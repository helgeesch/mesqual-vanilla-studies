{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MESQUAL Suite Documentation","text":"<p>MESQUAL (Modular Energy Scenario Comparison Library for Quantitative and Qualitative Analysis) is a platform-agnostic Python framework for multi-scenario energy systems analysis.</p>"},{"location":"#what-is-mesqual","title":"What is MESQUAL?","text":"<p>MESQUAL provides a unified data access layer, automatic scenario comparison, and comprehensive visualization capabilities that work seamlessly across any energy modeling platform (PyPSA, PLEXOS, SimFa, etc.) or custom data sources.</p>"},{"location":"#core-philosophy","title":"Core Philosophy","text":"<p>MESQUAL follows a three-tier code organization principle:</p> <ul> <li>General code (mesqual): Platform-agnostic framework available in all studies</li> <li>Platform-specific code (mesqual-pypsa, mesqual-plexos, etc.): Platform interfaces and interpreters</li> <li>Study-specific code (your-study-repo): Custom variables, analysis logic, and workflows</li> </ul>"},{"location":"#quick-navigation","title":"Quick Navigation","text":""},{"location":"#learn-about-mesqual","title":"\ud83d\udcda Learn About MESQUAL","text":"<ul> <li>MESQUAL Framework - Comprehensive overview of the core framework, architecture, and capabilities</li> <li>Vanilla Studies Repository - Template architecture for organizing energy modeling studies</li> </ul>"},{"location":"#explore-the-api","title":"\ud83d\udd0d Explore the API","text":"<ul> <li>MESQUAL API Reference - Complete API documentation for the core package</li> <li>StudyManager - Central orchestrator for multi-scenario analysis</li> <li>Datasets - Platform-agnostic data access layer</li> <li>KPI System - Advanced KPI framework with model object integration</li> <li>Visualization Modules - Folium maps, Plotly dashboards, and HTML reports</li> <li>Energy Data Handling - Area accounting, network flows, and more</li> </ul>"},{"location":"#hands-on-examples","title":"\ud83c\udf93 Hands-On Examples","text":"<ul> <li>Intro to MESQUAL - Comprehensive tutorial series covering:</li> <li>Data fetching and scenario comparison</li> <li>KPI calculation and analysis</li> <li>Interactive Folium map visualization</li> <li>Custom interpreter development</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<p>\ud83c\udfaf Unified Data Access - Single <code>.fetch(flag)</code> interface across all platforms - Consistent API for model data, time series, and computed metrics - Automatic MultiIndex handling for multi-scenario analysis</p> <p>\ud83d\udcca Multi-Scenario Management - Three-tier collection system (<code>.scen</code>, <code>.comp</code>, <code>.scen_comp</code>) - Automatic delta computation between scenarios - Unified access with type distinction</p> <p>\ud83d\uddfa\ufe0f Rich Visualization System - Interactive Folium maps with automatic feature generation - PropertyMapper system for data-driven styling - Time series dashboards and HTML reports</p> <p>\u26a1 Energy-Specific Tools - Area-level accounting for topological aggregation - Network flow analysis and capacity modeling - Volume-weighted price aggregation - Time series granularity conversion</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<p>Install the core framework from Git: <pre><code>pip install git+https://github.com/helgeesch/mesqual.git\n</code></pre></p> <p>Or for local development with the vanilla-studies repository: <pre><code>git clone https://github.com/helgeesch/mesqual-vanilla-studies.git\ncd mesqual-vanilla-studies\ngit submodule update --init\npip install -e ./submodules/mesqual\npip install -e ./submodules/mesqual-pypsa\npip install -r requirements.txt\n</code></pre></p>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import pypsa\nfrom mesqual import StudyManager\nfrom mesqual_pypsa import PyPSADataset\n\n# Load networks\nn_base = pypsa.Network('your_base_network.nc')\nn_scen1 = pypsa.Network('your_scen1_network.nc')\n\n# Initialize study manager\nstudy = StudyManager.factory_from_scenarios(\n    scenarios=[\n        PyPSADataset(n_base, name='base'),\n        PyPSADataset(n_scen1, name='scen1'),\n    ],\n    comparisons=[(\"scen1\", \"base\")],\n    export_folder=\"output\"\n)\n\n# Access MultiIndex df with data for all scenarios\ndf_prices = study.scen.fetch(\"buses_t.marginal_price\")\n\n# Access comparison deltas\ndf_price_deltas = study.comp.fetch(\"buses_t.marginal_price\")\n</code></pre>"},{"location":"#repository-structure","title":"Repository Structure","text":"<p>This documentation covers the entire MESQUAL suite:</p> <pre><code>mesqual/                         # Core framework (submodule)\nmesqual-pypsa/                   # PyPSA interface (submodule)\nmesqual-vanilla-studies/         # This repository\n\u251c\u2500\u2500 vanilla/                     # Generic utilities reused across all studies\n\u251c\u2500\u2500 studies/                     # Individual study folders\n\u2502   \u2514\u2500\u2500 study_01_intro_to_mesqual/  # Comprehensive tutorial\n\u251c\u2500\u2500 submodules/                  # Independent packages\n\u2514\u2500\u2500 docs/                        # This documentation\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please feel free to submit a pull request or open an issue.</p> <ul> <li>MESQUAL Core Repository</li> <li>Vanilla Studies Repository</li> <li>PyPSA Interface</li> </ul>"},{"location":"#license","title":"License","text":"<p>MESQUAL is licensed under the LGPL License - see the LICENSE file for details.</p>"},{"location":"#contact","title":"Contact","text":"<p>For questions or feedback, don't hesitate to get in touch!</p> <p></p>"},{"location":"about_mesqual/","title":"About MESQUAL Framework","text":""},{"location":"about_mesqual/#mesqual","title":"MESQUAL","text":"<p>Modular Energy Scenario Comparison Library for Quantitative and Qualitative Analysis</p> <p>A modular Python framework for energy market data analysis, with a focus on scenario comparison, KPI calculation and interactive visualizations.</p>"},{"location":"about_mesqual/#overview","title":"Overview","text":"<p>MESQUAL is a platform-agnostic Python framework for multi-scenario energy systems analysis. It provides a unified data access layer, automatic scenario comparison, and comprehensive visualization capabilities that work seamlessly across any energy modeling platform (PyPSA, PLEXOS, SimFa, etc.) or custom data sources.</p>"},{"location":"about_mesqual/#core-philosophy","title":"Core Philosophy","text":"<p>MESQUAL follows a three-tier code organization principle:</p> <ul> <li>General code (mesqual): Platform-agnostic framework available in all studies</li> <li>Platform-specific code (mesqual-pypsa, mesqual-plexos, etc.): Platform interfaces and interpreters</li> <li>Study-specific code (your-study-repo): Custom variables, analysis logic, and workflows</li> </ul>"},{"location":"about_mesqual/#key-capabilities","title":"Key Capabilities","text":"<p>\ud83c\udfaf Unified Data Access</p> <ul> <li>Single <code>.fetch(flag)</code> interface across all platforms and data types</li> <li>Consistent API for model data, time series, and computed metrics</li> <li>Automatic MultiIndex handling for multi-scenario and comparison analysis</li> </ul> <p>\ud83d\udcca Multi-Scenario Management</p> <ul> <li>Three-tier collection system (<code>.scen</code>, <code>.comp</code>, <code>.scen_comp</code>)</li> <li>Automatic delta computation between scenarios (and other types of comparisons)</li> <li>Unified access to scenarios and comparisons with type distinction</li> </ul> <p>\ud83d\udd27 Extensible Architecture</p> <ul> <li>Registry-based flag system with metadata management</li> <li>Custom interpreter registration for study-specific variables</li> <li>Platform interfaces enable integration with any energy modeling tool</li> </ul> <p>\ud83d\uddfa\ufe0f Rich Visualization System</p> <ul> <li>Interactive Folium maps with automatic feature generation</li> <li>PropertyMapper system for data-driven styling</li> <li>KPI collection visualizers with filtering and grouping</li> <li>Time series dashboards and HTML reports</li> </ul> <p>\u26a1 Energy-Specific Tools</p> <ul> <li>Area-level accounting for topological aggregation</li> <li>Network flow analysis and capacity modeling</li> <li>Volume-weighted price aggregation</li> <li>Time series granularity conversion and gap handling</li> </ul> <p>\ud83d\udcc8 Advanced KPI Framework</p> <ul> <li>Optional KPI system with model object integration</li> <li>Folium visualization integration with geographic context</li> <li>Unit handling and automatic conversion</li> <li>Multi-scenario bulk computation with progress tracking</li> <li>Note: For quick analysis, direct pandas processing is often faster</li> </ul> <p>\ud83d\udee0\ufe0f Comprehensive Utilities</p> <ul> <li>Pandas utilities for MultiIndex operations and filtering</li> <li>Folium helpers like automatic screenshotting tools</li> <li>Plotly theme system and visualization helpers</li> <li>Color scales and mapping systems</li> <li>Geographic and spatial analysis tools</li> </ul> <p>This is the foundation package for a whole suite of libraries and repositories.  In most cases, you will want to combine this foundation package with at least one existing mesqual-platform-interface (e.g. mesqual-pypsa, mesqual-plexos, ...), or build your own.</p> <p>To view a hands-on repository and see how the MESQUAL-suite is used in action, please visit the vanilla-studies repository. For platform-interfaces, visit those, respectively. The full list of the current MESQUAL-suite is:</p> <ul> <li>mesqual</li> <li>mesqual-vanilla-studies</li> <li>mesqual-pypsa</li> <li>mesqual-plexos (requires access)</li> </ul>"},{"location":"about_mesqual/#minimum-usage-examples","title":"Minimum usage examples","text":""},{"location":"about_mesqual/#example-using-pypsa-interface-to-set-up-a-study-with-multiple-scenarios-and-scenario-comparisons","title":"Example using PyPSA interface to set up a study with multiple scenarios and scenario comparisons","text":"<pre><code>import pypsa\nfrom mesqual import StudyManager\nfrom mesqual_pypsa import PyPSADataset\n\n# Load networks\nn_base = pypsa.Network('your_base_network.nc')\nn_scen1 = pypsa.Network('your_scen1_network.nc')\nn_scen2 = pypsa.Network('your_scen2_network.nc')\n\n# Initialize study manager\nstudy = StudyManager.factory_from_scenarios(\n    scenarios=[\n        PyPSADataset(n_base, name='base'),\n        PyPSADataset(n_scen1, name='scen1'),\n        PyPSADataset(n_scen2, name='scen2'),\n    ],\n    comparisons=[(\"scen1\", \"base\"), (\"scen2\", \"base\")],\n    export_folder=\"output\"\n)\n\n# Access MultiIndex df with data for all scenarios\ndf_prices = study.scen.fetch(\"buses_t.marginal_price\")\n\n# Access MultiIndex df with data for all comparisons (delta values)\ndf_price_deltas = study.comp.fetch(\"buses_t.marginal_price\")\n\n# Access buses model df of base case\ndf_bus_model = study.scen.get_dataset('base').fetch('buses')\n</code></pre>"},{"location":"about_mesqual/#example-using-plexos-interface-to-set-up-simple-dataset-and-fetch-data","title":"Example using Plexos interface to set up simple dataset and fetch data","text":"<pre><code>from mesqual_plexos import PlexosDataset\n\n# Initialize dataset\ndataset = PlexosDataset.from_xml_and_solution_zip(\n   model='path/to/my_plexos_model.xml', \n   solution='path/to/my_plexos_solution.zip',\n   name='my_name',\n)\n\n# Fetch data as DataFrame\ndf_prices = dataset.fetch(\"ST.Node.Price\")\ndf_nodes = dataset.fetch(\"Node.Model\")\n</code></pre> <p>For more elaborate and practical examples, please visit the mesqual-vanilla-studies repository.</p>"},{"location":"about_mesqual/#advanced-usage-building-platform-interfaces-and-study-specific-extensions","title":"Advanced Usage: Building Platform Interfaces and Study-Specific Extensions","text":"<p>MESQUAL's power comes from its layered architecture that separates platform-generic code, platform-specific code, and study-specific code. Here's how to extend MESQUAL for your platform and studies.</p>"},{"location":"about_mesqual/#1-building-a-platform-dataset","title":"1. Building a Platform Dataset","text":"<p>Platform datasets extend <code>PlatformDataset</code> with platform-specific data access. Here's how mesqual-pypsa implements it:</p> <pre><code># mesqual_pypsa/pypsa_dataset.py\nfrom pypsa import Network\nfrom mesqual.datasets import PlatformDataset\n\nclass PyPSADataset(PlatformDataset):\n    def __init__(self, network: Network, name: str = None, **kwargs):\n        super().__init__(\n            name=name or network.name,\n            flag_index=self.get_flag_index_type()(self),\n            **kwargs,\n            network=network,\n        )\n        self.n = network  # Store PyPSA network\n\n    @classmethod\n    def get_flag_type(cls) -&gt; Type[str]:\n        return str\n\n    @classmethod\n    def get_flag_index_type(cls) -&gt; type[PyPSAFlagIndex]:\n        return PyPSAFlagIndex\n\n    @classmethod\n    def _register_core_interpreters(cls):\n        from mesqual_pypsa.network_interpreters.model import PyPSAModelInterpreter\n        from mesqual_pypsa.network_interpreters.time_series import PyPSATimeSeriesInterpreter\n\n        cls.register_interpreter(PyPSAModelInterpreter)\n        cls.register_interpreter(PyPSATimeSeriesInterpreter)\n\n# Register platform interpreters\nPyPSADataset._register_core_interpreters()\n</code></pre>"},{"location":"about_mesqual/#2-platform-generic-interpreters","title":"2. Platform-Generic Interpreters","text":"<p>Interpreters define how to fetch specific flags from the platform. Each platform has its own interpreter base class:</p> <pre><code># Example: PyPSA Model Interpreter (platform-generic)\nfrom mesqual_pypsa.network_interpreters.base import PyPSAInterpreter\n\nclass PyPSAModelInterpreter(PyPSAInterpreter):\n    @property\n    def accepted_flags(self) -&gt; set[str]:\n        # Accepts any PyPSA component model flags\n        return {'buses', 'generators', 'loads', 'lines', 'links', ...}\n\n    def _fetch(self, flag: str, effective_config, **kwargs) -&gt; pd.DataFrame:\n        # Access PyPSA network via parent dataset\n        network = self.parent_dataset.n\n\n        # Return the component DataFrame\n        return getattr(network, flag)\n</code></pre>"},{"location":"about_mesqual/#3-study-specific-interpreters","title":"3. Study-Specific Interpreters","text":"<p>Studies can add custom interpreters for study-specific data or calculations. There are two main patterns:</p>"},{"location":"about_mesqual/#pattern-a-adding-new-model-data","title":"Pattern A: Adding New Model Data","text":"<pre><code># studies/study_01_intro_to_mesqual/src/study_specific_model_interpreters.py\nimport geopandas as gpd\nfrom mesqual_pypsa.network_interpreters.base import PyPSAInterpreter\n\nclass ControlAreaModelInterpreter(PyPSAInterpreter):\n    \"\"\"Provides control area geographic data (study-specific).\"\"\"\n\n    @property\n    def accepted_flags(self) -&gt; set[str]:\n        return {'control_areas'}\n\n    def _required_flags_for_flag(self, flag: str) -&gt; set[str]:\n        return set()\n\n    def _fetch(self, flag: str, effective_config, **kwargs) -&gt; gpd.GeoDataFrame:\n        # Load study-specific geospatial data\n        gdf = gpd.read_file('data/DE_control_areas.geojson')\n        gdf = gdf.set_index('control_area')\n        return gdf\n</code></pre>"},{"location":"about_mesqual/#pattern-b-enriching-existing-model-data","title":"Pattern B: Enriching Existing Model Data","text":"<pre><code>class ScigridDEBusModelInterpreter(PyPSAModelInterpreter):\n    \"\"\"Extends platform bus model with control area membership.\"\"\"\n\n    @property\n    def accepted_flags(self) -&gt; set[str]:\n        return {'buses'}\n\n    def _required_flags_for_flag(self, flag: str) -&gt; set[str]:\n        return {'control_areas'}  # Needs control area data\n\n    def _fetch(self, flag: str, effective_config, **kwargs) -&gt; pd.DataFrame:\n        # Get base bus data from platform interpreter\n        df_buses = super()._fetch(flag, effective_config, **kwargs)\n\n        # Fetch control area geodata\n        df_control_areas = self.parent_dataset.fetch('control_areas')\n\n        # Spatial join to assign buses to control areas\n        df_buses = gpd.GeoDataFrame(df_buses, geometry='location')\n        sjoined = gpd.sjoin(df_buses, df_control_areas, how='left', predicate='within')\n        df_buses['control_area'] = sjoined['control_area']\n\n        return df_buses\n</code></pre>"},{"location":"about_mesqual/#pattern-c-computing-custom-variables","title":"Pattern C: Computing Custom Variables","text":"<pre><code># studies/study_01_intro_to_mesqual/src/study_specific_variable_interpreters.py\nclass ControlAreaVolWeightedPrice(PyPSAInterpreter):\n    \"\"\"Calculates demand volume-weighted price per control area.\"\"\"\n\n    @property\n    def accepted_flags(self) -&gt; set[str]:\n        return {'control_areas_t.vol_weighted_marginal_price'}\n\n    def _required_flags_for_flag(self, flag: str) -&gt; set[str]:\n        return {'buses_t.marginal_price', 'loads_t.p'}\n\n    def _fetch(self, flag: str, effective_config, **kwargs) -&gt; pd.DataFrame:\n        # Fetch required data\n        price_per_bus = self.parent_dataset.fetch('buses_t.marginal_price')\n        load_per_load = self.parent_dataset.fetch('loads_t.p')\n        bus_model = self.parent_dataset.fetch('buses')\n        load_model = self.parent_dataset.fetch('loads')\n\n        # Aggregate loads to bus level\n        load_per_bus = prepend_model_prop_levels(load_per_load, load_model, 'bus')\n        load_per_bus = load_per_bus.T.groupby('bus').sum().T\n\n        # Aggregate to control area level\n        load_per_ca = prepend_model_prop_levels(load_per_bus, bus_model, 'control_area')\n        load_per_ca = load_per_ca.T.groupby('control_area').sum().T\n\n        # Calculate volume-weighted prices\n        price_load = price_per_bus.multiply(load_per_bus, fill_value=0)\n        price_load_per_ca = prepend_model_prop_levels(price_load, bus_model, 'control_area')\n        price_load_per_ca = price_load_per_ca.T.groupby('control_area').sum().T\n\n        vol_weighted_price = price_load_per_ca.divide(load_per_ca)\n        return vol_weighted_price\n</code></pre>"},{"location":"about_mesqual/#4-creating-study-specific-dataset-class","title":"4. Creating Study-Specific Dataset Class","text":"<p>Combine platform dataset with study interpreters:</p> <pre><code># studies/study_01_intro_to_mesqual/scripts/setup_study_manager.py\nfrom mesqual_pypsa import PyPSADataset\nfrom studies.study_01_intro_to_mesqual.src.study_specific_model_interpreters import (\n    ControlAreaModelInterpreter,\n    ScigridDEBusModelInterpreter\n)\nfrom studies.study_01_intro_to_mesqual.src.study_specific_variable_interpreters import (\n    ControlAreaVolWeightedPrice\n)\n\nclass ScigridDEDataset(PyPSADataset):\n    \"\"\"Study-specific dataset with control area support.\"\"\"\n\n    @classmethod\n    def _register_core_interpreters(cls):\n        # Register study-specific interpreters\n        cls.register_interpreter(ControlAreaModelInterpreter)\n        cls.register_interpreter(ScigridDEBusModelInterpreter)\n        cls.register_interpreter(ControlAreaVolWeightedPrice)\n\n# Register interpreters\nScigridDEDataset._register_core_interpreters()\n</code></pre>"},{"location":"about_mesqual/#5-setting-up-studymanager","title":"5. Setting Up StudyManager","text":"<p>Create a StudyManager with multiple scenarios and comparisons:</p> <pre><code>import pypsa\nfrom mesqual import StudyManager\n\ndef get_study_manager() -&gt; StudyManager:\n    # Load PyPSA networks\n    n_base = pypsa.Network('data/networks/base.nc')\n    n_wind_150 = pypsa.Network('data/networks/wind_150.nc')\n    n_solar_200 = pypsa.Network('data/networks/solar_200.nc')\n\n    # Create study manager\n    study = StudyManager.factory_from_scenarios(\n        scenarios=[\n            ScigridDEDataset(n_base, name='base'),\n            ScigridDEDataset(n_wind_150, name='wind_150'),\n            ScigridDEDataset(n_solar_200, name='solar_200'),\n        ],\n        comparisons=[\n            ('wind_150', 'base'),\n            ('solar_200', 'base'),\n        ],\n        export_folder='output/'\n    )\n\n    return study\n</code></pre>"},{"location":"about_mesqual/#6-complete-analysis-workflow","title":"6. Complete Analysis Workflow","text":"<pre><code># Initialize study\nstudy = get_study_manager()\n\n# === Data Access ===\n\n# Fetch platform-generic data across all scenarios\nbus_prices = study.scen.fetch('buses_t.marginal_price')  # PyPSA time series\ngenerators = study.scen.fetch('generators')  # PyPSA model data\n\n# Fetch study-specific data\ncontrol_areas = study.scen.fetch('control_areas')  # Study-specific GeoDataFrame\nca_prices = study.scen.fetch('control_areas_t.vol_weighted_marginal_price')  # Computed\n\n# Fetch comparison deltas\nprice_changes = study.comp.fetch('buses_t.marginal_price')\n\n# Individual dataset access\nds_base = study.scen.get_dataset('base')\nbase_buses = ds_base.fetch('buses')  # Has 'control_area' column from enrichment\n\n# === KPI Generation ===\n\nfrom mesqual.kpis import FlagAggKPIBuilder, Aggregations\n\n# Define KPIs\nkpi_defs = (\n    FlagAggKPIBuilder()\n    .for_flag('control_areas_t.vol_weighted_marginal_price')\n    .with_aggregation(Aggregations.Mean)\n    .build()\n)\n\n# Add to all scenarios\nstudy.scen.add_kpis_from_definitions_to_all_child_datasets(kpi_defs)\n\n# Get KPI collection\nkpi_collection = study.scen.get_merged_kpi_collection()\n\n# === Visualization ===\n\nimport folium\nfrom mesqual.visualizations import folium_viz_system as folviz\nfrom mesqual.visualizations import value_mapping_system as valmap\n\n# Create colorscale\nprice_scale = valmap.SegmentedContinuousColorscale(\n    segments={(0, 50): ['#2E86AB', '#A23B72', '#F18F01']}\n)\n\n# Create map\nm = folium.Map(location=[51, 10], zoom_start=6)\n\n# Visualize KPIs\nvisualizer = folviz.KPICollectionMapVisualizer(\n    generators=[\n        folviz.AreaGenerator(\n            folviz.AreaFeatureResolver(\n                fill_color=folviz.PropertyMapper.from_kpi_value(price_scale),\n                tooltip=True\n            )\n        )\n    ]\n)\n\nprice_kpis = kpi_collection.filter(flag='control_areas_t.vol_weighted_marginal_price')\nvisualizer.generate_and_add_feature_groups_to_map(price_kpis, m)\n\nm.save('output/price_map.html')\n</code></pre>"},{"location":"about_mesqual/#key-architecture-principles","title":"Key Architecture Principles","text":"<ol> <li> <p>Layered Responsibility:</p> </li> <li> <p>Platform dataset: Core data access from modeling tool</p> </li> <li>Platform interpreters: Generic data fetching for the platform</li> <li> <p>Study interpreters: Study-specific enrichments and calculations</p> </li> <li> <p>Interpreter Inheritance:</p> </li> <li> <p>Study interpreters extend platform interpreter base</p> </li> <li>Can call <code>super()._fetch()</code> to build on platform data</li> <li> <p>Declare dependencies via <code>_required_flags_for_flag()</code></p> </li> <li> <p>Flag Registration:</p> </li> <li> <p>Each interpreter declares <code>accepted_flags</code></p> </li> <li>StudyManager uses flag system to route fetch requests</li> <li> <p>Automatic dependency resolution</p> </li> <li> <p>Extensibility:</p> </li> <li> <p>Add interpreters without modifying platform code</p> </li> <li>Study-specific logic stays in study repository</li> <li>Platform code remains generic and reusable</li> </ol>"},{"location":"about_mesqual/#requirements","title":"Requirements","text":"<ul> <li>Python \u2265 3.10</li> <li>Install runtime dependencies with: <code>pip install -e .</code></li> </ul>"},{"location":"about_mesqual/#architecture","title":"Architecture","text":"<p>MESQUAL follows a modular design where platform-specific implementations are handled through separate packages:</p> <pre><code>mesqual/                         # Core package\nmesqual-pypsa/                   # PyPSA interface (separate package)\nmesqual-plexos/                  # PLEXOS interface (separate package)\n...                              # Other platform interfaces\nmesqual-your-custom-interface/   # Custom interface for your platform\n</code></pre> <p>The core package provides:</p> <ul> <li>Abstract interfaces for data handling</li> <li>Base classes for platform-specific implementations</li> <li>Scenario comparison tools</li> <li>KPI calculation framework</li> <li>Visualization modules</li> <li>Data transformation modules and utilities</li> <li>Pandas / Plotly / Folium utilities</li> </ul>"},{"location":"about_mesqual/#getting-started-integrate-mesqual-and-mesqual-interface-packages-in-your-project","title":"Getting Started: Integrate mesqual and mesqual-interface packages in your project","text":"<p>You have two ways to pull in the core library and any interfaces:</p>"},{"location":"about_mesqual/#option-a-install-from-git-easy-for-consumers","title":"Option A: Install from Git (easy for consumers)","text":"<pre><code>pip install git+https://github.com/helgeesch/mesqual.git\npip install git+https://github.com/path/to/any/mesqual-any-interface.git\n</code></pre>"},{"location":"about_mesqual/#option-b-local-dev-with-submodules-for-active-development","title":"Option B: Local dev with submodules (for active development)","text":""},{"location":"about_mesqual/#step-1-add-submodules-under-your-repo","title":"Step 1: Add submodules under your repo:","text":"<p>Add all required mesqual packages as submodules. If you want to build your own interface, just start by including the foundation package and start building your-custom-mesqual-interface. If you want to integrate an existing interface, just add that one as a submodule, respectively. <pre><code>git submodule add https://github.com/helgeesch/mesqual.git submodules/mesqual\ngit submodule add https://github.com/path/to/any/mesqual-any-interface.git submodules/mesqual-any-interface\ngit submodule update --init --recursive\n</code></pre> The folder <code>submodules/</code> should now include the respective packages.</p>"},{"location":"about_mesqual/#step-2-install-in-editable-mode-so-that-any-code-changes-just-work","title":"Step 2: Install in editable mode so that any code changes \u201cjust work\u201d:","text":"<pre><code>pip install -e ./submodules/mesqual\npip install -e ./submodules/mesqual-any-interface\n</code></pre>"},{"location":"about_mesqual/#step-3-optional-ide-tip","title":"Step 3 (optional): IDE tip","text":"<p>If you want full autocomplete and go-to-definition in PyCharm/VS Code, mark submodules/mesqual (and any other submodule) as a Sources Root in your IDE. This is purely for dev comfort and won\u2019t affect other users.</p>"},{"location":"about_mesqual/#attribution-and-licenses","title":"Attribution and Licenses","text":"<p>This project is licensed under the LGPL License - see the LICENSE file for details.</p>"},{"location":"about_mesqual/#third-party-assets","title":"Third-party assets:","text":"<ul> <li><code>countries.geojson</code>: Made with Natural Earth</li> </ul>"},{"location":"about_mesqual/#contact","title":"Contact","text":"<p>For questions or feedback, don't hesitate to open an issue or reach out via LinkedIn.</p> <p></p> <p>!!! info \"Source Documentation\"     The content above is pulled directly from the MESQUAL core repository README, ensuring you always have the most up-to-date information.</p>"},{"location":"about_vanilla_studies/","title":"About Vanilla Studies Repository","text":""},{"location":"about_vanilla_studies/#mesqual-vanilla-studies","title":"MESQUAL vanilla studies","text":""},{"location":"about_vanilla_studies/#overview","title":"Overview","text":"<p>MESQUAL (Modular Energy Scenario Comparison Library for Quantitative and Qualitative Analysis) is a Python framework for post-processing and analyzing energy systems data from any platform, with a focus on multi-scenario processing, scenario comparison, KPI calculation, and plotly / folium (map) visualizations.  </p> <p>This vanilla studies repository demonstrates MESQUAL's capabilities through practical examples and serves as a template architecture for organizing energy modeling studies. </p> <p>The repo integrates the foundation package mesqual, and the interface packages mesqual-pypsa, mesqual-plexos (requires access) as submodules. </p> <p>Visit the foundation package's README to find out more about the the MESQUAL value proposition or dive into the hands-on example studies in this repo to see MESQUAL in action.</p>"},{"location":"about_vanilla_studies/#example-studies","title":"Example Studies","text":"<p>Here is a list of all studies and examples currently part of the mesqual-vanilla-studies repo.</p>"},{"location":"about_vanilla_studies/#study-01-intro-to-mesqual-link","title":"Study 01: Intro to MESQUAL (link)","text":"<p>The intro study primarily uses a PyPSA example network to introduce the MESQUAL modules and framework. Click on the title of the study to see the full overview.</p>"},{"location":"about_vanilla_studies/#study-02-plexos-example-link","title":"Study 02: Plexos Example (link)","text":"<p>This study demonstrates how to use MESQUAL with Plexos simulation outputs, showing the platform-agnostic nature of the framework.  It includes examples of loading PLEXOS data, performing common analyses, and visualizing results using the same patterns established in Study 01.</p>"},{"location":"about_vanilla_studies/#repository-structure-as-framework-template","title":"Repository Structure as Framework Template","text":"<p>Beyond providing examples, this repository serves as a template architecture for organizing energy modeling studies. The structure enables:</p> <ul> <li>Clear separation between generic and study-specific code/data</li> <li>Clear separation between shared (version-controlled) and private (local) code/data</li> <li>Quick switching between studies, as many analysts need in their daily work</li> <li>Reuse of modules across studies</li> <li>Low entry barrier to get started, while enabling incremental and modular development</li> <li>more about this in </li> </ul>"},{"location":"about_vanilla_studies/#getting-started-setting-up-the-repo-locally","title":"Getting Started: Setting up the repo locally","text":""},{"location":"about_vanilla_studies/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python \u2265 3.10</li> <li>Git </li> <li>It is recommend to set up a virtual environment for this repository, but this is, of course, optional.</li> </ul>"},{"location":"about_vanilla_studies/#step-1-clone-mesqual-vanilla-studies","title":"Step 1: Clone mesqual-vanilla-studies","text":"<p>In your console, navigate to the directory in which you want to clone this repo. Then perform the clone: <pre><code>git clone https://github.com/helgeesch/mesqual-vanilla-studies.git\ncd mesqual-vanilla-studies\n</code></pre></p>"},{"location":"about_vanilla_studies/#step-2-install-sister-packages-or-add-them-as-submodules","title":"Step 2: Install sister packages or add them as submodules","text":""},{"location":"about_vanilla_studies/#option-a-install-submodules-from-git-with-pip-easy-for-consumers","title":"Option A: Install submodules from Git with pip (easy for consumers)","text":"<pre><code>pip install git+https://github.com/helgeesch/mesqual.git\npip install git+https://github.com/helgeesch/mesqual-pypsa.git\n</code></pre>"},{"location":"about_vanilla_studies/#option-b-local-dev-with-submodules-for-active-development","title":"Option B: Local dev with submodules (for active development)","text":""},{"location":"about_vanilla_studies/#b1-add-submodules-under-your-repo","title":"B.1: Add submodules under your repo:","text":"<p>Make sure your console is in the mesqual-vanilla-studies folder and then initialize all submodules with <pre><code>git submodule update --init\n</code></pre> The folder <code>submodules/</code> should now include the respective packages.</p>"},{"location":"about_vanilla_studies/#b2-install-in-editable-mode-so-that-any-code-changes-just-work","title":"B.2: Install in editable mode so that any code changes \u201cjust work\u201d:","text":"<pre><code>pip install -e ./submodules/mesqual\npip install -e ./submodules/mesqual-pypsa\n</code></pre>"},{"location":"about_vanilla_studies/#b3-optional-ide-tip","title":"B.3 (optional): IDE tip","text":"<p>If you want full autocomplete and go-to-definition in PyCharm/VS Code, mark submodules/mesqual (and any other submodule) as a Sources Root in your IDE. This is purely for dev comfort and won\u2019t affect other users.</p>"},{"location":"about_vanilla_studies/#step-4-install-requirements","title":"Step 4: Install requirements","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"about_vanilla_studies/#creating-a-new-study","title":"Creating a New Study","text":"<p>The vanilla repo comes with a generic folder structure template for new studies. You can check out the folder structure in study_structure.py.</p> <p>The fastest way to set up a new study from the template layout is to simply run: <pre><code>python -m vanilla.new_study studies/study_123_your_name\n</code></pre> This will create a new folder with the template structure in the studies directory.</p> <p>If you follow this structure, you can make use of the PathManager class for easy navigation between study-specific paths.</p>"},{"location":"about_vanilla_studies/#attribution-and-licenses","title":"Attribution and Licenses","text":"<p>This project is primarily licensed under the LGPL License - see the LICENSE file for details.</p>"},{"location":"about_vanilla_studies/#third-party-assets","title":"Third-party assets:","text":"<ul> <li>PyPSA and Scigrid-DE example network:  - [MIT License]</li> <li>GeoJSON of DE control areas:  - [CC BY 4.0]</li> </ul> <p>The example notebooks use these assets for demonstration purposes.</p>"},{"location":"about_vanilla_studies/#contributing","title":"Contributing","text":"<p>Contributions to improve examples or add new ones are welcome! Please feel free to submit a pull request or open an issue to discuss potential enhancements.</p>"},{"location":"about_vanilla_studies/#contact","title":"Contact","text":"<p>For questions or feedback about MESQUAL or these example studies, don't hesitate to get in touch!</p> <p></p> <p>!!! info \"Source Documentation\"     The content above is pulled directly from the vanilla-studies repository README, ensuring you always have the most up-to-date information.</p>"},{"location":"mesqual-package-documentation/api_reference/datasets/","title":"Dataset References","text":""},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset","title":"dataset","text":""},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset.Dataset","title":"Dataset","text":"<p>               Bases: <code>Generic[DatasetConfigType, FlagType, FlagIndexType]</code>, <code>ABC</code></p> <p>Abstract base class for all datasets in the MESQUAL framework.</p> <p>The Dataset class provides the fundamental interface for data access and manipulation in MESQUAL. It implements the core principle \"Everything is a Dataset\" where individual scenarios, collections of scenarios, and scenario comparisons all share the same unified interface.</p> Key Features <ul> <li>Unified <code>.fetch(flag)</code> interface for data access</li> <li>Attribute management for scenario metadata</li> <li>KPI calculation integration</li> <li>Database caching support</li> <li>Dot notation fetching via <code>dotfetch</code> property</li> <li>Type-safe generic implementation</li> </ul> <p>          Class Type Parameters:        </p> Name Bound or Constraints Description Default <code>DatasetConfigType</code> <p>Configuration class for dataset behavior</p> required <code>FlagType</code> <p>Type used for data flag identification (typically str)</p> required <code>FlagIndexType</code> <p>Flag index implementation for flag mapping</p> required <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Human-readable identifier for the dataset</p> <code>kpi_collection</code> <code>KPICollection</code> <p>Collection of KPIs associated with this dataset</p> <code>dotfetch</code> <code>_DotNotationFetcher</code> <p>Enables dot notation data access</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Basic usage pattern\n&gt;&gt;&gt; data = dataset.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; flags = dataset.accepted_flags\n&gt;&gt;&gt; if dataset.flag_is_accepted('generators_t.p'):\n...     gen_data = dataset.fetch('generators_t.p')\n</code></pre> Source code in <code>submodules/mesqual/mesqual/datasets/dataset.py</code> <pre><code>class Dataset(Generic[DatasetConfigType, FlagType, FlagIndexType], ABC):\n    \"\"\"\n    Abstract base class for all datasets in the MESQUAL framework.\n\n    The Dataset class provides the fundamental interface for data access and manipulation\n    in MESQUAL. It implements the core principle \"Everything is a Dataset\" where individual\n    scenarios, collections of scenarios, and scenario comparisons all share the same\n    unified interface.\n\n    Key Features:\n        - Unified `.fetch(flag)` interface for data access\n        - Attribute management for scenario metadata\n        - KPI calculation integration\n        - Database caching support\n        - Dot notation fetching via `dotfetch` property\n        - Type-safe generic implementation\n\n    Type Parameters:\n        DatasetConfigType: Configuration class for dataset behavior\n        FlagType: Type used for data flag identification (typically str)\n        FlagIndexType: Flag index implementation for flag mapping\n\n    Attributes:\n        name (str): Human-readable identifier for the dataset\n        kpi_collection (KPICollection): Collection of KPIs associated with this dataset\n        dotfetch (_DotNotationFetcher): Enables dot notation data access\n\n    Example:\n\n        &gt;&gt;&gt; # Basic usage pattern\n        &gt;&gt;&gt; data = dataset.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; flags = dataset.accepted_flags\n        &gt;&gt;&gt; if dataset.flag_is_accepted('generators_t.p'):\n        ...     gen_data = dataset.fetch('generators_t.p')\n    \"\"\"\n\n    def __init__(\n            self,\n            name: str = None,\n            parent_dataset: Dataset = None,\n            flag_index: FlagIndexType = None,\n            attributes: dict = None,\n            database: Database = None,\n            config: DatasetConfigType = None\n    ):\n        \"\"\"\n        Initialize a new Dataset instance.\n\n        Args:\n            name: Human-readable identifier. If None, auto-generates from class name\n            parent_dataset: Optional parent dataset for hierarchical relationships\n            flag_index: Index for mapping and validating data flags\n            attributes: Dictionary of metadata attributes for the dataset\n            database: Optional database for caching expensive computations\n            config: Configuration object controlling dataset behavior\n        \"\"\"\n        self.name = name or f'{self.__class__.__name__}_{str(id(self))}'\n        self._flag_index = flag_index or EmptyFlagIndex()\n        self._parent_dataset = parent_dataset\n        self._attributes: dict = attributes or dict()\n        self._database = database\n        self._config = config\n        self.dotfetch = _DotNotationFetcher(self)\n\n        from mesqual.kpis.collection import KPICollection\n        self.kpi_collection: KPICollection = KPICollection()\n\n    @property\n    def flag_index(self) -&gt; FlagIndexType:\n        if isinstance(self._flag_index, EmptyFlagIndex):\n            logger.info(\n                f\"Dataset {self.name}: \"\n                \"You're trying to use functionality of the FlagIndex but didn't define one. \"\n                \"The current FlagIndex in use is empty. \"\n                \"Make sure to set a flag_index in case you want to use full functionality of the flag_index.\"\n            )\n        return self._flag_index\n\n    @property\n    def database(self) -&gt; Database | None:\n        return self._database\n\n    def add_kpis_from_definitions(self, kpi_definitions: KPIDefinition | list[KPIDefinition]):\n        from mesqual.kpis.definitions.base import KPIDefinition\n        if isinstance(kpi_definitions, KPIDefinition):\n            kpis = kpi_definitions.generate_kpis(self)\n            self.add_kpis(kpis)\n        else:\n            for kpi_def in kpi_definitions:\n                kpis = kpi_def.generate_kpis(self)\n                self.add_kpis(kpis)\n\n    def add_kpis(self, kpis: Iterable[KPI]):\n        \"\"\"\n        Add multiple KPIs to this dataset's KPI collection.\n\n        Args:\n            kpis: Iterable of KPI instances, factories, or classes to add\n        \"\"\"\n        duplicates = []\n        for kpi in kpis:\n            if kpi in self.kpi_collection:\n                duplicates.append(kpi)\n            else:\n                self.add_kpi(kpi)\n        if duplicates:\n            _num_duplicates = len(duplicates)\n            logger.warning(f'{_num_duplicates} duplicates found and not added again or overwritten in {self.name}. ({duplicates[:3]}...)')\n\n    def add_kpi(self, kpi: KPI):\n        \"\"\"\n        Add a single KPI to this dataset's KPI collection.\n\n        Args:\n            kpi: KPI instance, factory, or class to add\n        \"\"\"\n        self.kpi_collection.add(kpi)\n\n    def clear_kpi_collection(self):\n        \"\"\"Clear the KPI collection.\"\"\"\n        from mesqual.kpis.collection import KPICollection\n        self.kpi_collection = KPICollection()\n\n    @property\n    def attributes(self) -&gt; dict:\n        return self._attributes\n\n    def get_attributes_series(self) -&gt; pd.Series:\n        att_series = pd.Series(self.attributes, name=self.name)\n        return att_series\n\n    def set_attributes(self, **kwargs):\n        for key, value in kwargs.items():\n            if not isinstance(key, str):\n                raise TypeError(f'Attribute keys must be of type str. Your key {key} is of type {type(key)}.')\n            if not isinstance(value, (bool, int, float, str)):\n                raise TypeError(\n                    f'Attribute values must be of type (bool, int, flaot, str). '\n                    f'Your value for {key} ({value}) is of type {type(value)}.'\n                )\n            self._attributes[key] = value\n\n    @property\n    def parent_dataset(self) -&gt; 'DatasetLinkCollection':\n        if self._parent_dataset is None:\n            raise RuntimeError(f\"Parent dataset called without / before assignment.\")\n        return self._parent_dataset\n\n    @parent_dataset.setter\n    def parent_dataset(self, parent_dataset: 'DatasetLinkCollection'):\n        from mesqual.datasets.dataset_collection import DatasetLinkCollection\n        if not isinstance(parent_dataset, DatasetLinkCollection):\n            raise TypeError(f\"Parent parent_dataset must be of type {DatasetLinkCollection.__name__}\")\n        self._parent_dataset = parent_dataset\n\n    @property\n    @abstractmethod\n    def accepted_flags(self) -&gt; set[FlagType]:\n        \"\"\"\n        Set of all flags accepted by this dataset.\n\n        This abstract property must be implemented by all concrete dataset classes\n        to define which data flags can be fetched from the dataset.\n\n        Returns:\n            Set of flags that can be used with the fetch() method\n\n        Example:\n\n            &gt;&gt;&gt; print(dataset.accepted_flags)\n                {'buses', 'buses_t.marginal_price', 'generators', 'generators_t.p', ...}\n        \"\"\"\n        return set()\n\n    def get_accepted_flags_containing_x(self, x: str, match_case: bool = False) -&gt; set[FlagType]:\n        \"\"\"\n        Find all accepted flags containing a specific substring.\n\n        Useful for discovering related data flags or filtering flags by category.\n\n        Args:\n            x: Substring to search for in flag names\n            match_case: If True, performs case-sensitive search. Default is False.\n\n        Returns:\n            Set of accepted flags containing the substring\n\n        Example:\n\n            &gt;&gt;&gt; ds = PyPSADataset()\n            &gt;&gt;&gt; ds.get_accepted_flags_containing_x('generators')\n                {'generators', 'generators_t.p', 'generators_t.efficiency', ...}\n            &gt;&gt;&gt; ds.get_accepted_flags_containing_x('BUSES', match_case=True)\n                set()  # Empty because case doesn't match\n        \"\"\"\n        if match_case:\n            return {f for f in self.accepted_flags if x in str(f)}\n        x_lower = x.lower()\n        return {f for f in self.accepted_flags if x_lower in str(f).lower()}\n\n    def flag_is_accepted(self, flag: FlagType) -&gt; bool:\n        \"\"\"\n        Boolean check whether a flag is accepted by the Dataset.\n\n        This method can be optionally overridden in any child-class\n        in case you want to follow logic instead of the explicit set of accepted_flags.\n        \"\"\"\n        return flag in self.accepted_flags\n\n    @flag_must_be_accepted\n    def required_flags_for_flag(self, flag: FlagType) -&gt; set[FlagType]:\n        return self._required_flags_for_flag(flag)\n\n    @abstractmethod\n    def _required_flags_for_flag(self, flag: FlagType) -&gt; set[FlagType]:\n        return set()\n\n    @flag_must_be_accepted\n    def fetch(self, flag: FlagType, config: dict | DatasetConfigType = None, **kwargs) -&gt; pd.Series | pd.DataFrame:\n        \"\"\"\n        Fetch data associated with a specific flag.\n\n        This is the primary method for data access in MESQUAL datasets. It provides\n        a unified interface for retrieving data regardless of the underlying source\n        or dataset type. The method includes automatic caching, post-processing,\n        and configuration management.\n\n        Args:\n            flag: Data identifier flag (must be in accepted_flags)\n            config: Optional configuration to override dataset defaults.\n                   Can be a dict or DatasetConfig instance.\n            **kwargs: Additional keyword arguments passed to the underlying\n                     data fetching implementation\n\n        Returns:\n            DataFrame or Series containing the requested data\n\n        Raises:\n            ValueError: If the flag is not accepted by this dataset\n\n        Examples:\n\n            &gt;&gt;&gt; # Basic usage\n            &gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price')\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # With custom configuration\n            &gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price', config={'use_database': False})\n        \"\"\"\n        effective_config = self._prepare_config(config)\n        use_database = self._database is not None and effective_config.use_database\n\n        if use_database:\n            if self._database.key_is_up_to_date(self, flag, config=effective_config, **kwargs):\n                return self._database.get(self, flag, config=effective_config, **kwargs)\n\n        raw_data = self._fetch(flag, effective_config, **kwargs)\n        processed_data = self._post_process_data(raw_data, flag, effective_config)\n\n        if use_database:\n            self._database.set(self, flag, config=effective_config, value=processed_data, **kwargs)\n\n        return processed_data.copy()\n\n    def _post_process_data(\n            self,\n            data: pd.Series | pd.DataFrame,\n            flag: FlagType,\n            config: DatasetConfigType\n    ) -&gt; pd.Series | pd.DataFrame:\n        if config.remove_duplicate_indices and any(data.index.duplicated()):\n            logger.info(\n                f'For some reason your data-set {self.name} returns an object with duplicate indices for flag {flag}.\\n'\n                f'We manually remove duplicate indices. Please make sure your data importer / converter is set up '\n                f'appropriately and that your raw data does not contain duplicate indices. \\n'\n                f'We will keep the first element of every duplicated index.'\n            )\n            data = data.loc[~data.index.duplicated()]\n        if config.auto_sort_datetime_index and isinstance(data.index, pd.DatetimeIndex):\n            data = data.sort_index()\n        return data\n\n    def _prepare_config(self, config: dict | DatasetConfigType = None) -&gt; DatasetConfigType:\n        if config is None:\n            return self.instance_config\n\n        if isinstance(config, dict):\n            temp_config = self.get_config_type()()\n            temp_config.__dict__.update(config)\n            return self.instance_config.merge(temp_config)\n\n        from mesqual.datasets.dataset_config import DatasetConfig\n        if isinstance(config, DatasetConfig):\n            return self.instance_config.merge(config)\n\n        raise TypeError(f\"Config must be dict or {DatasetConfig.__name__}, got {type(config)}\")\n\n    @abstractmethod\n    def _fetch(self, flag: FlagType, effective_config: DatasetConfigType, **kwargs) -&gt; pd.Series | pd.DataFrame:\n        return pd.DataFrame()\n\n    def fetch_multiple_flags_and_concat(\n            self,\n            flags: Iterable[FlagType],\n            concat_axis: int = 1,\n            concat_level_name: str = 'variable',\n            concat_level_at_top: bool = True,\n            config: dict | DatasetConfigType = None,\n            **kwargs\n    ) -&gt; Union[pd.Series, pd.DataFrame]:\n        dfs = {\n            str(flag): self.fetch(flag, config, **kwargs)\n            for flag in flags\n        }\n        df = pd.concat(\n            dfs,\n            axis=concat_axis,\n            names=[concat_level_name],\n        )\n        if not concat_level_at_top:\n            ax = df.axes[concat_axis]\n            ax = ax.reorder_levels(list(range(1, ax.nlevels)) + [0])\n            df.axes[concat_axis] = ax\n        return df\n\n    def fetch_filter_groupby_agg(\n            self,\n            flag: FlagType,\n            model_filter_query: str = None,\n            prop_groupby: str | list[str] = None,\n            prop_groupby_agg: str = None,\n            config: dict | DatasetConfigType = None,\n            **kwargs\n    ) -&gt; pd.Series | pd.DataFrame:\n        model_flag = self.flag_index.get_linked_model_flag(flag)\n        if not model_flag:\n            raise RuntimeError(f'FlagIndex could not successfully map flag {flag} to a model flag.')\n\n        from mesqual.utils import pandas_utils\n\n        data = self.fetch(flag, config, **kwargs)\n        model_df = self.fetch(model_flag, config, **kwargs)\n\n        if model_filter_query:\n            data = pandas_utils.filter_by_model_query(data, model_df, query=model_filter_query)\n\n        if prop_groupby:\n            if isinstance(prop_groupby, str):\n                prop_groupby = [prop_groupby]\n            data = pandas_utils.prepend_model_prop_levels(data, model_df, *prop_groupby)\n            data = data.groupby(prop_groupby)\n            if prop_groupby_agg:\n                data = data.agg(prop_groupby_agg)\n        elif prop_groupby_agg:\n            logger.warning(\n                f\"You provided a prop_groupby_agg operation, but didn't provide prop_groupby. \"\n                f\"No aggregation performed.\"\n            )\n        return data\n\n    @classmethod\n    def get_flag_type(cls) -&gt; Type[FlagType]:\n        from mesqual.flag.flag import FlagTypeProtocol\n        return FlagTypeProtocol\n\n    @classmethod\n    def get_flag_index_type(cls) -&gt; Type[FlagIndexType]:\n        from mesqual.flag.flag_index import FlagIndex\n        return FlagIndex\n\n    @classmethod\n    def get_config_type(cls) -&gt; Type[DatasetConfigType]:\n        from mesqual.datasets.dataset_config import DatasetConfig\n        return DatasetConfig\n\n    @property\n    def instance_config(self) -&gt; DatasetConfigType:\n        from mesqual.datasets.dataset_config import DatasetConfigManager\n        return DatasetConfigManager.get_effective_config(self.__class__, self._config)\n\n    def set_instance_config(self, config: DatasetConfigType) -&gt; None:\n        self._config = config\n\n    def set_instance_config_kwargs(self, **kwargs) -&gt; None:\n        for key, value in kwargs.items():\n            setattr(self._config, key, value)\n\n    @classmethod\n    def set_class_config(cls, config: DatasetConfigType) -&gt; None:\n        from mesqual.datasets.dataset_config import DatasetConfigManager\n        DatasetConfigManager.set_class_config(cls, config)\n\n    def __str__(self) -&gt; str:\n        return self.name\n\n    def __hash__(self):\n        return hash((self.name, self._config))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset.Dataset.accepted_flags","title":"accepted_flags  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>accepted_flags: set[FlagType]\n</code></pre> <p>Set of all flags accepted by this dataset.</p> <p>This abstract property must be implemented by all concrete dataset classes to define which data flags can be fetched from the dataset.</p> <p>Returns:</p> Type Description <code>set[FlagType]</code> <p>Set of flags that can be used with the fetch() method</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; print(dataset.accepted_flags)\n    {'buses', 'buses_t.marginal_price', 'generators', 'generators_t.p', ...}\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset.Dataset.__init__","title":"__init__","text":"<pre><code>__init__(name: str = None, parent_dataset: Dataset = None, flag_index: FlagIndexType = None, attributes: dict = None, database: Database = None, config: DatasetConfigType = None)\n</code></pre> <p>Initialize a new Dataset instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Human-readable identifier. If None, auto-generates from class name</p> <code>None</code> <code>parent_dataset</code> <code>Dataset</code> <p>Optional parent dataset for hierarchical relationships</p> <code>None</code> <code>flag_index</code> <code>FlagIndexType</code> <p>Index for mapping and validating data flags</p> <code>None</code> <code>attributes</code> <code>dict</code> <p>Dictionary of metadata attributes for the dataset</p> <code>None</code> <code>database</code> <code>Database</code> <p>Optional database for caching expensive computations</p> <code>None</code> <code>config</code> <code>DatasetConfigType</code> <p>Configuration object controlling dataset behavior</p> <code>None</code> Source code in <code>submodules/mesqual/mesqual/datasets/dataset.py</code> <pre><code>def __init__(\n        self,\n        name: str = None,\n        parent_dataset: Dataset = None,\n        flag_index: FlagIndexType = None,\n        attributes: dict = None,\n        database: Database = None,\n        config: DatasetConfigType = None\n):\n    \"\"\"\n    Initialize a new Dataset instance.\n\n    Args:\n        name: Human-readable identifier. If None, auto-generates from class name\n        parent_dataset: Optional parent dataset for hierarchical relationships\n        flag_index: Index for mapping and validating data flags\n        attributes: Dictionary of metadata attributes for the dataset\n        database: Optional database for caching expensive computations\n        config: Configuration object controlling dataset behavior\n    \"\"\"\n    self.name = name or f'{self.__class__.__name__}_{str(id(self))}'\n    self._flag_index = flag_index or EmptyFlagIndex()\n    self._parent_dataset = parent_dataset\n    self._attributes: dict = attributes or dict()\n    self._database = database\n    self._config = config\n    self.dotfetch = _DotNotationFetcher(self)\n\n    from mesqual.kpis.collection import KPICollection\n    self.kpi_collection: KPICollection = KPICollection()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset.Dataset.add_kpis","title":"add_kpis","text":"<pre><code>add_kpis(kpis: Iterable[KPI])\n</code></pre> <p>Add multiple KPIs to this dataset's KPI collection.</p> <p>Parameters:</p> Name Type Description Default <code>kpis</code> <code>Iterable[KPI]</code> <p>Iterable of KPI instances, factories, or classes to add</p> required Source code in <code>submodules/mesqual/mesqual/datasets/dataset.py</code> <pre><code>def add_kpis(self, kpis: Iterable[KPI]):\n    \"\"\"\n    Add multiple KPIs to this dataset's KPI collection.\n\n    Args:\n        kpis: Iterable of KPI instances, factories, or classes to add\n    \"\"\"\n    duplicates = []\n    for kpi in kpis:\n        if kpi in self.kpi_collection:\n            duplicates.append(kpi)\n        else:\n            self.add_kpi(kpi)\n    if duplicates:\n        _num_duplicates = len(duplicates)\n        logger.warning(f'{_num_duplicates} duplicates found and not added again or overwritten in {self.name}. ({duplicates[:3]}...)')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset.Dataset.add_kpi","title":"add_kpi","text":"<pre><code>add_kpi(kpi: KPI)\n</code></pre> <p>Add a single KPI to this dataset's KPI collection.</p> <p>Parameters:</p> Name Type Description Default <code>kpi</code> <code>KPI</code> <p>KPI instance, factory, or class to add</p> required Source code in <code>submodules/mesqual/mesqual/datasets/dataset.py</code> <pre><code>def add_kpi(self, kpi: KPI):\n    \"\"\"\n    Add a single KPI to this dataset's KPI collection.\n\n    Args:\n        kpi: KPI instance, factory, or class to add\n    \"\"\"\n    self.kpi_collection.add(kpi)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset.Dataset.clear_kpi_collection","title":"clear_kpi_collection","text":"<pre><code>clear_kpi_collection()\n</code></pre> <p>Clear the KPI collection.</p> Source code in <code>submodules/mesqual/mesqual/datasets/dataset.py</code> <pre><code>def clear_kpi_collection(self):\n    \"\"\"Clear the KPI collection.\"\"\"\n    from mesqual.kpis.collection import KPICollection\n    self.kpi_collection = KPICollection()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset.Dataset.get_accepted_flags_containing_x","title":"get_accepted_flags_containing_x","text":"<pre><code>get_accepted_flags_containing_x(x: str, match_case: bool = False) -&gt; set[FlagType]\n</code></pre> <p>Find all accepted flags containing a specific substring.</p> <p>Useful for discovering related data flags or filtering flags by category.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>Substring to search for in flag names</p> required <code>match_case</code> <code>bool</code> <p>If True, performs case-sensitive search. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>set[FlagType]</code> <p>Set of accepted flags containing the substring</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; ds = PyPSADataset()\n&gt;&gt;&gt; ds.get_accepted_flags_containing_x('generators')\n    {'generators', 'generators_t.p', 'generators_t.efficiency', ...}\n&gt;&gt;&gt; ds.get_accepted_flags_containing_x('BUSES', match_case=True)\n    set()  # Empty because case doesn't match\n</code></pre> Source code in <code>submodules/mesqual/mesqual/datasets/dataset.py</code> <pre><code>def get_accepted_flags_containing_x(self, x: str, match_case: bool = False) -&gt; set[FlagType]:\n    \"\"\"\n    Find all accepted flags containing a specific substring.\n\n    Useful for discovering related data flags or filtering flags by category.\n\n    Args:\n        x: Substring to search for in flag names\n        match_case: If True, performs case-sensitive search. Default is False.\n\n    Returns:\n        Set of accepted flags containing the substring\n\n    Example:\n\n        &gt;&gt;&gt; ds = PyPSADataset()\n        &gt;&gt;&gt; ds.get_accepted_flags_containing_x('generators')\n            {'generators', 'generators_t.p', 'generators_t.efficiency', ...}\n        &gt;&gt;&gt; ds.get_accepted_flags_containing_x('BUSES', match_case=True)\n            set()  # Empty because case doesn't match\n    \"\"\"\n    if match_case:\n        return {f for f in self.accepted_flags if x in str(f)}\n    x_lower = x.lower()\n    return {f for f in self.accepted_flags if x_lower in str(f).lower()}\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset.Dataset.flag_is_accepted","title":"flag_is_accepted","text":"<pre><code>flag_is_accepted(flag: FlagType) -&gt; bool\n</code></pre> <p>Boolean check whether a flag is accepted by the Dataset.</p> <p>This method can be optionally overridden in any child-class in case you want to follow logic instead of the explicit set of accepted_flags.</p> Source code in <code>submodules/mesqual/mesqual/datasets/dataset.py</code> <pre><code>def flag_is_accepted(self, flag: FlagType) -&gt; bool:\n    \"\"\"\n    Boolean check whether a flag is accepted by the Dataset.\n\n    This method can be optionally overridden in any child-class\n    in case you want to follow logic instead of the explicit set of accepted_flags.\n    \"\"\"\n    return flag in self.accepted_flags\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset.Dataset.fetch","title":"fetch","text":"<pre><code>fetch(flag: FlagType, config: dict | DatasetConfigType = None, **kwargs) -&gt; Series | DataFrame\n</code></pre> <p>Fetch data associated with a specific flag.</p> <p>This is the primary method for data access in MESQUAL datasets. It provides a unified interface for retrieving data regardless of the underlying source or dataset type. The method includes automatic caching, post-processing, and configuration management.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>FlagType</code> <p>Data identifier flag (must be in accepted_flags)</p> required <code>config</code> <code>dict | DatasetConfigType</code> <p>Optional configuration to override dataset defaults.    Can be a dict or DatasetConfig instance.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the underlying      data fetching implementation</p> <code>{}</code> <p>Returns:</p> Type Description <code>Series | DataFrame</code> <p>DataFrame or Series containing the requested data</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the flag is not accepted by this dataset</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic usage\n&gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With custom configuration\n&gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price', config={'use_database': False})\n</code></pre> Source code in <code>submodules/mesqual/mesqual/datasets/dataset.py</code> <pre><code>@flag_must_be_accepted\ndef fetch(self, flag: FlagType, config: dict | DatasetConfigType = None, **kwargs) -&gt; pd.Series | pd.DataFrame:\n    \"\"\"\n    Fetch data associated with a specific flag.\n\n    This is the primary method for data access in MESQUAL datasets. It provides\n    a unified interface for retrieving data regardless of the underlying source\n    or dataset type. The method includes automatic caching, post-processing,\n    and configuration management.\n\n    Args:\n        flag: Data identifier flag (must be in accepted_flags)\n        config: Optional configuration to override dataset defaults.\n               Can be a dict or DatasetConfig instance.\n        **kwargs: Additional keyword arguments passed to the underlying\n                 data fetching implementation\n\n    Returns:\n        DataFrame or Series containing the requested data\n\n    Raises:\n        ValueError: If the flag is not accepted by this dataset\n\n    Examples:\n\n        &gt;&gt;&gt; # Basic usage\n        &gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With custom configuration\n        &gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price', config={'use_database': False})\n    \"\"\"\n    effective_config = self._prepare_config(config)\n    use_database = self._database is not None and effective_config.use_database\n\n    if use_database:\n        if self._database.key_is_up_to_date(self, flag, config=effective_config, **kwargs):\n            return self._database.get(self, flag, config=effective_config, **kwargs)\n\n    raw_data = self._fetch(flag, effective_config, **kwargs)\n    processed_data = self._post_process_data(raw_data, flag, effective_config)\n\n    if use_database:\n        self._database.set(self, flag, config=effective_config, value=processed_data, **kwargs)\n\n    return processed_data.copy()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset.flag_must_be_accepted","title":"flag_must_be_accepted","text":"<pre><code>flag_must_be_accepted(method)\n</code></pre> <p>Decorator that validates flag acceptance before method execution.</p> <p>Ensures that only accepted flags are processed by dataset methods, providing clear error messages for invalid flag usage.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <p>The method to decorate</p> required <p>Returns:</p> Type Description <p>Decorated method that validates flag acceptance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the flag is not accepted by the dataset</p> Source code in <code>submodules/mesqual/mesqual/datasets/dataset.py</code> <pre><code>def flag_must_be_accepted(method):\n    \"\"\"\n    Decorator that validates flag acceptance before method execution.\n\n    Ensures that only accepted flags are processed by dataset methods,\n    providing clear error messages for invalid flag usage.\n\n    Args:\n        method: The method to decorate\n\n    Returns:\n        Decorated method that validates flag acceptance\n\n    Raises:\n        ValueError: If the flag is not accepted by the dataset\n    \"\"\"\n    def raise_if_flag_not_accepted(self: Dataset, flag: FlagType, config: DatasetConfigType = None, **kwargs):\n        if not self.flag_is_accepted(flag):\n            raise ValueError(f'Flag {flag} not accepted by Dataset \"{self.name}\" of type {type(self)}.')\n        return method(self, flag, config, **kwargs)\n    return raise_if_flag_not_accepted\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset_collection","title":"dataset_collection","text":""},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset_collection.DatasetCollection","title":"DatasetCollection","text":"<p>               Bases: <code>Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code>, <code>Dataset[DatasetConfigType, FlagType, FlagIndexType]</code>, <code>ABC</code></p> <p>Abstract base class for collections of datasets.</p> <p>DatasetCollection extends the Dataset interface to handle multiple child datasets while maintaining the same unified API. This enables complex hierarchical structures where collections themselves can be treated as datasets.</p> Key Features <ul> <li>Inherits all Dataset functionality</li> <li>Manages collections of child datasets</li> <li>Provides iteration and access methods</li> <li>Aggregates accepted flags from all children</li> <li>Supports KPI operations across all sub-datasets</li> </ul> <p>          Class Type Parameters:        </p> Name Bound or Constraints Description Default <code>DatasetType</code> <p>Type of datasets that can be collected</p> required <code>DatasetConfigType</code> <p>Configuration class for dataset behavior</p> required <code>FlagType</code> <p>Type used for data flag identification</p> required <code>FlagIndexType</code> <p>Flag index implementation for flag mapping</p> required <p>Attributes:</p> Name Type Description <code>datasets</code> <code>list[DatasetType]</code> <p>List of child datasets in this collection</p> Note <p>This class follows the \"Everything is a Dataset\" principle, allowing collections to be used anywhere a Dataset is expected.</p> Source code in <code>submodules/mesqual/mesqual/datasets/dataset_collection.py</code> <pre><code>class DatasetCollection(\n    Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType],\n    Dataset[DatasetConfigType, FlagType, FlagIndexType],\n    ABC\n):\n    \"\"\"\n    Abstract base class for collections of datasets.\n\n    DatasetCollection extends the Dataset interface to handle multiple child datasets\n    while maintaining the same unified API. This enables complex hierarchical structures\n    where collections themselves can be treated as datasets.\n\n    Key Features:\n        - Inherits all Dataset functionality\n        - Manages collections of child datasets\n        - Provides iteration and access methods\n        - Aggregates accepted flags from all children\n        - Supports KPI operations across all sub-datasets\n\n    Type Parameters:\n        DatasetType: Type of datasets that can be collected\n        DatasetConfigType: Configuration class for dataset behavior\n        FlagType: Type used for data flag identification\n        FlagIndexType: Flag index implementation for flag mapping\n\n    Attributes:\n        datasets (list[DatasetType]): List of child datasets in this collection\n\n    Note:\n        This class follows the \"Everything is a Dataset\" principle, allowing\n        collections to be used anywhere a Dataset is expected.\n    \"\"\"\n\n    def __init__(\n            self,\n            datasets: list[DatasetType] = None,\n            name: str = None,\n            parent_dataset: Dataset = None,\n            flag_index: FlagIndex = None,\n            attributes: dict = None,\n            database: Database = None,\n            config: DatasetConfigType = None\n    ):\n        super().__init__(\n            name=name,\n            parent_dataset=parent_dataset,\n            flag_index=flag_index,\n            attributes=attributes,\n            database=database,\n            config=config,\n        )\n        self.datasets: list[DatasetType] = datasets if datasets else []\n\n    @property\n    def dataset_iterator(self) -&gt; Iterator[DatasetType]:\n        for ds in self.datasets:\n            yield ds\n\n    @property\n    def flag_index(self) -&gt; FlagIndex:\n        from mesqual.flag.flag_index import EmptyFlagIndex\n        if (self._flag_index is None) or isinstance(self._flag_index, EmptyFlagIndex):\n            from mesqual.utils.check_all_same import all_same_object\n            if all_same_object(ds.flag_index for ds in self.datasets) and len(self.datasets):\n                return self.get_dataset().flag_index\n        return self._flag_index\n\n    @property\n    def attributes(self) -&gt; dict:\n        child_dataset_atts = [ds.attributes for ds in self.datasets]\n        attributes_that_all_childs_have_in_common = get_intersection_of_dicts(child_dataset_atts)\n        return {**attributes_that_all_childs_have_in_common, **self._attributes.copy()}\n\n    def get_merged_kpi_collection(self, deep: bool = True) -&gt; KPICollection:\n        \"\"\"\n        Merge KPI collections from all child datasets.\n\n        This method collects KPIs from all child datasets' kpi_collection\n        properties and returns a unified collection. Optionally recurses into\n        nested DatasetCollections.\n\n        Args:\n            deep: If True, recursively merge from nested DatasetCollections\n\n        Returns:\n            KPICollection containing all KPIs from all child datasets\n\n        Example:\n\n            &gt;&gt;&gt; # Create KPIs for all scenarios\n            &gt;&gt;&gt; study.scen: DatasetConcatCollection\n            &gt;&gt;&gt; study.scen.add_kpis_from_definitions_to_all_child_datasets(kpi_defs)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Get merged collection across all scenarios\n            &gt;&gt;&gt; all_kpis = study.scen.get_merged_kpi_collection()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Filter and export\n            &gt;&gt;&gt; mean_prices = all_kpis.filter_by(aggregation=Aggregations.Mean)\n            &gt;&gt;&gt; df = mean_prices.to_dataframe(unit_handling='auto_convert')\n        \"\"\"\n        from mesqual.kpis.collection import KPICollection\n        merged = KPICollection()\n\n        for ds in self.datasets:\n            # Add KPIs from this dataset\n            merged.extend(ds.kpi_collection._kpis)\n\n            # Recursively add from nested collections\n            if deep and isinstance(ds, DatasetCollection):\n                nested_merged = ds.get_merged_kpi_collection(deep=deep)\n                merged.extend(nested_merged._kpis)\n\n        return merged\n\n    def add_kpis_from_definitions_to_all_child_datasets(self, kpi_definitions: KPIDefinition | list[KPIDefinition]):\n        for ds in self.dataset_iterator:\n            ds.add_kpis_from_definitions(kpi_definitions)\n\n    def clear_kpi_collection_for_all_child_datasets(self, deep: bool = True):\n        for ds in self.datasets:\n            ds.clear_kpi_collection()\n            if deep and isinstance(ds, DatasetCollection):\n                ds.clear_kpi_collection_for_all_child_datasets(deep=deep)\n\n    @abstractmethod\n    def _fetch(\n            self,\n            flag: FlagType,\n            effective_config: DatasetConfigType,\n            **kwargs\n    ) -&gt; pd.Series | pd.DataFrame:\n        pass\n\n    def flag_is_accepted(self, flag: FlagType) -&gt; bool:\n        return any(ds.flag_is_accepted(flag) for ds in self.datasets)\n\n    @property\n    def accepted_flags(self) -&gt; set[FlagType]:\n        return nested_union([ds.accepted_flags for ds in self.datasets])\n\n    def _required_flags_for_flag(self, flag: FlagType) -&gt; set[FlagType]:\n        return nested_union([ds.accepted_flags for ds in self.datasets])\n\n    def get_dataset(self, key: str = None) -&gt; DatasetType:\n        if key is None:\n            if not self.datasets:\n                raise ValueError(\"No datasets available\")\n            return self.datasets[0]\n\n        for ds in self.datasets:\n            if ds.name == key:\n                return ds\n\n        raise KeyError(f\"Dataset with name '{key}' not found\")\n\n    def add_datasets(self, datasets: Iterable[DatasetType]):\n        for ds in datasets:\n            self.add_dataset(ds)\n\n    def add_dataset(self, dataset: DatasetType):\n        if not isinstance(dataset, self.get_child_dataset_type()):\n            raise TypeError(f\"Can only add data sets of type {self.get_child_dataset_type().__name__}.\")\n\n        for i, existing in enumerate(self.datasets):\n            if existing.name == dataset.name:\n                logger.warning(\n                    f\"Dataset {self.name}: \"\n                    f\"dataset {dataset.name} already in this collection. Replacing it.\"\n                )\n                self.datasets[i] = dataset\n                return\n\n        self.datasets.append(dataset)\n\n    @classmethod\n    def get_child_dataset_type(cls) -&gt; type[DatasetType]:\n        return Dataset\n\n    def fetch_merged(\n            self,\n            flag: FlagType,\n            config: dict | DatasetConfigType = None,\n            keep_first: bool = True,\n            **kwargs\n    ) -&gt; pd.Series | pd.DataFrame:\n        \"\"\"Fetch method that merges dataframes from all child datasets, similar to DatasetMergeCollection.\"\"\"\n        temp_merge_collection = self.get_merged_dataset_collection(keep_first)\n        return temp_merge_collection.fetch(flag, config, **kwargs)\n\n    def get_merged_dataset_collection(self, keep_first: bool = True) -&gt; 'DatasetMergeCollection':\n        return DatasetMergeCollection(\n            datasets=self.datasets,\n            name=f\"{self.name} merged\",\n            keep_first=keep_first\n        )\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset_collection.DatasetCollection.get_merged_kpi_collection","title":"get_merged_kpi_collection","text":"<pre><code>get_merged_kpi_collection(deep: bool = True) -&gt; KPICollection\n</code></pre> <p>Merge KPI collections from all child datasets.</p> <p>This method collects KPIs from all child datasets' kpi_collection properties and returns a unified collection. Optionally recurses into nested DatasetCollections.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, recursively merge from nested DatasetCollections</p> <code>True</code> <p>Returns:</p> Type Description <code>KPICollection</code> <p>KPICollection containing all KPIs from all child datasets</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Create KPIs for all scenarios\n&gt;&gt;&gt; study.scen: DatasetConcatCollection\n&gt;&gt;&gt; study.scen.add_kpis_from_definitions_to_all_child_datasets(kpi_defs)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get merged collection across all scenarios\n&gt;&gt;&gt; all_kpis = study.scen.get_merged_kpi_collection()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Filter and export\n&gt;&gt;&gt; mean_prices = all_kpis.filter_by(aggregation=Aggregations.Mean)\n&gt;&gt;&gt; df = mean_prices.to_dataframe(unit_handling='auto_convert')\n</code></pre> Source code in <code>submodules/mesqual/mesqual/datasets/dataset_collection.py</code> <pre><code>def get_merged_kpi_collection(self, deep: bool = True) -&gt; KPICollection:\n    \"\"\"\n    Merge KPI collections from all child datasets.\n\n    This method collects KPIs from all child datasets' kpi_collection\n    properties and returns a unified collection. Optionally recurses into\n    nested DatasetCollections.\n\n    Args:\n        deep: If True, recursively merge from nested DatasetCollections\n\n    Returns:\n        KPICollection containing all KPIs from all child datasets\n\n    Example:\n\n        &gt;&gt;&gt; # Create KPIs for all scenarios\n        &gt;&gt;&gt; study.scen: DatasetConcatCollection\n        &gt;&gt;&gt; study.scen.add_kpis_from_definitions_to_all_child_datasets(kpi_defs)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get merged collection across all scenarios\n        &gt;&gt;&gt; all_kpis = study.scen.get_merged_kpi_collection()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Filter and export\n        &gt;&gt;&gt; mean_prices = all_kpis.filter_by(aggregation=Aggregations.Mean)\n        &gt;&gt;&gt; df = mean_prices.to_dataframe(unit_handling='auto_convert')\n    \"\"\"\n    from mesqual.kpis.collection import KPICollection\n    merged = KPICollection()\n\n    for ds in self.datasets:\n        # Add KPIs from this dataset\n        merged.extend(ds.kpi_collection._kpis)\n\n        # Recursively add from nested collections\n        if deep and isinstance(ds, DatasetCollection):\n            nested_merged = ds.get_merged_kpi_collection(deep=deep)\n            merged.extend(nested_merged._kpis)\n\n    return merged\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset_collection.DatasetCollection.fetch_merged","title":"fetch_merged","text":"<pre><code>fetch_merged(flag: FlagType, config: dict | DatasetConfigType = None, keep_first: bool = True, **kwargs) -&gt; Series | DataFrame\n</code></pre> <p>Fetch method that merges dataframes from all child datasets, similar to DatasetMergeCollection.</p> Source code in <code>submodules/mesqual/mesqual/datasets/dataset_collection.py</code> <pre><code>def fetch_merged(\n        self,\n        flag: FlagType,\n        config: dict | DatasetConfigType = None,\n        keep_first: bool = True,\n        **kwargs\n) -&gt; pd.Series | pd.DataFrame:\n    \"\"\"Fetch method that merges dataframes from all child datasets, similar to DatasetMergeCollection.\"\"\"\n    temp_merge_collection = self.get_merged_dataset_collection(keep_first)\n    return temp_merge_collection.fetch(flag, config, **kwargs)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset_collection.DatasetLinkCollection","title":"DatasetLinkCollection","text":"<p>               Bases: <code>Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code>, <code>DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code></p> <p>Links multiple datasets to provide unified data access with automatic routing.</p> <p>DatasetLinkCollection acts as a unified interface to multiple child datasets, automatically routing data requests to the appropriate child dataset that  accepts the requested flag. This is the foundation for platform datasets that combine multiple data interpreters.</p> Key Features <ul> <li>Automatic flag routing to appropriate child dataset</li> <li>Bidirectional parent-child relationships</li> <li>First-match-wins routing strategy</li> <li>Overlap detection and warnings</li> <li>Maintains all Dataset interface compatibility</li> </ul> Routing Logic <p>When fetch() is called, iterates through child datasets in order and returns data from the first dataset that accepts the flag.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Platform dataset with multiple interpreters\n&gt;&gt;&gt; link_collection = DatasetLinkCollection([\n...     ModelInterpreter(network),\n...     TimeSeriesInterpreter(network),\n...     ObjectiveInterpreter(network)\n... ])\n&gt;&gt;&gt; # Automatically routes to appropriate interpreter\n&gt;&gt;&gt; buses = link_collection.fetch('buses')  # -&gt; ModelInterpreter\n&gt;&gt;&gt; prices = link_collection.fetch('buses_t.marginal_price')  # -&gt; TimeSeriesInterpreter\n</code></pre> Warning <p>If multiple child datasets accept the same flag, only the first one will be used. The constructor logs warnings for such overlaps.</p> Source code in <code>submodules/mesqual/mesqual/datasets/dataset_collection.py</code> <pre><code>class DatasetLinkCollection(\n    Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType],\n    DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]\n):\n    \"\"\"\n    Links multiple datasets to provide unified data access with automatic routing.\n\n    DatasetLinkCollection acts as a unified interface to multiple child datasets,\n    automatically routing data requests to the appropriate child dataset that \n    accepts the requested flag. This is the foundation for platform datasets\n    that combine multiple data interpreters.\n\n    Key Features:\n        - Automatic flag routing to appropriate child dataset\n        - Bidirectional parent-child relationships\n        - First-match-wins routing strategy\n        - Overlap detection and warnings\n        - Maintains all Dataset interface compatibility\n\n    Routing Logic:\n        When fetch() is called, iterates through child datasets in order and\n        returns data from the first dataset that accepts the flag.\n\n    Example:\n\n        &gt;&gt;&gt; # Platform dataset with multiple interpreters\n        &gt;&gt;&gt; link_collection = DatasetLinkCollection([\n        ...     ModelInterpreter(network),\n        ...     TimeSeriesInterpreter(network),\n        ...     ObjectiveInterpreter(network)\n        ... ])\n        &gt;&gt;&gt; # Automatically routes to appropriate interpreter\n        &gt;&gt;&gt; buses = link_collection.fetch('buses')  # -&gt; ModelInterpreter\n        &gt;&gt;&gt; prices = link_collection.fetch('buses_t.marginal_price')  # -&gt; TimeSeriesInterpreter\n\n    Warning:\n        If multiple child datasets accept the same flag, only the first one\n        will be used. The constructor logs warnings for such overlaps.\n    \"\"\"\n\n    def __init__(\n            self,\n            datasets: list[DatasetType],\n            name: str = None,\n            parent_dataset: Dataset = None,\n            flag_index: FlagIndex = None,\n            attributes: dict = None,\n            database: Database = None,\n            config: DatasetConfigType = None,\n    ):\n        super().__init__(\n            datasets=datasets,\n            name=name,\n            parent_dataset=parent_dataset,\n            flag_index=flag_index,\n            attributes=attributes,\n            database=database,\n            config=config,\n        )\n        self._warn_if_flags_overlap()\n\n    def _fetch(self, flag: FlagType, effective_config: DatasetConfigType, **kwargs) -&gt; pd.Series | pd.DataFrame:\n        for ds in self.datasets:\n            if ds.flag_is_accepted(flag):\n                return ds.fetch(flag, effective_config, **kwargs)\n        raise KeyError(f\"Key '{flag}' not recognized by any of the linked Datasets.\")\n\n    def _warn_if_flags_overlap(self):\n        from collections import Counter\n\n        accepted_flags = list()\n        for ds in self.datasets:\n            accepted_flags += list(ds.accepted_flags)\n\n        counts = Counter(accepted_flags)\n        duplicates = {k: v for k, v in counts.items() if v &gt; 1}\n        if any(duplicates.values()):\n            logger.warning(\n                f\"Dataset {self.name}: \"\n                f\"The following keys have multiple Dataset sources: {duplicates.keys()}. \\n\"\n                f\"Only the first one will be used! This might lead to unexpected behavior. \\n\"\n                f\"A potential reason could be the use of an inappropriate DatasetCollection Type.\"\n            )\n\n    def get_dataset_by_type(self, ds_type: type[Dataset]) -&gt; DatasetType:\n        \"\"\"Returns instance of child dataset that matches the ds_type.\"\"\"\n        for ds in self.datasets:\n            if isinstance(ds, ds_type):\n                return ds\n        raise KeyError(f'No Dataset of type {ds_type.__name__} found in {self.name}.')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset_collection.DatasetLinkCollection.get_dataset_by_type","title":"get_dataset_by_type","text":"<pre><code>get_dataset_by_type(ds_type: type[Dataset]) -&gt; DatasetType\n</code></pre> <p>Returns instance of child dataset that matches the ds_type.</p> Source code in <code>submodules/mesqual/mesqual/datasets/dataset_collection.py</code> <pre><code>def get_dataset_by_type(self, ds_type: type[Dataset]) -&gt; DatasetType:\n    \"\"\"Returns instance of child dataset that matches the ds_type.\"\"\"\n    for ds in self.datasets:\n        if isinstance(ds, ds_type):\n            return ds\n    raise KeyError(f'No Dataset of type {ds_type.__name__} found in {self.name}.')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset_collection.DatasetMergeCollection","title":"DatasetMergeCollection","text":"<p>               Bases: <code>Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code>, <code>DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code></p> <p>Fetch method will merge fragmented Datasets for same flag, e.g.:     - fragmented simulation runs, e.g. CW1, CW2, CW3, CWn.     - fragmented data sources, e.g. mapping from Excel file with model from simulation platform.</p> Source code in <code>submodules/mesqual/mesqual/datasets/dataset_collection.py</code> <pre><code>class DatasetMergeCollection(\n    Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType],\n    DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]\n):\n    \"\"\"\n    Fetch method will merge fragmented Datasets for same flag, e.g.:\n        - fragmented simulation runs, e.g. CW1, CW2, CW3, CWn.\n        - fragmented data sources, e.g. mapping from Excel file with model from simulation platform.\n    \"\"\"\n    def __init__(\n            self,\n            datasets: list[DatasetType],\n            name: str = None,\n            parent_dataset: Dataset = None,\n            flag_index: FlagIndex = None,\n            attributes: dict = None,\n            database: Database = None,\n            config: DatasetConfigType = None,\n            keep_first: bool = True,\n    ):\n        super().__init__(\n            datasets=datasets,\n            name=name,\n            parent_dataset=parent_dataset,\n            flag_index=flag_index,\n            attributes=attributes,\n            database=database,\n            config=config,\n        )\n        self.keep_first = keep_first\n\n    def _fetch(self, flag: FlagType, effective_config: DatasetConfigType, **kwargs) -&gt; pd.Series | pd.DataFrame:\n        data_frames = []\n        for ds in self.datasets:\n            if ds.flag_is_accepted(flag):\n                data_frames.append(ds.fetch(flag, effective_config, **kwargs))\n\n        if not data_frames:\n            raise KeyError(f\"Flag '{flag}' not recognized by any of the datasets.\")\n\n        from mesqual.utils.pandas_utils.combine_df import combine_dfs\n        df = combine_dfs(data_frames, keep_first=self.keep_first)\n        return df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset_collection.DatasetConcatCollection","title":"DatasetConcatCollection","text":"<p>               Bases: <code>Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code>, <code>DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code></p> <p>Concatenates data from multiple datasets with MultiIndex structure.</p> <p>DatasetConcatCollection is fundamental to MESQUAL's multi-scenario analysis capabilities. It fetches the same flag from multiple child datasets and concatenates the results into a single DataFrame/Series with an additional index level identifying the source dataset.</p> Key Features <ul> <li>Automatic MultiIndex creation with dataset names</li> <li>Configurable concatenation axis and level positioning  </li> <li>Preserves all dimensional relationships</li> <li>Supports scenario and comparison collections</li> <li>Enables unified analysis across multiple datasets</li> </ul> MultiIndex Structure <p>The resulting data structure includes an additional index level (typically named 'dataset') that identifies the source dataset for each data point.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Collection of scenario datasets\n&gt;&gt;&gt; scenarios = DatasetConcatCollection([\n...     PyPSADataset(base_network, name='base'),\n...     PyPSADataset(high_res_network, name='high_res'),\n...     PyPSADataset(low_gas_network, name='low_gas')\n... ])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Fetch creates MultiIndex DataFrame\n&gt;&gt;&gt; prices = scenarios.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; print(prices.columns.names)\n    ['dataset', 'Bus']  # Original Bus index + dataset level\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Access specific scenario data\n&gt;&gt;&gt; base_prices = prices['base']\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Analyze across scenarios\n&gt;&gt;&gt; mean_prices = prices.mean()  # Mean across all scenarios\n</code></pre> Source code in <code>submodules/mesqual/mesqual/datasets/dataset_collection.py</code> <pre><code>class DatasetConcatCollection(\n    Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType],\n    DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]\n):\n    \"\"\"\n    Concatenates data from multiple datasets with MultiIndex structure.\n\n    DatasetConcatCollection is fundamental to MESQUAL's multi-scenario analysis\n    capabilities. It fetches the same flag from multiple child datasets and\n    concatenates the results into a single DataFrame/Series with an additional\n    index level identifying the source dataset.\n\n    Key Features:\n        - Automatic MultiIndex creation with dataset names\n        - Configurable concatenation axis and level positioning  \n        - Preserves all dimensional relationships\n        - Supports scenario and comparison collections\n        - Enables unified analysis across multiple datasets\n\n    MultiIndex Structure:\n        The resulting data structure includes an additional index level\n        (typically named 'dataset') that identifies the source dataset\n        for each data point.\n\n    Example:\n\n        &gt;&gt;&gt; # Collection of scenario datasets\n        &gt;&gt;&gt; scenarios = DatasetConcatCollection([\n        ...     PyPSADataset(base_network, name='base'),\n        ...     PyPSADataset(high_res_network, name='high_res'),\n        ...     PyPSADataset(low_gas_network, name='low_gas')\n        ... ])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Fetch creates MultiIndex DataFrame\n        &gt;&gt;&gt; prices = scenarios.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; print(prices.columns.names)\n            ['dataset', 'Bus']  # Original Bus index + dataset level\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Access specific scenario data\n        &gt;&gt;&gt; base_prices = prices['base']\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Analyze across scenarios\n        &gt;&gt;&gt; mean_prices = prices.mean()  # Mean across all scenarios\n    \"\"\"\n    DEFAULT_CONCAT_LEVEL_NAME = 'dataset'\n    DEFAULT_ATT_LEVEL_NAME = 'attribute'\n\n    def __init__(\n            self,\n            datasets: list[DatasetType],\n            name: str = None,\n            parent_dataset: Dataset = None,\n            flag_index: FlagIndex = None,\n            attributes: dict = None,\n            database: Database = None,\n            config: DatasetConfigType = None,\n            default_concat_axis: int = 1,\n            concat_top: bool = True,\n            concat_level_name: str = None,\n    ):\n        super().__init__(\n            datasets=datasets,\n            name=name,\n            parent_dataset=parent_dataset,\n            flag_index=flag_index,\n            attributes=attributes,\n            database=database,\n            config=config,\n        )\n        super().__init__(datasets=datasets, name=name)\n        self.default_concat_axis = default_concat_axis\n        self.concat_top = concat_top\n        self.concat_level_name = concat_level_name or self.DEFAULT_CONCAT_LEVEL_NAME\n\n    def get_attributes_concat_df(self) -&gt; pd.DataFrame:\n        if all(isinstance(ds, DatasetConcatCollection) for ds in self.datasets):\n            use_att_df_instead_of_series = True\n        else:\n            use_att_df_instead_of_series = False\n\n        atts_per_dataset = dict()\n        for ds in self.datasets:\n            atts = ds.get_attributes_concat_df().T if use_att_df_instead_of_series else ds.get_attributes_series()\n            atts_per_dataset[ds.name] = atts\n\n        return pd.concat(\n            atts_per_dataset,\n            axis=1,\n            names=[self.concat_level_name]\n        ).rename_axis(self.DEFAULT_ATT_LEVEL_NAME).T\n\n    def _fetch(\n            self,\n            flag: FlagType,\n            effective_config: DatasetConfigType,\n            concat_axis: int = None,\n            **kwargs\n    ) -&gt; pd.Series | pd.DataFrame:\n        if concat_axis is None:\n            concat_axis = self.default_concat_axis\n\n        dfs = {}\n        for ds in self.datasets:\n            if ds.flag_is_accepted(flag):\n                dfs[ds.name] = ds.fetch(flag, effective_config, **kwargs)\n\n        if not dfs:\n            raise KeyError(f\"Flag '{flag}' not recognized by any of the datasets in {type(self)} {self.name}.\")\n\n        df0 = list(dfs.values())[0]\n        if not all(len(df.axes) == len(df0.axes) for df in dfs.values()):\n            raise NotImplementedError(f'Axes lengths do not match between dfs.')\n\n        for ax in range(len(df0.axes)):\n            if not all(set(df.axes[ax].names) == set(df0.axes[ax].names) for df in dfs.values()):\n                raise NotImplementedError(f'Axes names do not match between dfs.')\n\n        df = pd.concat(dfs, join='outer', axis=concat_axis, names=[self.concat_level_name])\n\n        if not self.concat_top:\n            ax = df.axes[concat_axis]\n            df.axes[concat_axis] = ax.reorder_levels([ax.nlevels - 1] + list(range(ax.nlevels - 1)))\n\n        return df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset_comparison","title":"dataset_comparison","text":""},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset_comparison.DatasetComparison","title":"DatasetComparison","text":"<p>               Bases: <code>Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code>, <code>DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code></p> <p>Computes and provides access to differences between two datasets.</p> <p>DatasetComparison is a core component of MESQUAL's scenario comparison capabilities. It automatically calculates deltas, ratios, or side-by-side comparisons between a variation dataset and a reference dataset, enabling systematic analysis of scenario differences.</p> Key Features <ul> <li>Automatic delta computation between datasets</li> <li>Multiple comparison types (DELTA, VARIATION, BOTH)</li> <li>Handles numeric and non-numeric data appropriately</li> <li>Preserves data structure and index relationships</li> <li>Configurable unchanged value handling</li> <li>Inherits full Dataset interface</li> </ul> Comparison Types <ul> <li>DELTA: Variation - Reference (default)</li> <li>VARIATION: Returns variation data with optional NaN for unchanged values</li> <li>BOTH: Side-by-side variation and reference data</li> </ul> <p>Attributes:</p> Name Type Description <code>variation_dataset</code> <p>The dataset representing the scenario being compared</p> <code>reference_dataset</code> <p>The dataset representing the baseline for comparison</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Compare high renewable scenario to base case\n&gt;&gt;&gt; comparison = DatasetComparison(\n...     variation_dataset=high_res_dataset,\n...     reference_dataset=base_dataset\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Get price differences\n&gt;&gt;&gt; price_deltas = comparison.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Get both datasets side-by-side (often used to show model changes)\n&gt;&gt;&gt; price_both = comparison.fetch('buses', comparison_type=ComparisonTypeEnum.BOTH)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Highlight only changes (often used to show model changes)\n&gt;&gt;&gt; price_changes = comparison.fetch('buses', replace_unchanged_values_by_nan=True)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/datasets/dataset_comparison.py</code> <pre><code>class DatasetComparison(\n    Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType],\n    DatasetCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]\n):\n    \"\"\"\n    Computes and provides access to differences between two datasets.\n\n    DatasetComparison is a core component of MESQUAL's scenario comparison capabilities.\n    It automatically calculates deltas, ratios, or side-by-side comparisons between\n    a variation dataset and a reference dataset, enabling systematic analysis of\n    scenario differences.\n\n    Key Features:\n        - Automatic delta computation between datasets\n        - Multiple comparison types (DELTA, VARIATION, BOTH)\n        - Handles numeric and non-numeric data appropriately\n        - Preserves data structure and index relationships\n        - Configurable unchanged value handling\n        - Inherits full Dataset interface\n\n    Comparison Types:\n        - DELTA: Variation - Reference (default)\n        - VARIATION: Returns variation data with optional NaN for unchanged values\n        - BOTH: Side-by-side variation and reference data\n\n    Attributes:\n        variation_dataset: The dataset representing the scenario being compared\n        reference_dataset: The dataset representing the baseline for comparison\n\n    Example:\n\n        &gt;&gt;&gt; # Compare high renewable scenario to base case\n        &gt;&gt;&gt; comparison = DatasetComparison(\n        ...     variation_dataset=high_res_dataset,\n        ...     reference_dataset=base_dataset\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Get price differences\n        &gt;&gt;&gt; price_deltas = comparison.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Get both datasets side-by-side (often used to show model changes)\n        &gt;&gt;&gt; price_both = comparison.fetch('buses', comparison_type=ComparisonTypeEnum.BOTH)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Highlight only changes (often used to show model changes)\n        &gt;&gt;&gt; price_changes = comparison.fetch('buses', replace_unchanged_values_by_nan=True)\n    \"\"\"\n    COMPARISON_ATTRIBUTES_SOURCE = ComparisonAttributesSourceEnum.USE_VARIATION_ATTS\n    COMPARISON_NAME_JOIN = ' vs '\n    VARIATION_DS_ATT_KEY = 'variation_dataset'\n    REFERENCE_DS_ATT_KEY = 'reference_dataset'\n\n    def __init__(\n            self,\n            variation_dataset: Dataset,\n            reference_dataset: Dataset,\n            name: str = None,\n            attributes: dict = None,\n            config: DatasetConfigType = None,\n    ):\n        name = name or self._get_auto_generated_name(variation_dataset, reference_dataset)\n\n        super().__init__(\n            [reference_dataset, variation_dataset],\n            name=name,\n            attributes=attributes,\n            config=config\n        )\n\n        self.variation_dataset = variation_dataset\n        self.reference_dataset = reference_dataset\n\n    def _get_auto_generated_name(self, variation_dataset: Dataset, reference_dataset: Dataset) -&gt; str:\n        return variation_dataset.name + self.COMPARISON_NAME_JOIN + reference_dataset.name\n\n    @property\n    def attributes(self) -&gt; dict:\n        match self.COMPARISON_ATTRIBUTES_SOURCE:\n            case ComparisonAttributesSourceEnum.USE_VARIATION_ATTS:\n                atts = self.variation_dataset.attributes.copy()\n            case ComparisonAttributesSourceEnum.USE_REFERENCE_ATTS:\n                atts = self.reference_dataset.attributes.copy()\n            case _:\n                atts = super().attributes\n        atts[self.VARIATION_DS_ATT_KEY] = self.variation_dataset.name\n        atts[self.REFERENCE_DS_ATT_KEY] = self.reference_dataset.name\n        return atts\n\n    def fetch(\n            self,\n            flag: FlagType,\n            config: dict | DatasetConfigType = None,\n            comparison_type: ComparisonTypeEnum = ComparisonTypeEnum.DELTA,\n            replace_unchanged_values_by_nan: bool = False,\n            fill_value: float | int | None = None,\n            **kwargs\n    ) -&gt; pd.Series | pd.DataFrame:\n        \"\"\"\n        Fetch comparison data between variation and reference datasets.\n\n        Extends the base Dataset.fetch() method with comparison-specific parameters\n        for controlling how the comparison is computed and formatted.\n\n        Args:\n            flag: Data identifier flag to fetch from both datasets\n            config: Optional configuration overrides\n            comparison_type: How to compare the datasets:\n                - DELTA: variation - reference (default)\n                - VARIATION: variation data only, optionally with NaN for unchanged\n                - BOTH: concatenated variation and reference data\n            replace_unchanged_values_by_nan: If True, replaces values that are\n                identical between datasets with NaN (useful for highlighting changes)\n            fill_value: Value to use for missing data in subtraction operations\n            **kwargs: Additional arguments passed to child dataset fetch methods\n\n        Returns:\n            DataFrame or Series with comparison results\n\n        Example:\n\n            &gt;&gt;&gt; # Basic delta comparison\n            &gt;&gt;&gt; deltas = comparison.fetch('buses_t.marginal_price')\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Highlight only changed values\n            &gt;&gt;&gt; changes_only = comparison.fetch(\n            ...     'buses_t.marginal_price',\n            ...     replace_unchanged_values_by_nan=True\n            ... )\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Side-by-side comparison\n            &gt;&gt;&gt; both = comparison.fetch(\n            ...     'buses_t.marginal_price',\n            ...     comparison_type=ComparisonTypeEnum.BOTH\n            ... )\n        \"\"\"\n        return super().fetch(\n            flag=flag,\n            config=config,\n            comparison_type=comparison_type,\n            replace_unchanged_values_by_nan=replace_unchanged_values_by_nan,\n            fill_value=fill_value,\n            **kwargs\n        )\n\n    def _fetch(\n            self,\n            flag: FlagType,\n            effective_config: DatasetConfigType,\n            comparison_type: ComparisonTypeEnum = ComparisonTypeEnum.DELTA,\n            replace_unchanged_values_by_nan: bool = False,\n            fill_value: float | int | None = None,\n            **kwargs\n    ) -&gt; pd.Series | pd.DataFrame:\n        df_var = self.variation_dataset.fetch(flag, effective_config, **kwargs)\n        df_ref = self.reference_dataset.fetch(flag, effective_config, **kwargs)\n\n        match comparison_type:\n            case ComparisonTypeEnum.VARIATION:\n                return self._get_variation_comparison(df_var, df_ref, replace_unchanged_values_by_nan)\n            case ComparisonTypeEnum.BOTH:\n                return self._get_both_comparison(df_var, df_ref, replace_unchanged_values_by_nan)\n            case ComparisonTypeEnum.DELTA:\n                return self._get_delta_comparison(df_var, df_ref, replace_unchanged_values_by_nan, fill_value)\n        raise ValueError(f\"Unsupported comparison_type: {comparison_type}\")\n\n    def _values_are_equal(self, val1, val2) -&gt; bool:\n        if pd.isna(val1) and pd.isna(val2):\n            return True\n        try:\n            return val1 == val2\n        except:\n            pass\n        try:\n            if str(val1) == str(val2):\n                return True\n        except:\n            pass\n        return False\n\n    def _get_variation_comparison(\n            self,\n            df_var: pd.DataFrame,\n            df_ref: pd.DataFrame,\n            replace_unchanged_values_by_nan: bool\n    ) -&gt; pd.DataFrame:\n        result = df_var.copy()\n\n        if replace_unchanged_values_by_nan:\n            common_indices = df_var.index.intersection(df_ref.index)\n            common_columns = df_var.columns.intersection(df_ref.columns)\n\n            for idx in common_indices:\n                for col in common_columns:\n                    if self._values_are_equal(df_var.loc[idx, col], df_ref.loc[idx, col]):\n                        result.loc[idx, col] = float('nan')\n\n        return result\n\n    def _get_both_comparison(\n            self,\n            df_var: pd.DataFrame,\n            df_ref: pd.DataFrame,\n            replace_unchanged_values_by_nan: bool\n    ) -&gt; pd.DataFrame:\n        var_name = self.variation_dataset.name\n        ref_name = self.reference_dataset.name\n\n        result = pd.concat([df_var, df_ref], keys=[var_name, ref_name])\n        result = result.sort_index(level=1)\n\n        if replace_unchanged_values_by_nan:\n            common_indices = df_var.index.intersection(df_ref.index)\n            common_columns = df_var.columns.intersection(df_ref.columns)\n\n            for idx in common_indices:\n                for col in common_columns:\n                    if self._values_are_equal(df_var.loc[idx, col], df_ref.loc[idx, col]):\n                        result.loc[(var_name, idx), col] = float('nan')\n                        result.loc[(ref_name, idx), col] = float('nan')\n\n        return result\n\n    def _get_delta_comparison(\n            self,\n            df_var: pd.DataFrame,\n            df_ref: pd.DataFrame,\n            replace_unchanged_values_by_nan: bool,\n            fill_value: float | int | None\n    ) -&gt; pd.DataFrame:\n        if pd_is_numeric(df_var) and pd_is_numeric(df_ref):\n            result = df_var.subtract(df_ref, fill_value=fill_value)\n\n            if replace_unchanged_values_by_nan:\n                result = result.replace(0, float('nan'))\n\n            return result\n\n        all_columns = df_var.columns.union(df_ref.columns)\n        all_indices = df_var.index.union(df_ref.index)\n\n        result = pd.DataFrame(index=all_indices, columns=all_columns)\n\n        for col in all_columns:\n            if col in df_var.columns and col in df_ref.columns:\n                var_col = df_var[col]\n                ref_col = df_ref[col]\n\n                # Special handling for boolean columns\n                if pd.api.types.is_bool_dtype(var_col) and pd.api.types.is_bool_dtype(ref_col):\n                    # For booleans, we can mark where they differ\n                    common_indices = var_col.index.intersection(ref_col.index)\n                    delta = pd.Series(index=all_indices)\n\n                    for idx in common_indices:\n                        if var_col.loc[idx] != ref_col.loc[idx]:\n                            delta.loc[idx] = f\"{var_col.loc[idx]} (was {ref_col.loc[idx]})\"\n                        elif not replace_unchanged_values_by_nan:\n                            delta.loc[idx] = var_col.loc[idx]\n\n                    # Handle indices only in variation\n                    for idx in var_col.index.difference(ref_col.index):\n                        delta.loc[idx] = f\"{var_col.loc[idx]} (new)\"\n\n                    # Handle indices only in reference\n                    for idx in ref_col.index.difference(var_col.index):\n                        delta.loc[idx] = f\"DELETED: {ref_col.loc[idx]}\"\n\n                    result[col] = delta\n\n                elif pd.api.types.is_numeric_dtype(var_col) and pd.api.types.is_numeric_dtype(ref_col):\n                    delta = var_col.subtract(ref_col, fill_value=fill_value)\n                    result[col] = delta\n\n                    if replace_unchanged_values_by_nan:\n                        result.loc[delta == 0, col] = float('nan')\n                else:\n                    common_indices = var_col.index.intersection(ref_col.index)\n                    var_only_indices = var_col.index.difference(ref_col.index)\n                    ref_only_indices = ref_col.index.difference(var_col.index)\n\n                    for idx in common_indices:\n                        if not self._values_are_equal(var_col.loc[idx], ref_col.loc[idx]):\n                            result.loc[idx, col] = f\"{var_col.loc[idx]} (was {ref_col.loc[idx]})\"\n                        elif not replace_unchanged_values_by_nan:\n                            result.loc[idx, col] = var_col.loc[idx]\n\n                    for idx in var_only_indices:\n                        result.loc[idx, col] = f\"{var_col.loc[idx]} (new)\"\n\n                    for idx in ref_only_indices:\n                        val = ref_col.loc[idx]\n                        if not pd.isna(val):\n                            result.loc[idx, col] = f\"DELETED: {val}\"\n\n            elif col in df_var.columns:\n                for idx in df_var.index:\n                    result.loc[idx, col] = f\"{df_var.loc[idx, col]} (new column)\"\n\n            else:  # Column only in reference\n                for idx in df_ref.index:\n                    val = df_ref.loc[idx, col]\n                    if not pd.isna(val):\n                        result.loc[idx, col] = f\"REMOVED: {val}\"\n\n        return result\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset_comparison.DatasetComparison.fetch","title":"fetch","text":"<pre><code>fetch(flag: FlagType, config: dict | DatasetConfigType = None, comparison_type: ComparisonTypeEnum = DELTA, replace_unchanged_values_by_nan: bool = False, fill_value: float | int | None = None, **kwargs) -&gt; Series | DataFrame\n</code></pre> <p>Fetch comparison data between variation and reference datasets.</p> <p>Extends the base Dataset.fetch() method with comparison-specific parameters for controlling how the comparison is computed and formatted.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>FlagType</code> <p>Data identifier flag to fetch from both datasets</p> required <code>config</code> <code>dict | DatasetConfigType</code> <p>Optional configuration overrides</p> <code>None</code> <code>comparison_type</code> <code>ComparisonTypeEnum</code> <p>How to compare the datasets: - DELTA: variation - reference (default) - VARIATION: variation data only, optionally with NaN for unchanged - BOTH: concatenated variation and reference data</p> <code>DELTA</code> <code>replace_unchanged_values_by_nan</code> <code>bool</code> <p>If True, replaces values that are identical between datasets with NaN (useful for highlighting changes)</p> <code>False</code> <code>fill_value</code> <code>float | int | None</code> <p>Value to use for missing data in subtraction operations</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to child dataset fetch methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>Series | DataFrame</code> <p>DataFrame or Series with comparison results</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Basic delta comparison\n&gt;&gt;&gt; deltas = comparison.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Highlight only changed values\n&gt;&gt;&gt; changes_only = comparison.fetch(\n...     'buses_t.marginal_price',\n...     replace_unchanged_values_by_nan=True\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Side-by-side comparison\n&gt;&gt;&gt; both = comparison.fetch(\n...     'buses_t.marginal_price',\n...     comparison_type=ComparisonTypeEnum.BOTH\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/datasets/dataset_comparison.py</code> <pre><code>def fetch(\n        self,\n        flag: FlagType,\n        config: dict | DatasetConfigType = None,\n        comparison_type: ComparisonTypeEnum = ComparisonTypeEnum.DELTA,\n        replace_unchanged_values_by_nan: bool = False,\n        fill_value: float | int | None = None,\n        **kwargs\n) -&gt; pd.Series | pd.DataFrame:\n    \"\"\"\n    Fetch comparison data between variation and reference datasets.\n\n    Extends the base Dataset.fetch() method with comparison-specific parameters\n    for controlling how the comparison is computed and formatted.\n\n    Args:\n        flag: Data identifier flag to fetch from both datasets\n        config: Optional configuration overrides\n        comparison_type: How to compare the datasets:\n            - DELTA: variation - reference (default)\n            - VARIATION: variation data only, optionally with NaN for unchanged\n            - BOTH: concatenated variation and reference data\n        replace_unchanged_values_by_nan: If True, replaces values that are\n            identical between datasets with NaN (useful for highlighting changes)\n        fill_value: Value to use for missing data in subtraction operations\n        **kwargs: Additional arguments passed to child dataset fetch methods\n\n    Returns:\n        DataFrame or Series with comparison results\n\n    Example:\n\n        &gt;&gt;&gt; # Basic delta comparison\n        &gt;&gt;&gt; deltas = comparison.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Highlight only changed values\n        &gt;&gt;&gt; changes_only = comparison.fetch(\n        ...     'buses_t.marginal_price',\n        ...     replace_unchanged_values_by_nan=True\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Side-by-side comparison\n        &gt;&gt;&gt; both = comparison.fetch(\n        ...     'buses_t.marginal_price',\n        ...     comparison_type=ComparisonTypeEnum.BOTH\n        ... )\n    \"\"\"\n    return super().fetch(\n        flag=flag,\n        config=config,\n        comparison_type=comparison_type,\n        replace_unchanged_values_by_nan=replace_unchanged_values_by_nan,\n        fill_value=fill_value,\n        **kwargs\n    )\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.platform_dataset","title":"platform_dataset","text":""},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.platform_dataset.PlatformDataset","title":"PlatformDataset","text":"<p>               Bases: <code>Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code>, <code>DatasetLinkCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType]</code>, <code>ABC</code></p> <p>Base class for platform-specific datasets with automatic interpreter management.</p> <p>PlatformDataset provides the foundation for integrating MESQUAL with specific energy modeling platforms (PyPSA, PLEXOS, etc.). It manages a registry of data interpreters and automatically instantiates them to handle different types of platform data.</p> Key Features <ul> <li>Automatic interpreter registration and instantiation</li> <li>Type-safe interpreter management through generics</li> <li>Flexible argument passing to interpreter constructors</li> <li>Support for study-specific interpreter extensions</li> <li>Unified data access through DatasetLinkCollection routing</li> </ul> Architecture <ul> <li>Uses DatasetLinkCollection for automatic flag routing</li> <li>Manages interpreter registry at class level</li> <li>Auto-instantiates all registered interpreters on construction</li> <li>Supports inheritance and interpreter registration on subclasses</li> </ul> <p>          Class Type Parameters:        </p> Name Bound or Constraints Description Default <code>DatasetType</code> <p>Base type for all interpreters (must be Dataset subclass)</p> required <code>DatasetConfigType</code> <p>Configuration class for dataset behavior</p> required <code>FlagType</code> <p>Type used for data flag identification</p> required <code>FlagIndexType</code> <p>Flag index implementation for flag mapping</p> required Class Attributes <p>_interpreter_registry: List of registered interpreter classes</p> Usage Pattern <ol> <li>Create platform dataset class inheriting from PlatformDataset</li> <li>Define get_child_dataset_type() to specify interpreter base class</li> <li>Create interpreter classes inheriting from the base interpreter</li> <li>Register interpreters using @PlatformDataset.register_interpreter</li> <li>Instantiate platform dataset - interpreters are auto-created</li> </ol> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Define platform dataset\n&gt;&gt;&gt; class PyPSADataset(PlatformDataset[PyPSAInterpreter, ...]):\n...     @classmethod\n...     def get_child_dataset_type(cls):\n...         return PyPSAInterpreter\n...\n&gt;&gt;&gt; # Register core interpreters\n&gt;&gt;&gt; @PyPSADataset.register_interpreter\n... class PyPSAModelInterpreter(PyPSAInterpreter):\n...     @property\n...     def accepted_flags(self):\n...         return {'buses', 'generators', 'lines'}\n...\n&gt;&gt;&gt; @PyPSADataset.register_interpreter  \n... class PyPSATimeSeriesInterpreter(PyPSAInterpreter):\n...     @property\n...     def accepted_flags(self):\n...         return {'buses_t.marginal_price', 'generators_t.p'}\n...\n&gt;&gt;&gt; # Register study-specific interpreter\n&gt;&gt;&gt; @PyPSADataset.register_interpreter\n... class CustomVariableInterpreter(PyPSAInterpreter):\n...     @property\n...     def accepted_flags(self):\n...         return {'custom_metric'}\n...\n&gt;&gt;&gt; # Use platform dataset\n&gt;&gt;&gt; dataset = PyPSADataset(network=my_network)\n&gt;&gt;&gt; buses = dataset.fetch('buses')  # Routes to ModelInterpreter\n&gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price')  # Routes to TimeSeriesInterpreter\n&gt;&gt;&gt; custom = dataset.fetch('custom_metric')  # Routes to CustomVariableInterpreter\n</code></pre> Notes <ul> <li>Interpreters are registered at class level and shared across instances</li> <li>Registration order affects routing (last registered = first checked)</li> <li>All registered interpreters are instantiated for each platform dataset</li> <li>Constructor arguments are automatically extracted and passed to interpreters</li> </ul> Source code in <code>submodules/mesqual/mesqual/datasets/platform_dataset.py</code> <pre><code>class PlatformDataset(\n    Generic[DatasetType, DatasetConfigType, FlagType, FlagIndexType],\n    DatasetLinkCollection[DatasetType, DatasetConfigType, FlagType, FlagIndexType],\n    ABC\n):\n    \"\"\"\n    Base class for platform-specific datasets with automatic interpreter management.\n\n    PlatformDataset provides the foundation for integrating MESQUAL with specific\n    energy modeling platforms (PyPSA, PLEXOS, etc.). It manages a registry of\n    data interpreters and automatically instantiates them to handle different\n    types of platform data.\n\n    Key Features:\n        - Automatic interpreter registration and instantiation\n        - Type-safe interpreter management through generics\n        - Flexible argument passing to interpreter constructors\n        - Support for study-specific interpreter extensions\n        - Unified data access through DatasetLinkCollection routing\n\n    Architecture:\n        - Uses DatasetLinkCollection for automatic flag routing\n        - Manages interpreter registry at class level\n        - Auto-instantiates all registered interpreters on construction\n        - Supports inheritance and interpreter registration on subclasses\n\n    Type Parameters:\n        DatasetType: Base type for all interpreters (must be Dataset subclass)\n        DatasetConfigType: Configuration class for dataset behavior\n        FlagType: Type used for data flag identification\n        FlagIndexType: Flag index implementation for flag mapping\n\n    Class Attributes:\n        _interpreter_registry: List of registered interpreter classes\n\n    Usage Pattern:\n        1. Create platform dataset class inheriting from PlatformDataset\n        2. Define get_child_dataset_type() to specify interpreter base class\n        3. Create interpreter classes inheriting from the base interpreter\n        4. Register interpreters using @PlatformDataset.register_interpreter\n        5. Instantiate platform dataset - interpreters are auto-created\n\n    Example:\n\n        &gt;&gt;&gt; # Define platform dataset\n        &gt;&gt;&gt; class PyPSADataset(PlatformDataset[PyPSAInterpreter, ...]):\n        ...     @classmethod\n        ...     def get_child_dataset_type(cls):\n        ...         return PyPSAInterpreter\n        ...\n        &gt;&gt;&gt; # Register core interpreters\n        &gt;&gt;&gt; @PyPSADataset.register_interpreter\n        ... class PyPSAModelInterpreter(PyPSAInterpreter):\n        ...     @property\n        ...     def accepted_flags(self):\n        ...         return {'buses', 'generators', 'lines'}\n        ...\n        &gt;&gt;&gt; @PyPSADataset.register_interpreter  \n        ... class PyPSATimeSeriesInterpreter(PyPSAInterpreter):\n        ...     @property\n        ...     def accepted_flags(self):\n        ...         return {'buses_t.marginal_price', 'generators_t.p'}\n        ...\n        &gt;&gt;&gt; # Register study-specific interpreter\n        &gt;&gt;&gt; @PyPSADataset.register_interpreter\n        ... class CustomVariableInterpreter(PyPSAInterpreter):\n        ...     @property\n        ...     def accepted_flags(self):\n        ...         return {'custom_metric'}\n        ...\n        &gt;&gt;&gt; # Use platform dataset\n        &gt;&gt;&gt; dataset = PyPSADataset(network=my_network)\n        &gt;&gt;&gt; buses = dataset.fetch('buses')  # Routes to ModelInterpreter\n        &gt;&gt;&gt; prices = dataset.fetch('buses_t.marginal_price')  # Routes to TimeSeriesInterpreter\n        &gt;&gt;&gt; custom = dataset.fetch('custom_metric')  # Routes to CustomVariableInterpreter\n\n    Notes:\n        - Interpreters are registered at class level and shared across instances\n        - Registration order affects routing (last registered = first checked)\n        - All registered interpreters are instantiated for each platform dataset\n        - Constructor arguments are automatically extracted and passed to interpreters\n    \"\"\"\n\n    _interpreter_registry: list[Type[DatasetType]] = []\n\n    def __init__(\n            self,\n            name: str = None,\n            flag_index: FlagIndexType = None,\n            attributes: dict = None,\n            database: Database = None,\n            config: DatasetConfigType = None,\n            **kwargs\n    ):\n        super().__init__(\n            datasets=[],\n            name=name,\n            flag_index=flag_index,\n            attributes=attributes,\n            database=database,\n            config=config,\n        )\n        interpreter_args = self._prepare_interpreter_initialization_args(kwargs)\n        datasets = self._initialize_registered_interpreters(interpreter_args)\n        self.add_datasets(datasets)\n\n    @classmethod\n    def register_interpreter(cls, interpreter: Type[DatasetType]) -&gt; Type['DatasetType']:\n        \"\"\"\n        Register a data interpreter class with this platform dataset.\n\n        This method is typically used as a decorator to register interpreter classes\n        that handle specific types of platform data. Registered interpreters are\n        automatically instantiated when the platform dataset is created.\n\n        Args:\n            interpreter: Interpreter class that must inherit from get_child_dataset_type()\n\n        Returns:\n            The interpreter class (unchanged) to support decorator usage\n\n        Raises:\n            TypeError: If interpreter doesn't inherit from the required base class\n\n        Example:\n\n            &gt;&gt;&gt; @PyPSADataset.register_interpreter\n            ... class CustomInterpreter(PyPSAInterpreter):\n            ...     @property\n            ...     def accepted_flags(self):\n            ...         return {'custom_flag'}\n            ...     \n            ...     def _fetch(self, flag, config, **kwargs):\n            ...         return compute_custom_data()\n        \"\"\"\n        cls._validate_interpreter_type(interpreter)\n        if interpreter not in cls._interpreter_registry:\n            cls._add_interpreter_to_registry(interpreter)\n        return interpreter\n\n    @classmethod\n    def get_registered_interpreters(cls) -&gt; list[Type[DatasetType]]:\n        return cls._interpreter_registry.copy()\n\n    def get_interpreter_instance(self, interpreter_type: Type[DatasetType]) -&gt; DatasetType:\n        interpreter = self._find_interpreter_instance(interpreter_type)\n        if interpreter is None:\n            raise ValueError(\n                f'No interpreter instance found for type {interpreter_type.__name__}'\n            )\n        return interpreter\n\n    def get_flags_by_interpreter(self) -&gt; dict[Type[DatasetType], set[FlagType]]:\n        return {\n            type(interpreter): interpreter.accepted_flags\n            for interpreter in self.datasets.values()\n        }\n\n    def _prepare_interpreter_initialization_args(self, kwargs: dict) -&gt; dict:\n        interpreter_signature = InterpreterSignature.from_interpreter(self.get_child_dataset_type())\n        return {\n            arg: kwargs.get(arg, default)\n            for arg, default in zip(interpreter_signature.args, interpreter_signature.defaults)\n        }\n\n    def _initialize_registered_interpreters(self, interpreter_args: dict) -&gt; list[DatasetType]:\n        return [\n            interpreter(**interpreter_args, parent_dataset=self)\n            for interpreter in self._interpreter_registry\n        ]\n\n    @classmethod\n    def _validate_interpreter_type(cls, interpreter: Type[DatasetType]) -&gt; None:\n        if not issubclass(interpreter, cls.get_child_dataset_type()):\n            raise TypeError(\n                f'Interpreter must be subclass of {cls.get_child_dataset_type().__name__}'\n            )\n\n    @classmethod\n    def _validate_interpreter_not_registered(cls, interpreter: Type[DatasetType]) -&gt; None:\n        if interpreter in cls._interpreter_registry:\n            raise ValueError(f'Interpreter {interpreter.__name__} already registered')\n\n    @classmethod\n    def _add_interpreter_to_registry(cls, interpreter: Type[DatasetType]) -&gt; None:\n        cls._interpreter_registry.insert(0, interpreter)\n\n    def _find_interpreter_instance(self, interpreter_type: Type[DatasetType]) -&gt; DatasetType | None:\n        for interpreter in self.datasets.values():\n            if isinstance(interpreter, interpreter_type):\n                return interpreter\n        return None\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.platform_dataset.PlatformDataset.register_interpreter","title":"register_interpreter  <code>classmethod</code>","text":"<pre><code>register_interpreter(interpreter: Type[DatasetType]) -&gt; Type['DatasetType']\n</code></pre> <p>Register a data interpreter class with this platform dataset.</p> <p>This method is typically used as a decorator to register interpreter classes that handle specific types of platform data. Registered interpreters are automatically instantiated when the platform dataset is created.</p> <p>Parameters:</p> Name Type Description Default <code>interpreter</code> <code>Type[DatasetType]</code> <p>Interpreter class that must inherit from get_child_dataset_type()</p> required <p>Returns:</p> Type Description <code>Type['DatasetType']</code> <p>The interpreter class (unchanged) to support decorator usage</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If interpreter doesn't inherit from the required base class</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; @PyPSADataset.register_interpreter\n... class CustomInterpreter(PyPSAInterpreter):\n...     @property\n...     def accepted_flags(self):\n...         return {'custom_flag'}\n...     \n...     def _fetch(self, flag, config, **kwargs):\n...         return compute_custom_data()\n</code></pre> Source code in <code>submodules/mesqual/mesqual/datasets/platform_dataset.py</code> <pre><code>@classmethod\ndef register_interpreter(cls, interpreter: Type[DatasetType]) -&gt; Type['DatasetType']:\n    \"\"\"\n    Register a data interpreter class with this platform dataset.\n\n    This method is typically used as a decorator to register interpreter classes\n    that handle specific types of platform data. Registered interpreters are\n    automatically instantiated when the platform dataset is created.\n\n    Args:\n        interpreter: Interpreter class that must inherit from get_child_dataset_type()\n\n    Returns:\n        The interpreter class (unchanged) to support decorator usage\n\n    Raises:\n        TypeError: If interpreter doesn't inherit from the required base class\n\n    Example:\n\n        &gt;&gt;&gt; @PyPSADataset.register_interpreter\n        ... class CustomInterpreter(PyPSAInterpreter):\n        ...     @property\n        ...     def accepted_flags(self):\n        ...         return {'custom_flag'}\n        ...     \n        ...     def _fetch(self, flag, config, **kwargs):\n        ...         return compute_custom_data()\n    \"\"\"\n    cls._validate_interpreter_type(interpreter)\n    if interpreter not in cls._interpreter_registry:\n        cls._add_interpreter_to_registry(interpreter)\n    return interpreter\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/datasets/#mesqual.datasets.dataset_config","title":"dataset_config","text":""},{"location":"mesqual-package-documentation/api_reference/study_manager/","title":"StudyManager References","text":""},{"location":"mesqual-package-documentation/api_reference/study_manager/#mesqual.study_manager.StudyManager","title":"StudyManager","text":"<p>Central orchestrator for multi-scenario energy systems analysis using MESQUAL's three-tier collection system.</p> <p>StudyManager provides unified access to scenarios, scenario comparisons, and combined datasets, implementing MESQUAL's core architectural principle that \"Everything is a Dataset\". It manages the complete lifecycle of multi-scenario studies including data organization, automatic delta computation, and export functionality.</p> The class implements three primary data access patterns <ul> <li><code>.scen</code>: Access to individual scenario data (DatasetConcatCollection)</li> <li><code>.comp</code>: Access to scenario comparison deltas (DatasetConcatCollectionOfComparisons)</li> <li><code>.scen_comp</code>: Unified access to both scenarios and comparisons with type distinction</li> </ul> <p>Attributes:</p> Name Type Description <code>scen</code> <code>DatasetConcatCollection</code> <p>Collection of scenario datasets with consistent .fetch() interface</p> <code>comp</code> <code>DatasetConcatCollectionOfComparisons</code> <p>Collection of scenario comparisons with automatic delta calculation</p> <code>scen_comp</code> <code>DatasetConcatCollection</code> <p>Unified collection combining scenarios and comparisons</p> <p>Examples:</p> <p>Basic multi-scenario study setup:</p> <pre><code>&gt;&gt;&gt; from mesqual import StudyManager\n&gt;&gt;&gt; from mesqual_pypsa import PyPSADataset\n&gt;&gt;&gt; \n&gt;&gt;&gt; study = StudyManager.factory_from_scenarios(\n...     scenarios=[\n...         PyPSADataset(base_network, name='base'),\n...         PyPSADataset(high_res_network, name='high_res'),\n...         PyPSADataset(low_gas_network, name='cheap_gas'),\n...     ],\n...     comparisons=[('high_res', 'base'), ('cheap_gas', 'base')],  # StudyManager handles automatic naming with '*variation_dataset_name* vs *reference_dataset_name*'\n... )\n</code></pre> <p>Three-tier data access:</p> <pre><code>&gt;&gt;&gt; # Access scenario data across all scenarios\n&gt;&gt;&gt; scenario_prices = study.scen.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; print(scenario_prices)  # Print price time-series df for all scenarios concatenated with MultiIndex\n    dataset              base        ... n_cheap_gas\n    Bus                     1    10  ...          99 99_220kV\n    snapshot                         ...\n    2011-01-01 00:00:00 -0.44  5.77  ...       23.83    23.79\n    2011-01-01 01:00:00 -0.58  6.10  ...       22.38    22.33\n    ...                   ...   ...  ...         ...      ...\n    2011-01-01 22:00:00 20.98 20.44  ...       22.72    22.67\n    2011-01-01 23:00:00 13.43 18.94  ...       22.59    22.54\n</code></pre> <pre><code>&gt;&gt;&gt; # Access comparison deltas automatically calculated across all comparisons\n&gt;&gt;&gt; price_changes = study.comp.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; print(price_changes)  # Print price time-series df for all comparisons concatenated with MultiIndex\n    dataset             high_res vs base        ... n_cheap_gas vs base\n    Bus                                1    10  ...                  99 99_220kV\n    snapshot                                    ...\n    2011-01-01 00:00:00             0.21  0.02  ...                0.10     0.11\n    2011-01-01 01:00:00             0.02  0.06  ...               -0.81    -0.82\n    ...                              ...   ...  ...                 ...      ...\n    2011-01-01 22:00:00             0.04 -0.05  ...               -2.10    -2.14\n    2011-01-01 23:00:00             2.33  0.46  ...               -1.74    -1.78\n</code></pre> <pre><code>&gt;&gt;&gt; # Access unified view with type-level distinction\n&gt;&gt;&gt; unified_scen_and_comp_price_data = study.scen_comp.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; print(unified_scen_and_comp_price_data)  # Print price time-series df for all scenarios and comparisons concatenated with additional level on MultiIndex:\n    type                scenario        ...          comparison\n    dataset                 base        ... n_cheap_gas vs base\n    Bus                        1    10  ...                  99 99_220kV\n    snapshot                            ...\n    2011-01-01 00:00:00    -0.44  5.77  ...                0.10     0.11\n    2011-01-01 01:00:00    -0.58  6.10  ...               -0.81    -0.82\n    ..                       ...   ...  ...                 ...      ...\n    2011-01-01 22:00:00    20.98 20.44  ...               -2.10    -2.14\n    2011-01-01 23:00:00    13.43 18.94  ...               -1.74    -1.78\n</code></pre> <p>Access individual datasets:</p> <pre><code>&gt;&gt;&gt; # Access to data from individual scenario\n&gt;&gt;&gt; ds_base = study.scen.get_dataset('base')\n&gt;&gt;&gt; base_prices = ds_base.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; print(base_prices)  # Print price time-series df for base scenario\n    Bus                     1    10  ...    99  99_220kV\n    snapshot                         ...\n    2011-01-01 00:00:00 -0.44  5.77  ... 23.72     23.69\n    2011-01-01 01:00:00 -0.58  6.10  ... 23.19     23.14\n    ...                   ...   ...  ...   ...       ...\n    2011-01-01 22:00:00 20.98 20.44  ... 24.81     24.81\n    2011-01-01 23:00:00 13.43 18.94  ... 24.33     24.32\n</code></pre> <pre><code>&gt;&gt;&gt; base_buses_model_df = ds_base.fetch('buses')\n&gt;&gt;&gt; print(base_buses_model_df)  # Print bus model df for base scenario\n               v_nom type  ...  v_mag_pu_max  control\n    Bus                    ...\n    1         220.00       ...           inf    Slack\n    2         380.00       ...           inf       PQ\n    ...          ...  ...  ...           ...      ...\n    450_220kV 220.00       ...           inf       PQ\n    458_220kV 220.00       ...           inf       PQ\n</code></pre> <pre><code>&gt;&gt;&gt; # Access to individual comparison dataset\n&gt;&gt;&gt; ds_comp_high_res = study.comp.get_dataset('high_res vs base')\n&gt;&gt;&gt; price_changes_high_res = ds_comp_high_res.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; print(price_changes_high_res)  # Print price time-series df changes (deltas) comparing high_res to base\n    Bus                    1    10  ...    99  99_220kV\n    snapshot                        ...\n    2011-01-01 00:00:00 0.21  0.02  ...  0.01      0.01\n    2011-01-01 01:00:00 0.02  0.06  ... -0.07     -0.07\n    ...                  ...   ...  ...   ...       ...\n    2011-01-01 22:00:00 0.04 -0.05  ... -0.00     -0.00\n    2011-01-01 23:00:00 2.33  0.46  ... -0.09     -0.09\n</code></pre> <pre><code>&gt;&gt;&gt; bus_model_changes_high_res = ds_comp_high_res.fetch('buses')\n&gt;&gt;&gt; print(bus_model_changes_high_res)  # Print bus model df changes comparing high_res to base\n               v_nom type  ...  v_mag_pu_max  control\n    Bus                    ...\n    1           0.00       ...           NaN    Slack\n    2           0.00       ...           NaN       PQ\n    ...          ...  ...  ...           ...      ...\n    450_220kV   0.00       ...           NaN       PQ\n    458_220kV   0.00       ...           NaN       PQ\n</code></pre> <p>Access to merged (model) dfs for all sub datasets:</p> <pre><code>&gt;&gt;&gt; # Access merged dataframe (useful to get unified model_df in case of different objects across scenarios)\n&gt;&gt;&gt; bus_model_df_merged = study.scen.fetch_merged('buses')\n&gt;&gt;&gt; print(bus_model_df_merged)  # Print merged bus model df for all scenarios\n               v_nom type  ...  v_mag_pu_max  control\n    Bus                    ...\n    1         220.00       ...           inf    Slack\n    2         380.00       ...           inf       PQ\n    ...          ...  ...  ...           ...      ...\n    450_220kV 220.00       ...           inf       PQ\n    458_220kV 220.00       ...           inf       PQ\n</code></pre> Notes <ul> <li>All collections share the same .fetch(flag) interface for consistent data access</li> <li>Scenario comparisons are computed automatically as deltas (variation - reference)</li> <li>The unified collection uses 'type' as the concat_level_name to distinguish data sources</li> <li>Supports dynamic addition of scenarios and comparisons after initialization</li> </ul> Source code in <code>submodules/mesqual/mesqual/study_manager.py</code> <pre><code>class StudyManager:\n    \"\"\"\n    Central orchestrator for multi-scenario energy systems analysis using MESQUAL's three-tier collection system.\n\n    StudyManager provides unified access to scenarios, scenario comparisons, and combined datasets,\n    implementing MESQUAL's core architectural principle that \"Everything is a Dataset\". It manages\n    the complete lifecycle of multi-scenario studies including data organization, automatic delta\n    computation, and export functionality.\n\n    The class implements three primary data access patterns:\n        - `.scen`: Access to individual scenario data (DatasetConcatCollection)\n        - `.comp`: Access to scenario comparison deltas (DatasetConcatCollectionOfComparisons)\n        - `.scen_comp`: Unified access to both scenarios and comparisons with type distinction\n\n    Attributes:\n        scen (DatasetConcatCollection): Collection of scenario datasets with consistent .fetch() interface\n        comp (DatasetConcatCollectionOfComparisons): Collection of scenario comparisons with automatic delta calculation\n        scen_comp (DatasetConcatCollection): Unified collection combining scenarios and comparisons\n\n    Examples:\n        Basic multi-scenario study setup:\n\n        &gt;&gt;&gt; from mesqual import StudyManager\n        &gt;&gt;&gt; from mesqual_pypsa import PyPSADataset\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; study = StudyManager.factory_from_scenarios(\n        ...     scenarios=[\n        ...         PyPSADataset(base_network, name='base'),\n        ...         PyPSADataset(high_res_network, name='high_res'),\n        ...         PyPSADataset(low_gas_network, name='cheap_gas'),\n        ...     ],\n        ...     comparisons=[('high_res', 'base'), ('cheap_gas', 'base')],  # StudyManager handles automatic naming with '*variation_dataset_name* vs *reference_dataset_name*'\n        ... )\n\n\n        Three-tier data access:\n\n        &gt;&gt;&gt; # Access scenario data across all scenarios\n        &gt;&gt;&gt; scenario_prices = study.scen.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; print(scenario_prices)  # Print price time-series df for all scenarios concatenated with MultiIndex\n            dataset              base        ... n_cheap_gas\n            Bus                     1    10  ...          99 99_220kV\n            snapshot                         ...\n            2011-01-01 00:00:00 -0.44  5.77  ...       23.83    23.79\n            2011-01-01 01:00:00 -0.58  6.10  ...       22.38    22.33\n            ...                   ...   ...  ...         ...      ...\n            2011-01-01 22:00:00 20.98 20.44  ...       22.72    22.67\n            2011-01-01 23:00:00 13.43 18.94  ...       22.59    22.54\n\n        &gt;&gt;&gt; # Access comparison deltas automatically calculated across all comparisons\n        &gt;&gt;&gt; price_changes = study.comp.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; print(price_changes)  # Print price time-series df for all comparisons concatenated with MultiIndex\n            dataset             high_res vs base        ... n_cheap_gas vs base\n            Bus                                1    10  ...                  99 99_220kV\n            snapshot                                    ...\n            2011-01-01 00:00:00             0.21  0.02  ...                0.10     0.11\n            2011-01-01 01:00:00             0.02  0.06  ...               -0.81    -0.82\n            ...                              ...   ...  ...                 ...      ...\n            2011-01-01 22:00:00             0.04 -0.05  ...               -2.10    -2.14\n            2011-01-01 23:00:00             2.33  0.46  ...               -1.74    -1.78\n\n        &gt;&gt;&gt; # Access unified view with type-level distinction\n        &gt;&gt;&gt; unified_scen_and_comp_price_data = study.scen_comp.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; print(unified_scen_and_comp_price_data)  # Print price time-series df for all scenarios and comparisons concatenated with additional level on MultiIndex:\n            type                scenario        ...          comparison\n            dataset                 base        ... n_cheap_gas vs base\n            Bus                        1    10  ...                  99 99_220kV\n            snapshot                            ...\n            2011-01-01 00:00:00    -0.44  5.77  ...                0.10     0.11\n            2011-01-01 01:00:00    -0.58  6.10  ...               -0.81    -0.82\n            ..                       ...   ...  ...                 ...      ...\n            2011-01-01 22:00:00    20.98 20.44  ...               -2.10    -2.14\n            2011-01-01 23:00:00    13.43 18.94  ...               -1.74    -1.78\n\n\n        Access individual datasets:\n\n        &gt;&gt;&gt; # Access to data from individual scenario\n        &gt;&gt;&gt; ds_base = study.scen.get_dataset('base')\n        &gt;&gt;&gt; base_prices = ds_base.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; print(base_prices)  # Print price time-series df for base scenario\n            Bus                     1    10  ...    99  99_220kV\n            snapshot                         ...\n            2011-01-01 00:00:00 -0.44  5.77  ... 23.72     23.69\n            2011-01-01 01:00:00 -0.58  6.10  ... 23.19     23.14\n            ...                   ...   ...  ...   ...       ...\n            2011-01-01 22:00:00 20.98 20.44  ... 24.81     24.81\n            2011-01-01 23:00:00 13.43 18.94  ... 24.33     24.32\n\n        &gt;&gt;&gt; base_buses_model_df = ds_base.fetch('buses')\n        &gt;&gt;&gt; print(base_buses_model_df)  # Print bus model df for base scenario\n                       v_nom type  ...  v_mag_pu_max  control\n            Bus                    ...\n            1         220.00       ...           inf    Slack\n            2         380.00       ...           inf       PQ\n            ...          ...  ...  ...           ...      ...\n            450_220kV 220.00       ...           inf       PQ\n            458_220kV 220.00       ...           inf       PQ\n\n        &gt;&gt;&gt; # Access to individual comparison dataset\n        &gt;&gt;&gt; ds_comp_high_res = study.comp.get_dataset('high_res vs base')\n        &gt;&gt;&gt; price_changes_high_res = ds_comp_high_res.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; print(price_changes_high_res)  # Print price time-series df changes (deltas) comparing high_res to base\n            Bus                    1    10  ...    99  99_220kV\n            snapshot                        ...\n            2011-01-01 00:00:00 0.21  0.02  ...  0.01      0.01\n            2011-01-01 01:00:00 0.02  0.06  ... -0.07     -0.07\n            ...                  ...   ...  ...   ...       ...\n            2011-01-01 22:00:00 0.04 -0.05  ... -0.00     -0.00\n            2011-01-01 23:00:00 2.33  0.46  ... -0.09     -0.09\n\n        &gt;&gt;&gt; bus_model_changes_high_res = ds_comp_high_res.fetch('buses')\n        &gt;&gt;&gt; print(bus_model_changes_high_res)  # Print bus model df changes comparing high_res to base\n                       v_nom type  ...  v_mag_pu_max  control\n            Bus                    ...\n            1           0.00       ...           NaN    Slack\n            2           0.00       ...           NaN       PQ\n            ...          ...  ...  ...           ...      ...\n            450_220kV   0.00       ...           NaN       PQ\n            458_220kV   0.00       ...           NaN       PQ\n\n\n        Access to merged (model) dfs for all sub datasets:\n\n        &gt;&gt;&gt; # Access merged dataframe (useful to get unified model_df in case of different objects across scenarios)\n        &gt;&gt;&gt; bus_model_df_merged = study.scen.fetch_merged('buses')\n        &gt;&gt;&gt; print(bus_model_df_merged)  # Print merged bus model df for all scenarios\n                       v_nom type  ...  v_mag_pu_max  control\n            Bus                    ...\n            1         220.00       ...           inf    Slack\n            2         380.00       ...           inf       PQ\n            ...          ...  ...  ...           ...      ...\n            450_220kV 220.00       ...           inf       PQ\n            458_220kV 220.00       ...           inf       PQ\n\n    Notes:\n        - All collections share the same .fetch(flag) interface for consistent data access\n        - Scenario comparisons are computed automatically as deltas (variation - reference)\n        - The unified collection uses 'type' as the concat_level_name to distinguish data sources\n        - Supports dynamic addition of scenarios and comparisons after initialization\n    \"\"\"\n    def __init__(\n            self,\n            scenarios: DatasetConcatCollection,\n            comparisons: DatasetConcatCollectionOfComparisons,\n            export_folder: str = None,\n    ):\n        self._scenarios = scenarios\n        self._comparisons = comparisons\n        self._scenarios_and_comparisons: DatasetConcatCollection = DatasetConcatCollection(\n            [\n                scenarios,\n                comparisons\n            ],\n            name='scenarios_and_comparisons',\n            concat_level_name='type',\n        )\n        self._export_folder: str = export_folder\n        if export_folder is not None:\n            self._ensure_folder_exists(export_folder)\n\n    @property\n    def scen(self) -&gt; DatasetConcatCollection:\n        return self._scenarios\n\n    def add_scenario(self, dataset: Dataset):\n        self._scenarios.add_dataset(dataset)\n\n    @property\n    def comp(self) -&gt; DatasetConcatCollectionOfComparisons:\n        return self._comparisons\n\n    def add_comparison(self, dataset: DatasetComparison):\n        self._comparisons.add_dataset(dataset)\n\n    @property\n    def scen_comp(self) -&gt; DatasetConcatCollection:\n        return self._scenarios_and_comparisons\n\n    @property\n    def export_folder(self) -&gt; str:\n        return self._export_folder\n\n    @export_folder.setter\n    def export_folder(self, folder_path: str):\n        self._ensure_folder_exists(folder_path)\n        self._export_folder = folder_path\n\n    @staticmethod\n    def _ensure_folder_exists(folder: str):\n        os.makedirs(folder, exist_ok=True)\n\n    def export_path(self, file_name: str) -&gt; str:\n        if self._export_folder is None:\n            raise RuntimeError(f'Export folder must be assigned first.')\n        return os.path.join(self._export_folder, file_name)\n\n    @classmethod\n    def factory_from_scenarios(\n            cls,\n            scenarios: list[Dataset],\n            comparisons: list[tuple[str, str]],\n            export_folder: str = None\n    ) -&gt; 'StudyManager':\n        scen = DatasetConcatCollection(scenarios, name='scenario', concat_level_name='dataset',)\n        comp = DatasetConcatCollectionOfComparisons(\n            datasets=[\n                DatasetComparison(scen.get_dataset(var), scen.get_dataset(ref))\n                for var, ref in comparisons\n            ],\n            name='comparison',\n            concat_level_name='dataset',\n        )\n        return cls(scen, comp, export_folder)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/","title":"Energy Data Handling Intro","text":"<p>Energy Data Handling Module.</p> <p>This comprehensive module provides utilities for handling energy-specific time series data operations, network flow analysis and area-level aggregations within the MESQUAL framework.</p> <p>The module is designed to work with pandas time series data and energy system networks, particularly for energy market analysis where different data streams may have varying temporal granularities and require specific handling based on whether they represent intensive (prices, power levels) or extensive (volumes, energy amounts) quantities.</p>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/#mesqual.energy_data_handling--core-components","title":"Core Components","text":"Time Series Processing <ul> <li>Granularity analysis and conversion between different temporal resolutions</li> <li>Gap detection and handling in time series data</li> <li>Support for intensive vs. extensive quantity conversions</li> </ul> Network Flow Analysis <ul> <li>Bidirectional transmission line flow data structures</li> <li>Network capacity modeling with loss considerations</li> <li>Flow direction conventions for complex network topologies</li> </ul> Area-Level Aggregations <ul> <li>Node-to-area aggregation modules with geographic modeling</li> <li>Cross-border flow analysis and capacity calculation modules</li> <li>Price aggregation modules using volume-weighted methods</li> </ul> Variable Utilities <ul> <li>Regional trade balance calculations</li> <li>Volume-weighted price aggregations</li> <li>Congestion rent analysis</li> <li>Directional data processing (up/down, net flows)</li> </ul> Model Handling <ul> <li>DataFrame enrichment with model properties</li> <li>Membership-based property propagation</li> <li>Combination identifier creation for paired relationships</li> </ul>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/granularity_modules/","title":"MESQUAL Granularity Modules","text":""},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/granularity_modules/#mesqual.energy_data_handling.granularity_analyzer.TimeSeriesGranularityAnalyzer","title":"TimeSeriesGranularityAnalyzer","text":"<p>Analyzes and validates time granularity in DatetimeIndex sequences.</p> <p>This class provides tools for working with time series data that may have varying granularities (e.g., hourly, quarter-hourly). It's particularly useful for electricity market data analysis where different market products can have different time resolutions.</p> Features <ul> <li>Granularity detection for time series data</li> <li>Support for mixed granularities within the same series</li> <li>Strict mode for validation scenarios</li> <li>Per-day granularity analysis</li> </ul> <p>Parameters:</p> Name Type Description Default <code>strict_mode</code> <code>bool</code> <p>If True, raises GranularityError when multiple granularities are detected. If False, only issues warnings. Default is True.</p> <code>True</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; analyzer = TimeSeriesGranularityAnalyzer()\n&gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=24, freq='h')\n&gt;&gt;&gt; analyzer.get_granularity_as_timedelta(index)\n    Timedelta('1 hours')\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/granularity_analyzer.py</code> <pre><code>class TimeSeriesGranularityAnalyzer:\n    \"\"\"Analyzes and validates time granularity in DatetimeIndex sequences.\n\n    This class provides tools for working with time series data that may have varying\n    granularities (e.g., hourly, quarter-hourly). It's particularly useful for\n    electricity market data analysis where different market products can have\n    different time resolutions.\n\n    Features:\n        - Granularity detection for time series data\n        - Support for mixed granularities within the same series\n        - Strict mode for validation scenarios\n        - Per-day granularity analysis\n\n    Args:\n        strict_mode: If True, raises GranularityError when multiple granularities\n            are detected. If False, only issues warnings. Default is True.\n\n    Example:\n\n        &gt;&gt;&gt; analyzer = TimeSeriesGranularityAnalyzer()\n        &gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=24, freq='h')\n        &gt;&gt;&gt; analyzer.get_granularity_as_timedelta(index)\n            Timedelta('1 hours')\n    \"\"\"\n\n    def __init__(self, strict_mode: bool = True):\n        self._strict_mode = strict_mode\n\n    @property\n    def strict_mode(self) -&gt; bool:\n        return self._strict_mode\n\n    @strict_mode.setter\n    def strict_mode(self, value: bool):\n        self._strict_mode = value\n\n    @_validate_dt_index\n    def get_granularity_as_series_of_timedeltas(self, dt_index: pd.DatetimeIndex) -&gt; pd.Series:\n        s = pd.Series(index=dt_index)\n        s = s.groupby(dt_index.date).apply(lambda g: self._get_granularity_for_day(g.index))\n        s = s.droplevel(0, axis=0)\n        if dt_index.name is not None:\n            s.index.name = dt_index.name\n        return s\n\n    @_validate_dt_index\n    def get_granularity_as_series_of_minutes(self, dt_index: pd.DatetimeIndex) -&gt; pd.Series:\n        return self.get_granularity_as_series_of_timedeltas(dt_index).apply(lambda x: x.total_seconds() / 60)\n\n    @_validate_dt_index\n    def get_granularity_as_series_of_hours(self, dt_index: pd.DatetimeIndex) -&gt; pd.Series:\n        return self.get_granularity_as_series_of_timedeltas(dt_index).apply(lambda x: x.total_seconds() / 3600)\n\n    def _get_granularity_for_day(self, dt_index: pd.DatetimeIndex) -&gt; pd.Series:\n        gran = dt_index.to_series().diff().shift(-1)\n        if len(gran) &gt; 1:\n            gran.iloc[-1] = gran.iloc[-2]\n\n        if gran.nunique() &gt; 1:\n            msg = f'Multiple granularities identified within a day: {gran.unique()}'\n            if self._strict_mode:\n                raise GranularityError(msg)\n            warnings.warn(msg)\n        return gran\n\n    @_validate_dt_index\n    def get_granularity_as_timedelta(self, dt_index: pd.DatetimeIndex) -&gt; pd.Timedelta:\n        if len(dt_index) == 0:\n            return pd.Timedelta(0)\n\n        gran = self.get_granularity_as_series_of_timedeltas(dt_index)\n        first_gran = gran.iloc[0]\n\n        if gran.nunique() &gt; 1:\n            msg = (f'Multiple granularities found: {gran.unique()}. '\n                   f'Using {first_gran} as the reference granularity.')\n            if self._strict_mode:\n                raise GranularityError(msg)\n            warnings.warn(msg)\n        return first_gran\n\n    @_validate_dt_index\n    def get_granularity_as_hours(self, dt_index: pd.DatetimeIndex) -&gt; float:\n        return self.get_granularity_as_timedelta(dt_index).total_seconds() / 3600\n\n    @_validate_dt_index\n    def get_granularity_as_minutes(self, dt_index: pd.DatetimeIndex) -&gt; float:\n        return self.get_granularity_as_timedelta(dt_index).total_seconds() / 60\n\n    @_validate_dt_index\n    def validate_constant_granularity(self, dt_index: pd.DatetimeIndex, expected_hours: float) -&gt; bool:\n        actual_hours = self.get_granularity_as_hours(dt_index)\n        return abs(actual_hours - expected_hours) &lt; 1e-10\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/granularity_modules/#mesqual.energy_data_handling.granularity_converter.TimeSeriesGranularityConverter","title":"TimeSeriesGranularityConverter","text":"<p>Converts time series between different granularities while respecting the nature of the quantity.</p> <p>This class handles the conversion of time series data between different granularities (e.g., hourly to 15-min or vice versa) while properly accounting for the physical nature of the quantity being converted:</p> <ul> <li> <p>Intensive quantities (e.g., prices, power levels) are replicated when increasing   granularity and averaged when decreasing granularity.</p> </li> <li> <p>Extensive quantities (e.g., volumes, welfare) are split when increasing   granularity and summed when decreasing granularity.</p> </li> </ul> Features <ul> <li>Automatic granularity detection using TimeGranularityAnalyzer</li> <li>Per-day processing to handle missing periods properly and prevent incorrect autofilling of missing days</li> <li>Support for both intensive and extensive quantities</li> <li>Timezone-aware processing including daylight saving transitions</li> </ul> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/granularity_converter.py</code> <pre><code>class TimeSeriesGranularityConverter:\n    \"\"\"Converts time series between different granularities while respecting the nature of the quantity.\n\n    This class handles the conversion of time series data between different granularities\n    (e.g., hourly to 15-min or vice versa) while properly accounting for the physical\n    nature of the quantity being converted:\n\n    - Intensive quantities (e.g., prices, power levels) are replicated when increasing\n      granularity and averaged when decreasing granularity.\n\n    - Extensive quantities (e.g., volumes, welfare) are split when increasing\n      granularity and summed when decreasing granularity.\n\n    Features:\n        - Automatic granularity detection using TimeGranularityAnalyzer\n        - Per-day processing to handle missing periods properly and prevent incorrect autofilling of missing days\n        - Support for both intensive and extensive quantities\n        - Timezone-aware processing including daylight saving transitions\n    \"\"\"\n    def __init__(self):\n        \"\"\"Initialize the granularity converter with analyzer instances.\n\n        Creates both strict and non-strict granularity analyzers for different\n        validation requirements during conversion operations.\n        \"\"\"\n        self._strict_gran_analyzer = TimeSeriesGranularityAnalyzer(strict_mode=True)\n        self._non_strict_gran_analyzer = TimeSeriesGranularityAnalyzer(strict_mode=False)\n\n    def _validate_series_format(self, series: pd.Series) -&gt; None:\n        \"\"\"Validate that the input series has the required DatetimeIndex format.\n\n        Args:\n            series: Time series to validate\n\n        Raises:\n            TypeError: If series index is not a DatetimeIndex\n        \"\"\"\n        if not isinstance(series.index, pd.DatetimeIndex):\n            raise TypeError(f\"Series index must be DatetimeIndex, got {type(series.index)}\")\n\n    def upsample_through_fillna(\n            self,\n            data: pd.DataFrame | pd.Series,\n            quantity_type: QuantityTypeEnum\n    ) -&gt; pd.DataFrame | pd.Series:\n        \"\"\"Upsample data using forward-fill strategy with quantity-type-aware scaling.\n\n        This method handles upsampling of sparse data where some values are missing.\n        It uses forward-fill to propagate values and applies appropriate scaling\n        based on the quantity type:\n\n        - For INTENSIVE quantities: Values are replicated without scaling\n        - For EXTENSIVE quantities: Values are divided by the number of periods\n          they are spread across within each hour-segment group\n\n        The method processes data per day and hour to handle missing periods properly\n        and prevent incorrect auto-filling across day boundaries.\n\n        Args:\n            data: Time series data to upsample (Series or DataFrame)\n            quantity_type: Type of quantity being converted (INTENSIVE or EXTENSIVE)\n\n        Returns:\n            Upsampled data with same type as input\n\n        Example:\n\n            &gt;&gt;&gt; # For extensive quantities (energy), values are divided\n            &gt;&gt;&gt; series = pd.Series([100, np.nan, np.nan, np.nan, 200, np.nan, np.nan, np.nan],\n            ...                   index=pd.date_range('2024-01-01', freq='15min', periods=5))\n            &gt;&gt;&gt; converter.upsample_through_fillna(series, QuantityTypeEnum.EXTENSIVE)\n                # Results in [25, 25, 25, 25, 50, 50, 50, 50]\n        \"\"\"\n        if isinstance(data, pd.Series):\n            return self._upsample_series(data, quantity_type)\n\n        tmp = data.copy().sort_index()\n        idx = tmp.index.tz_convert('UTC') if tmp.index.tz is not None else tmp.index\n\n        if quantity_type == QuantityTypeEnum.EXTENSIVE:\n            segment_patterns = tmp.notna().cumsum()\n\n            # Group columns by their segment pattern\n            pattern_to_cols = {}\n            for col in tmp.columns:\n                pattern = tuple(segment_patterns[col].values)  # Convert to tuple to make it hashable\n                pattern_to_cols.setdefault(pattern, []).append(col)\n\n            # Process each group of columns with same pattern\n            result_pieces = []\n            for pattern, cols in pattern_to_cols.items():\n                segments = segment_patterns[cols[0]]  # Take segments from first column (all are same)\n                piece = (\n                    tmp[cols]\n                    .groupby([idx.date, idx.hour, segments])\n                    .transform(lambda s: s.ffill() / len(s))\n                )\n                result_pieces.append(piece)\n\n            return pd.concat(result_pieces, axis=1).loc[data.index].rename_axis(data.columns.names, axis=1)\n        else:\n            return tmp.groupby([idx.date, idx.hour]).ffill().loc[data.index]\n\n    def _upsample_series(self, series: pd.Series, quantity_type: QuantityTypeEnum) -&gt; pd.Series:\n        \"\"\"Helper method to upsample a Series using DataFrame-based upsampling.\n\n        Args:\n            series: Time series to upsample\n            quantity_type: Type of quantity being converted\n\n        Returns:\n            Upsampled series\n        \"\"\"\n        return self.upsample_through_fillna(\n            series.to_frame(),\n            quantity_type\n        ).iloc[:, 0]\n\n    def convert_to_target_index(\n            self,\n            series: pd.Series,\n            target_index: pd.DatetimeIndex,\n            quantity_type: QuantityTypeEnum\n    ) -&gt; pd.Series:\n        \"\"\"Convert a time series to match a specific target DatetimeIndex.\n\n        This method converts the granularity of a time series to match the granularity\n        of a target index. The target index must have consistent granularity within\n        each day and consistent granularity across all days.\n\n        Args:\n            series: Source time series to convert\n            target_index: DatetimeIndex defining the target granularity and timestamps\n            quantity_type: Type of quantity (INTENSIVE or EXTENSIVE) for proper scaling\n\n        Returns:\n            Series converted to match target index granularity and timestamps\n\n        Raises:\n            ValueError: If target index has multiple granularities within days\n                       or inconsistent granularity across days\n\n        Example:\n\n            &gt;&gt;&gt; # Convert hourly to 15-min data\n            &gt;&gt;&gt; hourly_series = pd.Series([100, 150, 200], \n            ...                          index=pd.date_range('2024-01-01', freq='1H', periods=3))\n            &gt;&gt;&gt; target_idx = pd.date_range('2024-01-01', freq='15min', periods=12)\n            &gt;&gt;&gt; result = converter.convert_to_target_index(hourly_series, target_idx,\n            ...                                          QuantityTypeEnum.INTENSIVE)\n        \"\"\"\n        self._validate_series_format(series)\n        target_gran_series = self._strict_gran_analyzer.get_granularity_as_series_of_timedeltas(target_index)\n        _grouped = target_gran_series.groupby(target_gran_series.index.date)\n        if _grouped.nunique().max() &gt; 1:\n            raise ValueError(f\"Found some dates with multiple granularities within same day. Can't handle that!\")\n        if _grouped.first().nunique() &gt; 1:\n            raise ValueError(f\"Found multiple granularities. Can't handle that!\")\n        target_granularity = pd.Timedelta(target_gran_series.values[0])\n        return series.groupby(series.index.date).apply(\n            lambda x: self._convert_date_to_target_granularity(x, target_granularity, quantity_type)\n        ).droplevel(0).rename_axis(series.index.name).rename(series.name)\n\n    def convert_to_target_granularity(\n            self,\n            series: pd.Series,\n            target_granularity: pd.Timedelta,\n            quantity_type: QuantityTypeEnum\n    ) -&gt; pd.Series:\n        \"\"\"Convert a time series to a specific target granularity.\n\n        This method converts the temporal granularity of a time series while properly\n        handling the physical nature of the quantity. The conversion is performed\n        day-by-day to prevent incorrect handling of missing days or daylight saving\n        time transitions.\n\n        Args:\n            series: Source time series to convert\n            target_granularity: Target granularity as a pandas Timedelta\n                               (e.g., pd.Timedelta(minutes=15) for 15-minute data)\n            quantity_type: Type of quantity for proper scaling:\n                          - INTENSIVE: Values are averaged/replicated (prices, power)\n                          - EXTENSIVE: Values are summed/split (volumes, energy)\n\n        Returns:\n            Series with converted granularity, maintaining original naming and metadata\n\n        Raises:\n            GranularityConversionError: If conversion cannot be performed due to\n                                       unsupported granularities or data issues\n\n        Example:\n\n            &gt;&gt;&gt; # Convert 15-minute to hourly data (downsampling)\n            &gt;&gt;&gt; quarter_hourly = pd.Series([25, 30, 35, 40], \n            ...                           index=pd.date_range('2024-01-01', freq='15min', periods=4))\n            &gt;&gt;&gt; hourly = converter.convert_to_target_granularity(\n            ...     quarter_hourly, pd.Timedelta(hours=1), QuantityTypeEnum.EXTENSIVE)\n            &gt;&gt;&gt; print(hourly)  # Result: [130] (25+30+35+40)\n        \"\"\"\n        self._validate_series_format(series)\n        return series.groupby(series.index.date).apply(\n            lambda x: self._convert_date_to_target_granularity(x, target_granularity, quantity_type)\n        ).droplevel(0).rename_axis(series.index.name).rename(series.name)\n\n    def _convert_date_to_target_granularity(\n        self,\n        series: pd.Series,\n        target_granularity: pd.Timedelta,\n        quantity_type: QuantityTypeEnum\n    ) -&gt; pd.Series:\n        \"\"\"Convert granularity for a single date's worth of data.\n\n        This internal method handles the actual conversion logic for data within\n        a single date range. It determines whether upsampling, downsampling, or\n        no conversion is needed, then applies the appropriate method.\n\n        Args:\n            series: Time series data for a single date (may span into next date)\n            target_granularity: Target granularity as Timedelta\n            quantity_type: Type of quantity for scaling decisions\n\n        Returns:\n            Converted series for the date period\n\n        Raises:\n            GranularityConversionError: If conversion parameters are invalid or\n                                       unsupported granularities are encountered\n        \"\"\"\n        if len(set(series.index.date)) &gt; 2:\n            raise GranularityConversionError('This method is intended for single-date conversion only.')\n        source_gran = self._non_strict_gran_analyzer.get_granularity_as_series_of_minutes(series.index)\n        if len(source_gran.unique()) &gt; 1:\n            raise GranularityConversionError('Cannot convert data with changing granularity within a single day.')\n        source_gran_minutes = source_gran.values[0]\n        target_gran_minutes = target_granularity.total_seconds() / 60\n\n        _allowed_granularities = [1, 5, 15, 30, 60, 24*60]\n        if target_gran_minutes not in _allowed_granularities:\n            raise GranularityConversionError(\n                f'Target granularity {target_gran_minutes} minutes not supported. '\n                f'Allowed granularities: {_allowed_granularities} minutes'\n            )\n\n        if target_gran_minutes &gt; source_gran_minutes:\n            sampling = SamplingMethodEnum.DOWNSAMPLING\n        elif target_gran_minutes &lt; source_gran_minutes:\n            sampling = SamplingMethodEnum.UPSAMPLING\n        else:\n            sampling = SamplingMethodEnum.KEEP\n\n        if sampling == SamplingMethodEnum.UPSAMPLING:\n            scaling_factor = source_gran_minutes / target_gran_minutes\n            if (scaling_factor % 1) != 0:\n                raise GranularityConversionError(\n                    f'Source granularity ({source_gran_minutes} min) is not evenly divisible '\n                    f'by target granularity ({target_gran_minutes} min)'\n                )\n            else:\n                scaling_factor = int(scaling_factor)\n\n            new_index = pd.date_range(\n                start=series.index[0],\n                end=series.index[-1],\n                freq=f\"{target_gran_minutes}min\",\n                tz=series.index.tz\n            )\n\n            # For intensive quantities, replicate values; for extensive, divide by scaling factor\n            if quantity_type == QuantityTypeEnum.INTENSIVE:\n                return series.reindex(new_index, method='ffill')\n            else:  # EXTENSIVE\n                return series.reindex(new_index, method='ffill') / scaling_factor\n\n        elif sampling == SamplingMethodEnum.DOWNSAMPLING:\n            groups = series.groupby(pd.Grouper(freq=f\"{target_gran_minutes}min\"))\n            # For extensive quantities, sum the values; for intensive, take the mean\n            func = 'sum' if quantity_type == QuantityTypeEnum.EXTENSIVE else 'mean'\n            return groups.agg(func)\n\n        else:  # SamplingMethodEnum.KEEP\n            return series\n\n        return series\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/granularity_modules/#mesqual.energy_data_handling.granularity_converter.TimeSeriesGranularityConverter.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the granularity converter with analyzer instances.</p> <p>Creates both strict and non-strict granularity analyzers for different validation requirements during conversion operations.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/granularity_converter.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the granularity converter with analyzer instances.\n\n    Creates both strict and non-strict granularity analyzers for different\n    validation requirements during conversion operations.\n    \"\"\"\n    self._strict_gran_analyzer = TimeSeriesGranularityAnalyzer(strict_mode=True)\n    self._non_strict_gran_analyzer = TimeSeriesGranularityAnalyzer(strict_mode=False)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/granularity_modules/#mesqual.energy_data_handling.granularity_converter.TimeSeriesGranularityConverter.upsample_through_fillna","title":"upsample_through_fillna","text":"<pre><code>upsample_through_fillna(data: DataFrame | Series, quantity_type: QuantityTypeEnum) -&gt; DataFrame | Series\n</code></pre> <p>Upsample data using forward-fill strategy with quantity-type-aware scaling.</p> <p>This method handles upsampling of sparse data where some values are missing. It uses forward-fill to propagate values and applies appropriate scaling based on the quantity type:</p> <ul> <li>For INTENSIVE quantities: Values are replicated without scaling</li> <li>For EXTENSIVE quantities: Values are divided by the number of periods   they are spread across within each hour-segment group</li> </ul> <p>The method processes data per day and hour to handle missing periods properly and prevent incorrect auto-filling across day boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame | Series</code> <p>Time series data to upsample (Series or DataFrame)</p> required <code>quantity_type</code> <code>QuantityTypeEnum</code> <p>Type of quantity being converted (INTENSIVE or EXTENSIVE)</p> required <p>Returns:</p> Type Description <code>DataFrame | Series</code> <p>Upsampled data with same type as input</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # For extensive quantities (energy), values are divided\n&gt;&gt;&gt; series = pd.Series([100, np.nan, np.nan, np.nan, 200, np.nan, np.nan, np.nan],\n...                   index=pd.date_range('2024-01-01', freq='15min', periods=5))\n&gt;&gt;&gt; converter.upsample_through_fillna(series, QuantityTypeEnum.EXTENSIVE)\n    # Results in [25, 25, 25, 25, 50, 50, 50, 50]\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/granularity_converter.py</code> <pre><code>def upsample_through_fillna(\n        self,\n        data: pd.DataFrame | pd.Series,\n        quantity_type: QuantityTypeEnum\n) -&gt; pd.DataFrame | pd.Series:\n    \"\"\"Upsample data using forward-fill strategy with quantity-type-aware scaling.\n\n    This method handles upsampling of sparse data where some values are missing.\n    It uses forward-fill to propagate values and applies appropriate scaling\n    based on the quantity type:\n\n    - For INTENSIVE quantities: Values are replicated without scaling\n    - For EXTENSIVE quantities: Values are divided by the number of periods\n      they are spread across within each hour-segment group\n\n    The method processes data per day and hour to handle missing periods properly\n    and prevent incorrect auto-filling across day boundaries.\n\n    Args:\n        data: Time series data to upsample (Series or DataFrame)\n        quantity_type: Type of quantity being converted (INTENSIVE or EXTENSIVE)\n\n    Returns:\n        Upsampled data with same type as input\n\n    Example:\n\n        &gt;&gt;&gt; # For extensive quantities (energy), values are divided\n        &gt;&gt;&gt; series = pd.Series([100, np.nan, np.nan, np.nan, 200, np.nan, np.nan, np.nan],\n        ...                   index=pd.date_range('2024-01-01', freq='15min', periods=5))\n        &gt;&gt;&gt; converter.upsample_through_fillna(series, QuantityTypeEnum.EXTENSIVE)\n            # Results in [25, 25, 25, 25, 50, 50, 50, 50]\n    \"\"\"\n    if isinstance(data, pd.Series):\n        return self._upsample_series(data, quantity_type)\n\n    tmp = data.copy().sort_index()\n    idx = tmp.index.tz_convert('UTC') if tmp.index.tz is not None else tmp.index\n\n    if quantity_type == QuantityTypeEnum.EXTENSIVE:\n        segment_patterns = tmp.notna().cumsum()\n\n        # Group columns by their segment pattern\n        pattern_to_cols = {}\n        for col in tmp.columns:\n            pattern = tuple(segment_patterns[col].values)  # Convert to tuple to make it hashable\n            pattern_to_cols.setdefault(pattern, []).append(col)\n\n        # Process each group of columns with same pattern\n        result_pieces = []\n        for pattern, cols in pattern_to_cols.items():\n            segments = segment_patterns[cols[0]]  # Take segments from first column (all are same)\n            piece = (\n                tmp[cols]\n                .groupby([idx.date, idx.hour, segments])\n                .transform(lambda s: s.ffill() / len(s))\n            )\n            result_pieces.append(piece)\n\n        return pd.concat(result_pieces, axis=1).loc[data.index].rename_axis(data.columns.names, axis=1)\n    else:\n        return tmp.groupby([idx.date, idx.hour]).ffill().loc[data.index]\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/granularity_modules/#mesqual.energy_data_handling.granularity_converter.TimeSeriesGranularityConverter.convert_to_target_index","title":"convert_to_target_index","text":"<pre><code>convert_to_target_index(series: Series, target_index: DatetimeIndex, quantity_type: QuantityTypeEnum) -&gt; Series\n</code></pre> <p>Convert a time series to match a specific target DatetimeIndex.</p> <p>This method converts the granularity of a time series to match the granularity of a target index. The target index must have consistent granularity within each day and consistent granularity across all days.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series</code> <p>Source time series to convert</p> required <code>target_index</code> <code>DatetimeIndex</code> <p>DatetimeIndex defining the target granularity and timestamps</p> required <code>quantity_type</code> <code>QuantityTypeEnum</code> <p>Type of quantity (INTENSIVE or EXTENSIVE) for proper scaling</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Series converted to match target index granularity and timestamps</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If target index has multiple granularities within days        or inconsistent granularity across days</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Convert hourly to 15-min data\n&gt;&gt;&gt; hourly_series = pd.Series([100, 150, 200], \n...                          index=pd.date_range('2024-01-01', freq='1H', periods=3))\n&gt;&gt;&gt; target_idx = pd.date_range('2024-01-01', freq='15min', periods=12)\n&gt;&gt;&gt; result = converter.convert_to_target_index(hourly_series, target_idx,\n...                                          QuantityTypeEnum.INTENSIVE)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/granularity_converter.py</code> <pre><code>def convert_to_target_index(\n        self,\n        series: pd.Series,\n        target_index: pd.DatetimeIndex,\n        quantity_type: QuantityTypeEnum\n) -&gt; pd.Series:\n    \"\"\"Convert a time series to match a specific target DatetimeIndex.\n\n    This method converts the granularity of a time series to match the granularity\n    of a target index. The target index must have consistent granularity within\n    each day and consistent granularity across all days.\n\n    Args:\n        series: Source time series to convert\n        target_index: DatetimeIndex defining the target granularity and timestamps\n        quantity_type: Type of quantity (INTENSIVE or EXTENSIVE) for proper scaling\n\n    Returns:\n        Series converted to match target index granularity and timestamps\n\n    Raises:\n        ValueError: If target index has multiple granularities within days\n                   or inconsistent granularity across days\n\n    Example:\n\n        &gt;&gt;&gt; # Convert hourly to 15-min data\n        &gt;&gt;&gt; hourly_series = pd.Series([100, 150, 200], \n        ...                          index=pd.date_range('2024-01-01', freq='1H', periods=3))\n        &gt;&gt;&gt; target_idx = pd.date_range('2024-01-01', freq='15min', periods=12)\n        &gt;&gt;&gt; result = converter.convert_to_target_index(hourly_series, target_idx,\n        ...                                          QuantityTypeEnum.INTENSIVE)\n    \"\"\"\n    self._validate_series_format(series)\n    target_gran_series = self._strict_gran_analyzer.get_granularity_as_series_of_timedeltas(target_index)\n    _grouped = target_gran_series.groupby(target_gran_series.index.date)\n    if _grouped.nunique().max() &gt; 1:\n        raise ValueError(f\"Found some dates with multiple granularities within same day. Can't handle that!\")\n    if _grouped.first().nunique() &gt; 1:\n        raise ValueError(f\"Found multiple granularities. Can't handle that!\")\n    target_granularity = pd.Timedelta(target_gran_series.values[0])\n    return series.groupby(series.index.date).apply(\n        lambda x: self._convert_date_to_target_granularity(x, target_granularity, quantity_type)\n    ).droplevel(0).rename_axis(series.index.name).rename(series.name)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/granularity_modules/#mesqual.energy_data_handling.granularity_converter.TimeSeriesGranularityConverter.convert_to_target_granularity","title":"convert_to_target_granularity","text":"<pre><code>convert_to_target_granularity(series: Series, target_granularity: Timedelta, quantity_type: QuantityTypeEnum) -&gt; Series\n</code></pre> <p>Convert a time series to a specific target granularity.</p> <p>This method converts the temporal granularity of a time series while properly handling the physical nature of the quantity. The conversion is performed day-by-day to prevent incorrect handling of missing days or daylight saving time transitions.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series</code> <p>Source time series to convert</p> required <code>target_granularity</code> <code>Timedelta</code> <p>Target granularity as a pandas Timedelta                (e.g., pd.Timedelta(minutes=15) for 15-minute data)</p> required <code>quantity_type</code> <code>QuantityTypeEnum</code> <p>Type of quantity for proper scaling:           - INTENSIVE: Values are averaged/replicated (prices, power)           - EXTENSIVE: Values are summed/split (volumes, energy)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Series with converted granularity, maintaining original naming and metadata</p> <p>Raises:</p> Type Description <code>GranularityConversionError</code> <p>If conversion cannot be performed due to                        unsupported granularities or data issues</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Convert 15-minute to hourly data (downsampling)\n&gt;&gt;&gt; quarter_hourly = pd.Series([25, 30, 35, 40], \n...                           index=pd.date_range('2024-01-01', freq='15min', periods=4))\n&gt;&gt;&gt; hourly = converter.convert_to_target_granularity(\n...     quarter_hourly, pd.Timedelta(hours=1), QuantityTypeEnum.EXTENSIVE)\n&gt;&gt;&gt; print(hourly)  # Result: [130] (25+30+35+40)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/granularity_converter.py</code> <pre><code>def convert_to_target_granularity(\n        self,\n        series: pd.Series,\n        target_granularity: pd.Timedelta,\n        quantity_type: QuantityTypeEnum\n) -&gt; pd.Series:\n    \"\"\"Convert a time series to a specific target granularity.\n\n    This method converts the temporal granularity of a time series while properly\n    handling the physical nature of the quantity. The conversion is performed\n    day-by-day to prevent incorrect handling of missing days or daylight saving\n    time transitions.\n\n    Args:\n        series: Source time series to convert\n        target_granularity: Target granularity as a pandas Timedelta\n                           (e.g., pd.Timedelta(minutes=15) for 15-minute data)\n        quantity_type: Type of quantity for proper scaling:\n                      - INTENSIVE: Values are averaged/replicated (prices, power)\n                      - EXTENSIVE: Values are summed/split (volumes, energy)\n\n    Returns:\n        Series with converted granularity, maintaining original naming and metadata\n\n    Raises:\n        GranularityConversionError: If conversion cannot be performed due to\n                                   unsupported granularities or data issues\n\n    Example:\n\n        &gt;&gt;&gt; # Convert 15-minute to hourly data (downsampling)\n        &gt;&gt;&gt; quarter_hourly = pd.Series([25, 30, 35, 40], \n        ...                           index=pd.date_range('2024-01-01', freq='15min', periods=4))\n        &gt;&gt;&gt; hourly = converter.convert_to_target_granularity(\n        ...     quarter_hourly, pd.Timedelta(hours=1), QuantityTypeEnum.EXTENSIVE)\n        &gt;&gt;&gt; print(hourly)  # Result: [130] (25+30+35+40)\n    \"\"\"\n    self._validate_series_format(series)\n    return series.groupby(series.index.date).apply(\n        lambda x: self._convert_date_to_target_granularity(x, target_granularity, quantity_type)\n    ).droplevel(0).rename_axis(series.index.name).rename(series.name)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/granularity_modules/#mesqual.energy_data_handling.granularity_converter.SamplingMethodEnum","title":"SamplingMethodEnum","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of sampling methods for granularity conversion.</p> <p>Attributes:</p> Name Type Description <code>UPSAMPLING</code> <p>Converting from coarser to finer granularity (e.g., hourly to 15-min)</p> <code>DOWNSAMPLING</code> <p>Converting from finer to coarser granularity (e.g., 15-min to hourly)</p> <code>KEEP</code> <p>No conversion needed - source and target granularities are the same</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/granularity_converter.py</code> <pre><code>class SamplingMethodEnum(Enum):\n    \"\"\"Enumeration of sampling methods for granularity conversion.\n\n    Attributes:\n        UPSAMPLING: Converting from coarser to finer granularity (e.g., hourly to 15-min)\n        DOWNSAMPLING: Converting from finer to coarser granularity (e.g., 15-min to hourly)\n        KEEP: No conversion needed - source and target granularities are the same\n    \"\"\"\n    UPSAMPLING = 'upsampling'\n    DOWNSAMPLING = 'downsampling'\n    KEEP = 'keep'\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/network_lines_data/","title":"MESQUAL Network Lines Data","text":""},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/network_lines_data/#mesqual.energy_data_handling.network_lines_data.NetworkLineFlowsData","title":"NetworkLineFlowsData","text":"<p>Wrapper for bidirectional flow data of network transmission lines.</p> <p>This class encapsulates energy or power flow data for lines in both directions, accounting for transmission losses. It provides a standardized interface for handling complex flow patterns in electrical network analysis.</p> Flow Direction Conventions <ul> <li>sent_up: Flow entering line at node_from (towards node_to)</li> <li>received_up: Flow leaving line at node_from after losses (coming from node_to)</li> <li>sent_down: Flow entering line at node_to (towards node_from)  </li> <li>received_down: Flow leaving line at node_to after losses (coming from node_from)</li> </ul> <p>The distinction between 'sent' and 'received' allows for modeling transmission losses, where received_flow = sent_flow * (1 - loss_rate).</p> <p>Parameters:</p> Name Type Description Default <code>sent_up</code> <code>DataFrame</code> <p>DataFrame with flow data entering at node_from</p> required <code>received_up</code> <code>DataFrame</code> <p>DataFrame with flow data received at node_from after losses</p> required <code>sent_down</code> <code>DataFrame</code> <p>DataFrame with flow data entering at node_to</p> required <code>received_down</code> <code>DataFrame</code> <p>DataFrame with flow data received at node_to after losses</p> required <code>granularity</code> <code>None | float | Series</code> <p>Time granularity of the data (None, float in minutes, or Series)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If indices or columns of the four DataFrames don't match</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=24, freq='1H')\n&gt;&gt;&gt; columns = ['Line_A_B', 'Line_B_C']\n&gt;&gt;&gt; flows_up = pd.DataFrame(100, index=index, columns=columns)\n&gt;&gt;&gt; flows_down = pd.DataFrame(50, index=index, columns=columns)\n&gt;&gt;&gt; line_data = NetworkLineFlowsData.from_up_and_down_flow_without_losses(\n...     flows_up, flows_down)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/network_lines_data.py</code> <pre><code>class NetworkLineFlowsData:\n    \"\"\"Wrapper for bidirectional flow data of network transmission lines.\n\n    This class encapsulates energy or power flow data for lines in both directions,\n    accounting for transmission losses. It provides a standardized interface for\n    handling complex flow patterns in electrical network analysis.\n\n    Flow Direction Conventions:\n        - sent_up: Flow entering line at node_from (towards node_to)\n        - received_up: Flow leaving line at node_from after losses (coming from node_to)\n        - sent_down: Flow entering line at node_to (towards node_from)  \n        - received_down: Flow leaving line at node_to after losses (coming from node_from)\n\n    The distinction between 'sent' and 'received' allows for modeling transmission\n    losses, where received_flow = sent_flow * (1 - loss_rate).\n\n    Args:\n        sent_up: DataFrame with flow data entering at node_from\n        received_up: DataFrame with flow data received at node_from after losses\n        sent_down: DataFrame with flow data entering at node_to\n        received_down: DataFrame with flow data received at node_to after losses\n        granularity: Time granularity of the data (None, float in minutes, or Series)\n\n    Raises:\n        ValueError: If indices or columns of the four DataFrames don't match\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=24, freq='1H')\n        &gt;&gt;&gt; columns = ['Line_A_B', 'Line_B_C']\n        &gt;&gt;&gt; flows_up = pd.DataFrame(100, index=index, columns=columns)\n        &gt;&gt;&gt; flows_down = pd.DataFrame(50, index=index, columns=columns)\n        &gt;&gt;&gt; line_data = NetworkLineFlowsData.from_up_and_down_flow_without_losses(\n        ...     flows_up, flows_down)\n    \"\"\"\n    def __init__(\n            self,\n            sent_up: pd.DataFrame,\n            received_up: pd.DataFrame,\n            sent_down: pd.DataFrame,\n            received_down: pd.DataFrame,\n            granularity: None | float | pd.Series = None\n    ):\n        \"\"\"Initialize NetworkLineFlowsData with flow data in both directions.\n\n        Args:\n            sent_up: Flow data sent in up direction (node_from -&gt; node_to)\n            received_up: Flow data received in up direction after losses\n            sent_down: Flow data sent in down direction (node_to -&gt; node_from)\n            received_down: Flow data received in down direction after losses\n            granularity: Time granularity information for the data\n        \"\"\"\n        self.sent_up = sent_up\n        self.received_up = received_up\n        self.sent_down = sent_down\n        self.received_down = received_down\n        self.granularity = granularity\n        self.__post_init__()\n\n    def __post_init__(self):\n        \"\"\"Validate that all DataFrames have matching indices and columns.\n\n        Raises:\n            ValueError: If any DataFrame has mismatched indices or columns.\n        \"\"\"\n        dataframes = [self.received_up, self.sent_down, self.received_down]\n        for i, df in enumerate(dataframes):\n            df_name = ['received_up', 'sent_down', 'received_down'][i]\n            if not self.sent_up.index.equals(df.index):\n                raise ValueError(f'Index mismatch: sent_up vs {df_name}')\n            if not self.sent_up.columns.equals(df.columns):\n                raise ValueError(f'Columns mismatch: sent_up vs {df_name}')\n\n    def from_mw_to_mwh(self) -&gt; 'NetworkLineFlowsData':\n        \"\"\"Convert flow data from MW (power) to MWh (energy).\n\n        This conversion requires granularity information to properly scale the values.\n\n        Returns:\n            New NetworkLineFlowsData instance with energy values\n\n        Raises:\n            NotImplementedError: Method not yet implemented\n        \"\"\"\n        # TODO: Implement MW to MWh conversion using granularity\n        raise NotImplementedError(\"MW to MWh conversion not yet implemented\")\n\n    def from_mwh_to_mw(self) -&gt; 'NetworkLineFlowsData':\n        \"\"\"Convert flow data from MWh (energy) to MW (power).\n\n        This conversion requires granularity information to properly scale the values.\n\n        Returns:\n            New NetworkLineFlowsData instance with power values\n\n        Raises:\n            NotImplementedError: Method not yet implemented\n        \"\"\"\n        # TODO: Implement MWh to MW conversion using granularity  \n        raise NotImplementedError(\"MWh to MW conversion not yet implemented\")\n\n    @classmethod\n    def from_net_flow_without_losses(cls, net_flow: pd.DataFrame) -&gt; \"NetworkLineFlowsData\":\n        \"\"\"Create NetworkLineFlowsData from net line flow data assuming no transmission losses.\n\n        Converts net flow data (where positive values indicate flow in up direction\n        and negative values indicate flow in down direction) into the bidirectional\n        flow representation used by this class.\n\n        Args:\n            net_flow: DataFrame with net flow values. Positive = up direction,\n                negative = down direction\n\n        Returns:\n            NetworkLineFlowsData instance with flows split into up/down directions\n\n        Example:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; net_flows = pd.DataFrame({\n            ...     'Line_A_B': [100, -50, 75],\n            ...     'Line_B_C': [200, 150, -100]\n            ... })\n            &gt;&gt;&gt; line_data = NetworkLineFlowsData.from_net_flow_without_losses(net_flows)\n        \"\"\"\n        positive_flow = net_flow.clip(lower=0)\n        negative_flow = -net_flow.clip(upper=0)\n\n        return cls(\n            sent_up=positive_flow,\n            received_up=positive_flow,\n            sent_down=negative_flow,\n            received_down=negative_flow\n        )\n\n    @classmethod\n    def from_nodal_net_injection(\n            cls,\n            node_a_net_injection: pd.DataFrame,\n            node_b_net_injection: pd.DataFrame\n    ) -&gt; \"NetworkLineFlowsData\":\n        \"\"\"Create NetworkLineFlowsData from nodal net-injection data.\n\n        Converts nodal net-injection data (where positive values indicate flow from node towards line (injection)\n        and negative values indicate flow from line towards node (ejection)) into the bidirectional\n        flow representation used by this class. Automatically computes losses.\n        Node_A refers to the topological \"node_from\" and\n        Node_B to the topological \"node_to\" in the definition of the lines.\n\n        Args:\n            node_a_net_injection: DataFrame with net injection at Node_A.\n                positive = injection, negative = ejection\n            node_b_net_injection: DataFrame with net injection at Node_B.\n                positive = injection, negative = ejection\n\n        Returns:\n            NetworkLineFlowsData instance with flows split into up/down directions\n\n        Example:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; net_injection_a = pd.DataFrame({\n            ...     'power_a': [100, -49, -49],\n            ... })\n            &gt;&gt;&gt; net_injection_b = pd.DataFrame({\n            ...     'power_b': [-98, 50,  50]\n            ... })\n            &gt;&gt;&gt; line_data = NetworkLineFlowsData.from_nodal_net_injection(net_injection_a, net_injection_b)\n        \"\"\"\n\n        return cls(\n            sent_up=node_a_net_injection.clip(0),\n            received_up=-1 * node_b_net_injection.clip(None, 0),\n            sent_down=node_b_net_injection.clip(0),\n            received_down=-1 * node_a_net_injection.clip(None, 0)\n        )\n\n    @classmethod\n    def from_up_and_down_flow_without_losses(\n            cls,\n            flow_up: pd.DataFrame,\n            flow_down: pd.DataFrame\n    ) -&gt; \"NetworkLineFlowsData\":\n        \"\"\"Create NetworkLineFlowsData from separate up and down flow data without losses.\n\n        This constructor assumes that there are no transmission losses, so sent and\n        received flows are identical in each direction.\n\n        Args:\n            flow_up: DataFrame with flow data in up direction (node_from -&gt; node_to)\n            flow_down: DataFrame with flow data in down direction (node_to -&gt; node_from)\n\n        Returns:\n            NetworkLineFlowsData instance where sent and received flows are equal\n\n        Example:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; up_flows = pd.DataFrame({'Line_A_B': [100, 80, 120]})\n            &gt;&gt;&gt; down_flows = pd.DataFrame({'Line_A_B': [50, 60, 40]})\n            &gt;&gt;&gt; line_data = NetworkLineFlowsData.from_up_and_down_flow_without_losses(\n            ...     up_flows, down_flows)\n        \"\"\"\n        return cls(\n            sent_up=flow_up,\n            received_up=flow_up,\n            sent_down=flow_down,\n            received_down=flow_down\n        )\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/network_lines_data/#mesqual.energy_data_handling.network_lines_data.NetworkLineFlowsData.__init__","title":"__init__","text":"<pre><code>__init__(sent_up: DataFrame, received_up: DataFrame, sent_down: DataFrame, received_down: DataFrame, granularity: None | float | Series = None)\n</code></pre> <p>Initialize NetworkLineFlowsData with flow data in both directions.</p> <p>Parameters:</p> Name Type Description Default <code>sent_up</code> <code>DataFrame</code> <p>Flow data sent in up direction (node_from -&gt; node_to)</p> required <code>received_up</code> <code>DataFrame</code> <p>Flow data received in up direction after losses</p> required <code>sent_down</code> <code>DataFrame</code> <p>Flow data sent in down direction (node_to -&gt; node_from)</p> required <code>received_down</code> <code>DataFrame</code> <p>Flow data received in down direction after losses</p> required <code>granularity</code> <code>None | float | Series</code> <p>Time granularity information for the data</p> <code>None</code> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/network_lines_data.py</code> <pre><code>def __init__(\n        self,\n        sent_up: pd.DataFrame,\n        received_up: pd.DataFrame,\n        sent_down: pd.DataFrame,\n        received_down: pd.DataFrame,\n        granularity: None | float | pd.Series = None\n):\n    \"\"\"Initialize NetworkLineFlowsData with flow data in both directions.\n\n    Args:\n        sent_up: Flow data sent in up direction (node_from -&gt; node_to)\n        received_up: Flow data received in up direction after losses\n        sent_down: Flow data sent in down direction (node_to -&gt; node_from)\n        received_down: Flow data received in down direction after losses\n        granularity: Time granularity information for the data\n    \"\"\"\n    self.sent_up = sent_up\n    self.received_up = received_up\n    self.sent_down = sent_down\n    self.received_down = received_down\n    self.granularity = granularity\n    self.__post_init__()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/network_lines_data/#mesqual.energy_data_handling.network_lines_data.NetworkLineFlowsData.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate that all DataFrames have matching indices and columns.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any DataFrame has mismatched indices or columns.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/network_lines_data.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate that all DataFrames have matching indices and columns.\n\n    Raises:\n        ValueError: If any DataFrame has mismatched indices or columns.\n    \"\"\"\n    dataframes = [self.received_up, self.sent_down, self.received_down]\n    for i, df in enumerate(dataframes):\n        df_name = ['received_up', 'sent_down', 'received_down'][i]\n        if not self.sent_up.index.equals(df.index):\n            raise ValueError(f'Index mismatch: sent_up vs {df_name}')\n        if not self.sent_up.columns.equals(df.columns):\n            raise ValueError(f'Columns mismatch: sent_up vs {df_name}')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/network_lines_data/#mesqual.energy_data_handling.network_lines_data.NetworkLineFlowsData.from_mw_to_mwh","title":"from_mw_to_mwh","text":"<pre><code>from_mw_to_mwh() -&gt; NetworkLineFlowsData\n</code></pre> <p>Convert flow data from MW (power) to MWh (energy).</p> <p>This conversion requires granularity information to properly scale the values.</p> <p>Returns:</p> Type Description <code>NetworkLineFlowsData</code> <p>New NetworkLineFlowsData instance with energy values</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Method not yet implemented</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/network_lines_data.py</code> <pre><code>def from_mw_to_mwh(self) -&gt; 'NetworkLineFlowsData':\n    \"\"\"Convert flow data from MW (power) to MWh (energy).\n\n    This conversion requires granularity information to properly scale the values.\n\n    Returns:\n        New NetworkLineFlowsData instance with energy values\n\n    Raises:\n        NotImplementedError: Method not yet implemented\n    \"\"\"\n    # TODO: Implement MW to MWh conversion using granularity\n    raise NotImplementedError(\"MW to MWh conversion not yet implemented\")\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/network_lines_data/#mesqual.energy_data_handling.network_lines_data.NetworkLineFlowsData.from_mwh_to_mw","title":"from_mwh_to_mw","text":"<pre><code>from_mwh_to_mw() -&gt; NetworkLineFlowsData\n</code></pre> <p>Convert flow data from MWh (energy) to MW (power).</p> <p>This conversion requires granularity information to properly scale the values.</p> <p>Returns:</p> Type Description <code>NetworkLineFlowsData</code> <p>New NetworkLineFlowsData instance with power values</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Method not yet implemented</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/network_lines_data.py</code> <pre><code>def from_mwh_to_mw(self) -&gt; 'NetworkLineFlowsData':\n    \"\"\"Convert flow data from MWh (energy) to MW (power).\n\n    This conversion requires granularity information to properly scale the values.\n\n    Returns:\n        New NetworkLineFlowsData instance with power values\n\n    Raises:\n        NotImplementedError: Method not yet implemented\n    \"\"\"\n    # TODO: Implement MWh to MW conversion using granularity  \n    raise NotImplementedError(\"MWh to MW conversion not yet implemented\")\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/network_lines_data/#mesqual.energy_data_handling.network_lines_data.NetworkLineFlowsData.from_net_flow_without_losses","title":"from_net_flow_without_losses  <code>classmethod</code>","text":"<pre><code>from_net_flow_without_losses(net_flow: DataFrame) -&gt; NetworkLineFlowsData\n</code></pre> <p>Create NetworkLineFlowsData from net line flow data assuming no transmission losses.</p> <p>Converts net flow data (where positive values indicate flow in up direction and negative values indicate flow in down direction) into the bidirectional flow representation used by this class.</p> <p>Parameters:</p> Name Type Description Default <code>net_flow</code> <code>DataFrame</code> <p>DataFrame with net flow values. Positive = up direction, negative = down direction</p> required <p>Returns:</p> Type Description <code>NetworkLineFlowsData</code> <p>NetworkLineFlowsData instance with flows split into up/down directions</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; net_flows = pd.DataFrame({\n...     'Line_A_B': [100, -50, 75],\n...     'Line_B_C': [200, 150, -100]\n... })\n&gt;&gt;&gt; line_data = NetworkLineFlowsData.from_net_flow_without_losses(net_flows)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/network_lines_data.py</code> <pre><code>@classmethod\ndef from_net_flow_without_losses(cls, net_flow: pd.DataFrame) -&gt; \"NetworkLineFlowsData\":\n    \"\"\"Create NetworkLineFlowsData from net line flow data assuming no transmission losses.\n\n    Converts net flow data (where positive values indicate flow in up direction\n    and negative values indicate flow in down direction) into the bidirectional\n    flow representation used by this class.\n\n    Args:\n        net_flow: DataFrame with net flow values. Positive = up direction,\n            negative = down direction\n\n    Returns:\n        NetworkLineFlowsData instance with flows split into up/down directions\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; net_flows = pd.DataFrame({\n        ...     'Line_A_B': [100, -50, 75],\n        ...     'Line_B_C': [200, 150, -100]\n        ... })\n        &gt;&gt;&gt; line_data = NetworkLineFlowsData.from_net_flow_without_losses(net_flows)\n    \"\"\"\n    positive_flow = net_flow.clip(lower=0)\n    negative_flow = -net_flow.clip(upper=0)\n\n    return cls(\n        sent_up=positive_flow,\n        received_up=positive_flow,\n        sent_down=negative_flow,\n        received_down=negative_flow\n    )\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/network_lines_data/#mesqual.energy_data_handling.network_lines_data.NetworkLineFlowsData.from_nodal_net_injection","title":"from_nodal_net_injection  <code>classmethod</code>","text":"<pre><code>from_nodal_net_injection(node_a_net_injection: DataFrame, node_b_net_injection: DataFrame) -&gt; NetworkLineFlowsData\n</code></pre> <p>Create NetworkLineFlowsData from nodal net-injection data.</p> <p>Converts nodal net-injection data (where positive values indicate flow from node towards line (injection) and negative values indicate flow from line towards node (ejection)) into the bidirectional flow representation used by this class. Automatically computes losses. Node_A refers to the topological \"node_from\" and Node_B to the topological \"node_to\" in the definition of the lines.</p> <p>Parameters:</p> Name Type Description Default <code>node_a_net_injection</code> <code>DataFrame</code> <p>DataFrame with net injection at Node_A. positive = injection, negative = ejection</p> required <code>node_b_net_injection</code> <code>DataFrame</code> <p>DataFrame with net injection at Node_B. positive = injection, negative = ejection</p> required <p>Returns:</p> Type Description <code>NetworkLineFlowsData</code> <p>NetworkLineFlowsData instance with flows split into up/down directions</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; net_injection_a = pd.DataFrame({\n...     'power_a': [100, -49, -49],\n... })\n&gt;&gt;&gt; net_injection_b = pd.DataFrame({\n...     'power_b': [-98, 50,  50]\n... })\n&gt;&gt;&gt; line_data = NetworkLineFlowsData.from_nodal_net_injection(net_injection_a, net_injection_b)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/network_lines_data.py</code> <pre><code>@classmethod\ndef from_nodal_net_injection(\n        cls,\n        node_a_net_injection: pd.DataFrame,\n        node_b_net_injection: pd.DataFrame\n) -&gt; \"NetworkLineFlowsData\":\n    \"\"\"Create NetworkLineFlowsData from nodal net-injection data.\n\n    Converts nodal net-injection data (where positive values indicate flow from node towards line (injection)\n    and negative values indicate flow from line towards node (ejection)) into the bidirectional\n    flow representation used by this class. Automatically computes losses.\n    Node_A refers to the topological \"node_from\" and\n    Node_B to the topological \"node_to\" in the definition of the lines.\n\n    Args:\n        node_a_net_injection: DataFrame with net injection at Node_A.\n            positive = injection, negative = ejection\n        node_b_net_injection: DataFrame with net injection at Node_B.\n            positive = injection, negative = ejection\n\n    Returns:\n        NetworkLineFlowsData instance with flows split into up/down directions\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; net_injection_a = pd.DataFrame({\n        ...     'power_a': [100, -49, -49],\n        ... })\n        &gt;&gt;&gt; net_injection_b = pd.DataFrame({\n        ...     'power_b': [-98, 50,  50]\n        ... })\n        &gt;&gt;&gt; line_data = NetworkLineFlowsData.from_nodal_net_injection(net_injection_a, net_injection_b)\n    \"\"\"\n\n    return cls(\n        sent_up=node_a_net_injection.clip(0),\n        received_up=-1 * node_b_net_injection.clip(None, 0),\n        sent_down=node_b_net_injection.clip(0),\n        received_down=-1 * node_a_net_injection.clip(None, 0)\n    )\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/network_lines_data/#mesqual.energy_data_handling.network_lines_data.NetworkLineFlowsData.from_up_and_down_flow_without_losses","title":"from_up_and_down_flow_without_losses  <code>classmethod</code>","text":"<pre><code>from_up_and_down_flow_without_losses(flow_up: DataFrame, flow_down: DataFrame) -&gt; NetworkLineFlowsData\n</code></pre> <p>Create NetworkLineFlowsData from separate up and down flow data without losses.</p> <p>This constructor assumes that there are no transmission losses, so sent and received flows are identical in each direction.</p> <p>Parameters:</p> Name Type Description Default <code>flow_up</code> <code>DataFrame</code> <p>DataFrame with flow data in up direction (node_from -&gt; node_to)</p> required <code>flow_down</code> <code>DataFrame</code> <p>DataFrame with flow data in down direction (node_to -&gt; node_from)</p> required <p>Returns:</p> Type Description <code>NetworkLineFlowsData</code> <p>NetworkLineFlowsData instance where sent and received flows are equal</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; up_flows = pd.DataFrame({'Line_A_B': [100, 80, 120]})\n&gt;&gt;&gt; down_flows = pd.DataFrame({'Line_A_B': [50, 60, 40]})\n&gt;&gt;&gt; line_data = NetworkLineFlowsData.from_up_and_down_flow_without_losses(\n...     up_flows, down_flows)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/network_lines_data.py</code> <pre><code>@classmethod\ndef from_up_and_down_flow_without_losses(\n        cls,\n        flow_up: pd.DataFrame,\n        flow_down: pd.DataFrame\n) -&gt; \"NetworkLineFlowsData\":\n    \"\"\"Create NetworkLineFlowsData from separate up and down flow data without losses.\n\n    This constructor assumes that there are no transmission losses, so sent and\n    received flows are identical in each direction.\n\n    Args:\n        flow_up: DataFrame with flow data in up direction (node_from -&gt; node_to)\n        flow_down: DataFrame with flow data in down direction (node_to -&gt; node_from)\n\n    Returns:\n        NetworkLineFlowsData instance where sent and received flows are equal\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; up_flows = pd.DataFrame({'Line_A_B': [100, 80, 120]})\n        &gt;&gt;&gt; down_flows = pd.DataFrame({'Line_A_B': [50, 60, 40]})\n        &gt;&gt;&gt; line_data = NetworkLineFlowsData.from_up_and_down_flow_without_losses(\n        ...     up_flows, down_flows)\n    \"\"\"\n    return cls(\n        sent_up=flow_up,\n        received_up=flow_up,\n        sent_down=flow_down,\n        received_down=flow_down\n    )\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/network_lines_data/#mesqual.energy_data_handling.network_lines_data.NetworkLineCapacitiesData","title":"NetworkLineCapacitiesData  <code>dataclass</code>","text":"<p>Wrapper for bidirectional capacity data of network transmission lines.</p> <p>This dataclass encapsulates transmission capacity limits for network lines in both directions. Capacities can be asymmetric to reflect real-world transmission constraints or operational limits.</p> Capacity Direction Conventions <ul> <li>capacities_up: Maximum transmission capacity from node_from to node_to</li> <li>capacities_down: Maximum transmission capacity from node_to to node_from</li> </ul> <p>Parameters:</p> Name Type Description Default <code>capacities_up</code> <code>DataFrame</code> <p>DataFrame with capacity limits in up direction</p> required <code>capacities_down</code> <code>DataFrame</code> <p>DataFrame with capacity limits in down direction  </p> required <code>granularity</code> <code>None | float | Series</code> <p>Time granularity of capacity data (None, float in minutes, or Series)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If indices or columns of the two DataFrames don't match</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=24, freq='1H')\n&gt;&gt;&gt; columns = ['Line_A_B', 'Line_B_C']\n&gt;&gt;&gt; caps = pd.DataFrame(1000, index=index, columns=columns)\n&gt;&gt;&gt; capacity_data = NetworkLineCapacitiesData.from_symmetric_capacities(caps)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/network_lines_data.py</code> <pre><code>@dataclass\nclass NetworkLineCapacitiesData:\n    \"\"\"Wrapper for bidirectional capacity data of network transmission lines.\n\n    This dataclass encapsulates transmission capacity limits for network lines in both\n    directions. Capacities can be asymmetric to reflect real-world transmission\n    constraints or operational limits.\n\n    Capacity Direction Conventions:\n        - capacities_up: Maximum transmission capacity from node_from to node_to\n        - capacities_down: Maximum transmission capacity from node_to to node_from\n\n    Args:\n        capacities_up: DataFrame with capacity limits in up direction\n        capacities_down: DataFrame with capacity limits in down direction  \n        granularity: Time granularity of capacity data (None, float in minutes, or Series)\n\n    Raises:\n        ValueError: If indices or columns of the two DataFrames don't match\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=24, freq='1H')\n        &gt;&gt;&gt; columns = ['Line_A_B', 'Line_B_C']\n        &gt;&gt;&gt; caps = pd.DataFrame(1000, index=index, columns=columns)\n        &gt;&gt;&gt; capacity_data = NetworkLineCapacitiesData.from_symmetric_capacities(caps)\n    \"\"\"\n    capacities_up: pd.DataFrame\n    capacities_down: pd.DataFrame\n    granularity: None | float | pd.Series = None\n\n    def __post_init__(self):\n        \"\"\"Validate that both capacity DataFrames have matching indices and columns.\n\n        Raises:\n            ValueError: If DataFrames have mismatched indices or columns.\n        \"\"\"\n        if not self.capacities_up.index.equals(self.capacities_down.index):\n            raise ValueError('Index mismatch: capacities_up vs capacities_down')\n        if not self.capacities_up.columns.equals(self.capacities_down.columns):\n            raise ValueError('Columns mismatch: capacities_up vs capacities_down')\n\n    def from_mw_to_mwh(self) -&gt; 'NetworkLineCapacitiesData':\n        \"\"\"Convert capacity data from MW (power) to MWh (energy).\n\n        This conversion requires granularity information to properly scale the values.\n\n        Returns:\n            New NetworkLineCapacitiesData instance with energy capacity values\n\n        Raises:\n            NotImplementedError: Method not yet implemented\n        \"\"\"\n        # TODO: Implement MW to MWh conversion using granularity\n        raise NotImplementedError(\"MW to MWh conversion not yet implemented\")\n\n    def from_mwh_to_mw(self) -&gt; 'NetworkLineCapacitiesData':\n        \"\"\"Convert capacity data from MWh (energy) to MW (power).\n\n        This conversion requires granularity information to properly scale the values.\n\n        Returns:\n            New NetworkLineCapacitiesData instance with power capacity values\n\n        Raises:\n            NotImplementedError: Method not yet implemented\n        \"\"\"\n        # TODO: Implement MWh to MW conversion using granularity\n        raise NotImplementedError(\"MWh to MW conversion not yet implemented\")\n\n    @classmethod\n    def from_symmetric_capacities(cls, capacities: pd.DataFrame) -&gt; \"NetworkLineCapacitiesData\":\n        \"\"\"Create NetworkLineCapacitiesData with identical capacities in both directions.\n\n        This is a convenience constructor for cases where transmission lines have\n        the same capacity limit in both directions.\n\n        Args:\n            capacities: DataFrame with capacity values to use for both directions\n\n        Returns:\n            NetworkLineCapacitiesData instance with symmetric capacities\n\n        Example:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; caps = pd.DataFrame({\n            ...     'Line_A_B': [1000, 1200, 800],\n            ...     'Line_B_C': [1500, 1500, 1000]\n            ... })\n            &gt;&gt;&gt; capacity_data = NetworkLineCapacitiesData.from_symmetric_capacities(caps)\n        \"\"\"\n        return cls(capacities_up=capacities, capacities_down=capacities)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/network_lines_data/#mesqual.energy_data_handling.network_lines_data.NetworkLineCapacitiesData.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate that both capacity DataFrames have matching indices and columns.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If DataFrames have mismatched indices or columns.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/network_lines_data.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate that both capacity DataFrames have matching indices and columns.\n\n    Raises:\n        ValueError: If DataFrames have mismatched indices or columns.\n    \"\"\"\n    if not self.capacities_up.index.equals(self.capacities_down.index):\n        raise ValueError('Index mismatch: capacities_up vs capacities_down')\n    if not self.capacities_up.columns.equals(self.capacities_down.columns):\n        raise ValueError('Columns mismatch: capacities_up vs capacities_down')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/network_lines_data/#mesqual.energy_data_handling.network_lines_data.NetworkLineCapacitiesData.from_mw_to_mwh","title":"from_mw_to_mwh","text":"<pre><code>from_mw_to_mwh() -&gt; NetworkLineCapacitiesData\n</code></pre> <p>Convert capacity data from MW (power) to MWh (energy).</p> <p>This conversion requires granularity information to properly scale the values.</p> <p>Returns:</p> Type Description <code>NetworkLineCapacitiesData</code> <p>New NetworkLineCapacitiesData instance with energy capacity values</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Method not yet implemented</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/network_lines_data.py</code> <pre><code>def from_mw_to_mwh(self) -&gt; 'NetworkLineCapacitiesData':\n    \"\"\"Convert capacity data from MW (power) to MWh (energy).\n\n    This conversion requires granularity information to properly scale the values.\n\n    Returns:\n        New NetworkLineCapacitiesData instance with energy capacity values\n\n    Raises:\n        NotImplementedError: Method not yet implemented\n    \"\"\"\n    # TODO: Implement MW to MWh conversion using granularity\n    raise NotImplementedError(\"MW to MWh conversion not yet implemented\")\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/network_lines_data/#mesqual.energy_data_handling.network_lines_data.NetworkLineCapacitiesData.from_mwh_to_mw","title":"from_mwh_to_mw","text":"<pre><code>from_mwh_to_mw() -&gt; NetworkLineCapacitiesData\n</code></pre> <p>Convert capacity data from MWh (energy) to MW (power).</p> <p>This conversion requires granularity information to properly scale the values.</p> <p>Returns:</p> Type Description <code>NetworkLineCapacitiesData</code> <p>New NetworkLineCapacitiesData instance with power capacity values</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Method not yet implemented</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/network_lines_data.py</code> <pre><code>def from_mwh_to_mw(self) -&gt; 'NetworkLineCapacitiesData':\n    \"\"\"Convert capacity data from MWh (energy) to MW (power).\n\n    This conversion requires granularity information to properly scale the values.\n\n    Returns:\n        New NetworkLineCapacitiesData instance with power capacity values\n\n    Raises:\n        NotImplementedError: Method not yet implemented\n    \"\"\"\n    # TODO: Implement MWh to MW conversion using granularity\n    raise NotImplementedError(\"MWh to MW conversion not yet implemented\")\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/network_lines_data/#mesqual.energy_data_handling.network_lines_data.NetworkLineCapacitiesData.from_symmetric_capacities","title":"from_symmetric_capacities  <code>classmethod</code>","text":"<pre><code>from_symmetric_capacities(capacities: DataFrame) -&gt; NetworkLineCapacitiesData\n</code></pre> <p>Create NetworkLineCapacitiesData with identical capacities in both directions.</p> <p>This is a convenience constructor for cases where transmission lines have the same capacity limit in both directions.</p> <p>Parameters:</p> Name Type Description Default <code>capacities</code> <code>DataFrame</code> <p>DataFrame with capacity values to use for both directions</p> required <p>Returns:</p> Type Description <code>NetworkLineCapacitiesData</code> <p>NetworkLineCapacitiesData instance with symmetric capacities</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; caps = pd.DataFrame({\n...     'Line_A_B': [1000, 1200, 800],\n...     'Line_B_C': [1500, 1500, 1000]\n... })\n&gt;&gt;&gt; capacity_data = NetworkLineCapacitiesData.from_symmetric_capacities(caps)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/network_lines_data.py</code> <pre><code>@classmethod\ndef from_symmetric_capacities(cls, capacities: pd.DataFrame) -&gt; \"NetworkLineCapacitiesData\":\n    \"\"\"Create NetworkLineCapacitiesData with identical capacities in both directions.\n\n    This is a convenience constructor for cases where transmission lines have\n    the same capacity limit in both directions.\n\n    Args:\n        capacities: DataFrame with capacity values to use for both directions\n\n    Returns:\n        NetworkLineCapacitiesData instance with symmetric capacities\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; caps = pd.DataFrame({\n        ...     'Line_A_B': [1000, 1200, 800],\n        ...     'Line_B_C': [1500, 1500, 1000]\n        ... })\n        &gt;&gt;&gt; capacity_data = NetworkLineCapacitiesData.from_symmetric_capacities(caps)\n    \"\"\"\n    return cls(capacities_up=capacities, capacities_down=capacities)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/time_series_gap_handling/","title":"MESQUAL Time Series Gap Handling","text":""},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/time_series_gap_handling/#mesqual.energy_data_handling.time_series_gap_handling.TimeSeriesGapHandler","title":"TimeSeriesGapHandler","text":"<p>Handles gaps in time series data by inserting NaN values at detected gaps.</p> <p>This class is useful for identifying and marking gaps in time series data that exceed a specified threshold. The handler inserts NaN values at the beginning of detected gaps to make gaps visible for downstream processing. This is particularly useful for line plots, where you often prefer to have a visual line-gap in a data gap instead of the line bridging the two surrounding values.</p> <p>Parameters:</p> Name Type Description Default <code>max_gap_in_minutes</code> <code>float</code> <p>Maximum allowed gap duration in minutes. Gaps longer than this threshold will have NaN values inserted. Default is 60 minutes.</p> <code>60</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=5, freq='30min')\n&gt;&gt;&gt; dates = dates.delete(2)  # Create a gap\n&gt;&gt;&gt; values = np.random.rand(len(dates))\n&gt;&gt;&gt; series = pd.Series(values, index=dates)\n&gt;&gt;&gt; handler = TimeSeriesGapHandler(max_gap_in_minutes=30)\n&gt;&gt;&gt; result = handler.insert_nans_at_gaps(series)\n&gt;&gt;&gt; print(result)  # Will show NaN inserted at gap location\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/time_series_gap_handling.py</code> <pre><code>class TimeSeriesGapHandler:\n    \"\"\"Handles gaps in time series data by inserting NaN values at detected gaps.\n\n    This class is useful for identifying and marking gaps in time series data that exceed\n    a specified threshold. The handler inserts NaN values at the beginning of detected gaps\n    to make gaps visible for downstream processing. This is particularly useful for line plots,\n    where you often prefer to have a visual line-gap in a data gap instead of the line bridging\n    the two surrounding values.\n\n    Args:\n        max_gap_in_minutes: Maximum allowed gap duration in minutes. Gaps longer than\n            this threshold will have NaN values inserted. Default is 60 minutes.\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=5, freq='30min')\n        &gt;&gt;&gt; dates = dates.delete(2)  # Create a gap\n        &gt;&gt;&gt; values = np.random.rand(len(dates))\n        &gt;&gt;&gt; series = pd.Series(values, index=dates)\n        &gt;&gt;&gt; handler = TimeSeriesGapHandler(max_gap_in_minutes=30)\n        &gt;&gt;&gt; result = handler.insert_nans_at_gaps(series)\n        &gt;&gt;&gt; print(result)  # Will show NaN inserted at gap location\n    \"\"\"\n\n    def __init__(self, max_gap_in_minutes: float = 60):\n        \"\"\"Initialize the gap handler with specified maximum gap threshold.\n\n        Args:\n            max_gap_in_minutes: Maximum allowed gap duration in minutes before\n                inserting NaN markers.\n        \"\"\"\n        self._max_gap_in_minutes = max_gap_in_minutes\n\n    def insert_nans_at_gaps(self, data: pd.Series | pd.DataFrame) -&gt; pd.Series | pd.DataFrame:\n        \"\"\"Insert NaN values at the beginning of gaps that exceed the maximum threshold.\n\n        This method identifies time gaps in the data that are longer than the configured\n        maximum gap duration and inserts NaN values at the start of these gaps. This\n        approach ensures that gaps are explicitly marked in the data rather than being\n        silently filled by interpolation methods.\n\n        Args:\n            data: Time series data with DatetimeIndex. Can be either a Series or DataFrame.\n\n        Returns:\n            Same type as input with NaN values inserted at gap locations. The returned\n            data will be sorted by index.\n\n        Raises:\n            TypeError: If data index is not a DatetimeIndex.\n\n        Example:\n\n            &gt;&gt;&gt; dates = pd.date_range('2024-01-01', freq='1H', periods=5)\n            &gt;&gt;&gt; dates = dates.delete([2, 3])  # Create 2-hour gap\n            &gt;&gt;&gt; series = pd.Series([1, 2, 3], index=dates[[0, 1, 4]])\n            &gt;&gt;&gt; handler = TimeSeriesGapHandler(max_gap_in_minutes=90)\n            &gt;&gt;&gt; result = handler.insert_nans_at_gaps(series)\n            &gt;&gt;&gt; print(result)  # Shows NaN inserted at gap start\n        \"\"\"\n        if not isinstance(data.index, pd.DatetimeIndex):\n            raise TypeError(f\"Data index must be DatetimeIndex, got {type(data.index)}\")\n\n        diffs = data.index.to_series().diff()\n        mask = diffs &gt; pd.Timedelta(minutes=self._max_gap_in_minutes)\n        gap_indices = np.where(mask)[0]\n\n        if len(gap_indices) == 0:\n            return data  # No gaps found\n\n        new_timestamps = data.index[gap_indices - 1] + pd.Timedelta(minutes=self._max_gap_in_minutes)\n\n        if isinstance(data, pd.Series):\n            new_values = pd.Series(np.nan, index=new_timestamps, name=data.name)\n        else:\n            new_values = pd.DataFrame(np.nan, index=new_timestamps, columns=data.columns)\n\n        return pd.concat([data, new_values]).sort_index()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/time_series_gap_handling/#mesqual.energy_data_handling.time_series_gap_handling.TimeSeriesGapHandler.__init__","title":"__init__","text":"<pre><code>__init__(max_gap_in_minutes: float = 60)\n</code></pre> <p>Initialize the gap handler with specified maximum gap threshold.</p> <p>Parameters:</p> Name Type Description Default <code>max_gap_in_minutes</code> <code>float</code> <p>Maximum allowed gap duration in minutes before inserting NaN markers.</p> <code>60</code> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/time_series_gap_handling.py</code> <pre><code>def __init__(self, max_gap_in_minutes: float = 60):\n    \"\"\"Initialize the gap handler with specified maximum gap threshold.\n\n    Args:\n        max_gap_in_minutes: Maximum allowed gap duration in minutes before\n            inserting NaN markers.\n    \"\"\"\n    self._max_gap_in_minutes = max_gap_in_minutes\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/time_series_gap_handling/#mesqual.energy_data_handling.time_series_gap_handling.TimeSeriesGapHandler.insert_nans_at_gaps","title":"insert_nans_at_gaps","text":"<pre><code>insert_nans_at_gaps(data: Series | DataFrame) -&gt; Series | DataFrame\n</code></pre> <p>Insert NaN values at the beginning of gaps that exceed the maximum threshold.</p> <p>This method identifies time gaps in the data that are longer than the configured maximum gap duration and inserts NaN values at the start of these gaps. This approach ensures that gaps are explicitly marked in the data rather than being silently filled by interpolation methods.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series | DataFrame</code> <p>Time series data with DatetimeIndex. Can be either a Series or DataFrame.</p> required <p>Returns:</p> Type Description <code>Series | DataFrame</code> <p>Same type as input with NaN values inserted at gap locations. The returned</p> <code>Series | DataFrame</code> <p>data will be sorted by index.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If data index is not a DatetimeIndex.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; dates = pd.date_range('2024-01-01', freq='1H', periods=5)\n&gt;&gt;&gt; dates = dates.delete([2, 3])  # Create 2-hour gap\n&gt;&gt;&gt; series = pd.Series([1, 2, 3], index=dates[[0, 1, 4]])\n&gt;&gt;&gt; handler = TimeSeriesGapHandler(max_gap_in_minutes=90)\n&gt;&gt;&gt; result = handler.insert_nans_at_gaps(series)\n&gt;&gt;&gt; print(result)  # Shows NaN inserted at gap start\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/time_series_gap_handling.py</code> <pre><code>def insert_nans_at_gaps(self, data: pd.Series | pd.DataFrame) -&gt; pd.Series | pd.DataFrame:\n    \"\"\"Insert NaN values at the beginning of gaps that exceed the maximum threshold.\n\n    This method identifies time gaps in the data that are longer than the configured\n    maximum gap duration and inserts NaN values at the start of these gaps. This\n    approach ensures that gaps are explicitly marked in the data rather than being\n    silently filled by interpolation methods.\n\n    Args:\n        data: Time series data with DatetimeIndex. Can be either a Series or DataFrame.\n\n    Returns:\n        Same type as input with NaN values inserted at gap locations. The returned\n        data will be sorted by index.\n\n    Raises:\n        TypeError: If data index is not a DatetimeIndex.\n\n    Example:\n\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', freq='1H', periods=5)\n        &gt;&gt;&gt; dates = dates.delete([2, 3])  # Create 2-hour gap\n        &gt;&gt;&gt; series = pd.Series([1, 2, 3], index=dates[[0, 1, 4]])\n        &gt;&gt;&gt; handler = TimeSeriesGapHandler(max_gap_in_minutes=90)\n        &gt;&gt;&gt; result = handler.insert_nans_at_gaps(series)\n        &gt;&gt;&gt; print(result)  # Shows NaN inserted at gap start\n    \"\"\"\n    if not isinstance(data.index, pd.DatetimeIndex):\n        raise TypeError(f\"Data index must be DatetimeIndex, got {type(data.index)}\")\n\n    diffs = data.index.to_series().diff()\n    mask = diffs &gt; pd.Timedelta(minutes=self._max_gap_in_minutes)\n    gap_indices = np.where(mask)[0]\n\n    if len(gap_indices) == 0:\n        return data  # No gaps found\n\n    new_timestamps = data.index[gap_indices - 1] + pd.Timedelta(minutes=self._max_gap_in_minutes)\n\n    if isinstance(data, pd.Series):\n        new_values = pd.Series(np.nan, index=new_timestamps, name=data.name)\n    else:\n        new_values = pd.DataFrame(np.nan, index=new_timestamps, columns=data.columns)\n\n    return pd.concat([data, new_values]).sort_index()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/","title":"MESQUAL Area and AreaBorder Accounting","text":""},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/#mesqual.energy_data_handling.area_accounting","title":"area_accounting","text":"<p>MESQUAL Area and Area Border Accounting Package</p> <p>Tools for transforming node-line topology into area-based models and variables.  Supports multi-level aggregation (countries, bidding zones, market regions) and  cross-border flow analysis for energy market studies.</p>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/#mesqual.energy_data_handling.area_accounting--core-components","title":"Core Components:","text":"Model Generators <ul> <li>Area model creation from node-to-area mappings with geographic visualization</li> <li>Border identification from transmission topology with standardized naming</li> <li>Network graph generation and geometric analysis for spatial representation</li> </ul> Variable Calculators <ul> <li>Area variables: Price aggregation (volume-weighted) and sum-based calculations</li> <li>Border variables: Cross-border flows, capacity aggregation, and price spreads</li> <li>Flexible node-to-area mapping with time series support</li> </ul>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/","title":"Area Variable Accounting","text":""},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mesqual.energy_data_handling.area_accounting.area_variable_price_calculator.AreaPriceCalculator","title":"AreaPriceCalculator","text":"<p>               Bases: <code>AreaVariableCalculatorBase</code></p> <p>Calculates area-level prices from node prices using simple or weighted averaging.</p> <p>This calculator aggregates node-level electricity prices to area-level (e.g., bidding zones, countries) using either simple averaging or weighted averaging based on demand, supply, or other energy quantities. It's particularly useful in energy market analysis where different regions may have multiple price nodes that need to be consolidated into representative area prices.</p> <p>The class inherits from AreaVariableCalculatorBase and provides energy-aware price aggregation that handles edge cases like zero weights and missing data appropriately.</p> <p>Typical use cases: - Aggregating nodal prices to bidding zone prices - Creating country-level price indices from multiple market nodes - Volume-weighted price calculations for regional analysis</p> <p>Parameters:</p> Name Type Description Default <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame with node-area mappings</p> required <code>area_column</code> <code>str</code> <p>Column name containing area identifiers</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Node model with area mapping\n&gt;&gt;&gt; node_model = pd.DataFrame({\n...     'bidding_zone': ['DE_LU', 'DE_LU', 'FR', 'FR']\n... }, index=['DE1', 'DE2', 'FR1', 'FR2'])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Price calculator\n&gt;&gt;&gt; calc = AreaPriceCalculator(node_model, 'bidding_zone')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Node prices\n&gt;&gt;&gt; prices = pd.DataFrame({\n...     'DE1': [50.0, 45.0], 'DE2': [52.0, 47.0],\n...     'FR1': [55.0, 48.0], 'FR2': [53.0, 46.0]\n... }, index=pd.date_range('2024-01-01', periods=2, freq='h'))\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple average\n&gt;&gt;&gt; area_prices = calc.calculate(prices)\n&gt;&gt;&gt; print(area_prices)\n           bidding_zone  DE_LU FR\n    datetime\n    2024-01-01 00:00:00  51.0  54.0\n    2024-01-01 01:00:00  46.0  47.0\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_variable_price_calculator.py</code> <pre><code>class AreaPriceCalculator(AreaVariableCalculatorBase):\n    \"\"\"Calculates area-level prices from node prices using simple or weighted averaging.\n\n    This calculator aggregates node-level electricity prices to area-level (e.g., bidding zones,\n    countries) using either simple averaging or weighted averaging based on demand, supply, or\n    other energy quantities. It's particularly useful in energy market analysis where different\n    regions may have multiple price nodes that need to be consolidated into representative area\n    prices.\n\n    The class inherits from AreaVariableCalculatorBase and provides energy-aware price aggregation\n    that handles edge cases like zero weights and missing data appropriately.\n\n    Typical use cases:\n    - Aggregating nodal prices to bidding zone prices\n    - Creating country-level price indices from multiple market nodes\n    - Volume-weighted price calculations for regional analysis\n\n    Args:\n        node_model_df: DataFrame with node-area mappings\n        area_column: Column name containing area identifiers\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Node model with area mapping\n        &gt;&gt;&gt; node_model = pd.DataFrame({\n        ...     'bidding_zone': ['DE_LU', 'DE_LU', 'FR', 'FR']\n        ... }, index=['DE1', 'DE2', 'FR1', 'FR2'])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Price calculator\n        &gt;&gt;&gt; calc = AreaPriceCalculator(node_model, 'bidding_zone')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Node prices\n        &gt;&gt;&gt; prices = pd.DataFrame({\n        ...     'DE1': [50.0, 45.0], 'DE2': [52.0, 47.0],\n        ...     'FR1': [55.0, 48.0], 'FR2': [53.0, 46.0]\n        ... }, index=pd.date_range('2024-01-01', periods=2, freq='h'))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Simple average\n        &gt;&gt;&gt; area_prices = calc.calculate(prices)\n        &gt;&gt;&gt; print(area_prices)\n                   bidding_zone  DE_LU FR\n            datetime\n            2024-01-01 00:00:00  51.0  54.0\n            2024-01-01 01:00:00  46.0  47.0\n    \"\"\"\n\n    def calculate(\n        self,\n        node_price_df: pd.DataFrame,\n        weighting_factor_df: pd.DataFrame = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Calculate area prices with different weighting options.\n\n        Aggregates node-level prices to area-level using simple averaging (when no weights\n        provided) or weighted averaging (when weights provided). The method handles missing\n        nodes gracefully and ensures proper handling of zero weights and NaN values.\n\n        In case you want to exclude certain nodes from the aggregation (e.g. because they\n        are virtual or synthetic nodes), you can simply remove them from the node_price_df\n        before passing it to this method.\n\n        Args:\n            node_price_df: Node-level price time series with datetime index and node columns.\n                Values represent electricity prices in \u20ac/MWh or similar units.\n            weighting_factor_df: Optional weighting factor DataFrame with same structure as\n                node_price_df. Common weighting factors include:\n                - node_demand_df: Demand-weighted prices\n                - node_supply_df: Supply-weighted prices  \n                - node_capacity_df: Capacity-weighted prices\n                If None, simple arithmetic average is used.\n\n        Returns:\n            DataFrame with area-level prices. Index matches input time series, columns\n            represent areas with prices in same units as input.\n\n        Raises:\n            ValueError: If node_price_df structure is invalid\n            KeyError: If required nodes are missing from weighting_factor_df\n\n        Example:\n\n            &gt;&gt;&gt; # Simple average\n            &gt;&gt;&gt; area_prices = calc.calculate(node_prices)\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Demand-weighted average  \n            &gt;&gt;&gt; weighted_prices = calc.calculate(node_prices, node_demand)\n        \"\"\"\n        self._validate_node_data(node_price_df, 'node_price_df')\n\n        area_prices = {}\n\n        for area in self.areas:\n            area_nodes = self.get_area_nodes(area)\n            area_nodes = [n for n in area_nodes if n in node_price_df.columns]\n\n            if not area_nodes:\n                continue\n\n            prices = node_price_df[area_nodes]\n\n            if weighting_factor_df is None:\n                area_prices[area] = self._calculate_simple_average(prices)\n            else:\n                self._validate_node_data(weighting_factor_df, 'weighting_factor_df')\n                area_prices[area] = self._calculate_weighted_average(\n                    prices, weighting_factor_df[area_nodes]\n                )\n\n        result = pd.DataFrame(area_prices)\n        result.columns.name = self.area_column\n        return result\n\n    def _calculate_simple_average(self, prices: pd.DataFrame) -&gt; pd.Series:\n        return prices.mean(axis=1)\n\n    def _calculate_weighted_average(\n        self, \n        prices: pd.DataFrame, \n        weights: pd.DataFrame\n    ) -&gt; pd.Series:\n        \"\"\"Calculate weighted average of prices using provided weights.\n\n        Computes volume-weighted or otherwise weighted prices while handling edge cases\n        appropriately. When weights sum to zero, the method defaults to weight of 1 to\n        avoid division errors. When all prices are NaN for a time period, the result\n        is also NaN.\n\n        Args:\n            prices: DataFrame with price time series for nodes in an area\n            weights: DataFrame with weighting factors (e.g., demand, supply) with same\n                structure as prices. Must have non-negative values.\n\n        Returns:\n            Series with weighted average prices over time\n\n        Note:\n            This method assumes weights are extensive quantities (like energy volumes)\n            while prices are intensive quantities (like \u20ac/MWh).\n        \"\"\"\n        weighted_sum = (prices * weights).sum(axis=1)\n        weight_sum = weights.sum(axis=1).replace(0, 1)\n        weighted_price = weighted_sum / weight_sum\n        weighted_price[prices.isna().all(axis=1)] = np.nan\n        return weighted_price\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mesqual.energy_data_handling.area_accounting.area_variable_price_calculator.AreaPriceCalculator.calculate","title":"calculate","text":"<pre><code>calculate(node_price_df: DataFrame, weighting_factor_df: DataFrame = None) -&gt; DataFrame\n</code></pre> <p>Calculate area prices with different weighting options.</p> <p>Aggregates node-level prices to area-level using simple averaging (when no weights provided) or weighted averaging (when weights provided). The method handles missing nodes gracefully and ensures proper handling of zero weights and NaN values.</p> <p>In case you want to exclude certain nodes from the aggregation (e.g. because they are virtual or synthetic nodes), you can simply remove them from the node_price_df before passing it to this method.</p> <p>Parameters:</p> Name Type Description Default <code>node_price_df</code> <code>DataFrame</code> <p>Node-level price time series with datetime index and node columns. Values represent electricity prices in \u20ac/MWh or similar units.</p> required <code>weighting_factor_df</code> <code>DataFrame</code> <p>Optional weighting factor DataFrame with same structure as node_price_df. Common weighting factors include: - node_demand_df: Demand-weighted prices - node_supply_df: Supply-weighted prices - node_capacity_df: Capacity-weighted prices If None, simple arithmetic average is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with area-level prices. Index matches input time series, columns</p> <code>DataFrame</code> <p>represent areas with prices in same units as input.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If node_price_df structure is invalid</p> <code>KeyError</code> <p>If required nodes are missing from weighting_factor_df</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Simple average\n&gt;&gt;&gt; area_prices = calc.calculate(node_prices)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Demand-weighted average  \n&gt;&gt;&gt; weighted_prices = calc.calculate(node_prices, node_demand)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_variable_price_calculator.py</code> <pre><code>def calculate(\n    self,\n    node_price_df: pd.DataFrame,\n    weighting_factor_df: pd.DataFrame = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate area prices with different weighting options.\n\n    Aggregates node-level prices to area-level using simple averaging (when no weights\n    provided) or weighted averaging (when weights provided). The method handles missing\n    nodes gracefully and ensures proper handling of zero weights and NaN values.\n\n    In case you want to exclude certain nodes from the aggregation (e.g. because they\n    are virtual or synthetic nodes), you can simply remove them from the node_price_df\n    before passing it to this method.\n\n    Args:\n        node_price_df: Node-level price time series with datetime index and node columns.\n            Values represent electricity prices in \u20ac/MWh or similar units.\n        weighting_factor_df: Optional weighting factor DataFrame with same structure as\n            node_price_df. Common weighting factors include:\n            - node_demand_df: Demand-weighted prices\n            - node_supply_df: Supply-weighted prices  \n            - node_capacity_df: Capacity-weighted prices\n            If None, simple arithmetic average is used.\n\n    Returns:\n        DataFrame with area-level prices. Index matches input time series, columns\n        represent areas with prices in same units as input.\n\n    Raises:\n        ValueError: If node_price_df structure is invalid\n        KeyError: If required nodes are missing from weighting_factor_df\n\n    Example:\n\n        &gt;&gt;&gt; # Simple average\n        &gt;&gt;&gt; area_prices = calc.calculate(node_prices)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Demand-weighted average  \n        &gt;&gt;&gt; weighted_prices = calc.calculate(node_prices, node_demand)\n    \"\"\"\n    self._validate_node_data(node_price_df, 'node_price_df')\n\n    area_prices = {}\n\n    for area in self.areas:\n        area_nodes = self.get_area_nodes(area)\n        area_nodes = [n for n in area_nodes if n in node_price_df.columns]\n\n        if not area_nodes:\n            continue\n\n        prices = node_price_df[area_nodes]\n\n        if weighting_factor_df is None:\n            area_prices[area] = self._calculate_simple_average(prices)\n        else:\n            self._validate_node_data(weighting_factor_df, 'weighting_factor_df')\n            area_prices[area] = self._calculate_weighted_average(\n                prices, weighting_factor_df[area_nodes]\n            )\n\n    result = pd.DataFrame(area_prices)\n    result.columns.name = self.area_column\n    return result\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mesqual.energy_data_handling.area_accounting.area_variable_sum_calculator.AreaSumCalculator","title":"AreaSumCalculator","text":"<p>               Bases: <code>AreaVariableCalculatorBase</code></p> <p>General calculator for summing node-level extensive quantities to area level.</p> <p>This calculator aggregates extensive quantities (values that scale with system size) from node-level to area-level using summation. Typical use cases include power generation, demand, energy volumes, reserves, and other additive quantities in energy systems analysis.</p> <p>Unlike intensive quantities (like prices), extensive quantities should be summed when aggregating to higher geographic levels, making this calculator appropriate for many physical quantities in energy modeling.</p> <p>Inherits from AreaVariableCalculatorBase and provides the MESQUAL framework's standard approach for area-level aggregation of extensive variables.</p> <p>Parameters:</p> Name Type Description Default <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame mapping nodes to areas</p> required <code>area_column</code> <code>str</code> <p>Column name containing area identifiers</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Node model\n&gt;&gt;&gt; node_model = pd.DataFrame({\n...     'bidding_zone': ['DE_LU', 'DE_LU', 'FR', 'FR']\n... }, index=['DE1', 'DE2', 'FR1', 'FR2'])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Sum calculator\n&gt;&gt;&gt; calc = AreaSumCalculator(node_model, 'bidding_zone')\n&gt;&gt;&gt; # Node generation data\n&gt;&gt;&gt; generation = pd.DataFrame({\n...     'DE1': [800, 850], 'DE2': [750, 780],\n...     'FR1': [900, 920], 'FR2': [850, 870]\n... }, index=pd.date_range('2024-01-01', periods=2, freq='h'))\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Sum to areas\n&gt;&gt;&gt; area_generation = calc.calculate(generation)\n&gt;&gt;&gt; print(area_generation)\n    bidding_zone  DE_LU   FR\n    2024-01-01 00:00:00  1550  1750\n    2024-01-01 01:00:00  1630  1790\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_variable_sum_calculator.py</code> <pre><code>class AreaSumCalculator(AreaVariableCalculatorBase):\n    \"\"\"General calculator for summing node-level extensive quantities to area level.\n\n    This calculator aggregates extensive quantities (values that scale with system size)\n    from node-level to area-level using summation. Typical use cases include power\n    generation, demand, energy volumes, reserves, and other additive quantities in\n    energy systems analysis.\n\n    Unlike intensive quantities (like prices), extensive quantities should be summed\n    when aggregating to higher geographic levels, making this calculator appropriate\n    for many physical quantities in energy modeling.\n\n    Inherits from AreaVariableCalculatorBase and provides the MESQUAL framework's\n    standard approach for area-level aggregation of extensive variables.\n\n    Args:\n        node_model_df: DataFrame mapping nodes to areas\n        area_column: Column name containing area identifiers\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Node model\n        &gt;&gt;&gt; node_model = pd.DataFrame({\n        ...     'bidding_zone': ['DE_LU', 'DE_LU', 'FR', 'FR']\n        ... }, index=['DE1', 'DE2', 'FR1', 'FR2'])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Sum calculator\n        &gt;&gt;&gt; calc = AreaSumCalculator(node_model, 'bidding_zone')\n        &gt;&gt;&gt; # Node generation data\n        &gt;&gt;&gt; generation = pd.DataFrame({\n        ...     'DE1': [800, 850], 'DE2': [750, 780],\n        ...     'FR1': [900, 920], 'FR2': [850, 870]\n        ... }, index=pd.date_range('2024-01-01', periods=2, freq='h'))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Sum to areas\n        &gt;&gt;&gt; area_generation = calc.calculate(generation)\n        &gt;&gt;&gt; print(area_generation)\n            bidding_zone  DE_LU   FR\n            2024-01-01 00:00:00  1550  1750\n            2024-01-01 01:00:00  1630  1790\n    \"\"\"\n\n    def calculate(self, node_data_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Calculate area sums from node-level extensive quantity data.\n\n        Sums node-level values within each area to create area-level aggregates.\n        This method is designed for extensive quantities where summation is the\n        appropriate aggregation method (e.g., generation, demand, volumes).\n\n        Missing nodes are handled gracefully - if a node exists in the node model\n        but not in the data, it's simply ignored. Areas with no available nodes\n        are omitted from the output.\n\n        In case you want to exclude certain nodes from the aggregation (e.g. because\n        they are virtual or synthetic nodes), you can simply remove them from the\n        node_data_df before passing it to this method.\n\n        Args:\n            node_data_df: DataFrame with node-level time series data. Index should\n                be datetime, columns should be node identifiers. Values represent\n                extensive quantities (MW, MWh, etc.) that should be summed.\n\n        Returns:\n            DataFrame with area-level aggregated data. Index matches input time series,\n            columns represent areas. Units are preserved from input data.\n\n        Raises:\n            ValueError: If node_data_df structure is invalid\n\n        Example:\n\n            &gt;&gt;&gt; # Sum generation across nodes\n            &gt;&gt;&gt; area_generation = calc.calculate(node_generation_df)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Sum demand across nodes  \n            &gt;&gt;&gt; area_demand = calc.calculate(node_demand_df)\n        \"\"\"\n\n        self._validate_node_data(node_data_df, 'node_data_df')\n\n        area_sums = {}\n        for area in self.areas:\n            area_nodes = self.get_area_nodes(area)\n            area_nodes = [n for n in area_nodes if n in node_data_df.columns]\n            if area_nodes:\n                area_sums[area] = node_data_df[area_nodes].sum(axis=1)\n\n        result = pd.DataFrame(area_sums)\n        result.columns.name = self.area_column\n        return result\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mesqual.energy_data_handling.area_accounting.area_variable_sum_calculator.AreaSumCalculator.calculate","title":"calculate","text":"<pre><code>calculate(node_data_df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Calculate area sums from node-level extensive quantity data.</p> <p>Sums node-level values within each area to create area-level aggregates. This method is designed for extensive quantities where summation is the appropriate aggregation method (e.g., generation, demand, volumes).</p> <p>Missing nodes are handled gracefully - if a node exists in the node model but not in the data, it's simply ignored. Areas with no available nodes are omitted from the output.</p> <p>In case you want to exclude certain nodes from the aggregation (e.g. because they are virtual or synthetic nodes), you can simply remove them from the node_data_df before passing it to this method.</p> <p>Parameters:</p> Name Type Description Default <code>node_data_df</code> <code>DataFrame</code> <p>DataFrame with node-level time series data. Index should be datetime, columns should be node identifiers. Values represent extensive quantities (MW, MWh, etc.) that should be summed.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with area-level aggregated data. Index matches input time series,</p> <code>DataFrame</code> <p>columns represent areas. Units are preserved from input data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If node_data_df structure is invalid</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Sum generation across nodes\n&gt;&gt;&gt; area_generation = calc.calculate(node_generation_df)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Sum demand across nodes  \n&gt;&gt;&gt; area_demand = calc.calculate(node_demand_df)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_variable_sum_calculator.py</code> <pre><code>def calculate(self, node_data_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Calculate area sums from node-level extensive quantity data.\n\n    Sums node-level values within each area to create area-level aggregates.\n    This method is designed for extensive quantities where summation is the\n    appropriate aggregation method (e.g., generation, demand, volumes).\n\n    Missing nodes are handled gracefully - if a node exists in the node model\n    but not in the data, it's simply ignored. Areas with no available nodes\n    are omitted from the output.\n\n    In case you want to exclude certain nodes from the aggregation (e.g. because\n    they are virtual or synthetic nodes), you can simply remove them from the\n    node_data_df before passing it to this method.\n\n    Args:\n        node_data_df: DataFrame with node-level time series data. Index should\n            be datetime, columns should be node identifiers. Values represent\n            extensive quantities (MW, MWh, etc.) that should be summed.\n\n    Returns:\n        DataFrame with area-level aggregated data. Index matches input time series,\n        columns represent areas. Units are preserved from input data.\n\n    Raises:\n        ValueError: If node_data_df structure is invalid\n\n    Example:\n\n        &gt;&gt;&gt; # Sum generation across nodes\n        &gt;&gt;&gt; area_generation = calc.calculate(node_generation_df)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Sum demand across nodes  \n        &gt;&gt;&gt; area_demand = calc.calculate(node_demand_df)\n    \"\"\"\n\n    self._validate_node_data(node_data_df, 'node_data_df')\n\n    area_sums = {}\n    for area in self.areas:\n        area_nodes = self.get_area_nodes(area)\n        area_nodes = [n for n in area_nodes if n in node_data_df.columns]\n        if area_nodes:\n            area_sums[area] = node_data_df[area_nodes].sum(axis=1)\n\n    result = pd.DataFrame(area_sums)\n    result.columns.name = self.area_column\n    return result\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mesqual.energy_data_handling.area_accounting.area_variable_base.AreaVariableCalculatorBase","title":"AreaVariableCalculatorBase","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for calculating energy variables aggregated at area level.</p> <p>This base class provides common functionality for aggregating node-level energy data (such as generation, demand, prices) to higher-level areas (countries, bidding zones, market areas). It handles the mapping between nodes and areas and provides validation and utility methods for area-based calculations.</p> <p>The class is designed to be subclassed for specific variable types, with each subclass implementing its own calculation logic while leveraging the common area mapping and validation functionality provided here.</p> <p>Energy market context: In electricity markets, many variables are naturally defined at the nodal level  (generators, loads, prices) but need to be aggregated to market or geographical areas for analysis, reporting, and trading. This aggregation must handle missing data, different node counts per area, and preserve energy-specific semantics.</p> <p>Parameters:</p> Name Type Description Default <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame containing node information with area assignments. Index should be node identifiers, must contain the specified area_column.</p> required <code>area_column</code> <code>str</code> <p>Name of the column in node_model_df that contains area assignments. Each node should be assigned to exactly one area (NaN values are allowed).</p> required <p>Attributes:</p> Name Type Description <code>node_model_df</code> <p>The input node model DataFrame</p> <code>area_column</code> <p>Name of the area assignment column</p> <code>node_to_area_map</code> <p>Dictionary mapping node IDs to area names</p> <code>areas</code> <p>Sorted list of unique area names (excluding NaN)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If area_column is not found in node_model_df</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # Node model with area assignments\n&gt;&gt;&gt; node_model = pd.DataFrame({\n...     'country': ['DE', 'DE', 'FR', 'FR', 'BE'],\n...     'voltage': [380, 220, 380, 220, 380]\n... }, index=['DE1', 'DE2', 'FR1', 'FR2', 'BE1'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Subclass implementation\n&gt;&gt;&gt; class MyAreaCalculator(AreaVariableCalculatorBase):\n...     def calculate(self, **kwargs):\n...         return pd.DataFrame()  # Implementation here\n&gt;&gt;&gt; \n&gt;&gt;&gt; calculator = MyAreaCalculator(node_model, 'country')\n&gt;&gt;&gt; print(calculator.areas)  # ['BE', 'DE', 'FR']\n&gt;&gt;&gt; print(calculator.get_area_nodes('DE'))  # ['DE1', 'DE2']\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_variable_base.py</code> <pre><code>class AreaVariableCalculatorBase(ABC):\n    \"\"\"Abstract base class for calculating energy variables aggregated at area level.\n\n    This base class provides common functionality for aggregating node-level energy data\n    (such as generation, demand, prices) to higher-level areas (countries, bidding zones,\n    market areas). It handles the mapping between nodes and areas and provides validation\n    and utility methods for area-based calculations.\n\n    The class is designed to be subclassed for specific variable types, with each subclass\n    implementing its own calculation logic while leveraging the common area mapping and\n    validation functionality provided here.\n\n    Energy market context:\n    In electricity markets, many variables are naturally defined at the nodal level \n    (generators, loads, prices) but need to be aggregated to market or geographical\n    areas for analysis, reporting, and trading. This aggregation must handle missing\n    data, different node counts per area, and preserve energy-specific semantics.\n\n    Args:\n        node_model_df: DataFrame containing node information with area assignments.\n            Index should be node identifiers, must contain the specified area_column.\n        area_column: Name of the column in node_model_df that contains area assignments.\n            Each node should be assigned to exactly one area (NaN values are allowed).\n\n    Attributes:\n        node_model_df: The input node model DataFrame\n        area_column: Name of the area assignment column\n        node_to_area_map: Dictionary mapping node IDs to area names\n        areas: Sorted list of unique area names (excluding NaN)\n\n    Raises:\n        ValueError: If area_column is not found in node_model_df\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; # Node model with area assignments\n        &gt;&gt;&gt; node_model = pd.DataFrame({\n        ...     'country': ['DE', 'DE', 'FR', 'FR', 'BE'],\n        ...     'voltage': [380, 220, 380, 220, 380]\n        ... }, index=['DE1', 'DE2', 'FR1', 'FR2', 'BE1'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Subclass implementation\n        &gt;&gt;&gt; class MyAreaCalculator(AreaVariableCalculatorBase):\n        ...     def calculate(self, **kwargs):\n        ...         return pd.DataFrame()  # Implementation here\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; calculator = MyAreaCalculator(node_model, 'country')\n        &gt;&gt;&gt; print(calculator.areas)  # ['BE', 'DE', 'FR']\n        &gt;&gt;&gt; print(calculator.get_area_nodes('DE'))  # ['DE1', 'DE2']\n    \"\"\"\n\n    def __init__(self, node_model_df: pd.DataFrame, area_column: str):\n        \"\"\"Initialize the area variable calculator.\n\n        Args:\n            node_model_df: DataFrame with node-to-area mapping\n            area_column: Column name containing area assignments\n\n        Raises:\n            ValueError: If area_column not found in node_model_df\n        \"\"\"\n        self.node_model_df = node_model_df\n        self.area_column = area_column\n        self.node_to_area_map = self._create_node_to_area_map()\n        self.areas = sorted(self.node_model_df[area_column].dropna().unique())\n        self._validate_inputs()\n\n    def _validate_inputs(self):\n        \"\"\"Validate input parameters during initialization.\n\n        Raises:\n            ValueError: If area_column is not found in node_model_df\n        \"\"\"\n        if self.area_column not in self.node_model_df.columns:\n            raise ValueError(f\"Column '{self.area_column}' not found in node_model_df\")\n\n    def _create_node_to_area_map(self) -&gt; dict[str, str]:\n        return self.node_model_df[self.area_column].to_dict()\n\n    def get_area_nodes(self, area: str) -&gt; list[str]:\n        \"\"\"Get all nodes belonging to a specific area.\n\n        Args:\n            area: Area name to get nodes for\n\n        Returns:\n            List of node IDs that belong to the specified area\n\n        Example:\n\n            &gt;&gt;&gt; calculator = MyAreaCalculator(node_model, 'country')\n            &gt;&gt;&gt; german_nodes = calculator.get_area_nodes('DE')\n            &gt;&gt;&gt; print(german_nodes)  # ['DE1', 'DE2']\n        \"\"\"\n        return self.node_model_df[self.node_model_df[self.area_column] == area].index.tolist()\n\n    @abstractmethod\n    def calculate(self, **kwargs) -&gt; pd.DataFrame:\n        \"\"\"Calculate the area variable. Must be implemented by subclasses.\n\n        This method should contain the specific logic for aggregating node-level\n        data to area level for the particular variable type. The implementation\n        will vary depending on whether the variable is extensive (additive like\n        energy volumes) or intensive (averaged like prices).\n\n        Args:\n            **kwargs: Variable-specific parameters for the calculation\n\n        Returns:\n            DataFrame with area-level aggregated data. Index should be datetime\n            for time series data, columns should be area identifiers.\n\n        Raises:\n            NotImplementedError: This is an abstract method\n        \"\"\"\n        pass\n\n    def _validate_node_data(self, node_df: pd.DataFrame, data_name: str):\n        \"\"\"Validate that required nodes are present in node_model_df.\n\n        Logs warnings for any nodes found in the data that are not in the node model.\n        This is important for detecting data inconsistencies or model updates.\n\n        Args:\n            node_df: DataFrame containing node-level data to validate\n            data_name: Descriptive name of the data being validated (for logging)\n\n        Example:\n\n            &gt;&gt;&gt; # Log warning if generation_data has nodes not in node_model_df\n            &gt;&gt;&gt; calculator._validate_node_data(generation_data, \"generation\")\n        \"\"\"\n        missing_nodes = set(node_df.columns) - set(self.node_model_df.index)\n        if missing_nodes:\n            logger.warning(f\"{len(missing_nodes)} nodes missing in node_model_df from {data_name}\")\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mesqual.energy_data_handling.area_accounting.area_variable_base.AreaVariableCalculatorBase.__init__","title":"__init__","text":"<pre><code>__init__(node_model_df: DataFrame, area_column: str)\n</code></pre> <p>Initialize the area variable calculator.</p> <p>Parameters:</p> Name Type Description Default <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame with node-to-area mapping</p> required <code>area_column</code> <code>str</code> <p>Column name containing area assignments</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If area_column not found in node_model_df</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_variable_base.py</code> <pre><code>def __init__(self, node_model_df: pd.DataFrame, area_column: str):\n    \"\"\"Initialize the area variable calculator.\n\n    Args:\n        node_model_df: DataFrame with node-to-area mapping\n        area_column: Column name containing area assignments\n\n    Raises:\n        ValueError: If area_column not found in node_model_df\n    \"\"\"\n    self.node_model_df = node_model_df\n    self.area_column = area_column\n    self.node_to_area_map = self._create_node_to_area_map()\n    self.areas = sorted(self.node_model_df[area_column].dropna().unique())\n    self._validate_inputs()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mesqual.energy_data_handling.area_accounting.area_variable_base.AreaVariableCalculatorBase.get_area_nodes","title":"get_area_nodes","text":"<pre><code>get_area_nodes(area: str) -&gt; list[str]\n</code></pre> <p>Get all nodes belonging to a specific area.</p> <p>Parameters:</p> Name Type Description Default <code>area</code> <code>str</code> <p>Area name to get nodes for</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of node IDs that belong to the specified area</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; calculator = MyAreaCalculator(node_model, 'country')\n&gt;&gt;&gt; german_nodes = calculator.get_area_nodes('DE')\n&gt;&gt;&gt; print(german_nodes)  # ['DE1', 'DE2']\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_variable_base.py</code> <pre><code>def get_area_nodes(self, area: str) -&gt; list[str]:\n    \"\"\"Get all nodes belonging to a specific area.\n\n    Args:\n        area: Area name to get nodes for\n\n    Returns:\n        List of node IDs that belong to the specified area\n\n    Example:\n\n        &gt;&gt;&gt; calculator = MyAreaCalculator(node_model, 'country')\n        &gt;&gt;&gt; german_nodes = calculator.get_area_nodes('DE')\n        &gt;&gt;&gt; print(german_nodes)  # ['DE1', 'DE2']\n    \"\"\"\n    return self.node_model_df[self.node_model_df[self.area_column] == area].index.tolist()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mesqual.energy_data_handling.area_accounting.area_variable_base.AreaVariableCalculatorBase.calculate","title":"calculate  <code>abstractmethod</code>","text":"<pre><code>calculate(**kwargs) -&gt; DataFrame\n</code></pre> <p>Calculate the area variable. Must be implemented by subclasses.</p> <p>This method should contain the specific logic for aggregating node-level data to area level for the particular variable type. The implementation will vary depending on whether the variable is extensive (additive like energy volumes) or intensive (averaged like prices).</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Variable-specific parameters for the calculation</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with area-level aggregated data. Index should be datetime</p> <code>DataFrame</code> <p>for time series data, columns should be area identifiers.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This is an abstract method</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_variable_base.py</code> <pre><code>@abstractmethod\ndef calculate(self, **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Calculate the area variable. Must be implemented by subclasses.\n\n    This method should contain the specific logic for aggregating node-level\n    data to area level for the particular variable type. The implementation\n    will vary depending on whether the variable is extensive (additive like\n    energy volumes) or intensive (averaged like prices).\n\n    Args:\n        **kwargs: Variable-specific parameters for the calculation\n\n    Returns:\n        DataFrame with area-level aggregated data. Index should be datetime\n        for time series data, columns should be area identifiers.\n\n    Raises:\n        NotImplementedError: This is an abstract method\n    \"\"\"\n    pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mesqual.energy_data_handling.area_accounting.area_variable_base.ExampleSumCalculator","title":"ExampleSumCalculator","text":"<p>               Bases: <code>AreaVariableCalculatorBase</code></p> <p>Example implementation that sums node-level data to area level.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_variable_base.py</code> <pre><code>class ExampleSumCalculator(AreaVariableCalculatorBase):\n    \"\"\"Example implementation that sums node-level data to area level.\"\"\"\n\n    def calculate(self, node_data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Sum node data for each area (extensive variable aggregation).\"\"\"\n        self._validate_node_data(node_data, \"example_data\")\n\n        result_dict = {}\n        for area in self.areas:\n            area_nodes = self.get_area_nodes(area)\n            # Filter to nodes that exist in both model and data\n            available_nodes = [n for n in area_nodes if n in node_data.columns]\n            if available_nodes:\n                result_dict[area] = node_data[available_nodes].sum(axis=1)\n            else:\n                # Create empty series with same index if no data available\n                result_dict[area] = pd.Series(index=node_data.index, dtype=float)\n\n        result_df = pd.DataFrame(result_dict)\n        result_df.columns.name = self.area_column\n        return result_df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/area_variables/#mesqual.energy_data_handling.area_accounting.area_variable_base.ExampleSumCalculator.calculate","title":"calculate","text":"<pre><code>calculate(node_data: DataFrame) -&gt; DataFrame\n</code></pre> <p>Sum node data for each area (extensive variable aggregation).</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_variable_base.py</code> <pre><code>def calculate(self, node_data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Sum node data for each area (extensive variable aggregation).\"\"\"\n    self._validate_node_data(node_data, \"example_data\")\n\n    result_dict = {}\n    for area in self.areas:\n        area_nodes = self.get_area_nodes(area)\n        # Filter to nodes that exist in both model and data\n        available_nodes = [n for n in area_nodes if n in node_data.columns]\n        if available_nodes:\n            result_dict[area] = node_data[available_nodes].sum(axis=1)\n        else:\n            # Create empty series with same index if no data available\n            result_dict[area] = pd.Series(index=node_data.index, dtype=float)\n\n    result_df = pd.DataFrame(result_dict)\n    result_df.columns.name = self.area_column\n    return result_df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/","title":"AreaBorder Variable Accounting","text":""},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mesqual.energy_data_handling.area_accounting.border_variable_base.AreaBorderVariableCalculatorBase","title":"AreaBorderVariableCalculatorBase","text":"<p>               Bases: <code>ABC</code>, <code>AreaBorderNamingConventions</code></p> <p>Abstract base class for calculating energy variables at area border level.</p> <p>This base class provides functionality for aggregating line-level energy data  (flows, capacities, price spreads) to area border level. An area border represents the interface between two areas (countries, bidding zones, etc.).</p> <p>The class handles the complex mapping from transmission lines to area borders, including proper handling of line directionality. Lines are classified as either \"up\" or \"down\" relative to the border direction based on their node endpoints.</p> <p>Border directionality: - \"Up\" direction: From area_from to area_to (as defined in border naming) - \"Down\" direction: From area_to to area_from - Line direction is determined by comparing line endpoints to border areas</p> <p>Parameters:</p> Name Type Description Default <code>area_border_model_df</code> <code>DataFrame</code> <p>DataFrame containing area border definitions. Index should be border identifiers (e.g., 'DE-FR', 'FR-BE').</p> required <code>line_model_df</code> <code>DataFrame</code> <p>DataFrame containing transmission line information. Must include node_from_col and node_to_col columns.</p> required <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame containing node information with area assignments. Must include area_column for mapping nodes to areas.</p> required <code>area_column</code> <code>str</code> <p>Column name in node_model_df containing area assignments.</p> required <code>node_from_col</code> <code>str</code> <p>Column name in line_model_df for line starting node.</p> <code>'node_from'</code> <code>node_to_col</code> <code>str</code> <p>Column name in line_model_df for line ending node.</p> <code>'node_to'</code> <p>Attributes:</p> Name Type Description <code>area_border_model_df</code> <p>Border model DataFrame</p> <code>line_model_df</code> <p>Line model DataFrame  </p> <code>node_model_df</code> <p>Node model DataFrame</p> <code>area_column</code> <p>Name of area assignment column</p> <code>node_from_col</code> <p>Name of line from-node column</p> <code>node_to_col</code> <p>Name of line to-node column</p> <code>node_to_area_map</code> <p>Dictionary mapping node IDs to area names</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing from input DataFrames</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # Define borders between areas\n&gt;&gt;&gt; border_model = pd.DataFrame(index=['DE-FR', 'FR-BE'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Define transmission lines  \n&gt;&gt;&gt; line_model = pd.DataFrame({\n...     'node_from': ['DE1', 'FR1'],\n...     'node_to': ['FR1', 'BE1'],\n...     'capacity': [1000, 800]\n... }, index=['Line1', 'Line2'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Node-to-area mapping\n&gt;&gt;&gt; node_model = pd.DataFrame({\n...     'country': ['DE', 'FR', 'BE']\n... }, index=['DE1', 'FR1', 'BE1'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Subclass for specific calculation\n&gt;&gt;&gt; class MyBorderCalculator(AreaBorderVariableCalculatorBase):\n...     @property\n...     def variable_name(self):\n...         return \"my_variable\"\n...     def calculate(self, **kwargs):\n...         return pd.DataFrame()\n&gt;&gt;&gt; \n&gt;&gt;&gt; calculator = MyBorderCalculator(\n...     border_model, line_model, node_model, 'country'\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_variable_base.py</code> <pre><code>class AreaBorderVariableCalculatorBase(ABC, AreaBorderNamingConventions):\n    \"\"\"Abstract base class for calculating energy variables at area border level.\n\n    This base class provides functionality for aggregating line-level energy data \n    (flows, capacities, price spreads) to area border level. An area border represents\n    the interface between two areas (countries, bidding zones, etc.).\n\n    The class handles the complex mapping from transmission lines to area borders,\n    including proper handling of line directionality. Lines are classified as either\n    \"up\" or \"down\" relative to the border direction based on their node endpoints.\n\n    Border directionality:\n    - \"Up\" direction: From area_from to area_to (as defined in border naming)\n    - \"Down\" direction: From area_to to area_from\n    - Line direction is determined by comparing line endpoints to border areas\n\n    Args:\n        area_border_model_df: DataFrame containing area border definitions.\n            Index should be border identifiers (e.g., 'DE-FR', 'FR-BE').\n        line_model_df: DataFrame containing transmission line information.\n            Must include node_from_col and node_to_col columns.\n        node_model_df: DataFrame containing node information with area assignments.\n            Must include area_column for mapping nodes to areas.\n        area_column: Column name in node_model_df containing area assignments.\n        node_from_col: Column name in line_model_df for line starting node.\n        node_to_col: Column name in line_model_df for line ending node.\n\n    Attributes:\n        area_border_model_df: Border model DataFrame\n        line_model_df: Line model DataFrame  \n        node_model_df: Node model DataFrame\n        area_column: Name of area assignment column\n        node_from_col: Name of line from-node column\n        node_to_col: Name of line to-node column\n        node_to_area_map: Dictionary mapping node IDs to area names\n\n    Raises:\n        ValueError: If required columns are missing from input DataFrames\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; # Define borders between areas\n        &gt;&gt;&gt; border_model = pd.DataFrame(index=['DE-FR', 'FR-BE'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Define transmission lines  \n        &gt;&gt;&gt; line_model = pd.DataFrame({\n        ...     'node_from': ['DE1', 'FR1'],\n        ...     'node_to': ['FR1', 'BE1'],\n        ...     'capacity': [1000, 800]\n        ... }, index=['Line1', 'Line2'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Node-to-area mapping\n        &gt;&gt;&gt; node_model = pd.DataFrame({\n        ...     'country': ['DE', 'FR', 'BE']\n        ... }, index=['DE1', 'FR1', 'BE1'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Subclass for specific calculation\n        &gt;&gt;&gt; class MyBorderCalculator(AreaBorderVariableCalculatorBase):\n        ...     @property\n        ...     def variable_name(self):\n        ...         return \"my_variable\"\n        ...     def calculate(self, **kwargs):\n        ...         return pd.DataFrame()\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; calculator = MyBorderCalculator(\n        ...     border_model, line_model, node_model, 'country'\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        area_border_model_df: pd.DataFrame,\n        line_model_df: pd.DataFrame,\n        node_model_df: pd.DataFrame,\n        area_column: str,\n        node_from_col: str = 'node_from',\n        node_to_col: str = 'node_to'\n    ):\n        \"\"\"Initialize the area border variable calculator.\n\n        Args:\n            area_border_model_df: DataFrame with border definitions\n            line_model_df: DataFrame with line information including endpoints\n            node_model_df: DataFrame with node-to-area mapping\n            area_column: Column name for area assignments in node_model_df\n            node_from_col: Column name for line starting node in line_model_df\n            node_to_col: Column name for line ending node in line_model_df\n\n        Raises:\n            ValueError: If required columns are missing from DataFrames\n        \"\"\"\n        super().__init__(area_column)\n        self.area_border_model_df = area_border_model_df\n        self.line_model_df = line_model_df\n        self.node_model_df = node_model_df\n        self.area_column = area_column\n        self.node_from_col = node_from_col\n        self.node_to_col = node_to_col\n        self.node_to_area_map = self._create_node_to_area_map()\n        self._validate_inputs()\n\n    def _validate_inputs(self):\n        \"\"\"Validate input parameters during initialization.\n\n        Raises:\n            ValueError: If required columns are missing from DataFrames\n        \"\"\"\n        if self.area_column not in self.node_model_df.columns:\n            raise ValueError(f\"Column '{self.area_column}' not found in node_model_df\")\n        if self.node_from_col not in self.line_model_df.columns:\n            raise ValueError(f\"Column '{self.node_from_col}' not found in line_model_df\")\n        if self.node_to_col not in self.line_model_df.columns:\n            raise ValueError(f\"Column '{self.node_to_col}' not found in line_model_df\")\n\n    def _create_node_to_area_map(self) -&gt; dict[Hashable, str]:\n        \"\"\"Create a mapping dictionary from node IDs to area names.\n\n        Returns:\n            Dictionary with node IDs as keys and area names as values.\n            Nodes with NaN area assignments will have NaN values.\n        \"\"\"\n        return self.node_model_df[self.area_column].to_dict()\n\n    def get_border_lines_in_topological_up_and_down_direction(self, border_id: str) -&gt; tuple[list[Hashable], list[Hashable]]:\n        \"\"\"Get transmission lines for a border classified by topological direction.\n\n        This method identifies which transmission lines connect the two areas of a border\n        and classifies them based on their topological direction relative to the border.\n\n        Border directionality logic:\n        - \"Up\" direction: Lines where node_from is in area_from and node_to is in area_to\n        - \"Down\" direction: Lines where node_from is in area_to and node_to is in area_from\n\n        This classification is essential for correctly aggregating directional quantities\n        like power flows, where the sign and direction matter for market analysis.\n\n        Args:\n            border_id: Border identifier (e.g., 'DE-FR') that will be decomposed\n                into area_from and area_to using the naming convention.\n\n        Returns:\n            Tuple containing two lists:\n            - lines_up: Line IDs for lines in the \"up\" direction\n            - lines_down: Line IDs for lines in the \"down\" direction\n\n        Example:\n\n            &gt;&gt;&gt; # For border 'DE-FR'\n            &gt;&gt;&gt; lines_up, lines_down = calculator.get_border_lines_in_topological_up_and_down_direction('DE-FR')\n            &gt;&gt;&gt; # lines_up: Lines from German nodes to French nodes  \n            &gt;&gt;&gt; # lines_down: Lines from French nodes to German nodes\n        \"\"\"\n        area_from, area_to = self.decompose_area_border_name_to_areas(border_id)\n        nodes_in_area_from = self.node_model_df.loc[self.node_model_df[self.area_column] == area_from].index.to_list()\n        nodes_in_area_to = self.node_model_df.loc[self.node_model_df[self.area_column] == area_to].index.to_list()\n        lines_up = self.line_model_df.loc[\n                self.line_model_df[self.node_from_col].isin(nodes_in_area_from)\n                &amp; self.line_model_df[self.node_to_col].isin(nodes_in_area_to)\n            ].index.to_list()\n        lines_down = self.line_model_df.loc[\n                self.line_model_df[self.node_from_col].isin(nodes_in_area_to)\n                &amp; self.line_model_df[self.node_to_col].isin(nodes_in_area_from)\n            ].index.to_list()\n        return lines_up, lines_down\n\n    @abstractmethod\n    def calculate(self, **kwargs) -&gt; pd.DataFrame:\n        \"\"\"Calculate the border variable. Must be implemented by subclasses.\n\n        This method should contain the specific logic for aggregating line-level\n        data to border level for the particular variable type. The implementation\n        will vary based on the variable (flows, capacities, prices, etc.) and\n        should handle directional aggregation appropriately.\n\n        Args:\n            **kwargs: Variable-specific parameters for the calculation\n\n        Returns:\n            DataFrame with border-level aggregated data. Index should be datetime\n            for time series data, columns should be border identifiers.\n\n        Raises:\n            NotImplementedError: This is an abstract method\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def variable_name(self) -&gt; str:\n        \"\"\"Name of the variable being calculated.\n\n        This property should return a descriptive name for the variable being\n        calculated by this calculator. Used for naming output columns and logging.\n\n        Returns:\n            String name of the variable (e.g., 'border_flow', 'border_capacity')\n        \"\"\"\n        pass\n\n    def _validate_time_series_data(self, df: pd.DataFrame, data_name: str):\n        \"\"\"Validate that time series data has appropriate datetime index.\n\n        Logs warnings if the data doesn't have a DatetimeIndex, which may indicate\n        data formatting issues or non-time-series data being used inappropriately.\n\n        Args:\n            df: DataFrame to validate\n            data_name: Descriptive name of the data for logging purposes\n        \"\"\"\n        if not isinstance(df.index, pd.DatetimeIndex):\n            logger.warning(f\"{data_name} does not have DatetimeIndex\")\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mesqual.energy_data_handling.area_accounting.border_variable_base.AreaBorderVariableCalculatorBase.variable_name","title":"variable_name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>variable_name: str\n</code></pre> <p>Name of the variable being calculated.</p> <p>This property should return a descriptive name for the variable being calculated by this calculator. Used for naming output columns and logging.</p> <p>Returns:</p> Type Description <code>str</code> <p>String name of the variable (e.g., 'border_flow', 'border_capacity')</p>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mesqual.energy_data_handling.area_accounting.border_variable_base.AreaBorderVariableCalculatorBase.__init__","title":"__init__","text":"<pre><code>__init__(area_border_model_df: DataFrame, line_model_df: DataFrame, node_model_df: DataFrame, area_column: str, node_from_col: str = 'node_from', node_to_col: str = 'node_to')\n</code></pre> <p>Initialize the area border variable calculator.</p> <p>Parameters:</p> Name Type Description Default <code>area_border_model_df</code> <code>DataFrame</code> <p>DataFrame with border definitions</p> required <code>line_model_df</code> <code>DataFrame</code> <p>DataFrame with line information including endpoints</p> required <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame with node-to-area mapping</p> required <code>area_column</code> <code>str</code> <p>Column name for area assignments in node_model_df</p> required <code>node_from_col</code> <code>str</code> <p>Column name for line starting node in line_model_df</p> <code>'node_from'</code> <code>node_to_col</code> <code>str</code> <p>Column name for line ending node in line_model_df</p> <code>'node_to'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing from DataFrames</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_variable_base.py</code> <pre><code>def __init__(\n    self,\n    area_border_model_df: pd.DataFrame,\n    line_model_df: pd.DataFrame,\n    node_model_df: pd.DataFrame,\n    area_column: str,\n    node_from_col: str = 'node_from',\n    node_to_col: str = 'node_to'\n):\n    \"\"\"Initialize the area border variable calculator.\n\n    Args:\n        area_border_model_df: DataFrame with border definitions\n        line_model_df: DataFrame with line information including endpoints\n        node_model_df: DataFrame with node-to-area mapping\n        area_column: Column name for area assignments in node_model_df\n        node_from_col: Column name for line starting node in line_model_df\n        node_to_col: Column name for line ending node in line_model_df\n\n    Raises:\n        ValueError: If required columns are missing from DataFrames\n    \"\"\"\n    super().__init__(area_column)\n    self.area_border_model_df = area_border_model_df\n    self.line_model_df = line_model_df\n    self.node_model_df = node_model_df\n    self.area_column = area_column\n    self.node_from_col = node_from_col\n    self.node_to_col = node_to_col\n    self.node_to_area_map = self._create_node_to_area_map()\n    self._validate_inputs()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mesqual.energy_data_handling.area_accounting.border_variable_base.AreaBorderVariableCalculatorBase.get_border_lines_in_topological_up_and_down_direction","title":"get_border_lines_in_topological_up_and_down_direction","text":"<pre><code>get_border_lines_in_topological_up_and_down_direction(border_id: str) -&gt; tuple[list[Hashable], list[Hashable]]\n</code></pre> <p>Get transmission lines for a border classified by topological direction.</p> <p>This method identifies which transmission lines connect the two areas of a border and classifies them based on their topological direction relative to the border.</p> <p>Border directionality logic: - \"Up\" direction: Lines where node_from is in area_from and node_to is in area_to - \"Down\" direction: Lines where node_from is in area_to and node_to is in area_from</p> <p>This classification is essential for correctly aggregating directional quantities like power flows, where the sign and direction matter for market analysis.</p> <p>Parameters:</p> Name Type Description Default <code>border_id</code> <code>str</code> <p>Border identifier (e.g., 'DE-FR') that will be decomposed into area_from and area_to using the naming convention.</p> required <p>Returns:</p> Type Description <code>list[Hashable]</code> <p>Tuple containing two lists:</p> <code>list[Hashable]</code> <ul> <li>lines_up: Line IDs for lines in the \"up\" direction</li> </ul> <code>tuple[list[Hashable], list[Hashable]]</code> <ul> <li>lines_down: Line IDs for lines in the \"down\" direction</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; # For border 'DE-FR'\n&gt;&gt;&gt; lines_up, lines_down = calculator.get_border_lines_in_topological_up_and_down_direction('DE-FR')\n&gt;&gt;&gt; # lines_up: Lines from German nodes to French nodes  \n&gt;&gt;&gt; # lines_down: Lines from French nodes to German nodes\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_variable_base.py</code> <pre><code>def get_border_lines_in_topological_up_and_down_direction(self, border_id: str) -&gt; tuple[list[Hashable], list[Hashable]]:\n    \"\"\"Get transmission lines for a border classified by topological direction.\n\n    This method identifies which transmission lines connect the two areas of a border\n    and classifies them based on their topological direction relative to the border.\n\n    Border directionality logic:\n    - \"Up\" direction: Lines where node_from is in area_from and node_to is in area_to\n    - \"Down\" direction: Lines where node_from is in area_to and node_to is in area_from\n\n    This classification is essential for correctly aggregating directional quantities\n    like power flows, where the sign and direction matter for market analysis.\n\n    Args:\n        border_id: Border identifier (e.g., 'DE-FR') that will be decomposed\n            into area_from and area_to using the naming convention.\n\n    Returns:\n        Tuple containing two lists:\n        - lines_up: Line IDs for lines in the \"up\" direction\n        - lines_down: Line IDs for lines in the \"down\" direction\n\n    Example:\n\n        &gt;&gt;&gt; # For border 'DE-FR'\n        &gt;&gt;&gt; lines_up, lines_down = calculator.get_border_lines_in_topological_up_and_down_direction('DE-FR')\n        &gt;&gt;&gt; # lines_up: Lines from German nodes to French nodes  \n        &gt;&gt;&gt; # lines_down: Lines from French nodes to German nodes\n    \"\"\"\n    area_from, area_to = self.decompose_area_border_name_to_areas(border_id)\n    nodes_in_area_from = self.node_model_df.loc[self.node_model_df[self.area_column] == area_from].index.to_list()\n    nodes_in_area_to = self.node_model_df.loc[self.node_model_df[self.area_column] == area_to].index.to_list()\n    lines_up = self.line_model_df.loc[\n            self.line_model_df[self.node_from_col].isin(nodes_in_area_from)\n            &amp; self.line_model_df[self.node_to_col].isin(nodes_in_area_to)\n        ].index.to_list()\n    lines_down = self.line_model_df.loc[\n            self.line_model_df[self.node_from_col].isin(nodes_in_area_to)\n            &amp; self.line_model_df[self.node_to_col].isin(nodes_in_area_from)\n        ].index.to_list()\n    return lines_up, lines_down\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mesqual.energy_data_handling.area_accounting.border_variable_base.AreaBorderVariableCalculatorBase.calculate","title":"calculate  <code>abstractmethod</code>","text":"<pre><code>calculate(**kwargs) -&gt; DataFrame\n</code></pre> <p>Calculate the border variable. Must be implemented by subclasses.</p> <p>This method should contain the specific logic for aggregating line-level data to border level for the particular variable type. The implementation will vary based on the variable (flows, capacities, prices, etc.) and should handle directional aggregation appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Variable-specific parameters for the calculation</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with border-level aggregated data. Index should be datetime</p> <code>DataFrame</code> <p>for time series data, columns should be border identifiers.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This is an abstract method</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_variable_base.py</code> <pre><code>@abstractmethod\ndef calculate(self, **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Calculate the border variable. Must be implemented by subclasses.\n\n    This method should contain the specific logic for aggregating line-level\n    data to border level for the particular variable type. The implementation\n    will vary based on the variable (flows, capacities, prices, etc.) and\n    should handle directional aggregation appropriately.\n\n    Args:\n        **kwargs: Variable-specific parameters for the calculation\n\n    Returns:\n        DataFrame with border-level aggregated data. Index should be datetime\n        for time series data, columns should be border identifiers.\n\n    Raises:\n        NotImplementedError: This is an abstract method\n    \"\"\"\n    pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mesqual.energy_data_handling.area_accounting.border_variable_capacity_calculator.BorderCapacityCalculator","title":"BorderCapacityCalculator","text":"<p>               Bases: <code>AreaBorderVariableCalculatorBase</code></p> <p>Calculates aggregated transmission capacities for area borders.</p> <p>This calculator aggregates line-level transmission capacities to border level, handling bidirectional capacity data with proper directional aggregation.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from mesqual.energy_data_handling.network_lines_data import NetworkLineCapacitiesData\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Create capacity data\n&gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n&gt;&gt;&gt; capacities = NetworkLineCapacitiesData(\n...     capacities_up=pd.DataFrame({...}),\n...     capacities_down=pd.DataFrame({...})\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; calculator = BorderCapacityCalculator(\n...     area_border_model_df, line_model_df, node_model_df, 'country'\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Calculate capacities for up direction (area_from \u2192 area_to)\n&gt;&gt;&gt; up_capacities = calculator.calculate(capacities, direction='up')\n&gt;&gt;&gt; print(up_capacities)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_variable_capacity_calculator.py</code> <pre><code>class BorderCapacityCalculator(AreaBorderVariableCalculatorBase):\n    \"\"\"Calculates aggregated transmission capacities for area borders.\n\n    This calculator aggregates line-level transmission capacities to border level,\n    handling bidirectional capacity data with proper directional aggregation.\n\n    Example:\n\n        &gt;&gt;&gt; from mesqual.energy_data_handling.network_lines_data import NetworkLineCapacitiesData\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Create capacity data\n        &gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n        &gt;&gt;&gt; capacities = NetworkLineCapacitiesData(\n        ...     capacities_up=pd.DataFrame({...}),\n        ...     capacities_down=pd.DataFrame({...})\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; calculator = BorderCapacityCalculator(\n        ...     area_border_model_df, line_model_df, node_model_df, 'country'\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Calculate capacities for up direction (area_from \u2192 area_to)\n        &gt;&gt;&gt; up_capacities = calculator.calculate(capacities, direction='up')\n        &gt;&gt;&gt; print(up_capacities)\n    \"\"\"\n\n    @property\n    def variable_name(self) -&gt; str:\n        return \"border_capacity\"\n\n    def calculate(\n            self,\n            line_capacity_data: NetworkLineCapacitiesData,\n            direction: Literal['up', 'down'] = 'up'\n    ) -&gt; pd.DataFrame:\n        \"\"\"Aggregate line-level transmission capacities to border level.\n\n        Sums transmission capacities of all lines belonging to each border,\n        respecting the specified direction and handling bidirectional capacity data.\n        Lines are aggregated based on their topological relationship to the border.\n\n        Direction logic:\n        - 'up': Capacities for flows from area_from to area_to\n        - 'down': Capacities for flows from area_to to area_from\n\n        For each border, the method:\n        1. Identifies lines in 'up' and 'down' topological directions\n        2. Selects appropriate capacity data based on requested direction\n        3. Sums capacities across all border lines\n        4. Handles missing data by excluding unavailable lines\n\n        Args:\n            line_capacity_data: NetworkLineCapacitiesData containing bidirectional\n                capacity time series. Must include capacities_up and capacities_down\n                DataFrames with line IDs as columns and timestamps as index.\n            direction: Direction for capacity aggregation:\n                - 'up': Sum capacities for area_from \u2192 area_to flows\n                - 'down': Sum capacities for area_to \u2192 area_from flows\n\n        Returns:\n            DataFrame with border-level capacity aggregations. Index matches the \n            input capacity data, columns are border identifiers. Values represent\n            total transmission capacity in MW for each border and timestamp.\n\n        Raises:\n            ValueError: If direction is not 'up' or 'down'\n\n        Example:\n\n            &gt;&gt;&gt; # Calculate up-direction capacities (exports from area_from)\n            &gt;&gt;&gt; up_caps = calculator.calculate(capacity_data, direction='up')\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Calculate down-direction capacities (imports to area_from)  \n            &gt;&gt;&gt; down_caps = calculator.calculate(capacity_data, direction='down')\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; print(f\"DE\u2192FR capacity: {up_caps.loc['2024-01-01 12:00', 'DE-FR']:.0f} MW\")\n        \"\"\"\n        self._validate_time_series_data(line_capacity_data.capacities_up, \"capacities_up\")\n        self._validate_time_series_data(line_capacity_data.capacities_down, \"capacities_down\")\n\n        border_capacities = {}\n\n        for border_id, border in self.area_border_model_df.iterrows():\n            lines_up, lines_down = self.get_border_lines_in_topological_up_and_down_direction(border_id)\n\n            if not lines_up and not lines_down:\n                # No lines found for this border - create empty series\n                index = line_capacity_data.capacities_up.index\n                border_capacities[border_id] = pd.Series(index=index, dtype=float)\n                continue\n\n            if direction == 'up':\n                # For 'up' direction: use up capacities of lines_up + down capacities of lines_down\n                capacity_parts = []\n                if lines_up:\n                    available_lines_up = [line for line in lines_up if line in line_capacity_data.capacities_up.columns]\n                    if available_lines_up:\n                        capacity_parts.append(line_capacity_data.capacities_up[available_lines_up])\n\n                if lines_down:\n                    available_lines_down = [line for line in lines_down if line in line_capacity_data.capacities_down.columns]\n                    if available_lines_down:\n                        capacity_parts.append(line_capacity_data.capacities_down[available_lines_down])\n\n            elif direction == 'down':\n                # For 'down' direction: use down capacities of lines_up + up capacities of lines_down\n                capacity_parts = []\n                if lines_up:\n                    available_lines_up = [line for line in lines_up if line in line_capacity_data.capacities_down.columns]\n                    if available_lines_up:\n                        capacity_parts.append(line_capacity_data.capacities_down[available_lines_up])\n\n                if lines_down:\n                    available_lines_down = [line for line in lines_down if line in line_capacity_data.capacities_up.columns]\n                    if available_lines_down:\n                        capacity_parts.append(line_capacity_data.capacities_up[available_lines_down])\n            else:\n                raise ValueError(f\"Unknown capacity direction: {direction}. Must be 'up' or 'down'\")\n\n            # Combine and sum capacities\n            if capacity_parts:\n                all_capacities = pd.concat(capacity_parts, axis=1)\n                border_capacities[border_id] = all_capacities.sum(axis=1)\n            else:\n                # No capacity data available for any lines\n                index = line_capacity_data.capacities_up.index\n                border_capacities[border_id] = pd.Series(index=index, dtype=float)\n\n        result = pd.DataFrame(border_capacities)\n        result.columns.name = self.border_identifier\n        return result\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mesqual.energy_data_handling.area_accounting.border_variable_capacity_calculator.BorderCapacityCalculator.calculate","title":"calculate","text":"<pre><code>calculate(line_capacity_data: NetworkLineCapacitiesData, direction: Literal['up', 'down'] = 'up') -&gt; DataFrame\n</code></pre> <p>Aggregate line-level transmission capacities to border level.</p> <p>Sums transmission capacities of all lines belonging to each border, respecting the specified direction and handling bidirectional capacity data. Lines are aggregated based on their topological relationship to the border.</p> <p>Direction logic: - 'up': Capacities for flows from area_from to area_to - 'down': Capacities for flows from area_to to area_from</p> <p>For each border, the method: 1. Identifies lines in 'up' and 'down' topological directions 2. Selects appropriate capacity data based on requested direction 3. Sums capacities across all border lines 4. Handles missing data by excluding unavailable lines</p> <p>Parameters:</p> Name Type Description Default <code>line_capacity_data</code> <code>NetworkLineCapacitiesData</code> <p>NetworkLineCapacitiesData containing bidirectional capacity time series. Must include capacities_up and capacities_down DataFrames with line IDs as columns and timestamps as index.</p> required <code>direction</code> <code>Literal['up', 'down']</code> <p>Direction for capacity aggregation: - 'up': Sum capacities for area_from \u2192 area_to flows - 'down': Sum capacities for area_to \u2192 area_from flows</p> <code>'up'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with border-level capacity aggregations. Index matches the </p> <code>DataFrame</code> <p>input capacity data, columns are border identifiers. Values represent</p> <code>DataFrame</code> <p>total transmission capacity in MW for each border and timestamp.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If direction is not 'up' or 'down'</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Calculate up-direction capacities (exports from area_from)\n&gt;&gt;&gt; up_caps = calculator.calculate(capacity_data, direction='up')\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Calculate down-direction capacities (imports to area_from)  \n&gt;&gt;&gt; down_caps = calculator.calculate(capacity_data, direction='down')\n&gt;&gt;&gt; \n&gt;&gt;&gt; print(f\"DE\u2192FR capacity: {up_caps.loc['2024-01-01 12:00', 'DE-FR']:.0f} MW\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_variable_capacity_calculator.py</code> <pre><code>def calculate(\n        self,\n        line_capacity_data: NetworkLineCapacitiesData,\n        direction: Literal['up', 'down'] = 'up'\n) -&gt; pd.DataFrame:\n    \"\"\"Aggregate line-level transmission capacities to border level.\n\n    Sums transmission capacities of all lines belonging to each border,\n    respecting the specified direction and handling bidirectional capacity data.\n    Lines are aggregated based on their topological relationship to the border.\n\n    Direction logic:\n    - 'up': Capacities for flows from area_from to area_to\n    - 'down': Capacities for flows from area_to to area_from\n\n    For each border, the method:\n    1. Identifies lines in 'up' and 'down' topological directions\n    2. Selects appropriate capacity data based on requested direction\n    3. Sums capacities across all border lines\n    4. Handles missing data by excluding unavailable lines\n\n    Args:\n        line_capacity_data: NetworkLineCapacitiesData containing bidirectional\n            capacity time series. Must include capacities_up and capacities_down\n            DataFrames with line IDs as columns and timestamps as index.\n        direction: Direction for capacity aggregation:\n            - 'up': Sum capacities for area_from \u2192 area_to flows\n            - 'down': Sum capacities for area_to \u2192 area_from flows\n\n    Returns:\n        DataFrame with border-level capacity aggregations. Index matches the \n        input capacity data, columns are border identifiers. Values represent\n        total transmission capacity in MW for each border and timestamp.\n\n    Raises:\n        ValueError: If direction is not 'up' or 'down'\n\n    Example:\n\n        &gt;&gt;&gt; # Calculate up-direction capacities (exports from area_from)\n        &gt;&gt;&gt; up_caps = calculator.calculate(capacity_data, direction='up')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Calculate down-direction capacities (imports to area_from)  \n        &gt;&gt;&gt; down_caps = calculator.calculate(capacity_data, direction='down')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; print(f\"DE\u2192FR capacity: {up_caps.loc['2024-01-01 12:00', 'DE-FR']:.0f} MW\")\n    \"\"\"\n    self._validate_time_series_data(line_capacity_data.capacities_up, \"capacities_up\")\n    self._validate_time_series_data(line_capacity_data.capacities_down, \"capacities_down\")\n\n    border_capacities = {}\n\n    for border_id, border in self.area_border_model_df.iterrows():\n        lines_up, lines_down = self.get_border_lines_in_topological_up_and_down_direction(border_id)\n\n        if not lines_up and not lines_down:\n            # No lines found for this border - create empty series\n            index = line_capacity_data.capacities_up.index\n            border_capacities[border_id] = pd.Series(index=index, dtype=float)\n            continue\n\n        if direction == 'up':\n            # For 'up' direction: use up capacities of lines_up + down capacities of lines_down\n            capacity_parts = []\n            if lines_up:\n                available_lines_up = [line for line in lines_up if line in line_capacity_data.capacities_up.columns]\n                if available_lines_up:\n                    capacity_parts.append(line_capacity_data.capacities_up[available_lines_up])\n\n            if lines_down:\n                available_lines_down = [line for line in lines_down if line in line_capacity_data.capacities_down.columns]\n                if available_lines_down:\n                    capacity_parts.append(line_capacity_data.capacities_down[available_lines_down])\n\n        elif direction == 'down':\n            # For 'down' direction: use down capacities of lines_up + up capacities of lines_down\n            capacity_parts = []\n            if lines_up:\n                available_lines_up = [line for line in lines_up if line in line_capacity_data.capacities_down.columns]\n                if available_lines_up:\n                    capacity_parts.append(line_capacity_data.capacities_down[available_lines_up])\n\n            if lines_down:\n                available_lines_down = [line for line in lines_down if line in line_capacity_data.capacities_up.columns]\n                if available_lines_down:\n                    capacity_parts.append(line_capacity_data.capacities_up[available_lines_down])\n        else:\n            raise ValueError(f\"Unknown capacity direction: {direction}. Must be 'up' or 'down'\")\n\n        # Combine and sum capacities\n        if capacity_parts:\n            all_capacities = pd.concat(capacity_parts, axis=1)\n            border_capacities[border_id] = all_capacities.sum(axis=1)\n        else:\n            # No capacity data available for any lines\n            index = line_capacity_data.capacities_up.index\n            border_capacities[border_id] = pd.Series(index=index, dtype=float)\n\n    result = pd.DataFrame(border_capacities)\n    result.columns.name = self.border_identifier\n    return result\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mesqual.energy_data_handling.area_accounting.border_variable_flow_calculator.BorderFlowCalculator","title":"BorderFlowCalculator","text":"<p>               Bases: <code>AreaBorderVariableCalculatorBase</code></p> <p>Calculates aggregated power flows for area borders.</p> <p>This calculator aggregates line-level power flows to border level, handling bidirectional flow data and transmission losses. The calculator can aggregate both sent and received flows, accounting for transmission losses that occur between sending and receiving ends. It supports multiple output formats including directional flows and net flows.</p> <p>Flow aggregation logic: - Lines and flows are classified as \"up\" or \"down\" based on topological direction - Flows are aggregated respecting directionality and loss conventions - Net flows represent the algebraic sum (up_flow - down_flow)</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from mesqual.energy_data_handling.network_lines_data import NetworkLineFlowsData\n&gt;&gt;&gt; calculator = BorderFlowCalculator(\n...     area_border_model_df, line_model_df, node_model_df, 'country'\n... )\n&gt;&gt;&gt; # Calculate net sent flows (before losses)\n&gt;&gt;&gt; net_flows = calculator.calculate(flow_data, flow_type='sent', direction='net')\n&gt;&gt;&gt; print(net_flows)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_variable_flow_calculator.py</code> <pre><code>class BorderFlowCalculator(AreaBorderVariableCalculatorBase):\n    \"\"\"Calculates aggregated power flows for area borders.\n\n    This calculator aggregates line-level power flows to border level, handling\n    bidirectional flow data and transmission losses.\n    The calculator can aggregate both sent and received flows, accounting for\n    transmission losses that occur between sending and receiving ends. It supports\n    multiple output formats including directional flows and net flows.\n\n    Flow aggregation logic:\n    - Lines and flows are classified as \"up\" or \"down\" based on topological direction\n    - Flows are aggregated respecting directionality and loss conventions\n    - Net flows represent the algebraic sum (up_flow - down_flow)\n\n    Example:\n\n        &gt;&gt;&gt; from mesqual.energy_data_handling.network_lines_data import NetworkLineFlowsData\n        &gt;&gt;&gt; calculator = BorderFlowCalculator(\n        ...     area_border_model_df, line_model_df, node_model_df, 'country'\n        ... )\n        &gt;&gt;&gt; # Calculate net sent flows (before losses)\n        &gt;&gt;&gt; net_flows = calculator.calculate(flow_data, flow_type='sent', direction='net')\n        &gt;&gt;&gt; print(net_flows)\n    \"\"\"\n\n    @property\n    def variable_name(self) -&gt; str:\n        return \"border_flow\"\n\n    def calculate(\n        self,\n        line_flow_data: NetworkLineFlowsData,\n        flow_type: Literal['sent', 'received'] = 'sent',\n        direction: Literal['up', 'down', 'net'] = 'net'\n    ) -&gt; pd.DataFrame:\n        \"\"\"Aggregate line-level power flows to border level.\n\n        Sums power flows of all lines belonging to each border, respecting flow\n        directionality and transmission loss conventions. The aggregation handles\n        both pre-loss (sent) and post-loss (received) flows.\n\n        Flow type selection:\n        - 'sent': Flows before transmission losses (injected into lines)  \n        - 'received': Flows after transmission losses (withdrawn from lines)\n\n        Direction options:\n        - 'up': Flows from area_from to area_to only\n        - 'down': Flows from area_to to area_from only\n        - 'net': Net flows (up - down), positive means net export from area_from\n\n        The method handles missing data by preserving NaN values when all \n        constituent flows are missing for a given timestamp.\n\n        Args:\n            line_flow_data: NetworkLineFlowsData containing bidirectional flow\n                time series. Must include sent_up, received_up, sent_down, and\n                received_down DataFrames with line IDs as columns.\n            flow_type: Type of flows to aggregate:\n                - 'sent': Pre-loss flows (power injected into transmission)\n                - 'received': Post-loss flows (power withdrawn after losses)\n            direction: Flow direction to calculate:\n                - 'up': Flows from area_from \u2192 area_to\n                - 'down': Flows from area_to \u2192 area_from  \n                - 'net': Net flows (up - down)\n\n        Returns:\n            DataFrame with border-level flow aggregations. Index matches input\n            flow data, columns are border identifiers. Values represent power\n            flows in MW. For net flows, positive values indicate net export\n            from area_from to area_to.\n\n        Raises:\n            ValueError: If flow_type not in ['sent', 'received'] or direction\n                not in ['up', 'down', 'net']\n\n        Example:\n\n            &gt;&gt;&gt; # Calculate net sent flows (most common use case)\n            &gt;&gt;&gt; net_sent = calculator.calculate(flows, 'sent', 'net')\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Calculate received flows in up direction only\n            &gt;&gt;&gt; up_received = calculator.calculate(flows, 'received', 'up')\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; print(f\"DE\u2192FR net flow: {net_sent.loc['2024-01-01 12:00', 'DE-FR']:.0f} MW\")\n        \"\"\"\n        # Validate inputs\n        if flow_type not in ['sent', 'received']:\n            raise ValueError(f\"Unknown flow_type: {flow_type}. Must be 'sent' or 'received'\")\n        if direction not in ['up', 'down', 'net']:\n            raise ValueError(f\"Unknown flow direction: {direction}. Must be 'up', 'down', or 'net'\")\n\n        self._validate_time_series_data(line_flow_data.sent_up, \"sent_up\")\n        self._validate_time_series_data(line_flow_data.received_up, \"received_up\")\n\n        border_flows = {}\n\n        for border_id, border in self.area_border_model_df.iterrows():\n            lines_up, lines_down = self.get_border_lines_in_topological_up_and_down_direction(border_id)\n\n            if not lines_up and not lines_down:\n                # No lines for this border - create empty series\n                index = line_flow_data.sent_up.index\n                border_flows[border_id] = pd.Series(index=index, dtype=float)\n                continue\n\n            # Select appropriate flow data based on flow_type\n            if flow_type == 'sent':\n                flow_data_up = line_flow_data.sent_up\n                flow_data_down = line_flow_data.sent_down\n            else:  # flow_type == 'received'\n                flow_data_up = line_flow_data.received_up  \n                flow_data_down = line_flow_data.received_down\n\n            # Aggregate flows by direction relative to border\n            flow_parts_up = []\n            flow_parts_down = []\n\n            if lines_up:\n                # Lines in topological \"up\" direction\n                available_lines_up = [line for line in lines_up if line in flow_data_up.columns]\n                if available_lines_up:\n                    flow_parts_up.append(flow_data_up[available_lines_up])\n\n            if lines_down:  \n                # Lines in topological \"down\" direction contribute to opposite border flow\n                available_lines_down = [line for line in lines_down if line in flow_data_down.columns]\n                if available_lines_down:\n                    flow_parts_up.append(flow_data_down[available_lines_down])\n\n            if lines_down:\n                # Lines in topological \"down\" direction  \n                available_lines_down = [line for line in lines_down if line in flow_data_up.columns]\n                if available_lines_down:\n                    flow_parts_down.append(flow_data_up[available_lines_down])\n\n            if lines_up:\n                # Lines in topological \"up\" direction contribute to opposite border flow\n                available_lines_up = [line for line in lines_up if line in flow_data_down.columns]\n                if available_lines_up:\n                    flow_parts_down.append(flow_data_down[available_lines_up])\n\n            # Sum flows for each direction\n            if flow_parts_up:\n                flows_up_combined = pd.concat(flow_parts_up, axis=1)\n                flow_up = flows_up_combined.sum(axis=1)\n                flow_up[flows_up_combined.isna().all(axis=1)] = np.nan\n            else:\n                flow_up = pd.Series(index=line_flow_data.sent_up.index, dtype=float)\n\n            if flow_parts_down:\n                flows_down_combined = pd.concat(flow_parts_down, axis=1)  \n                flow_down = flows_down_combined.sum(axis=1)\n                flow_down[flows_down_combined.isna().all(axis=1)] = np.nan\n            else:\n                flow_down = pd.Series(index=line_flow_data.sent_up.index, dtype=float)\n\n            # Select final output based on direction parameter\n            if direction == 'up':\n                border_flows[border_id] = flow_up\n            elif direction == 'down':\n                border_flows[border_id] = flow_down\n            else:  # direction == 'net'\n                flow_net = flow_up.subtract(flow_down, fill_value=0)\n                # Preserve NaN when both directions are NaN\n                flow_net[flow_up.isna() &amp; flow_down.isna()] = np.nan\n                border_flows[border_id] = flow_net\n\n        result = pd.DataFrame(border_flows)\n        result.columns.name = self.border_identifier\n        return result\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mesqual.energy_data_handling.area_accounting.border_variable_flow_calculator.BorderFlowCalculator.calculate","title":"calculate","text":"<pre><code>calculate(line_flow_data: NetworkLineFlowsData, flow_type: Literal['sent', 'received'] = 'sent', direction: Literal['up', 'down', 'net'] = 'net') -&gt; DataFrame\n</code></pre> <p>Aggregate line-level power flows to border level.</p> <p>Sums power flows of all lines belonging to each border, respecting flow directionality and transmission loss conventions. The aggregation handles both pre-loss (sent) and post-loss (received) flows.</p> <p>Flow type selection: - 'sent': Flows before transmission losses (injected into lines) - 'received': Flows after transmission losses (withdrawn from lines)</p> <p>Direction options: - 'up': Flows from area_from to area_to only - 'down': Flows from area_to to area_from only - 'net': Net flows (up - down), positive means net export from area_from</p> <p>The method handles missing data by preserving NaN values when all  constituent flows are missing for a given timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>line_flow_data</code> <code>NetworkLineFlowsData</code> <p>NetworkLineFlowsData containing bidirectional flow time series. Must include sent_up, received_up, sent_down, and received_down DataFrames with line IDs as columns.</p> required <code>flow_type</code> <code>Literal['sent', 'received']</code> <p>Type of flows to aggregate: - 'sent': Pre-loss flows (power injected into transmission) - 'received': Post-loss flows (power withdrawn after losses)</p> <code>'sent'</code> <code>direction</code> <code>Literal['up', 'down', 'net']</code> <p>Flow direction to calculate: - 'up': Flows from area_from \u2192 area_to - 'down': Flows from area_to \u2192 area_from - 'net': Net flows (up - down)</p> <code>'net'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with border-level flow aggregations. Index matches input</p> <code>DataFrame</code> <p>flow data, columns are border identifiers. Values represent power</p> <code>DataFrame</code> <p>flows in MW. For net flows, positive values indicate net export</p> <code>DataFrame</code> <p>from area_from to area_to.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If flow_type not in ['sent', 'received'] or direction not in ['up', 'down', 'net']</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Calculate net sent flows (most common use case)\n&gt;&gt;&gt; net_sent = calculator.calculate(flows, 'sent', 'net')\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Calculate received flows in up direction only\n&gt;&gt;&gt; up_received = calculator.calculate(flows, 'received', 'up')\n&gt;&gt;&gt; \n&gt;&gt;&gt; print(f\"DE\u2192FR net flow: {net_sent.loc['2024-01-01 12:00', 'DE-FR']:.0f} MW\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_variable_flow_calculator.py</code> <pre><code>def calculate(\n    self,\n    line_flow_data: NetworkLineFlowsData,\n    flow_type: Literal['sent', 'received'] = 'sent',\n    direction: Literal['up', 'down', 'net'] = 'net'\n) -&gt; pd.DataFrame:\n    \"\"\"Aggregate line-level power flows to border level.\n\n    Sums power flows of all lines belonging to each border, respecting flow\n    directionality and transmission loss conventions. The aggregation handles\n    both pre-loss (sent) and post-loss (received) flows.\n\n    Flow type selection:\n    - 'sent': Flows before transmission losses (injected into lines)  \n    - 'received': Flows after transmission losses (withdrawn from lines)\n\n    Direction options:\n    - 'up': Flows from area_from to area_to only\n    - 'down': Flows from area_to to area_from only\n    - 'net': Net flows (up - down), positive means net export from area_from\n\n    The method handles missing data by preserving NaN values when all \n    constituent flows are missing for a given timestamp.\n\n    Args:\n        line_flow_data: NetworkLineFlowsData containing bidirectional flow\n            time series. Must include sent_up, received_up, sent_down, and\n            received_down DataFrames with line IDs as columns.\n        flow_type: Type of flows to aggregate:\n            - 'sent': Pre-loss flows (power injected into transmission)\n            - 'received': Post-loss flows (power withdrawn after losses)\n        direction: Flow direction to calculate:\n            - 'up': Flows from area_from \u2192 area_to\n            - 'down': Flows from area_to \u2192 area_from  \n            - 'net': Net flows (up - down)\n\n    Returns:\n        DataFrame with border-level flow aggregations. Index matches input\n        flow data, columns are border identifiers. Values represent power\n        flows in MW. For net flows, positive values indicate net export\n        from area_from to area_to.\n\n    Raises:\n        ValueError: If flow_type not in ['sent', 'received'] or direction\n            not in ['up', 'down', 'net']\n\n    Example:\n\n        &gt;&gt;&gt; # Calculate net sent flows (most common use case)\n        &gt;&gt;&gt; net_sent = calculator.calculate(flows, 'sent', 'net')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Calculate received flows in up direction only\n        &gt;&gt;&gt; up_received = calculator.calculate(flows, 'received', 'up')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; print(f\"DE\u2192FR net flow: {net_sent.loc['2024-01-01 12:00', 'DE-FR']:.0f} MW\")\n    \"\"\"\n    # Validate inputs\n    if flow_type not in ['sent', 'received']:\n        raise ValueError(f\"Unknown flow_type: {flow_type}. Must be 'sent' or 'received'\")\n    if direction not in ['up', 'down', 'net']:\n        raise ValueError(f\"Unknown flow direction: {direction}. Must be 'up', 'down', or 'net'\")\n\n    self._validate_time_series_data(line_flow_data.sent_up, \"sent_up\")\n    self._validate_time_series_data(line_flow_data.received_up, \"received_up\")\n\n    border_flows = {}\n\n    for border_id, border in self.area_border_model_df.iterrows():\n        lines_up, lines_down = self.get_border_lines_in_topological_up_and_down_direction(border_id)\n\n        if not lines_up and not lines_down:\n            # No lines for this border - create empty series\n            index = line_flow_data.sent_up.index\n            border_flows[border_id] = pd.Series(index=index, dtype=float)\n            continue\n\n        # Select appropriate flow data based on flow_type\n        if flow_type == 'sent':\n            flow_data_up = line_flow_data.sent_up\n            flow_data_down = line_flow_data.sent_down\n        else:  # flow_type == 'received'\n            flow_data_up = line_flow_data.received_up  \n            flow_data_down = line_flow_data.received_down\n\n        # Aggregate flows by direction relative to border\n        flow_parts_up = []\n        flow_parts_down = []\n\n        if lines_up:\n            # Lines in topological \"up\" direction\n            available_lines_up = [line for line in lines_up if line in flow_data_up.columns]\n            if available_lines_up:\n                flow_parts_up.append(flow_data_up[available_lines_up])\n\n        if lines_down:  \n            # Lines in topological \"down\" direction contribute to opposite border flow\n            available_lines_down = [line for line in lines_down if line in flow_data_down.columns]\n            if available_lines_down:\n                flow_parts_up.append(flow_data_down[available_lines_down])\n\n        if lines_down:\n            # Lines in topological \"down\" direction  \n            available_lines_down = [line for line in lines_down if line in flow_data_up.columns]\n            if available_lines_down:\n                flow_parts_down.append(flow_data_up[available_lines_down])\n\n        if lines_up:\n            # Lines in topological \"up\" direction contribute to opposite border flow\n            available_lines_up = [line for line in lines_up if line in flow_data_down.columns]\n            if available_lines_up:\n                flow_parts_down.append(flow_data_down[available_lines_up])\n\n        # Sum flows for each direction\n        if flow_parts_up:\n            flows_up_combined = pd.concat(flow_parts_up, axis=1)\n            flow_up = flows_up_combined.sum(axis=1)\n            flow_up[flows_up_combined.isna().all(axis=1)] = np.nan\n        else:\n            flow_up = pd.Series(index=line_flow_data.sent_up.index, dtype=float)\n\n        if flow_parts_down:\n            flows_down_combined = pd.concat(flow_parts_down, axis=1)  \n            flow_down = flows_down_combined.sum(axis=1)\n            flow_down[flows_down_combined.isna().all(axis=1)] = np.nan\n        else:\n            flow_down = pd.Series(index=line_flow_data.sent_up.index, dtype=float)\n\n        # Select final output based on direction parameter\n        if direction == 'up':\n            border_flows[border_id] = flow_up\n        elif direction == 'down':\n            border_flows[border_id] = flow_down\n        else:  # direction == 'net'\n            flow_net = flow_up.subtract(flow_down, fill_value=0)\n            # Preserve NaN when both directions are NaN\n            flow_net[flow_up.isna() &amp; flow_down.isna()] = np.nan\n            border_flows[border_id] = flow_net\n\n    result = pd.DataFrame(border_flows)\n    result.columns.name = self.border_identifier\n    return result\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mesqual.energy_data_handling.area_accounting.border_variable_price_spread_calculator.BorderPriceSpreadCalculator","title":"BorderPriceSpreadCalculator","text":"<p>               Bases: <code>AreaBorderVariableCalculatorBase</code></p> <p>Calculates electricity price spreads between areas for each border.</p> <p>This calculator computes price differences between connected areas. Price spreads are fundamental indicators in electricity markets for: - Market integration analysis (zero spreads indicate perfect coupling) - Congestion identification (non-zero spreads suggest transmission constraints) - Arbitrage opportunity assessment (price differences drive trading incentives) - Market efficiency evaluation (persistent spreads may indicate inefficiencies) - Cross-border flow direction prediction (flows typically follow price gradients)</p> <p>The calculator supports multiple spread calculation methods (Spread Types): - 'raw': price_to - price_from (preserves direction and sign) - 'absolute': |price_to - price_from| (magnitude only) - 'directional_up': max(price_to - price_from, 0) (only positive spreads) - 'directional_down': max(price_from - price_to, 0) (only negative spreads as positive)</p> <p>Attributes:</p> Name Type Description <code>variable_name</code> <code>str</code> <p>Returns 'price_spread' for identification</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample area price data\n&gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n&gt;&gt;&gt; area_prices = pd.DataFrame({\n...     'DE': np.random.uniform(40, 80, 24),  # German prices\n...     'FR': np.random.uniform(35, 75, 24),  # French prices\n...     'BE': np.random.uniform(45, 85, 24)   # Belgian prices\n... }, index=time_index)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Set up border model and calculator (see base class docs for setup)\n&gt;&gt;&gt; calculator = BorderPriceSpreadCalculator(\n...     border_model_df, line_model_df, node_model_df, 'country'\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate raw price spreads\n&gt;&gt;&gt; raw_spreads = calculator.calculate(area_prices, spread_type='raw')\n&gt;&gt;&gt; print(f\"Average spread DE-FR: {raw_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate all spread types at once\n&gt;&gt;&gt; all_spreads = calculator.calculate_all_spread_types(area_prices)\n&gt;&gt;&gt; print(all_spreads.head())\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_variable_price_spread_calculator.py</code> <pre><code>class BorderPriceSpreadCalculator(AreaBorderVariableCalculatorBase):\n    \"\"\"Calculates electricity price spreads between areas for each border.\n\n    This calculator computes price differences between connected areas.\n    Price spreads are fundamental indicators in electricity markets for:\n    - Market integration analysis (zero spreads indicate perfect coupling)\n    - Congestion identification (non-zero spreads suggest transmission constraints)\n    - Arbitrage opportunity assessment (price differences drive trading incentives)\n    - Market efficiency evaluation (persistent spreads may indicate inefficiencies)\n    - Cross-border flow direction prediction (flows typically follow price gradients)\n\n    The calculator supports multiple spread calculation methods (Spread Types):\n    - 'raw': price_to - price_from (preserves direction and sign)\n    - 'absolute': |price_to - price_from| (magnitude only)\n    - 'directional_up': max(price_to - price_from, 0) (only positive spreads)\n    - 'directional_down': max(price_from - price_to, 0) (only negative spreads as positive)\n\n    Attributes:\n        variable_name (str): Returns 'price_spread' for identification\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample area price data\n        &gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n        &gt;&gt;&gt; area_prices = pd.DataFrame({\n        ...     'DE': np.random.uniform(40, 80, 24),  # German prices\n        ...     'FR': np.random.uniform(35, 75, 24),  # French prices\n        ...     'BE': np.random.uniform(45, 85, 24)   # Belgian prices\n        ... }, index=time_index)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Set up border model and calculator (see base class docs for setup)\n        &gt;&gt;&gt; calculator = BorderPriceSpreadCalculator(\n        ...     border_model_df, line_model_df, node_model_df, 'country'\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate raw price spreads\n        &gt;&gt;&gt; raw_spreads = calculator.calculate(area_prices, spread_type='raw')\n        &gt;&gt;&gt; print(f\"Average spread DE-FR: {raw_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate all spread types at once\n        &gt;&gt;&gt; all_spreads = calculator.calculate_all_spread_types(area_prices)\n        &gt;&gt;&gt; print(all_spreads.head())\n    \"\"\"\n\n    @property\n    def variable_name(self) -&gt; str:\n        return \"price_spread\"\n\n    def calculate(\n        self,\n        area_price_df: pd.DataFrame,\n        spread_type: Literal['raw', 'absolute', 'directional_up', 'directional_down'] = 'raw'\n    ) -&gt; pd.DataFrame:\n        \"\"\"Calculate electricity price spreads between connected market areas.\n\n        Computes price differences across transmission borders using the specified\n        calculation method. Price spreads are calculated as directional differences\n        based on the border naming convention (area_from \u2192 area_to).\n\n        The calculation handles missing area data gracefully by excluding borders\n        where either area lacks price data. This is common when analyzing subsets\n        of larger energy systems or when dealing with data availability issues.\n\n        Args:\n            area_price_df (pd.DataFrame): Time series of area-level electricity prices.\n                - Index: DateTime index for time series analysis\n                - Columns: Area identifiers matching border area names\n                - Values: Prices in consistent units (e.g., EUR/MWh, USD/MWh)\n                - Example shape: (8760 hours, N areas) for annual analysis\n\n            spread_type (Literal): Method for calculating price spreads.\n                - 'raw': Directional price differences (default, preserves sign)\n                - 'absolute': Magnitude of price differences (always non-negative)\n                - 'directional_up': Only spreads where price_to &gt; price_from\n                - 'directional_down': Only spreads where price_from &gt; price_to\n\n        Returns:\n            pd.DataFrame: Border-level price spreads with temporal dimension.\n                - Index: Same as input area_price_df (typically DatetimeIndex)\n                - Columns: Border identifiers (e.g., 'DE-FR', 'FR-BE')\n                - Column name: Set to self.border_identifier for consistency\n                - Values: Price spreads in same units as input prices\n                - Missing data: NaN where area price data is unavailable\n\n        Raises:\n            ValueError: If spread_type is not one of the supported options\n\n        Example:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Create hourly price data for German and French markets\n            &gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n            &gt;&gt;&gt; prices = pd.DataFrame({\n            ...     'DE': [45.2, 43.1, 41.8, 39.5, 38.2, 42.1, 52.3, 65.4,\n            ...            72.1, 68.9, 64.2, 58.7, 55.1, 53.8, 56.2, 61.4,\n            ...            67.8, 74.2, 69.1, 64.3, 58.9, 52.1, 48.7, 46.3],\n            ...     'FR': [42.8, 41.2, 39.1, 37.8, 36.4, 40.3, 49.8, 62.1,\n            ...            68.9, 65.2, 61.4, 56.8, 53.2, 51.9, 54.1, 58.7,\n            ...            64.3, 70.8, 66.2, 61.1, 56.3, 49.8, 46.1, 43.9]\n            ... }, index=time_index)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Calculate raw spreads (FR - DE for DE-FR border)\n            &gt;&gt;&gt; raw_spreads = calculator.calculate(prices, 'raw')\n            &gt;&gt;&gt; print(f\"Average DE-FR spread: {raw_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n            &gt;&gt;&gt; # Output: Average DE-FR spread: -2.15 EUR/MWh (German prices higher)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Calculate absolute spreads for congestion analysis\n            &gt;&gt;&gt; abs_spreads = calculator.calculate(prices, 'absolute')\n            &gt;&gt;&gt; print(f\"Average absolute spread: {abs_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n            &gt;&gt;&gt; # Output: Average absolute spread: 2.15 EUR/MWh\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Analyze directional spreads for flow prediction\n            &gt;&gt;&gt; up_spreads = calculator.calculate(prices, 'directional_up')\n            &gt;&gt;&gt; down_spreads = calculator.calculate(prices, 'directional_down')\n            &gt;&gt;&gt; print(f\"Hours with FR &gt; DE prices: {(up_spreads['DE-FR'] &gt; 0).sum()}\")\n            &gt;&gt;&gt; print(f\"Hours with DE &gt; FR prices: {(down_spreads['DE-FR'] &gt; 0).sum()}\")\n        \"\"\"\n        self._validate_time_series_data(area_price_df, 'area_price_df')\n\n        spreads = {}\n\n        for border_id, border in self.area_border_model_df.iterrows():\n            area_from = border[self.source_area_identifier]\n            area_to = border[self.target_area_identifier]\n\n            if area_from in area_price_df.columns and area_to in area_price_df.columns:\n                price_from = area_price_df[area_from]\n                price_to = area_price_df[area_to]\n\n                raw_spread = price_to - price_from\n\n                if spread_type == 'raw':\n                    spreads[border_id] = raw_spread\n                elif spread_type == 'absolute':\n                    spreads[border_id] = raw_spread.abs()\n                elif spread_type == 'directional_up':\n                    spreads[border_id] = raw_spread.clip(lower=0)\n                elif spread_type == 'directional_down':\n                    spreads[border_id] = (-1 * raw_spread).clip(lower=0)\n                else:\n                    raise ValueError(f\"Unknown spread_type: {spread_type}\")\n\n        result = pd.DataFrame(spreads)\n        result.columns.name = self.border_identifier\n        return result\n\n    def calculate_all_spread_types(self, area_price_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Calculate all price spread types simultaneously for comprehensive analysis.\n\n        Returns a MultiIndex DataFrame with all four spread calculation methods\n        (raw, absolute, directional_up, directional_down) in a single DataFrame,\n        providing a complete view of price relationships across all borders.\n\n        Args:\n            area_price_df (pd.DataFrame): Time series of area-level electricity prices.\n                Same format as required by the calculate() method:\n                - Index: DateTime index for temporal analysis\n                - Columns: Area identifiers matching border definitions\n                - Values: Prices in consistent units (e.g., EUR/MWh, USD/MWh)\n\n        Returns:\n            pd.DataFrame: MultiIndex DataFrame with comprehensive spread analysis.\n                - Index: Same temporal index as input area_price_df\n                - Columns: MultiIndex with two levels:\n                    - Level 0 ('spread_type'): ['raw', 'absolute', 'directional_up', 'directional_down']\n                    - Level 1 (border_identifier): Border names (e.g., 'DE-FR', 'FR-BE')\n                - Values: Price spreads in same units as input prices\n                - Structure: (time_periods, spread_types \u00d7 borders)\n\n        Example:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Create sample price data\n            &gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n            &gt;&gt;&gt; prices = pd.DataFrame({\n            ...     'DE': np.random.uniform(40, 80, 24),\n            ...     'FR': np.random.uniform(35, 75, 24),\n            ...     'BE': np.random.uniform(45, 85, 24)\n            ... }, index=time_index)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Calculate all spread types\n            &gt;&gt;&gt; all_spreads = calculator.calculate_all_spread_types(prices)\n            &gt;&gt;&gt; print(all_spreads.columns.names)\n            &gt;&gt;&gt; # Output: ['spread_type', 'country_border']\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Access specific spread types\n            &gt;&gt;&gt; raw_spreads = all_spreads['raw']\n            &gt;&gt;&gt; absolute_spreads = all_spreads['absolute']\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Analyze spread statistics by type\n            &gt;&gt;&gt; spread_stats = all_spreads.groupby(level='spread_type', axis=1).mean()\n            &gt;&gt;&gt; print(spread_stats)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Compare directional flows\n            &gt;&gt;&gt; up_flows = all_spreads['directional_up'].sum(axis=1)\n            &gt;&gt;&gt; down_flows = all_spreads['directional_down'].sum(axis=1)\n            &gt;&gt;&gt; net_spread_pressure = up_flows - down_flows\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Identify hours with high price volatility\n            &gt;&gt;&gt; high_volatility_hours = absolute_spreads.mean(axis=1) &gt; 10  # EUR/MWh threshold\n            &gt;&gt;&gt; print(f\"Hours with high spread volatility: {high_volatility_hours.sum()}\")\n        \"\"\"\n        results = {}\n        for spread_type in ['raw', 'absolute', 'directional_up', 'directional_down']:\n            results[spread_type] = self.calculate(area_price_df, spread_type)\n\n        return pd.concat(results, axis=1, names=['spread_type'])\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mesqual.energy_data_handling.area_accounting.border_variable_price_spread_calculator.BorderPriceSpreadCalculator.calculate","title":"calculate","text":"<pre><code>calculate(area_price_df: DataFrame, spread_type: Literal['raw', 'absolute', 'directional_up', 'directional_down'] = 'raw') -&gt; DataFrame\n</code></pre> <p>Calculate electricity price spreads between connected market areas.</p> <p>Computes price differences across transmission borders using the specified calculation method. Price spreads are calculated as directional differences based on the border naming convention (area_from \u2192 area_to).</p> <p>The calculation handles missing area data gracefully by excluding borders where either area lacks price data. This is common when analyzing subsets of larger energy systems or when dealing with data availability issues.</p> <p>Parameters:</p> Name Type Description Default <code>area_price_df</code> <code>DataFrame</code> <p>Time series of area-level electricity prices. - Index: DateTime index for time series analysis - Columns: Area identifiers matching border area names - Values: Prices in consistent units (e.g., EUR/MWh, USD/MWh) - Example shape: (8760 hours, N areas) for annual analysis</p> required <code>spread_type</code> <code>Literal</code> <p>Method for calculating price spreads. - 'raw': Directional price differences (default, preserves sign) - 'absolute': Magnitude of price differences (always non-negative) - 'directional_up': Only spreads where price_to &gt; price_from - 'directional_down': Only spreads where price_from &gt; price_to</p> <code>'raw'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Border-level price spreads with temporal dimension. - Index: Same as input area_price_df (typically DatetimeIndex) - Columns: Border identifiers (e.g., 'DE-FR', 'FR-BE') - Column name: Set to self.border_identifier for consistency - Values: Price spreads in same units as input prices - Missing data: NaN where area price data is unavailable</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If spread_type is not one of the supported options</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create hourly price data for German and French markets\n&gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n&gt;&gt;&gt; prices = pd.DataFrame({\n...     'DE': [45.2, 43.1, 41.8, 39.5, 38.2, 42.1, 52.3, 65.4,\n...            72.1, 68.9, 64.2, 58.7, 55.1, 53.8, 56.2, 61.4,\n...            67.8, 74.2, 69.1, 64.3, 58.9, 52.1, 48.7, 46.3],\n...     'FR': [42.8, 41.2, 39.1, 37.8, 36.4, 40.3, 49.8, 62.1,\n...            68.9, 65.2, 61.4, 56.8, 53.2, 51.9, 54.1, 58.7,\n...            64.3, 70.8, 66.2, 61.1, 56.3, 49.8, 46.1, 43.9]\n... }, index=time_index)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate raw spreads (FR - DE for DE-FR border)\n&gt;&gt;&gt; raw_spreads = calculator.calculate(prices, 'raw')\n&gt;&gt;&gt; print(f\"Average DE-FR spread: {raw_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n&gt;&gt;&gt; # Output: Average DE-FR spread: -2.15 EUR/MWh (German prices higher)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate absolute spreads for congestion analysis\n&gt;&gt;&gt; abs_spreads = calculator.calculate(prices, 'absolute')\n&gt;&gt;&gt; print(f\"Average absolute spread: {abs_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n&gt;&gt;&gt; # Output: Average absolute spread: 2.15 EUR/MWh\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Analyze directional spreads for flow prediction\n&gt;&gt;&gt; up_spreads = calculator.calculate(prices, 'directional_up')\n&gt;&gt;&gt; down_spreads = calculator.calculate(prices, 'directional_down')\n&gt;&gt;&gt; print(f\"Hours with FR &gt; DE prices: {(up_spreads['DE-FR'] &gt; 0).sum()}\")\n&gt;&gt;&gt; print(f\"Hours with DE &gt; FR prices: {(down_spreads['DE-FR'] &gt; 0).sum()}\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_variable_price_spread_calculator.py</code> <pre><code>def calculate(\n    self,\n    area_price_df: pd.DataFrame,\n    spread_type: Literal['raw', 'absolute', 'directional_up', 'directional_down'] = 'raw'\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate electricity price spreads between connected market areas.\n\n    Computes price differences across transmission borders using the specified\n    calculation method. Price spreads are calculated as directional differences\n    based on the border naming convention (area_from \u2192 area_to).\n\n    The calculation handles missing area data gracefully by excluding borders\n    where either area lacks price data. This is common when analyzing subsets\n    of larger energy systems or when dealing with data availability issues.\n\n    Args:\n        area_price_df (pd.DataFrame): Time series of area-level electricity prices.\n            - Index: DateTime index for time series analysis\n            - Columns: Area identifiers matching border area names\n            - Values: Prices in consistent units (e.g., EUR/MWh, USD/MWh)\n            - Example shape: (8760 hours, N areas) for annual analysis\n\n        spread_type (Literal): Method for calculating price spreads.\n            - 'raw': Directional price differences (default, preserves sign)\n            - 'absolute': Magnitude of price differences (always non-negative)\n            - 'directional_up': Only spreads where price_to &gt; price_from\n            - 'directional_down': Only spreads where price_from &gt; price_to\n\n    Returns:\n        pd.DataFrame: Border-level price spreads with temporal dimension.\n            - Index: Same as input area_price_df (typically DatetimeIndex)\n            - Columns: Border identifiers (e.g., 'DE-FR', 'FR-BE')\n            - Column name: Set to self.border_identifier for consistency\n            - Values: Price spreads in same units as input prices\n            - Missing data: NaN where area price data is unavailable\n\n    Raises:\n        ValueError: If spread_type is not one of the supported options\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create hourly price data for German and French markets\n        &gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n        &gt;&gt;&gt; prices = pd.DataFrame({\n        ...     'DE': [45.2, 43.1, 41.8, 39.5, 38.2, 42.1, 52.3, 65.4,\n        ...            72.1, 68.9, 64.2, 58.7, 55.1, 53.8, 56.2, 61.4,\n        ...            67.8, 74.2, 69.1, 64.3, 58.9, 52.1, 48.7, 46.3],\n        ...     'FR': [42.8, 41.2, 39.1, 37.8, 36.4, 40.3, 49.8, 62.1,\n        ...            68.9, 65.2, 61.4, 56.8, 53.2, 51.9, 54.1, 58.7,\n        ...            64.3, 70.8, 66.2, 61.1, 56.3, 49.8, 46.1, 43.9]\n        ... }, index=time_index)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate raw spreads (FR - DE for DE-FR border)\n        &gt;&gt;&gt; raw_spreads = calculator.calculate(prices, 'raw')\n        &gt;&gt;&gt; print(f\"Average DE-FR spread: {raw_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n        &gt;&gt;&gt; # Output: Average DE-FR spread: -2.15 EUR/MWh (German prices higher)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate absolute spreads for congestion analysis\n        &gt;&gt;&gt; abs_spreads = calculator.calculate(prices, 'absolute')\n        &gt;&gt;&gt; print(f\"Average absolute spread: {abs_spreads['DE-FR'].mean():.2f} EUR/MWh\")\n        &gt;&gt;&gt; # Output: Average absolute spread: 2.15 EUR/MWh\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Analyze directional spreads for flow prediction\n        &gt;&gt;&gt; up_spreads = calculator.calculate(prices, 'directional_up')\n        &gt;&gt;&gt; down_spreads = calculator.calculate(prices, 'directional_down')\n        &gt;&gt;&gt; print(f\"Hours with FR &gt; DE prices: {(up_spreads['DE-FR'] &gt; 0).sum()}\")\n        &gt;&gt;&gt; print(f\"Hours with DE &gt; FR prices: {(down_spreads['DE-FR'] &gt; 0).sum()}\")\n    \"\"\"\n    self._validate_time_series_data(area_price_df, 'area_price_df')\n\n    spreads = {}\n\n    for border_id, border in self.area_border_model_df.iterrows():\n        area_from = border[self.source_area_identifier]\n        area_to = border[self.target_area_identifier]\n\n        if area_from in area_price_df.columns and area_to in area_price_df.columns:\n            price_from = area_price_df[area_from]\n            price_to = area_price_df[area_to]\n\n            raw_spread = price_to - price_from\n\n            if spread_type == 'raw':\n                spreads[border_id] = raw_spread\n            elif spread_type == 'absolute':\n                spreads[border_id] = raw_spread.abs()\n            elif spread_type == 'directional_up':\n                spreads[border_id] = raw_spread.clip(lower=0)\n            elif spread_type == 'directional_down':\n                spreads[border_id] = (-1 * raw_spread).clip(lower=0)\n            else:\n                raise ValueError(f\"Unknown spread_type: {spread_type}\")\n\n    result = pd.DataFrame(spreads)\n    result.columns.name = self.border_identifier\n    return result\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/border_variables/#mesqual.energy_data_handling.area_accounting.border_variable_price_spread_calculator.BorderPriceSpreadCalculator.calculate_all_spread_types","title":"calculate_all_spread_types","text":"<pre><code>calculate_all_spread_types(area_price_df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Calculate all price spread types simultaneously for comprehensive analysis.</p> <p>Returns a MultiIndex DataFrame with all four spread calculation methods (raw, absolute, directional_up, directional_down) in a single DataFrame, providing a complete view of price relationships across all borders.</p> <p>Parameters:</p> Name Type Description Default <code>area_price_df</code> <code>DataFrame</code> <p>Time series of area-level electricity prices. Same format as required by the calculate() method: - Index: DateTime index for temporal analysis - Columns: Area identifiers matching border definitions - Values: Prices in consistent units (e.g., EUR/MWh, USD/MWh)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: MultiIndex DataFrame with comprehensive spread analysis. - Index: Same temporal index as input area_price_df - Columns: MultiIndex with two levels:     - Level 0 ('spread_type'): ['raw', 'absolute', 'directional_up', 'directional_down']     - Level 1 (border_identifier): Border names (e.g., 'DE-FR', 'FR-BE') - Values: Price spreads in same units as input prices - Structure: (time_periods, spread_types \u00d7 borders)</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample price data\n&gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n&gt;&gt;&gt; prices = pd.DataFrame({\n...     'DE': np.random.uniform(40, 80, 24),\n...     'FR': np.random.uniform(35, 75, 24),\n...     'BE': np.random.uniform(45, 85, 24)\n... }, index=time_index)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate all spread types\n&gt;&gt;&gt; all_spreads = calculator.calculate_all_spread_types(prices)\n&gt;&gt;&gt; print(all_spreads.columns.names)\n&gt;&gt;&gt; # Output: ['spread_type', 'country_border']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Access specific spread types\n&gt;&gt;&gt; raw_spreads = all_spreads['raw']\n&gt;&gt;&gt; absolute_spreads = all_spreads['absolute']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Analyze spread statistics by type\n&gt;&gt;&gt; spread_stats = all_spreads.groupby(level='spread_type', axis=1).mean()\n&gt;&gt;&gt; print(spread_stats)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compare directional flows\n&gt;&gt;&gt; up_flows = all_spreads['directional_up'].sum(axis=1)\n&gt;&gt;&gt; down_flows = all_spreads['directional_down'].sum(axis=1)\n&gt;&gt;&gt; net_spread_pressure = up_flows - down_flows\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Identify hours with high price volatility\n&gt;&gt;&gt; high_volatility_hours = absolute_spreads.mean(axis=1) &gt; 10  # EUR/MWh threshold\n&gt;&gt;&gt; print(f\"Hours with high spread volatility: {high_volatility_hours.sum()}\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_variable_price_spread_calculator.py</code> <pre><code>def calculate_all_spread_types(self, area_price_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Calculate all price spread types simultaneously for comprehensive analysis.\n\n    Returns a MultiIndex DataFrame with all four spread calculation methods\n    (raw, absolute, directional_up, directional_down) in a single DataFrame,\n    providing a complete view of price relationships across all borders.\n\n    Args:\n        area_price_df (pd.DataFrame): Time series of area-level electricity prices.\n            Same format as required by the calculate() method:\n            - Index: DateTime index for temporal analysis\n            - Columns: Area identifiers matching border definitions\n            - Values: Prices in consistent units (e.g., EUR/MWh, USD/MWh)\n\n    Returns:\n        pd.DataFrame: MultiIndex DataFrame with comprehensive spread analysis.\n            - Index: Same temporal index as input area_price_df\n            - Columns: MultiIndex with two levels:\n                - Level 0 ('spread_type'): ['raw', 'absolute', 'directional_up', 'directional_down']\n                - Level 1 (border_identifier): Border names (e.g., 'DE-FR', 'FR-BE')\n            - Values: Price spreads in same units as input prices\n            - Structure: (time_periods, spread_types \u00d7 borders)\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample price data\n        &gt;&gt;&gt; time_index = pd.date_range('2024-01-01', periods=24, freq='h')\n        &gt;&gt;&gt; prices = pd.DataFrame({\n        ...     'DE': np.random.uniform(40, 80, 24),\n        ...     'FR': np.random.uniform(35, 75, 24),\n        ...     'BE': np.random.uniform(45, 85, 24)\n        ... }, index=time_index)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate all spread types\n        &gt;&gt;&gt; all_spreads = calculator.calculate_all_spread_types(prices)\n        &gt;&gt;&gt; print(all_spreads.columns.names)\n        &gt;&gt;&gt; # Output: ['spread_type', 'country_border']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Access specific spread types\n        &gt;&gt;&gt; raw_spreads = all_spreads['raw']\n        &gt;&gt;&gt; absolute_spreads = all_spreads['absolute']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Analyze spread statistics by type\n        &gt;&gt;&gt; spread_stats = all_spreads.groupby(level='spread_type', axis=1).mean()\n        &gt;&gt;&gt; print(spread_stats)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compare directional flows\n        &gt;&gt;&gt; up_flows = all_spreads['directional_up'].sum(axis=1)\n        &gt;&gt;&gt; down_flows = all_spreads['directional_down'].sum(axis=1)\n        &gt;&gt;&gt; net_spread_pressure = up_flows - down_flows\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Identify hours with high price volatility\n        &gt;&gt;&gt; high_volatility_hours = absolute_spreads.mean(axis=1) &gt; 10  # EUR/MWh threshold\n        &gt;&gt;&gt; print(f\"Hours with high spread volatility: {high_volatility_hours.sum()}\")\n    \"\"\"\n    results = {}\n    for spread_type in ['raw', 'absolute', 'directional_up', 'directional_down']:\n        results[spread_type] = self.calculate(area_price_df, spread_type)\n\n    return pd.concat(results, axis=1, names=['spread_type'])\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/","title":"Area and AreaBorder Model-DF Generators","text":"<p>Border model generation for energy system area connectivity analysis.</p> <p>This module provides functionality for identifying and modeling borders between energy system areas based on line topologies. It supports the creation of comprehensive border_model_dfs that capture directional relationships, naming conventions, and geometric properties essential for energy systems analysis.</p> Key Capabilities <ul> <li>Automatic border identification from line topology</li> <li>Standardized border naming conventions with directional awareness</li> <li>Integration with geometric border calculators</li> <li>Network graph generation for area connectivity analysis</li> <li>Support for both physical and logical borders (geographically touching borders vs geographically separated borders)</li> </ul> Typical Energy Use Cases <ul> <li>Modeling interconnections between countries, control areas, or market zones</li> <li>Cross-border capacity and flow analysis</li> <li>Network visualization and analysis</li> </ul> MESQUAL Integration <p>This module integrates with MESQUAL's area accounting system to provide border_model_df building capabilities that support spatial energy system analysis and cross-border flow calculations.</p>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator","title":"AreaModelGenerator","text":"<p>               Bases: <code>GeoModelGeneratorBase</code></p> <p>Generates comprehensive area model DataFrames from node-to-area mappings.</p> <p>This class creates detailed area model DataFrames that aggregate node-level data into area-level representations for energy system analysis. It supports automatic area discovery, node counting, and geographic representative point calculation for visualization and spatial analysis.</p> <p>The generator processes node model data with area assignments to create comprehensive area models suitable for energy system aggregation, market analysis, and spatial visualization workflows.</p> Key Features <ul> <li>Automatic area discovery from node-to-area mappings</li> <li>Representative geographic point calculation for visualization</li> <li>Integration with geometric area data (polygons, boundaries)</li> <li>Support for different area granularities (countries, bidding zones, regions)</li> <li>Robust handling of missing or incomplete area assignments</li> </ul> MESQUAL Integration <p>Designed to work with MESQUAL's area accounting system, providing area model building capabilities that support spatial energy system analysis, capacity aggregation, and visualization workflows.</p> <p>Attributes:</p> Name Type Description <code>node_model_df</code> <code>DataFrame</code> <p>Node-level data with area assignments</p> <code>area_column</code> <code>str</code> <p>Column name containing area identifiers</p> <code>geo_location_column</code> <code>str</code> <p>Column name containing geographic Point objects</p> <p>Examples:</p> <pre><code>Basic area model generation:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from shapely.geometry import Point\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create node model with area assignments\n&gt;&gt;&gt; node_data = pd.DataFrame({\n&gt;&gt;&gt;     'voltage': [380, 380, 220, 380, 220],\n&gt;&gt;&gt;     'country': ['DE', 'DE', 'FR', 'FR', 'BE'],\n&gt;&gt;&gt;     'capacity_mw': [2000, 1500, 800, 1200, 600],\n&gt;&gt;&gt;     'location': [Point(10, 52), Point(11, 53), Point(2, 48),\n&gt;&gt;&gt;                  Point(3, 49), Point(4, 50)]\n&gt;&gt;&gt; }, index=['DE1', 'DE2', 'FR1', 'FR2', 'BE1'])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Generate area model\n&gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n&gt;&gt;&gt; area_model = generator.generate_area_model()\n&gt;&gt;&gt; print(area_model)\n              node_count projection_point\n    country\n    DE               2    POINT (10.5 52.5)\n    FR               2    POINT (2.5 48.5)\n    BE               1    POINT (4 50)\n\nEnhanced area model with geometry:\n&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; from shapely.geometry import Polygon\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create area geometries\n&gt;&gt;&gt; area_polygons = gpd.GeoDataFrame({\n&gt;&gt;&gt;     'geometry': [\n&gt;&gt;&gt;         Polygon([(9, 51), (12, 51), (12, 54), (9, 54)]),  # DE\n&gt;&gt;&gt;         Polygon([(1, 47), (4, 47), (4, 50), (1, 50)]),   # FR\n&gt;&gt;&gt;         Polygon([(3, 49), (5, 49), (5, 51), (3, 51)])    # BE\n&gt;&gt;&gt;     ]\n&gt;&gt;&gt; }, index=['DE', 'FR', 'BE'])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Enhance with geometry\n&gt;&gt;&gt; area_model_geo = generator.enhance_with_geometry(area_model, area_polygons)\n&gt;&gt;&gt; print(f\"Enhanced model has geometry: {'geometry' in area_model_geo.columns}\")\n\nCustom enhancement workflow:\n&gt;&gt;&gt; # Step-by-step area model building\n&gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n&gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(base_model)\n&gt;&gt;&gt; final_model = generator.enhance_area_model_df_by_adding_representative_geo_point(enhanced_model)\n&gt;&gt;&gt; print(f\"Created area model with {len(final_model)} areas\")\n</code></pre> Energy Domain Context <ul> <li>Area models are fundamental for energy system analysis, enabling:<ul> <li>Projection of node-level data to area-level data (e.g. nodal prices -&gt; area prices)</li> <li>Market zone aggregation and analysis</li> <li>Regional energy balance studies</li> <li>...</li> </ul> </li> </ul> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>class AreaModelGenerator(GeoModelGeneratorBase):\n    \"\"\"Generates comprehensive area model DataFrames from node-to-area mappings.\n\n    This class creates detailed area model DataFrames that aggregate node-level data\n    into area-level representations for energy system analysis. It supports\n    automatic area discovery, node counting, and geographic representative point\n    calculation for visualization and spatial analysis.\n\n    The generator processes node model data with area assignments to create\n    comprehensive area models suitable for energy system aggregation, market\n    analysis, and spatial visualization workflows.\n\n    Key Features:\n        - Automatic area discovery from node-to-area mappings\n        - Representative geographic point calculation for visualization\n        - Integration with geometric area data (polygons, boundaries)\n        - Support for different area granularities (countries, bidding zones, regions)\n        - Robust handling of missing or incomplete area assignments\n\n    MESQUAL Integration:\n        Designed to work with MESQUAL's area accounting system, providing\n        area model building capabilities that support spatial energy system analysis,\n        capacity aggregation, and visualization workflows.\n\n    Attributes:\n        node_model_df (pd.DataFrame): Node-level data with area assignments\n        area_column (str): Column name containing area identifiers\n        geo_location_column (str): Column name containing geographic Point objects\n\n    Examples:\n\n        Basic area model generation:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from shapely.geometry import Point\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create node model with area assignments\n        &gt;&gt;&gt; node_data = pd.DataFrame({\n        &gt;&gt;&gt;     'voltage': [380, 380, 220, 380, 220],\n        &gt;&gt;&gt;     'country': ['DE', 'DE', 'FR', 'FR', 'BE'],\n        &gt;&gt;&gt;     'capacity_mw': [2000, 1500, 800, 1200, 600],\n        &gt;&gt;&gt;     'location': [Point(10, 52), Point(11, 53), Point(2, 48),\n        &gt;&gt;&gt;                  Point(3, 49), Point(4, 50)]\n        &gt;&gt;&gt; }, index=['DE1', 'DE2', 'FR1', 'FR2', 'BE1'])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Generate area model\n        &gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n        &gt;&gt;&gt; area_model = generator.generate_area_model()\n        &gt;&gt;&gt; print(area_model)\n                      node_count projection_point\n            country\n            DE               2    POINT (10.5 52.5)\n            FR               2    POINT (2.5 48.5)\n            BE               1    POINT (4 50)\n\n        Enhanced area model with geometry:\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; from shapely.geometry import Polygon\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create area geometries\n        &gt;&gt;&gt; area_polygons = gpd.GeoDataFrame({\n        &gt;&gt;&gt;     'geometry': [\n        &gt;&gt;&gt;         Polygon([(9, 51), (12, 51), (12, 54), (9, 54)]),  # DE\n        &gt;&gt;&gt;         Polygon([(1, 47), (4, 47), (4, 50), (1, 50)]),   # FR\n        &gt;&gt;&gt;         Polygon([(3, 49), (5, 49), (5, 51), (3, 51)])    # BE\n        &gt;&gt;&gt;     ]\n        &gt;&gt;&gt; }, index=['DE', 'FR', 'BE'])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Enhance with geometry\n        &gt;&gt;&gt; area_model_geo = generator.enhance_with_geometry(area_model, area_polygons)\n        &gt;&gt;&gt; print(f\"Enhanced model has geometry: {'geometry' in area_model_geo.columns}\")\n\n        Custom enhancement workflow:\n        &gt;&gt;&gt; # Step-by-step area model building\n        &gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n        &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(base_model)\n        &gt;&gt;&gt; final_model = generator.enhance_area_model_df_by_adding_representative_geo_point(enhanced_model)\n        &gt;&gt;&gt; print(f\"Created area model with {len(final_model)} areas\")\n\n    Energy Domain Context:\n        - Area models are fundamental for energy system analysis, enabling:\n            - Projection of node-level data to area-level data (e.g. nodal prices -&gt; area prices)\n            - Market zone aggregation and analysis\n            - Regional energy balance studies\n            - ...\n    \"\"\"\n\n    def __init__(\n            self,\n            node_model_df: pd.DataFrame,\n            area_column: str,\n            geo_location_column: str = None,\n    ):\n        \"\"\"Initialize the area model generator.\n\n        Args:\n            node_model_df: DataFrame containing node-level data with area assignments.\n                Must contain area_column with area identifiers for each node.\n                May contain geographic Point objects for spatial analysis.\n            area_column: Column name in node_model_df containing area assignments\n                (e.g., 'country', 'bidding_zone', 'market_region', 'control_area').\n            geo_location_column: Column name containing geographic Point objects\n                for representative point calculation. If None, automatically\n                detects column containing Point geometries.\n\n        Raises:\n            ValueError: If area_column is not found in node_model_df columns.\n\n        Example:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from shapely.geometry import Point\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Node data with area assignments\n            &gt;&gt;&gt; nodes = pd.DataFrame({\n            &gt;&gt;&gt;     'voltage_kv': [380, 220, 380, 150],\n            &gt;&gt;&gt;     'country': ['DE', 'DE', 'FR', 'FR'], \n            &gt;&gt;&gt;     'bidding_zone': ['DE_LU', 'DE_LU', 'FR', 'FR'],\n            &gt;&gt;&gt;     'coordinates': [Point(10, 52), Point(11, 53), Point(2, 48), Point(3, 49)]\n            &gt;&gt;&gt; }, index=['DE1', 'DE2', 'FR1', 'FR2'])\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Initialize for country-level analysis\n            &gt;&gt;&gt; generator = AreaModelGenerator(nodes, 'country', 'coordinates')\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Or let it auto-detect geographic column\n            &gt;&gt;&gt; generator = AreaModelGenerator(nodes, 'bidding_zone')\n        \"\"\"\n        self.node_model_df = node_model_df\n        self.area_column = area_column\n        self.geo_location_column = geo_location_column or self._identify_geo_location_column()\n        self._validate_inputs()\n\n    def _validate_inputs(self):\n        if self.area_column not in self.node_model_df.columns:\n            raise ValueError(\n                f\"Area column '{self.area_column}' not found in node_model_df. \"\n                f\"Available columns: {list(self.node_model_df.columns)}\"\n            )\n\n    def _identify_geo_location_column(self) -&gt; str | None:\n        for c in self.node_model_df.columns:\n            if all(isinstance(i, Point) or i is None for i in self.node_model_df[c].values):\n                return c\n        return None\n\n    def generate_base_area_model_from_area_names_in_node_model_df(self) -&gt; pd.DataFrame:\n        \"\"\"Generate base area model DataFrame from unique area names in node data.\n\n        Creates a minimal area model DataFrame containing only the unique area\n        identifiers found in the node model data. This forms the foundation\n        for building comprehensive area models.\n\n        Returns:\n            pd.DataFrame: Base area model with area identifiers as index.\n                Contains no additional columns - serves as starting point\n                for enhancement with node counts, geographic data, etc.\n\n        Example:\n\n            &gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n            &gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n            &gt;&gt;&gt; print(base_model)\n                Empty DataFrame\n                Columns: []\n                Index: ['DE', 'FR', 'BE']\n\n        Note:\n            Areas with None or NaN values in the area_column are excluded\n            from the generated model.\n        \"\"\"\n        unique_areas = self.node_model_df[self.area_column].dropna().unique()\n        area_model_df = pd.DataFrame(index=unique_areas)\n        area_model_df.index.name = self.area_column\n        return area_model_df\n\n    def ensure_completeness_of_area_model_df(self, area_model_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Ensure area model contains all areas present in node data.\n\n        Validates and extends an existing area model DataFrame to include\n        any areas found in the node data that might be missing from the\n        provided area model. This is useful when working with predefined\n        area models that may not cover all areas in the dataset.\n\n        Args:\n            area_model_df: Existing area model DataFrame to validate and extend.\n\n        Returns:\n            pd.DataFrame: Complete area model containing all areas from node data.\n                Existing data is preserved, new areas are added with NaN values\n                for existing columns.\n\n        Example:\n\n            &gt;&gt;&gt; # Predefined area model missing some areas\n            &gt;&gt;&gt; partial_model = pd.DataFrame({\n            &gt;&gt;&gt;     'max_price': [5000, 3000]\n            &gt;&gt;&gt; }, index=['DE', 'FR'])\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Ensure completeness (adds 'BE' if present in node data)\n            &gt;&gt;&gt; complete_model = generator.ensure_completeness_of_area_model_df(partial_model)\n            &gt;&gt;&gt; print(complete_model)\n                          max_price\n                country\n                DE             5000\n                FR             3000\n                BE              NaN\n\n        Use Case:\n            Essential for maintaining data consistency when combining\n            predefined area models with dynamic node-based area discovery.\n        \"\"\"\n        complete_set = area_model_df.index.to_list()\n        for a in self.node_model_df[self.area_column].unique():\n            if a is not None and a not in complete_set:\n                complete_set.append(a)\n        return area_model_df.reindex(complete_set)\n\n    def enhance_area_model_df_by_adding_node_count_per_area(\n            self,\n            area_model_df: pd.DataFrame,\n            node_count_column_name: str = 'node_count'\n    ) -&gt; pd.DataFrame:\n        \"\"\"Enhance area model by adding node count statistics per area.\n\n        Aggregates the number of nodes assigned to each area and adds this\n        information to the area model DataFrame. Node counts are essential\n        for understanding infrastructure density and capacity distribution.\n\n        Args:\n            area_model_df: Base area model DataFrame to enhance.\n            node_count_column_name: Name for the new node count column.\n                Defaults to 'node_count'.\n\n        Returns:\n            pd.DataFrame: Enhanced area model with node count column added.\n                Existing data is preserved, node counts are added for all areas.\n                Areas not present in node data will have NaN node counts.\n\n        Example:\n\n            &gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n            &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(base_model)\n            &gt;&gt;&gt; print(enhanced_model)\n                          node_count\n                country\n                DE               2\n                FR               2\n                BE               1\n\n            &gt;&gt;&gt; # Custom column name\n            &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(\n            &gt;&gt;&gt;     base_model, 'infrastructure_count'\n            &gt;&gt;&gt; )\n        \"\"\"\n        enhanced_df = area_model_df.copy()\n        node_counts = self.node_model_df[self.area_column].value_counts().to_dict()\n        for node, count in node_counts.items():\n            if node in enhanced_df.index:\n                enhanced_df.loc[node, node_count_column_name] = count\n        return enhanced_df\n\n    def enhance_area_model_df_by_adding_representative_geo_point(\n            self,\n            area_model_df: pd.DataFrame | gpd.GeoDataFrame,\n            target_column_name: str = 'projection_point',\n            round_point_decimals: int = 4,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Enhance area model by adding representative geographic points for\n        labeling and KPI printing in map visualizations.\n\n        Calculates representative geographic points for each area based on\n        either area geometries (if available) or node locations within each area.\n        These points are useful for visualization, labeling, and spatial analysis.\n\n        The method supports two calculation modes:\n        1. Geometry-based: Uses area polygon centroids or representative points\n        2. Node-based: Calculates centroid from node locations within each area\n\n        Args:\n            area_model_df: Area model DataFrame to enhance. Can be regular DataFrame\n                or GeoDataFrame with 'geometry' column.\n            target_column_name: Name for the new representative point column.\n                Defaults to 'projection_point'.\n            round_point_decimals: Number of decimal places for coordinate rounding.\n                Set to None to disable rounding. Defaults to 4.\n\n        Returns:\n            pd.DataFrame: Enhanced area model with representative points added.\n                Points are added as Shapely Point objects suitable for mapping\n                and spatial analysis.\n\n        Raises:\n            TypeError: If geo_location_column contains non-Point objects.\n\n        Example:\n\n            &gt;&gt;&gt; # Node-based representative points\n            &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_representative_geo_point(base_model)\n            &gt;&gt;&gt; print(enhanced_model)\n                          projection_point\n                country\n                DE        POINT (10.5 52.5)\n                FR        POINT (2.5 48.5)\n                BE        POINT (4 50)\n\n            &gt;&gt;&gt; # With custom column name and precision\n            &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_representative_geo_point(\n            &gt;&gt;&gt;     base_model, 'center_point', round_point_decimals=2\n            &gt;&gt;&gt; )\n\n            &gt;&gt;&gt; # Access coordinates for mapping\n            &gt;&gt;&gt; center = enhanced_model.loc['DE', 'projection_point']\n            &gt;&gt;&gt; print(f\"DE center: {center.x:.2f}, {center.y:.2f}\")\n        \"\"\"\n        enhanced_df = area_model_df.copy()\n\n        def round_point(point: Point | None) -&gt; Point | None:\n            if point is None or round_point_decimals is None:\n                return point\n            return type(point)(round(point.x, round_point_decimals), round(point.y, round_point_decimals))\n\n        if target_column_name not in enhanced_df:\n            enhanced_df[target_column_name] = None\n\n        for area in enhanced_df.index:\n            if pd.notna(enhanced_df.loc[area, target_column_name]):\n                continue\n            if 'geometry' in enhanced_df.columns:\n                geo = enhanced_df.loc[area, 'geometry']\n                if pd.notna(geo) and isinstance(geo, (Polygon, MultiPolygon)):\n                    enhanced_df.loc[area, target_column_name] = round_point(self.get_representative_area_point(geo))\n            elif self.geo_location_column:\n                nodes = self.node_model_df.loc[self.node_model_df[self.area_column] == area, self.geo_location_column]\n                nodes = nodes.dropna()\n                if not nodes.empty:\n                    locations = [n for n in nodes.values if n is not None]\n                    if not all(isinstance(i, Point) for i in locations):\n                        raise TypeError(\n                            f'Geographic location column \"{self.geo_location_column}\" must contain only '\n                            f'Point objects. Found: {[type(i).__name__ for i in locations if not isinstance(i, Point)]}'\n                        )\n                    representative_point = self._compute_representative_point_from_cloud_of_2d_points(locations)\n                    enhanced_df.loc[area, target_column_name] = round_point(representative_point)\n        return enhanced_df\n\n    def generate_area_model(self) -&gt; pd.DataFrame:\n        \"\"\"Generate complete area model with node counts and representative points.\n\n        Creates a comprehensive area model DataFrame by combining base area\n        discovery, node count aggregation, and representative geographic point\n        calculation. This is the main method for generating complete area models.\n\n        The generated model includes:\n            - All unique areas from node data\n            - Node count per area for capacity/infrastructure analysis\n            - Representative geographic points for visualization\n\n        Returns:\n            pd.DataFrame: Complete area model with node counts and geographic data.\n                Index contains area identifiers, columns include 'node_count'\n                and 'projection_point' (if geographic data available).\n\n        Example:\n\n            &gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n            &gt;&gt;&gt; area_model = generator.generate_area_model()\n            &gt;&gt;&gt; print(area_model)\n                          node_count projection_point\n                country\n                DE               2    POINT (10.5 52.5)\n                FR               2    POINT (2.5 48.5)\n                BE               1    POINT (4 50)\n        \"\"\"\n        area_model_df = self.generate_base_area_model_from_area_names_in_node_model_df()\n        area_model_df = self.enhance_area_model_df_by_adding_node_count_per_area(area_model_df)\n        area_model_df = self.enhance_area_model_df_by_adding_representative_geo_point(area_model_df)\n        return area_model_df\n\n    def enhance_with_geometry(\n        self,\n        area_model_df: pd.DataFrame,\n        area_gdf: gpd.GeoDataFrame\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Enhance area model with geometric polygon data for spatial analysis.\n\n        Integrates area polygon geometries from a GeoDataFrame into the area model,\n        enabling advanced spatial analysis, visualization, and border calculations.\n        The method matches areas by index and creates a proper GeoDataFrame output.\n\n        Args:\n            area_model_df: Area model DataFrame to enhance with geometry.\n            area_gdf: GeoDataFrame containing area polygon geometries.\n                Must have 'geometry' column with Polygon or MultiPolygon objects.\n                Areas are matched by index values.\n\n        Returns:\n            gpd.GeoDataFrame: Enhanced area model as GeoDataFrame with geometry column.\n                All original data is preserved, geometry column is added for areas\n                that exist in both DataFrames. Missing geometries are set to None.\n\n        Example:\n\n            &gt;&gt;&gt; import geopandas as gpd\n            &gt;&gt;&gt; from shapely.geometry import Polygon\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Create area geometries\n            &gt;&gt;&gt; area_polygons = gpd.GeoDataFrame({\n            &gt;&gt;&gt;     'area_name': ['Germany', 'France', 'Belgium'],\n            &gt;&gt;&gt;     'geometry': [\n            &gt;&gt;&gt;         Polygon([(9, 51), (12, 51), (12, 54), (9, 54)]),\n            &gt;&gt;&gt;         Polygon([(1, 47), (4, 47), (4, 50), (1, 50)]),\n            &gt;&gt;&gt;         Polygon([(3, 49), (5, 49), (5, 51), (3, 51)])\n            &gt;&gt;&gt;     ]\n            &gt;&gt;&gt; }, index=['DE', 'FR', 'BE'])\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Enhance area model with geometry\n            &gt;&gt;&gt; geo_model = generator.enhance_with_geometry(area_model, area_polygons)\n            &gt;&gt;&gt; print(f\"Model has geometry: {isinstance(geo_model, gpd.GeoDataFrame)}\")\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Use for spatial operations\n            &gt;&gt;&gt; total_area = geo_model['geometry'].area.sum()\n            &gt;&gt;&gt; print(f\"Total area: {total_area:.0f} square units\")\n        \"\"\"\n        enhanced_df = area_model_df.copy()\n        if 'geometry' not in enhanced_df.columns:\n            enhanced_df['geometry'] = None\n        for area in area_model_df.index:\n            if area in area_gdf.index:\n                enhanced_df.loc[area, 'geometry'] = area_gdf.loc[area, 'geometry']\n        if not isinstance(enhanced_df, gpd.GeoDataFrame):\n            enhanced_df = gpd.GeoDataFrame(enhanced_df, geometry='geometry')\n        return enhanced_df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator.__init__","title":"__init__","text":"<pre><code>__init__(node_model_df: DataFrame, area_column: str, geo_location_column: str = None)\n</code></pre> <p>Initialize the area model generator.</p> <p>Parameters:</p> Name Type Description Default <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame containing node-level data with area assignments. Must contain area_column with area identifiers for each node. May contain geographic Point objects for spatial analysis.</p> required <code>area_column</code> <code>str</code> <p>Column name in node_model_df containing area assignments (e.g., 'country', 'bidding_zone', 'market_region', 'control_area').</p> required <code>geo_location_column</code> <code>str</code> <p>Column name containing geographic Point objects for representative point calculation. If None, automatically detects column containing Point geometries.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If area_column is not found in node_model_df columns.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from shapely.geometry import Point\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Node data with area assignments\n&gt;&gt;&gt; nodes = pd.DataFrame({\n&gt;&gt;&gt;     'voltage_kv': [380, 220, 380, 150],\n&gt;&gt;&gt;     'country': ['DE', 'DE', 'FR', 'FR'], \n&gt;&gt;&gt;     'bidding_zone': ['DE_LU', 'DE_LU', 'FR', 'FR'],\n&gt;&gt;&gt;     'coordinates': [Point(10, 52), Point(11, 53), Point(2, 48), Point(3, 49)]\n&gt;&gt;&gt; }, index=['DE1', 'DE2', 'FR1', 'FR2'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Initialize for country-level analysis\n&gt;&gt;&gt; generator = AreaModelGenerator(nodes, 'country', 'coordinates')\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Or let it auto-detect geographic column\n&gt;&gt;&gt; generator = AreaModelGenerator(nodes, 'bidding_zone')\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>def __init__(\n        self,\n        node_model_df: pd.DataFrame,\n        area_column: str,\n        geo_location_column: str = None,\n):\n    \"\"\"Initialize the area model generator.\n\n    Args:\n        node_model_df: DataFrame containing node-level data with area assignments.\n            Must contain area_column with area identifiers for each node.\n            May contain geographic Point objects for spatial analysis.\n        area_column: Column name in node_model_df containing area assignments\n            (e.g., 'country', 'bidding_zone', 'market_region', 'control_area').\n        geo_location_column: Column name containing geographic Point objects\n            for representative point calculation. If None, automatically\n            detects column containing Point geometries.\n\n    Raises:\n        ValueError: If area_column is not found in node_model_df columns.\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from shapely.geometry import Point\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Node data with area assignments\n        &gt;&gt;&gt; nodes = pd.DataFrame({\n        &gt;&gt;&gt;     'voltage_kv': [380, 220, 380, 150],\n        &gt;&gt;&gt;     'country': ['DE', 'DE', 'FR', 'FR'], \n        &gt;&gt;&gt;     'bidding_zone': ['DE_LU', 'DE_LU', 'FR', 'FR'],\n        &gt;&gt;&gt;     'coordinates': [Point(10, 52), Point(11, 53), Point(2, 48), Point(3, 49)]\n        &gt;&gt;&gt; }, index=['DE1', 'DE2', 'FR1', 'FR2'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Initialize for country-level analysis\n        &gt;&gt;&gt; generator = AreaModelGenerator(nodes, 'country', 'coordinates')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Or let it auto-detect geographic column\n        &gt;&gt;&gt; generator = AreaModelGenerator(nodes, 'bidding_zone')\n    \"\"\"\n    self.node_model_df = node_model_df\n    self.area_column = area_column\n    self.geo_location_column = geo_location_column or self._identify_geo_location_column()\n    self._validate_inputs()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator.generate_base_area_model_from_area_names_in_node_model_df","title":"generate_base_area_model_from_area_names_in_node_model_df","text":"<pre><code>generate_base_area_model_from_area_names_in_node_model_df() -&gt; DataFrame\n</code></pre> <p>Generate base area model DataFrame from unique area names in node data.</p> <p>Creates a minimal area model DataFrame containing only the unique area identifiers found in the node model data. This forms the foundation for building comprehensive area models.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Base area model with area identifiers as index. Contains no additional columns - serves as starting point for enhancement with node counts, geographic data, etc.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n&gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n&gt;&gt;&gt; print(base_model)\n    Empty DataFrame\n    Columns: []\n    Index: ['DE', 'FR', 'BE']\n</code></pre> Note <p>Areas with None or NaN values in the area_column are excluded from the generated model.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>def generate_base_area_model_from_area_names_in_node_model_df(self) -&gt; pd.DataFrame:\n    \"\"\"Generate base area model DataFrame from unique area names in node data.\n\n    Creates a minimal area model DataFrame containing only the unique area\n    identifiers found in the node model data. This forms the foundation\n    for building comprehensive area models.\n\n    Returns:\n        pd.DataFrame: Base area model with area identifiers as index.\n            Contains no additional columns - serves as starting point\n            for enhancement with node counts, geographic data, etc.\n\n    Example:\n\n        &gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n        &gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n        &gt;&gt;&gt; print(base_model)\n            Empty DataFrame\n            Columns: []\n            Index: ['DE', 'FR', 'BE']\n\n    Note:\n        Areas with None or NaN values in the area_column are excluded\n        from the generated model.\n    \"\"\"\n    unique_areas = self.node_model_df[self.area_column].dropna().unique()\n    area_model_df = pd.DataFrame(index=unique_areas)\n    area_model_df.index.name = self.area_column\n    return area_model_df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator.ensure_completeness_of_area_model_df","title":"ensure_completeness_of_area_model_df","text":"<pre><code>ensure_completeness_of_area_model_df(area_model_df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Ensure area model contains all areas present in node data.</p> <p>Validates and extends an existing area model DataFrame to include any areas found in the node data that might be missing from the provided area model. This is useful when working with predefined area models that may not cover all areas in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>area_model_df</code> <code>DataFrame</code> <p>Existing area model DataFrame to validate and extend.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Complete area model containing all areas from node data. Existing data is preserved, new areas are added with NaN values for existing columns.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Predefined area model missing some areas\n&gt;&gt;&gt; partial_model = pd.DataFrame({\n&gt;&gt;&gt;     'max_price': [5000, 3000]\n&gt;&gt;&gt; }, index=['DE', 'FR'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Ensure completeness (adds 'BE' if present in node data)\n&gt;&gt;&gt; complete_model = generator.ensure_completeness_of_area_model_df(partial_model)\n&gt;&gt;&gt; print(complete_model)\n              max_price\n    country\n    DE             5000\n    FR             3000\n    BE              NaN\n</code></pre> Use Case <p>Essential for maintaining data consistency when combining predefined area models with dynamic node-based area discovery.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>def ensure_completeness_of_area_model_df(self, area_model_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Ensure area model contains all areas present in node data.\n\n    Validates and extends an existing area model DataFrame to include\n    any areas found in the node data that might be missing from the\n    provided area model. This is useful when working with predefined\n    area models that may not cover all areas in the dataset.\n\n    Args:\n        area_model_df: Existing area model DataFrame to validate and extend.\n\n    Returns:\n        pd.DataFrame: Complete area model containing all areas from node data.\n            Existing data is preserved, new areas are added with NaN values\n            for existing columns.\n\n    Example:\n\n        &gt;&gt;&gt; # Predefined area model missing some areas\n        &gt;&gt;&gt; partial_model = pd.DataFrame({\n        &gt;&gt;&gt;     'max_price': [5000, 3000]\n        &gt;&gt;&gt; }, index=['DE', 'FR'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Ensure completeness (adds 'BE' if present in node data)\n        &gt;&gt;&gt; complete_model = generator.ensure_completeness_of_area_model_df(partial_model)\n        &gt;&gt;&gt; print(complete_model)\n                      max_price\n            country\n            DE             5000\n            FR             3000\n            BE              NaN\n\n    Use Case:\n        Essential for maintaining data consistency when combining\n        predefined area models with dynamic node-based area discovery.\n    \"\"\"\n    complete_set = area_model_df.index.to_list()\n    for a in self.node_model_df[self.area_column].unique():\n        if a is not None and a not in complete_set:\n            complete_set.append(a)\n    return area_model_df.reindex(complete_set)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator.enhance_area_model_df_by_adding_node_count_per_area","title":"enhance_area_model_df_by_adding_node_count_per_area","text":"<pre><code>enhance_area_model_df_by_adding_node_count_per_area(area_model_df: DataFrame, node_count_column_name: str = 'node_count') -&gt; DataFrame\n</code></pre> <p>Enhance area model by adding node count statistics per area.</p> <p>Aggregates the number of nodes assigned to each area and adds this information to the area model DataFrame. Node counts are essential for understanding infrastructure density and capacity distribution.</p> <p>Parameters:</p> Name Type Description Default <code>area_model_df</code> <code>DataFrame</code> <p>Base area model DataFrame to enhance.</p> required <code>node_count_column_name</code> <code>str</code> <p>Name for the new node count column. Defaults to 'node_count'.</p> <code>'node_count'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Enhanced area model with node count column added. Existing data is preserved, node counts are added for all areas. Areas not present in node data will have NaN node counts.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n&gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(base_model)\n&gt;&gt;&gt; print(enhanced_model)\n              node_count\n    country\n    DE               2\n    FR               2\n    BE               1\n\n&gt;&gt;&gt; # Custom column name\n&gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(\n&gt;&gt;&gt;     base_model, 'infrastructure_count'\n&gt;&gt;&gt; )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>def enhance_area_model_df_by_adding_node_count_per_area(\n        self,\n        area_model_df: pd.DataFrame,\n        node_count_column_name: str = 'node_count'\n) -&gt; pd.DataFrame:\n    \"\"\"Enhance area model by adding node count statistics per area.\n\n    Aggregates the number of nodes assigned to each area and adds this\n    information to the area model DataFrame. Node counts are essential\n    for understanding infrastructure density and capacity distribution.\n\n    Args:\n        area_model_df: Base area model DataFrame to enhance.\n        node_count_column_name: Name for the new node count column.\n            Defaults to 'node_count'.\n\n    Returns:\n        pd.DataFrame: Enhanced area model with node count column added.\n            Existing data is preserved, node counts are added for all areas.\n            Areas not present in node data will have NaN node counts.\n\n    Example:\n\n        &gt;&gt;&gt; base_model = generator.generate_base_area_model_from_area_names_in_node_model_df()\n        &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(base_model)\n        &gt;&gt;&gt; print(enhanced_model)\n                      node_count\n            country\n            DE               2\n            FR               2\n            BE               1\n\n        &gt;&gt;&gt; # Custom column name\n        &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_node_count_per_area(\n        &gt;&gt;&gt;     base_model, 'infrastructure_count'\n        &gt;&gt;&gt; )\n    \"\"\"\n    enhanced_df = area_model_df.copy()\n    node_counts = self.node_model_df[self.area_column].value_counts().to_dict()\n    for node, count in node_counts.items():\n        if node in enhanced_df.index:\n            enhanced_df.loc[node, node_count_column_name] = count\n    return enhanced_df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator.enhance_area_model_df_by_adding_representative_geo_point","title":"enhance_area_model_df_by_adding_representative_geo_point","text":"<pre><code>enhance_area_model_df_by_adding_representative_geo_point(area_model_df: DataFrame | GeoDataFrame, target_column_name: str = 'projection_point', round_point_decimals: int = 4) -&gt; DataFrame\n</code></pre> <p>Enhance area model by adding representative geographic points for labeling and KPI printing in map visualizations.</p> <p>Calculates representative geographic points for each area based on either area geometries (if available) or node locations within each area. These points are useful for visualization, labeling, and spatial analysis.</p> <p>The method supports two calculation modes: 1. Geometry-based: Uses area polygon centroids or representative points 2. Node-based: Calculates centroid from node locations within each area</p> <p>Parameters:</p> Name Type Description Default <code>area_model_df</code> <code>DataFrame | GeoDataFrame</code> <p>Area model DataFrame to enhance. Can be regular DataFrame or GeoDataFrame with 'geometry' column.</p> required <code>target_column_name</code> <code>str</code> <p>Name for the new representative point column. Defaults to 'projection_point'.</p> <code>'projection_point'</code> <code>round_point_decimals</code> <code>int</code> <p>Number of decimal places for coordinate rounding. Set to None to disable rounding. Defaults to 4.</p> <code>4</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Enhanced area model with representative points added. Points are added as Shapely Point objects suitable for mapping and spatial analysis.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If geo_location_column contains non-Point objects.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Node-based representative points\n&gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_representative_geo_point(base_model)\n&gt;&gt;&gt; print(enhanced_model)\n              projection_point\n    country\n    DE        POINT (10.5 52.5)\n    FR        POINT (2.5 48.5)\n    BE        POINT (4 50)\n\n&gt;&gt;&gt; # With custom column name and precision\n&gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_representative_geo_point(\n&gt;&gt;&gt;     base_model, 'center_point', round_point_decimals=2\n&gt;&gt;&gt; )\n\n&gt;&gt;&gt; # Access coordinates for mapping\n&gt;&gt;&gt; center = enhanced_model.loc['DE', 'projection_point']\n&gt;&gt;&gt; print(f\"DE center: {center.x:.2f}, {center.y:.2f}\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>def enhance_area_model_df_by_adding_representative_geo_point(\n        self,\n        area_model_df: pd.DataFrame | gpd.GeoDataFrame,\n        target_column_name: str = 'projection_point',\n        round_point_decimals: int = 4,\n) -&gt; pd.DataFrame:\n    \"\"\"Enhance area model by adding representative geographic points for\n    labeling and KPI printing in map visualizations.\n\n    Calculates representative geographic points for each area based on\n    either area geometries (if available) or node locations within each area.\n    These points are useful for visualization, labeling, and spatial analysis.\n\n    The method supports two calculation modes:\n    1. Geometry-based: Uses area polygon centroids or representative points\n    2. Node-based: Calculates centroid from node locations within each area\n\n    Args:\n        area_model_df: Area model DataFrame to enhance. Can be regular DataFrame\n            or GeoDataFrame with 'geometry' column.\n        target_column_name: Name for the new representative point column.\n            Defaults to 'projection_point'.\n        round_point_decimals: Number of decimal places for coordinate rounding.\n            Set to None to disable rounding. Defaults to 4.\n\n    Returns:\n        pd.DataFrame: Enhanced area model with representative points added.\n            Points are added as Shapely Point objects suitable for mapping\n            and spatial analysis.\n\n    Raises:\n        TypeError: If geo_location_column contains non-Point objects.\n\n    Example:\n\n        &gt;&gt;&gt; # Node-based representative points\n        &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_representative_geo_point(base_model)\n        &gt;&gt;&gt; print(enhanced_model)\n                      projection_point\n            country\n            DE        POINT (10.5 52.5)\n            FR        POINT (2.5 48.5)\n            BE        POINT (4 50)\n\n        &gt;&gt;&gt; # With custom column name and precision\n        &gt;&gt;&gt; enhanced_model = generator.enhance_area_model_df_by_adding_representative_geo_point(\n        &gt;&gt;&gt;     base_model, 'center_point', round_point_decimals=2\n        &gt;&gt;&gt; )\n\n        &gt;&gt;&gt; # Access coordinates for mapping\n        &gt;&gt;&gt; center = enhanced_model.loc['DE', 'projection_point']\n        &gt;&gt;&gt; print(f\"DE center: {center.x:.2f}, {center.y:.2f}\")\n    \"\"\"\n    enhanced_df = area_model_df.copy()\n\n    def round_point(point: Point | None) -&gt; Point | None:\n        if point is None or round_point_decimals is None:\n            return point\n        return type(point)(round(point.x, round_point_decimals), round(point.y, round_point_decimals))\n\n    if target_column_name not in enhanced_df:\n        enhanced_df[target_column_name] = None\n\n    for area in enhanced_df.index:\n        if pd.notna(enhanced_df.loc[area, target_column_name]):\n            continue\n        if 'geometry' in enhanced_df.columns:\n            geo = enhanced_df.loc[area, 'geometry']\n            if pd.notna(geo) and isinstance(geo, (Polygon, MultiPolygon)):\n                enhanced_df.loc[area, target_column_name] = round_point(self.get_representative_area_point(geo))\n        elif self.geo_location_column:\n            nodes = self.node_model_df.loc[self.node_model_df[self.area_column] == area, self.geo_location_column]\n            nodes = nodes.dropna()\n            if not nodes.empty:\n                locations = [n for n in nodes.values if n is not None]\n                if not all(isinstance(i, Point) for i in locations):\n                    raise TypeError(\n                        f'Geographic location column \"{self.geo_location_column}\" must contain only '\n                        f'Point objects. Found: {[type(i).__name__ for i in locations if not isinstance(i, Point)]}'\n                    )\n                representative_point = self._compute_representative_point_from_cloud_of_2d_points(locations)\n                enhanced_df.loc[area, target_column_name] = round_point(representative_point)\n    return enhanced_df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator.generate_area_model","title":"generate_area_model","text":"<pre><code>generate_area_model() -&gt; DataFrame\n</code></pre> <p>Generate complete area model with node counts and representative points.</p> <p>Creates a comprehensive area model DataFrame by combining base area discovery, node count aggregation, and representative geographic point calculation. This is the main method for generating complete area models.</p> The generated model includes <ul> <li>All unique areas from node data</li> <li>Node count per area for capacity/infrastructure analysis</li> <li>Representative geographic points for visualization</li> </ul> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Complete area model with node counts and geographic data. Index contains area identifiers, columns include 'node_count' and 'projection_point' (if geographic data available).</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n&gt;&gt;&gt; area_model = generator.generate_area_model()\n&gt;&gt;&gt; print(area_model)\n              node_count projection_point\n    country\n    DE               2    POINT (10.5 52.5)\n    FR               2    POINT (2.5 48.5)\n    BE               1    POINT (4 50)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>def generate_area_model(self) -&gt; pd.DataFrame:\n    \"\"\"Generate complete area model with node counts and representative points.\n\n    Creates a comprehensive area model DataFrame by combining base area\n    discovery, node count aggregation, and representative geographic point\n    calculation. This is the main method for generating complete area models.\n\n    The generated model includes:\n        - All unique areas from node data\n        - Node count per area for capacity/infrastructure analysis\n        - Representative geographic points for visualization\n\n    Returns:\n        pd.DataFrame: Complete area model with node counts and geographic data.\n            Index contains area identifiers, columns include 'node_count'\n            and 'projection_point' (if geographic data available).\n\n    Example:\n\n        &gt;&gt;&gt; generator = AreaModelGenerator(node_data, 'country')\n        &gt;&gt;&gt; area_model = generator.generate_area_model()\n        &gt;&gt;&gt; print(area_model)\n                      node_count projection_point\n            country\n            DE               2    POINT (10.5 52.5)\n            FR               2    POINT (2.5 48.5)\n            BE               1    POINT (4 50)\n    \"\"\"\n    area_model_df = self.generate_base_area_model_from_area_names_in_node_model_df()\n    area_model_df = self.enhance_area_model_df_by_adding_node_count_per_area(area_model_df)\n    area_model_df = self.enhance_area_model_df_by_adding_representative_geo_point(area_model_df)\n    return area_model_df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.area_model_generator.AreaModelGenerator.enhance_with_geometry","title":"enhance_with_geometry","text":"<pre><code>enhance_with_geometry(area_model_df: DataFrame, area_gdf: GeoDataFrame) -&gt; GeoDataFrame\n</code></pre> <p>Enhance area model with geometric polygon data for spatial analysis.</p> <p>Integrates area polygon geometries from a GeoDataFrame into the area model, enabling advanced spatial analysis, visualization, and border calculations. The method matches areas by index and creates a proper GeoDataFrame output.</p> <p>Parameters:</p> Name Type Description Default <code>area_model_df</code> <code>DataFrame</code> <p>Area model DataFrame to enhance with geometry.</p> required <code>area_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing area polygon geometries. Must have 'geometry' column with Polygon or MultiPolygon objects. Areas are matched by index values.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Enhanced area model as GeoDataFrame with geometry column. All original data is preserved, geometry column is added for areas that exist in both DataFrames. Missing geometries are set to None.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; from shapely.geometry import Polygon\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Create area geometries\n&gt;&gt;&gt; area_polygons = gpd.GeoDataFrame({\n&gt;&gt;&gt;     'area_name': ['Germany', 'France', 'Belgium'],\n&gt;&gt;&gt;     'geometry': [\n&gt;&gt;&gt;         Polygon([(9, 51), (12, 51), (12, 54), (9, 54)]),\n&gt;&gt;&gt;         Polygon([(1, 47), (4, 47), (4, 50), (1, 50)]),\n&gt;&gt;&gt;         Polygon([(3, 49), (5, 49), (5, 51), (3, 51)])\n&gt;&gt;&gt;     ]\n&gt;&gt;&gt; }, index=['DE', 'FR', 'BE'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Enhance area model with geometry\n&gt;&gt;&gt; geo_model = generator.enhance_with_geometry(area_model, area_polygons)\n&gt;&gt;&gt; print(f\"Model has geometry: {isinstance(geo_model, gpd.GeoDataFrame)}\")\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Use for spatial operations\n&gt;&gt;&gt; total_area = geo_model['geometry'].area.sum()\n&gt;&gt;&gt; print(f\"Total area: {total_area:.0f} square units\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/area_model_generator.py</code> <pre><code>def enhance_with_geometry(\n    self,\n    area_model_df: pd.DataFrame,\n    area_gdf: gpd.GeoDataFrame\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Enhance area model with geometric polygon data for spatial analysis.\n\n    Integrates area polygon geometries from a GeoDataFrame into the area model,\n    enabling advanced spatial analysis, visualization, and border calculations.\n    The method matches areas by index and creates a proper GeoDataFrame output.\n\n    Args:\n        area_model_df: Area model DataFrame to enhance with geometry.\n        area_gdf: GeoDataFrame containing area polygon geometries.\n            Must have 'geometry' column with Polygon or MultiPolygon objects.\n            Areas are matched by index values.\n\n    Returns:\n        gpd.GeoDataFrame: Enhanced area model as GeoDataFrame with geometry column.\n            All original data is preserved, geometry column is added for areas\n            that exist in both DataFrames. Missing geometries are set to None.\n\n    Example:\n\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; from shapely.geometry import Polygon\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Create area geometries\n        &gt;&gt;&gt; area_polygons = gpd.GeoDataFrame({\n        &gt;&gt;&gt;     'area_name': ['Germany', 'France', 'Belgium'],\n        &gt;&gt;&gt;     'geometry': [\n        &gt;&gt;&gt;         Polygon([(9, 51), (12, 51), (12, 54), (9, 54)]),\n        &gt;&gt;&gt;         Polygon([(1, 47), (4, 47), (4, 50), (1, 50)]),\n        &gt;&gt;&gt;         Polygon([(3, 49), (5, 49), (5, 51), (3, 51)])\n        &gt;&gt;&gt;     ]\n        &gt;&gt;&gt; }, index=['DE', 'FR', 'BE'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Enhance area model with geometry\n        &gt;&gt;&gt; geo_model = generator.enhance_with_geometry(area_model, area_polygons)\n        &gt;&gt;&gt; print(f\"Model has geometry: {isinstance(geo_model, gpd.GeoDataFrame)}\")\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Use for spatial operations\n        &gt;&gt;&gt; total_area = geo_model['geometry'].area.sum()\n        &gt;&gt;&gt; print(f\"Total area: {total_area:.0f} square units\")\n    \"\"\"\n    enhanced_df = area_model_df.copy()\n    if 'geometry' not in enhanced_df.columns:\n        enhanced_df['geometry'] = None\n    for area in area_model_df.index:\n        if area in area_gdf.index:\n            enhanced_df.loc[area, 'geometry'] = area_gdf.loc[area, 'geometry']\n    if not isinstance(enhanced_df, gpd.GeoDataFrame):\n        enhanced_df = gpd.GeoDataFrame(enhanced_df, geometry='geometry')\n    return enhanced_df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_generator.AreaBorderNamingConventions","title":"AreaBorderNamingConventions","text":"<p>Standardized naming conventions for energy system area borders.</p> <p>This class provides consistent naming patterns for borders between energy system areas (countries, bidding zones, market regions). It ensures standardized naming across different analysis workflows and supports bidirectional relationship management.</p> The naming system supports <ul> <li>Configurable separators and prefixes/suffixes</li> <li>Bidirectional border identification (A-B and B-A)</li> <li>Alphabetically sorted canonical border names</li> <li>Consistent column naming for source and target areas</li> </ul> Key Features <ul> <li>Configurable naming patterns for different use cases</li> <li>Automatic opposite border name generation</li> <li>Alphabetical sorting for canonical border representation</li> <li>Consistent identifier generation for database/DataFrame columns</li> </ul> <p>Attributes:</p> Name Type Description <code>JOIN_AREA_NAMES_BY</code> <code>str</code> <p>Separator for area names in border identifiers</p> <code>SOURCE_AREA_IDENTIFIER_SUFFIX</code> <code>str</code> <p>Suffix for source area column names</p> <code>TARGET_AREA_IDENTIFIER_SUFFIX</code> <code>str</code> <p>Suffix for target area column names</p> <code>OPPOSITE_BORDER_IDENTIFIER</code> <code>str</code> <p>Column name for opposite border references</p> <code>SORTED_BORDER_IDENTIFIER</code> <code>str</code> <p>Column name for alphabetically sorted borders</p> <code>NAME_IS_ALPHABETICALLY_SORTED_IDENTIFIER</code> <code>str</code> <p>Boolean indicator column</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n&gt;&gt;&gt; border_name = conventions.get_area_border_name('DE', 'FR')\n&gt;&gt;&gt; print(border_name)  # 'DE - FR'\n&gt;&gt;&gt; opposite = conventions.get_opposite_area_border_name(border_name)\n&gt;&gt;&gt; print(opposite)  # 'FR - DE'\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>class AreaBorderNamingConventions:\n    \"\"\"Standardized naming conventions for energy system area borders.\n\n    This class provides consistent naming patterns for borders between energy\n    system areas (countries, bidding zones, market regions). It ensures\n    standardized naming across different analysis workflows and supports\n    bidirectional relationship management.\n\n    The naming system supports:\n        - Configurable separators and prefixes/suffixes\n        - Bidirectional border identification (A-B and B-A)\n        - Alphabetically sorted canonical border names\n        - Consistent column naming for source and target areas\n\n    Key Features:\n        - Configurable naming patterns for different use cases\n        - Automatic opposite border name generation\n        - Alphabetical sorting for canonical border representation\n        - Consistent identifier generation for database/DataFrame columns\n\n    Attributes:\n        JOIN_AREA_NAMES_BY (str): Separator for area names in border identifiers\n        SOURCE_AREA_IDENTIFIER_SUFFIX (str): Suffix for source area column names\n        TARGET_AREA_IDENTIFIER_SUFFIX (str): Suffix for target area column names\n        OPPOSITE_BORDER_IDENTIFIER (str): Column name for opposite border references\n        SORTED_BORDER_IDENTIFIER (str): Column name for alphabetically sorted borders\n        NAME_IS_ALPHABETICALLY_SORTED_IDENTIFIER (str): Boolean indicator column\n\n    Example:\n\n        &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n        &gt;&gt;&gt; border_name = conventions.get_area_border_name('DE', 'FR')\n        &gt;&gt;&gt; print(border_name)  # 'DE - FR'\n        &gt;&gt;&gt; opposite = conventions.get_opposite_area_border_name(border_name)\n        &gt;&gt;&gt; print(opposite)  # 'FR - DE'\n    \"\"\"\n\n    JOIN_AREA_NAMES_BY = ' - '\n    SOURCE_AREA_IDENTIFIER_PREFIX = ''\n    TARGET_AREA_IDENTIFIER_PREFIX = ''\n    SOURCE_AREA_IDENTIFIER_SUFFIX = '_from'\n    TARGET_AREA_IDENTIFIER_SUFFIX = '_to'\n    OPPOSITE_BORDER_IDENTIFIER = 'opposite_border'\n    SORTED_BORDER_IDENTIFIER = 'sorted_border'\n    NAME_IS_ALPHABETICALLY_SORTED_IDENTIFIER = 'name_is_alphabetically_sorted'\n    PROJECTION_POINT_IDENTIFIER = 'projection_point'\n    AZIMUTH_ANGLE_IDENTIFIER = 'azimuth_angle'\n    BORDER_IS_PHYSICAL_IDENTIFIER = 'is_physical'\n    BORDER_LINE_STRING_IDENTIFIER = 'geo_line_string'\n\n    def __init__(\n            self,\n            area_column: str,\n            border_identifier: str = None,\n            source_area_identifier: str = None,\n            target_area_identifier: str = None,\n    ):\n        \"\"\"Initialize border naming conventions.\n\n        Args:\n            area_column: Name of the area column (e.g., 'country', 'bidding_zone')\n            border_identifier: Custom name for border identifier column.\n                Defaults to '{area_column}_border'\n            source_area_identifier: Custom name for source area column.\n                Defaults to '{area_column}_from'\n            target_area_identifier: Custom name for target area column.\n                Defaults to '{area_column}_to'\n\n        Example:\n\n            &gt;&gt;&gt; # Standard naming\n            &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n            &gt;&gt;&gt; print(conventions.border_identifier)  # 'country_border'\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Custom naming\n            &gt;&gt;&gt; conventions = AreaBorderNamingConventions(\n            ...     'bidding_zone',\n            ...     border_identifier='interconnection',\n            ...     source_area_identifier='origin_zone'\n            ... )\n        \"\"\"\n        self.area_column = area_column\n        self.border_identifier = border_identifier or self._default_border_identifier()\n        self.source_area_identifier = source_area_identifier or self._default_source_area_identifier()\n        self.target_area_identifier = target_area_identifier or self._default_target_area_identifier()\n\n    def _default_border_identifier(self) -&gt; str:\n        return f'{self.area_column}_border'\n\n    def _default_source_area_identifier(self) -&gt; str:\n        return f'{self.SOURCE_AREA_IDENTIFIER_PREFIX}{self.area_column}{self.SOURCE_AREA_IDENTIFIER_SUFFIX}'\n\n    def _default_target_area_identifier(self) -&gt; str:\n        return f'{self.TARGET_AREA_IDENTIFIER_PREFIX}{self.area_column}{self.TARGET_AREA_IDENTIFIER_SUFFIX}'\n\n    def get_area_border_name(self, area_from: str, area_to: str) -&gt; str:\n        \"\"\"Generate standardized border name from source and target areas.\n\n        Args:\n            area_from: Source area identifier (e.g., 'DE', 'FR_North')\n            area_to: Target area identifier (e.g., 'FR', 'DE_South')\n\n        Returns:\n            str: Formatted border name using the configured separator (e.g. 'DE - FR', 'FR_North - DE_South')\n\n        Example:\n\n            &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n            &gt;&gt;&gt; border_name = conventions.get_area_border_name('DE', 'FR')\n            &gt;&gt;&gt; print(border_name)  # 'DE - FR'\n        \"\"\"\n        return f'{area_from}{self.JOIN_AREA_NAMES_BY}{area_to}'\n\n    def decompose_area_border_name_to_areas(self, border_name: str) -&gt; Tuple[str, str]:\n        \"\"\"Extract source and target area names from border identifier.\n\n        Args:\n            border_name: Border name in standard format (e.g., 'DE - FR')\n\n        Returns:\n            Tuple[str, str]: Source and target area names\n\n        Raises:\n            ValueError: If border_name doesn't contain the expected separator\n\n        Example:\n\n            &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n            &gt;&gt;&gt; area_from, area_to = conventions.decompose_area_border_name_to_areas('DE - FR')\n            &gt;&gt;&gt; print(f\"From: {area_from}, To: {area_to}\")  # From: DE, To: FR\n        \"\"\"\n        area_from, area_to = border_name.split(self.JOIN_AREA_NAMES_BY)\n        return area_from, area_to\n\n    def get_opposite_area_border_name(self, border_name: str) -&gt; str:\n        \"\"\"Generate the opposite direction border name.\n\n        Args:\n            border_name: Original border name (e.g., 'DE - FR')\n\n        Returns:\n            str: Opposite direction border name (e.g., 'FR - DE')\n\n        Example:\n\n            &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n            &gt;&gt;&gt; opposite = conventions.get_opposite_area_border_name('DE - FR')\n            &gt;&gt;&gt; print(opposite)  # 'FR - DE'\n\n        Energy Domain Context:\n            Energy flows and capacities are often directional, requiring\n            tracking of both A\u2192B and B\u2192A relationships for comprehensive\n            border analysis.\n        \"\"\"\n        area_from, area_to = self.decompose_area_border_name_to_areas(border_name)\n        return self.get_area_border_name(area_to, area_from)\n\n    def get_alphabetically_sorted_border(self, border_name: str) -&gt; str:\n        \"\"\"Generate alphabetically sorted canonical border name.\n\n        Creates a canonical representation where area names are sorted\n        alphabetically, useful for identifying unique borders regardless\n        of direction specification, or for matching borders of opposite direction.\n\n        Args:\n            border_name: Border name in any direction (e.g., 'FR - DE' or 'DE - FR')\n\n        Returns:\n            str: Alphabetically sorted border name (e.g., 'DE - FR')\n\n        Example:\n\n            &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n            &gt;&gt;&gt; sorted_border = conventions.get_alphabetically_sorted_border('FR - DE')\n            &gt;&gt;&gt; print(sorted_border)  # 'DE - FR'\n\n        Use Case:\n            Canonical naming is essential for border deduplication and\n            consistent reference in energy system databases and analysis.\n        \"\"\"\n        area_from, area_to = self.decompose_area_border_name_to_areas(border_name)\n        return self.get_area_border_name(*list(sorted([area_from, area_to])))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_generator.AreaBorderNamingConventions.__init__","title":"__init__","text":"<pre><code>__init__(area_column: str, border_identifier: str = None, source_area_identifier: str = None, target_area_identifier: str = None)\n</code></pre> <p>Initialize border naming conventions.</p> <p>Parameters:</p> Name Type Description Default <code>area_column</code> <code>str</code> <p>Name of the area column (e.g., 'country', 'bidding_zone')</p> required <code>border_identifier</code> <code>str</code> <p>Custom name for border identifier column. Defaults to '{area_column}_border'</p> <code>None</code> <code>source_area_identifier</code> <code>str</code> <p>Custom name for source area column. Defaults to '{area_column}_from'</p> <code>None</code> <code>target_area_identifier</code> <code>str</code> <p>Custom name for target area column. Defaults to '{area_column}_to'</p> <code>None</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Standard naming\n&gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n&gt;&gt;&gt; print(conventions.border_identifier)  # 'country_border'\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Custom naming\n&gt;&gt;&gt; conventions = AreaBorderNamingConventions(\n...     'bidding_zone',\n...     border_identifier='interconnection',\n...     source_area_identifier='origin_zone'\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def __init__(\n        self,\n        area_column: str,\n        border_identifier: str = None,\n        source_area_identifier: str = None,\n        target_area_identifier: str = None,\n):\n    \"\"\"Initialize border naming conventions.\n\n    Args:\n        area_column: Name of the area column (e.g., 'country', 'bidding_zone')\n        border_identifier: Custom name for border identifier column.\n            Defaults to '{area_column}_border'\n        source_area_identifier: Custom name for source area column.\n            Defaults to '{area_column}_from'\n        target_area_identifier: Custom name for target area column.\n            Defaults to '{area_column}_to'\n\n    Example:\n\n        &gt;&gt;&gt; # Standard naming\n        &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n        &gt;&gt;&gt; print(conventions.border_identifier)  # 'country_border'\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Custom naming\n        &gt;&gt;&gt; conventions = AreaBorderNamingConventions(\n        ...     'bidding_zone',\n        ...     border_identifier='interconnection',\n        ...     source_area_identifier='origin_zone'\n        ... )\n    \"\"\"\n    self.area_column = area_column\n    self.border_identifier = border_identifier or self._default_border_identifier()\n    self.source_area_identifier = source_area_identifier or self._default_source_area_identifier()\n    self.target_area_identifier = target_area_identifier or self._default_target_area_identifier()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_generator.AreaBorderNamingConventions.get_area_border_name","title":"get_area_border_name","text":"<pre><code>get_area_border_name(area_from: str, area_to: str) -&gt; str\n</code></pre> <p>Generate standardized border name from source and target areas.</p> <p>Parameters:</p> Name Type Description Default <code>area_from</code> <code>str</code> <p>Source area identifier (e.g., 'DE', 'FR_North')</p> required <code>area_to</code> <code>str</code> <p>Target area identifier (e.g., 'FR', 'DE_South')</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted border name using the configured separator (e.g. 'DE - FR', 'FR_North - DE_South')</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n&gt;&gt;&gt; border_name = conventions.get_area_border_name('DE', 'FR')\n&gt;&gt;&gt; print(border_name)  # 'DE - FR'\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def get_area_border_name(self, area_from: str, area_to: str) -&gt; str:\n    \"\"\"Generate standardized border name from source and target areas.\n\n    Args:\n        area_from: Source area identifier (e.g., 'DE', 'FR_North')\n        area_to: Target area identifier (e.g., 'FR', 'DE_South')\n\n    Returns:\n        str: Formatted border name using the configured separator (e.g. 'DE - FR', 'FR_North - DE_South')\n\n    Example:\n\n        &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n        &gt;&gt;&gt; border_name = conventions.get_area_border_name('DE', 'FR')\n        &gt;&gt;&gt; print(border_name)  # 'DE - FR'\n    \"\"\"\n    return f'{area_from}{self.JOIN_AREA_NAMES_BY}{area_to}'\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_generator.AreaBorderNamingConventions.decompose_area_border_name_to_areas","title":"decompose_area_border_name_to_areas","text":"<pre><code>decompose_area_border_name_to_areas(border_name: str) -&gt; Tuple[str, str]\n</code></pre> <p>Extract source and target area names from border identifier.</p> <p>Parameters:</p> Name Type Description Default <code>border_name</code> <code>str</code> <p>Border name in standard format (e.g., 'DE - FR')</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>Tuple[str, str]: Source and target area names</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If border_name doesn't contain the expected separator</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n&gt;&gt;&gt; area_from, area_to = conventions.decompose_area_border_name_to_areas('DE - FR')\n&gt;&gt;&gt; print(f\"From: {area_from}, To: {area_to}\")  # From: DE, To: FR\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def decompose_area_border_name_to_areas(self, border_name: str) -&gt; Tuple[str, str]:\n    \"\"\"Extract source and target area names from border identifier.\n\n    Args:\n        border_name: Border name in standard format (e.g., 'DE - FR')\n\n    Returns:\n        Tuple[str, str]: Source and target area names\n\n    Raises:\n        ValueError: If border_name doesn't contain the expected separator\n\n    Example:\n\n        &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n        &gt;&gt;&gt; area_from, area_to = conventions.decompose_area_border_name_to_areas('DE - FR')\n        &gt;&gt;&gt; print(f\"From: {area_from}, To: {area_to}\")  # From: DE, To: FR\n    \"\"\"\n    area_from, area_to = border_name.split(self.JOIN_AREA_NAMES_BY)\n    return area_from, area_to\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_generator.AreaBorderNamingConventions.get_opposite_area_border_name","title":"get_opposite_area_border_name","text":"<pre><code>get_opposite_area_border_name(border_name: str) -&gt; str\n</code></pre> <p>Generate the opposite direction border name.</p> <p>Parameters:</p> Name Type Description Default <code>border_name</code> <code>str</code> <p>Original border name (e.g., 'DE - FR')</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Opposite direction border name (e.g., 'FR - DE')</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n&gt;&gt;&gt; opposite = conventions.get_opposite_area_border_name('DE - FR')\n&gt;&gt;&gt; print(opposite)  # 'FR - DE'\n</code></pre> Energy Domain Context <p>Energy flows and capacities are often directional, requiring tracking of both A\u2192B and B\u2192A relationships for comprehensive border analysis.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def get_opposite_area_border_name(self, border_name: str) -&gt; str:\n    \"\"\"Generate the opposite direction border name.\n\n    Args:\n        border_name: Original border name (e.g., 'DE - FR')\n\n    Returns:\n        str: Opposite direction border name (e.g., 'FR - DE')\n\n    Example:\n\n        &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n        &gt;&gt;&gt; opposite = conventions.get_opposite_area_border_name('DE - FR')\n        &gt;&gt;&gt; print(opposite)  # 'FR - DE'\n\n    Energy Domain Context:\n        Energy flows and capacities are often directional, requiring\n        tracking of both A\u2192B and B\u2192A relationships for comprehensive\n        border analysis.\n    \"\"\"\n    area_from, area_to = self.decompose_area_border_name_to_areas(border_name)\n    return self.get_area_border_name(area_to, area_from)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_generator.AreaBorderNamingConventions.get_alphabetically_sorted_border","title":"get_alphabetically_sorted_border","text":"<pre><code>get_alphabetically_sorted_border(border_name: str) -&gt; str\n</code></pre> <p>Generate alphabetically sorted canonical border name.</p> <p>Creates a canonical representation where area names are sorted alphabetically, useful for identifying unique borders regardless of direction specification, or for matching borders of opposite direction.</p> <p>Parameters:</p> Name Type Description Default <code>border_name</code> <code>str</code> <p>Border name in any direction (e.g., 'FR - DE' or 'DE - FR')</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Alphabetically sorted border name (e.g., 'DE - FR')</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n&gt;&gt;&gt; sorted_border = conventions.get_alphabetically_sorted_border('FR - DE')\n&gt;&gt;&gt; print(sorted_border)  # 'DE - FR'\n</code></pre> Use Case <p>Canonical naming is essential for border deduplication and consistent reference in energy system databases and analysis.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def get_alphabetically_sorted_border(self, border_name: str) -&gt; str:\n    \"\"\"Generate alphabetically sorted canonical border name.\n\n    Creates a canonical representation where area names are sorted\n    alphabetically, useful for identifying unique borders regardless\n    of direction specification, or for matching borders of opposite direction.\n\n    Args:\n        border_name: Border name in any direction (e.g., 'FR - DE' or 'DE - FR')\n\n    Returns:\n        str: Alphabetically sorted border name (e.g., 'DE - FR')\n\n    Example:\n\n        &gt;&gt;&gt; conventions = AreaBorderNamingConventions('country')\n        &gt;&gt;&gt; sorted_border = conventions.get_alphabetically_sorted_border('FR - DE')\n        &gt;&gt;&gt; print(sorted_border)  # 'DE - FR'\n\n    Use Case:\n        Canonical naming is essential for border deduplication and\n        consistent reference in energy system databases and analysis.\n    \"\"\"\n    area_from, area_to = self.decompose_area_border_name_to_areas(border_name)\n    return self.get_area_border_name(*list(sorted([area_from, area_to])))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_generator.AreaBorderModelGenerator","title":"AreaBorderModelGenerator","text":"<p>               Bases: <code>AreaBorderNamingConventions</code></p> <p>Generates comprehensive border models from energy system topology.</p> <p>This class analyzes line connectivity and node-to-area mappings to automatically identify borders between energy system areas. It creates a comprehensive border_model_df with standardized naming, directional relationships, and integration points for geometric analysis.</p> <p>The generator processes line topology data to identify cross-area connections. It supports bidirectional relationship tracking and provides network graph representations for connectivity analysis.</p> Key Features <ul> <li>Automatic border discovery from line topology</li> <li>Bidirectional border relationship management</li> <li>Standardized naming conventions with configurable patterns</li> <li>Network graph generation for connectivity analysis</li> <li>Integration with geometric border calculators</li> <li>Support for different area granularities (countries, bidding zones, etc.)</li> </ul> MESQUAL Integration <p>Designed to work with MESQUAL's area accounting system, providing border modeling capabilities that integrate with flow calculators, capacity analyzers, and visualization tools.</p> <p>Attributes:</p> Name Type Description <code>line_model_df</code> <code>DataFrame</code> <p>Transmission line data with topology information</p> <code>node_model_df</code> <code>DataFrame</code> <p>Node data with area assignments</p> <code>node_from_col</code> <code>str</code> <p>Column name for line source nodes</p> <code>node_to_col</code> <code>str</code> <p>Column name for line target nodes</p> <code>node_to_area_map</code> <code>dict</code> <p>Mapping from nodes to their assigned areas</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Create border model from transmission data\n&gt;&gt;&gt; generator = AreaBorderModelGenerator(\n...     node_df, line_df, 'country', 'node_from', 'node_to'\n... )\n&gt;&gt;&gt; border_model = generator.generate_area_border_model()\n&gt;&gt;&gt; print(f\"Found {len(border_model)} directional borders\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>class AreaBorderModelGenerator(AreaBorderNamingConventions):\n    \"\"\"Generates comprehensive border models from energy system topology.\n\n    This class analyzes line connectivity and node-to-area mappings\n    to automatically identify borders between energy system areas. It creates\n    a comprehensive border_model_df with standardized naming, directional relationships,\n    and integration points for geometric analysis.\n\n    The generator processes line topology data to identify cross-area connections.\n    It supports bidirectional relationship tracking and provides network graph\n    representations for connectivity analysis.\n\n    Key Features:\n        - Automatic border discovery from line topology\n        - Bidirectional border relationship management\n        - Standardized naming conventions with configurable patterns\n        - Network graph generation for connectivity analysis\n        - Integration with geometric border calculators\n        - Support for different area granularities (countries, bidding zones, etc.)\n\n    MESQUAL Integration:\n        Designed to work with MESQUAL's area accounting system, providing\n        border modeling capabilities that integrate with flow calculators,\n        capacity analyzers, and visualization tools.\n\n    Attributes:\n        line_model_df (pd.DataFrame): Transmission line data with topology information\n        node_model_df (pd.DataFrame): Node data with area assignments\n        node_from_col (str): Column name for line source nodes\n        node_to_col (str): Column name for line target nodes\n        node_to_area_map (dict): Mapping from nodes to their assigned areas\n\n    Example:\n\n        &gt;&gt;&gt; # Create border model from transmission data\n        &gt;&gt;&gt; generator = AreaBorderModelGenerator(\n        ...     node_df, line_df, 'country', 'node_from', 'node_to'\n        ... )\n        &gt;&gt;&gt; border_model = generator.generate_area_border_model()\n        &gt;&gt;&gt; print(f\"Found {len(border_model)} directional borders\")\n    \"\"\"\n\n    def __init__(\n        self, \n        node_model_df: pd.DataFrame,\n        line_model_df: pd.DataFrame,\n        area_column: str,\n        node_from_col: str,\n        node_to_col: str,\n        border_identifier: str = None,\n        source_area_identifier: str = None,\n        target_area_identifier: str = None,\n    ):\n        \"\"\"Initialize the area border model generator.\n\n        Args:\n            node_model_df: DataFrame containing node-level data with area assignments.\n                Must contain area_column with area identifiers for each node.\n            line_model_df: DataFrame containing transmission line topology data.\n                Must contain node_from_col and node_to_col with node identifiers.\n            area_column: Column name in node_model_df containing area assignments\n                (e.g., 'country', 'bidding_zone', 'market_region')\n            node_from_col: Column name in line_model_df for source node identifiers\n            node_to_col: Column name in line_model_df for target node identifiers\n            border_identifier: Custom border column name (optional)\n            source_area_identifier: Custom source area column name (optional)\n            target_area_identifier: Custom target area column name (optional)\n\n        Raises:\n            ValueError: If required columns are not found in input DataFrames\n\n        Example:\n\n            &gt;&gt;&gt; generator = AreaBorderModelGenerator(\n            ...     nodes_df=node_data,\n            ...     lines_df=transmission_data,\n            ...     area_column='bidding_zone',\n            ...     node_from_col='bus_from',\n            ...     node_to_col='bus_to'\n            ... )\n        \"\"\"\n        super().__init__(area_column, border_identifier, source_area_identifier, target_area_identifier)\n        self.line_model_df = line_model_df\n        self.node_model_df = node_model_df\n        self.node_from_col = node_from_col\n        self.node_to_col = node_to_col\n\n        self._validate_inputs()\n        self.node_to_area_map = self._create_node_to_area_map()\n\n    def _validate_inputs(self):\n        if self.area_column not in self.node_model_df.columns:\n            raise ValueError(\n                f\"Area column '{self.area_column}' not found in node_model_df. \"\n                f\"Available columns: {list(self.node_model_df.columns)}\"\n            )\n        if self.node_from_col not in self.line_model_df.columns:\n            raise ValueError(\n                f\"Source node column '{self.node_from_col}' not found in line_model_df. \"\n                f\"Available columns: {list(self.line_model_df.columns)}\"\n            )\n        if self.node_to_col not in self.line_model_df.columns:\n            raise ValueError(\n                f\"Target node column '{self.node_to_col}' not found in line_model_df. \"\n                f\"Available columns: {list(self.line_model_df.columns)}\"\n            )\n\n    def _create_node_to_area_map(self) -&gt; dict:\n        \"\"\"Create mapping from node identifiers to their assigned areas.\n\n        Returns:\n            dict: Mapping from node IDs to area assignments\n\n        Note:\n            Nodes with None or NaN area assignments are included in the mapping\n            but will be filtered out during border identification.\n        \"\"\"\n        return self.node_model_df[self.area_column].to_dict()\n\n    def generate_area_border_model(self) -&gt; pd.DataFrame:\n        \"\"\"Generate comprehensive border model with all relationship data.\n\n        Analyzes transmission line topology to identify borders between areas,\n        creating a comprehensive DataFrame with directional relationships,\n        naming conventions, and reference data for further analysis.\n\n        The generated model includes:\n            - Border identifiers in both directions (A\u2192B and B\u2192A)\n            - Source and target area columns\n            - Opposite border references for bidirectional analysis\n            - Alphabetically sorted canonical border names\n            - Boolean indicators for alphabetical sorting\n\n        Returns:\n            pd.DataFrame: Comprehensive border model indexed by border identifiers.\n                Returns empty DataFrame with proper column structure if no borders found.\n\n        Example:\n\n            &gt;&gt;&gt; border_model = generator.generate_area_border_model()\n            &gt;&gt;&gt; print(border_model.columns)\n            ['country_from', 'country_to', 'opposite_border', 'sorted_border', 'name_is_alphabetically_sorted']\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Access border relationships\n            &gt;&gt;&gt; for border_id, row in border_model.iterrows():\n            ...     print(f\"{border_id}: {row['country_from']} \u2192 {row['country_to']}\")\n        \"\"\"\n        borders = self._identify_borders()\n\n        if not borders:\n            return pd.DataFrame(\n                columns=[\n                    self.border_identifier,\n                    self.source_area_identifier,\n                    self.target_area_identifier,\n                    self.OPPOSITE_BORDER_IDENTIFIER,\n                    self.SORTED_BORDER_IDENTIFIER,\n                    self.NAME_IS_ALPHABETICALLY_SORTED_IDENTIFIER,\n                ]\n            )\n\n        border_data = []\n        for area_from, area_to in borders:\n            border_id = self.get_area_border_name(area_from, area_to)\n            opposite_id = self.get_opposite_area_border_name(border_id)\n\n            sorted_border = self.get_alphabetically_sorted_border(border_id)\n\n            border_data.append({\n                self.border_identifier: border_id,\n                self.source_area_identifier: area_from,\n                self.target_area_identifier: area_to,\n                self.OPPOSITE_BORDER_IDENTIFIER: opposite_id,\n                self.SORTED_BORDER_IDENTIFIER: sorted_border,\n                self.NAME_IS_ALPHABETICALLY_SORTED_IDENTIFIER: sorted_border == border_id,\n            })\n\n        border_model_df = pd.DataFrame(border_data).set_index(self.border_identifier)\n\n        return border_model_df\n\n    def _identify_borders(self) -&gt; set[tuple[str, str]]:\n        \"\"\"Identify borders from line topology.\n\n        Analyzes line connectivity to find areas that are connected by\n        lines, creating bidirectional border relationships.\n\n        Returns:\n            set: Set of (area_from, area_to) tuples representing directional borders.\n                Includes both directions for each physical connection.\n\n        Note:\n            - Lines connecting nodes within the same area are ignored\n            - Lines with nodes having None/NaN area assignments are ignored\n            - Both directions (A\u2192B and B\u2192A) are included for each connection\n        \"\"\"\n        borders = set()\n\n        for _, line in self.line_model_df.iterrows():\n            node_from = line[self.node_from_col]\n            node_to = line[self.node_to_col]\n\n            area_from = self.node_to_area_map.get(node_from)\n            area_to = self.node_to_area_map.get(node_to)\n\n            if area_from and area_to and area_from != area_to:\n                borders.add((area_from, area_to))\n                borders.add((area_to, area_from))\n\n        return borders\n\n    def _get_lines_for_border(self, area_from: str, area_to: str) -&gt; list[str]:\n        \"\"\"Get all lines that cross a specific directional border.\n\n        Args:\n            area_from: Source area identifier\n            area_to: Target area identifier\n\n        Returns:\n            list[str]: List of line identifiers that connect the specified areas\n                in the given direction\n\n        Example:\n\n            &gt;&gt;&gt; lines = generator._get_lines_for_border('DE', 'FR')\n            &gt;&gt;&gt; print(f\"Lines from DE to FR: {lines}\")\n        \"\"\"\n        lines = []\n\n        for line_id, line in self.line_model_df.iterrows():\n            node_from = line[self.node_from_col]\n            node_to = line[self.node_to_col]\n\n            node_area_from = self.node_to_area_map.get(node_from)\n            node_area_to = self.node_to_area_map.get(node_to)\n\n            if node_area_from == area_from and node_area_to == area_to:\n                lines.append(line_id)\n\n        return lines\n\n    def get_area_graph(self) -&gt; nx.Graph:\n        \"\"\"Generate NetworkX graph representation of area connectivity.\n\n        Creates an undirected graph where nodes represent areas and edges\n        represent borders. This is useful for network analysis, path finding,\n        and connectivity studies in multi-area energy systems.\n\n        Returns:\n            nx.Graph: Undirected graph with areas as nodes and borders as edges.\n                Graph may contain multiple disconnected components if areas\n                are not fully interconnected.\n\n        Example:\n\n            &gt;&gt;&gt; graph = generator.get_area_graph()\n            &gt;&gt;&gt; print(f\"Areas: {list(graph.nodes())}\")\n            &gt;&gt;&gt; print(f\"Borders: {list(graph.edges())}\")\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Check connectivity\n            &gt;&gt;&gt; connected = nx.is_connected(graph)\n            &gt;&gt;&gt; print(f\"All areas connected: {connected}\")\n        \"\"\"\n        graph = nx.Graph()\n        borders = self._identify_borders()\n\n        for area_from, area_to in borders:\n            if not graph.has_edge(area_from, area_to):\n                graph.add_edge(area_from, area_to)\n\n        return graph\n\n    def enhance_with_geometry(\n        self, \n        border_model_df: pd.DataFrame,\n        area_geometry_calculator: AreaBorderGeometryCalculator\n    ) -&gt; pd.DataFrame:\n        \"\"\"Enhance border model with geometric properties for visualization.\n\n        Integrates with AreaBorderGeometryCalculator to add geometric information\n        to borders, including representative points, directional angles, and\n        line geometries. This enables advanced visualization of energy system borders.\n\n        Args:\n            border_model_df: Border model DataFrame to enhance\n            area_geometry_calculator: Configured geometry calculator with area\n                polygon data for geometric computations\n\n        Returns:\n            pd.DataFrame: Enhanced border model with additional geometric columns:\n                - projection_point: Point for label/arrow placement\n                - azimuth_angle: Directional angle in degrees\n                - is_physical: Boolean indicating if border is physical (touching areas)\n                - geo_line_string: LineString geometry representing the border\n\n        Example:\n\n            &gt;&gt;&gt; # Setup geometry calculator with area polygons\n            &gt;&gt;&gt; geo_calc = AreaBorderGeometryCalculator(area_polygons_gdf)\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Enhance border model\n            &gt;&gt;&gt; enhanced_borders = generator.enhance_with_geometry(border_model, geo_calc)\n            &gt;&gt;&gt; print(enhanced_borders.columns)  # Includes geometric properties\n\n        Note:\n            Geometric enhancement may fail for some borders due to missing\n            area geometries or calculation errors. Such failures are logged\n            as warnings without stopping the overall process.\n        \"\"\"\n        enhanced_df = border_model_df.copy()\n\n        for border_id, border in border_model_df.iterrows():\n            area_from = border[self.source_area_identifier]\n            area_to = border[self.target_area_identifier]\n\n            try:\n                geometry_info = area_geometry_calculator.calculate_border_geometry(\n                    area_from, area_to\n                )\n\n                enhanced_df.loc[border_id, self.PROJECTION_POINT_IDENTIFIER] = geometry_info[area_geometry_calculator.PROJECTION_POINT_IDENTIFIER]\n                enhanced_df.loc[border_id, self.AZIMUTH_ANGLE_IDENTIFIER] = geometry_info[area_geometry_calculator.AZIMUTH_ANGLE_IDENTIFIER]\n                enhanced_df.loc[border_id, self.BORDER_IS_PHYSICAL_IDENTIFIER] = geometry_info[area_geometry_calculator.BORDER_IS_PHYSICAL_IDENTIFIER]\n                enhanced_df.loc[border_id, self.BORDER_LINE_STRING_IDENTIFIER] = geometry_info[area_geometry_calculator.BORDER_LINE_STRING_IDENTIFIER]\n\n            except Exception as e:\n                print(f\"Warning: Could not calculate geometry for border {border_id} \"\n                      f\"({area_from} \u2192 {area_to}): {e}\")\n\n        return enhanced_df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_generator.AreaBorderModelGenerator.__init__","title":"__init__","text":"<pre><code>__init__(node_model_df: DataFrame, line_model_df: DataFrame, area_column: str, node_from_col: str, node_to_col: str, border_identifier: str = None, source_area_identifier: str = None, target_area_identifier: str = None)\n</code></pre> <p>Initialize the area border model generator.</p> <p>Parameters:</p> Name Type Description Default <code>node_model_df</code> <code>DataFrame</code> <p>DataFrame containing node-level data with area assignments. Must contain area_column with area identifiers for each node.</p> required <code>line_model_df</code> <code>DataFrame</code> <p>DataFrame containing transmission line topology data. Must contain node_from_col and node_to_col with node identifiers.</p> required <code>area_column</code> <code>str</code> <p>Column name in node_model_df containing area assignments (e.g., 'country', 'bidding_zone', 'market_region')</p> required <code>node_from_col</code> <code>str</code> <p>Column name in line_model_df for source node identifiers</p> required <code>node_to_col</code> <code>str</code> <p>Column name in line_model_df for target node identifiers</p> required <code>border_identifier</code> <code>str</code> <p>Custom border column name (optional)</p> <code>None</code> <code>source_area_identifier</code> <code>str</code> <p>Custom source area column name (optional)</p> <code>None</code> <code>target_area_identifier</code> <code>str</code> <p>Custom target area column name (optional)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are not found in input DataFrames</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; generator = AreaBorderModelGenerator(\n...     nodes_df=node_data,\n...     lines_df=transmission_data,\n...     area_column='bidding_zone',\n...     node_from_col='bus_from',\n...     node_to_col='bus_to'\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def __init__(\n    self, \n    node_model_df: pd.DataFrame,\n    line_model_df: pd.DataFrame,\n    area_column: str,\n    node_from_col: str,\n    node_to_col: str,\n    border_identifier: str = None,\n    source_area_identifier: str = None,\n    target_area_identifier: str = None,\n):\n    \"\"\"Initialize the area border model generator.\n\n    Args:\n        node_model_df: DataFrame containing node-level data with area assignments.\n            Must contain area_column with area identifiers for each node.\n        line_model_df: DataFrame containing transmission line topology data.\n            Must contain node_from_col and node_to_col with node identifiers.\n        area_column: Column name in node_model_df containing area assignments\n            (e.g., 'country', 'bidding_zone', 'market_region')\n        node_from_col: Column name in line_model_df for source node identifiers\n        node_to_col: Column name in line_model_df for target node identifiers\n        border_identifier: Custom border column name (optional)\n        source_area_identifier: Custom source area column name (optional)\n        target_area_identifier: Custom target area column name (optional)\n\n    Raises:\n        ValueError: If required columns are not found in input DataFrames\n\n    Example:\n\n        &gt;&gt;&gt; generator = AreaBorderModelGenerator(\n        ...     nodes_df=node_data,\n        ...     lines_df=transmission_data,\n        ...     area_column='bidding_zone',\n        ...     node_from_col='bus_from',\n        ...     node_to_col='bus_to'\n        ... )\n    \"\"\"\n    super().__init__(area_column, border_identifier, source_area_identifier, target_area_identifier)\n    self.line_model_df = line_model_df\n    self.node_model_df = node_model_df\n    self.node_from_col = node_from_col\n    self.node_to_col = node_to_col\n\n    self._validate_inputs()\n    self.node_to_area_map = self._create_node_to_area_map()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_generator.AreaBorderModelGenerator.generate_area_border_model","title":"generate_area_border_model","text":"<pre><code>generate_area_border_model() -&gt; DataFrame\n</code></pre> <p>Generate comprehensive border model with all relationship data.</p> <p>Analyzes transmission line topology to identify borders between areas, creating a comprehensive DataFrame with directional relationships, naming conventions, and reference data for further analysis.</p> The generated model includes <ul> <li>Border identifiers in both directions (A\u2192B and B\u2192A)</li> <li>Source and target area columns</li> <li>Opposite border references for bidirectional analysis</li> <li>Alphabetically sorted canonical border names</li> <li>Boolean indicators for alphabetical sorting</li> </ul> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Comprehensive border model indexed by border identifiers. Returns empty DataFrame with proper column structure if no borders found.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; border_model = generator.generate_area_border_model()\n&gt;&gt;&gt; print(border_model.columns)\n['country_from', 'country_to', 'opposite_border', 'sorted_border', 'name_is_alphabetically_sorted']\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Access border relationships\n&gt;&gt;&gt; for border_id, row in border_model.iterrows():\n...     print(f\"{border_id}: {row['country_from']} \u2192 {row['country_to']}\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def generate_area_border_model(self) -&gt; pd.DataFrame:\n    \"\"\"Generate comprehensive border model with all relationship data.\n\n    Analyzes transmission line topology to identify borders between areas,\n    creating a comprehensive DataFrame with directional relationships,\n    naming conventions, and reference data for further analysis.\n\n    The generated model includes:\n        - Border identifiers in both directions (A\u2192B and B\u2192A)\n        - Source and target area columns\n        - Opposite border references for bidirectional analysis\n        - Alphabetically sorted canonical border names\n        - Boolean indicators for alphabetical sorting\n\n    Returns:\n        pd.DataFrame: Comprehensive border model indexed by border identifiers.\n            Returns empty DataFrame with proper column structure if no borders found.\n\n    Example:\n\n        &gt;&gt;&gt; border_model = generator.generate_area_border_model()\n        &gt;&gt;&gt; print(border_model.columns)\n        ['country_from', 'country_to', 'opposite_border', 'sorted_border', 'name_is_alphabetically_sorted']\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Access border relationships\n        &gt;&gt;&gt; for border_id, row in border_model.iterrows():\n        ...     print(f\"{border_id}: {row['country_from']} \u2192 {row['country_to']}\")\n    \"\"\"\n    borders = self._identify_borders()\n\n    if not borders:\n        return pd.DataFrame(\n            columns=[\n                self.border_identifier,\n                self.source_area_identifier,\n                self.target_area_identifier,\n                self.OPPOSITE_BORDER_IDENTIFIER,\n                self.SORTED_BORDER_IDENTIFIER,\n                self.NAME_IS_ALPHABETICALLY_SORTED_IDENTIFIER,\n            ]\n        )\n\n    border_data = []\n    for area_from, area_to in borders:\n        border_id = self.get_area_border_name(area_from, area_to)\n        opposite_id = self.get_opposite_area_border_name(border_id)\n\n        sorted_border = self.get_alphabetically_sorted_border(border_id)\n\n        border_data.append({\n            self.border_identifier: border_id,\n            self.source_area_identifier: area_from,\n            self.target_area_identifier: area_to,\n            self.OPPOSITE_BORDER_IDENTIFIER: opposite_id,\n            self.SORTED_BORDER_IDENTIFIER: sorted_border,\n            self.NAME_IS_ALPHABETICALLY_SORTED_IDENTIFIER: sorted_border == border_id,\n        })\n\n    border_model_df = pd.DataFrame(border_data).set_index(self.border_identifier)\n\n    return border_model_df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_generator.AreaBorderModelGenerator.get_area_graph","title":"get_area_graph","text":"<pre><code>get_area_graph() -&gt; Graph\n</code></pre> <p>Generate NetworkX graph representation of area connectivity.</p> <p>Creates an undirected graph where nodes represent areas and edges represent borders. This is useful for network analysis, path finding, and connectivity studies in multi-area energy systems.</p> <p>Returns:</p> Type Description <code>Graph</code> <p>nx.Graph: Undirected graph with areas as nodes and borders as edges. Graph may contain multiple disconnected components if areas are not fully interconnected.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; graph = generator.get_area_graph()\n&gt;&gt;&gt; print(f\"Areas: {list(graph.nodes())}\")\n&gt;&gt;&gt; print(f\"Borders: {list(graph.edges())}\")\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Check connectivity\n&gt;&gt;&gt; connected = nx.is_connected(graph)\n&gt;&gt;&gt; print(f\"All areas connected: {connected}\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def get_area_graph(self) -&gt; nx.Graph:\n    \"\"\"Generate NetworkX graph representation of area connectivity.\n\n    Creates an undirected graph where nodes represent areas and edges\n    represent borders. This is useful for network analysis, path finding,\n    and connectivity studies in multi-area energy systems.\n\n    Returns:\n        nx.Graph: Undirected graph with areas as nodes and borders as edges.\n            Graph may contain multiple disconnected components if areas\n            are not fully interconnected.\n\n    Example:\n\n        &gt;&gt;&gt; graph = generator.get_area_graph()\n        &gt;&gt;&gt; print(f\"Areas: {list(graph.nodes())}\")\n        &gt;&gt;&gt; print(f\"Borders: {list(graph.edges())}\")\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Check connectivity\n        &gt;&gt;&gt; connected = nx.is_connected(graph)\n        &gt;&gt;&gt; print(f\"All areas connected: {connected}\")\n    \"\"\"\n    graph = nx.Graph()\n    borders = self._identify_borders()\n\n    for area_from, area_to in borders:\n        if not graph.has_edge(area_from, area_to):\n            graph.add_edge(area_from, area_to)\n\n    return graph\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_generator.AreaBorderModelGenerator.enhance_with_geometry","title":"enhance_with_geometry","text":"<pre><code>enhance_with_geometry(border_model_df: DataFrame, area_geometry_calculator: AreaBorderGeometryCalculator) -&gt; DataFrame\n</code></pre> <p>Enhance border model with geometric properties for visualization.</p> <p>Integrates with AreaBorderGeometryCalculator to add geometric information to borders, including representative points, directional angles, and line geometries. This enables advanced visualization of energy system borders.</p> <p>Parameters:</p> Name Type Description Default <code>border_model_df</code> <code>DataFrame</code> <p>Border model DataFrame to enhance</p> required <code>area_geometry_calculator</code> <code>AreaBorderGeometryCalculator</code> <p>Configured geometry calculator with area polygon data for geometric computations</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Enhanced border model with additional geometric columns: - projection_point: Point for label/arrow placement - azimuth_angle: Directional angle in degrees - is_physical: Boolean indicating if border is physical (touching areas) - geo_line_string: LineString geometry representing the border</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Setup geometry calculator with area polygons\n&gt;&gt;&gt; geo_calc = AreaBorderGeometryCalculator(area_polygons_gdf)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Enhance border model\n&gt;&gt;&gt; enhanced_borders = generator.enhance_with_geometry(border_model, geo_calc)\n&gt;&gt;&gt; print(enhanced_borders.columns)  # Includes geometric properties\n</code></pre> Note <p>Geometric enhancement may fail for some borders due to missing area geometries or calculation errors. Such failures are logged as warnings without stopping the overall process.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_generator.py</code> <pre><code>def enhance_with_geometry(\n    self, \n    border_model_df: pd.DataFrame,\n    area_geometry_calculator: AreaBorderGeometryCalculator\n) -&gt; pd.DataFrame:\n    \"\"\"Enhance border model with geometric properties for visualization.\n\n    Integrates with AreaBorderGeometryCalculator to add geometric information\n    to borders, including representative points, directional angles, and\n    line geometries. This enables advanced visualization of energy system borders.\n\n    Args:\n        border_model_df: Border model DataFrame to enhance\n        area_geometry_calculator: Configured geometry calculator with area\n            polygon data for geometric computations\n\n    Returns:\n        pd.DataFrame: Enhanced border model with additional geometric columns:\n            - projection_point: Point for label/arrow placement\n            - azimuth_angle: Directional angle in degrees\n            - is_physical: Boolean indicating if border is physical (touching areas)\n            - geo_line_string: LineString geometry representing the border\n\n    Example:\n\n        &gt;&gt;&gt; # Setup geometry calculator with area polygons\n        &gt;&gt;&gt; geo_calc = AreaBorderGeometryCalculator(area_polygons_gdf)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Enhance border model\n        &gt;&gt;&gt; enhanced_borders = generator.enhance_with_geometry(border_model, geo_calc)\n        &gt;&gt;&gt; print(enhanced_borders.columns)  # Includes geometric properties\n\n    Note:\n        Geometric enhancement may fail for some borders due to missing\n        area geometries or calculation errors. Such failures are logged\n        as warnings without stopping the overall process.\n    \"\"\"\n    enhanced_df = border_model_df.copy()\n\n    for border_id, border in border_model_df.iterrows():\n        area_from = border[self.source_area_identifier]\n        area_to = border[self.target_area_identifier]\n\n        try:\n            geometry_info = area_geometry_calculator.calculate_border_geometry(\n                area_from, area_to\n            )\n\n            enhanced_df.loc[border_id, self.PROJECTION_POINT_IDENTIFIER] = geometry_info[area_geometry_calculator.PROJECTION_POINT_IDENTIFIER]\n            enhanced_df.loc[border_id, self.AZIMUTH_ANGLE_IDENTIFIER] = geometry_info[area_geometry_calculator.AZIMUTH_ANGLE_IDENTIFIER]\n            enhanced_df.loc[border_id, self.BORDER_IS_PHYSICAL_IDENTIFIER] = geometry_info[area_geometry_calculator.BORDER_IS_PHYSICAL_IDENTIFIER]\n            enhanced_df.loc[border_id, self.BORDER_LINE_STRING_IDENTIFIER] = geometry_info[area_geometry_calculator.BORDER_LINE_STRING_IDENTIFIER]\n\n        except Exception as e:\n            print(f\"Warning: Could not calculate geometry for border {border_id} \"\n                  f\"({area_from} \u2192 {area_to}): {e}\")\n\n    return enhanced_df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator","title":"AreaBorderGeometryCalculator","text":"<p>               Bases: <code>GeoModelGeneratorBase</code></p> <p>Advanced geometric calculator for energy system area border analysis.</p> <p>This class provides sophisticated geometric calculations for borders between energy system areas, handling both physical borders (adjacent areas sharing geographic boundaries) and logical borders (non-adjacent areas requiring connection paths, e.g. through the sea). It's specifically designed to generate properties for energy market cross-border visualizations.</p> <p>The calculator combines multiple geometric algorithms: - Physical border extraction using geometric intersection - Logical geo-line-border path finding with obstacle avoidance - Representative point computation using pole of inaccessibility for label placements on maps - Azimuth angle calculation for flow icon (arrow) visualization - Geometric validation and optimization</p> Key Features <ul> <li>Automatic detection of physical vs logical borders</li> <li>Optimal path finding for non-crossing connections</li> <li>Representative point calculation for label placement</li> <li>Directional angle computation for arrow orientation</li> <li>Performance optimization with geometric caching</li> <li>Integration with MESQUAL area accounting workflows</li> </ul> Energy Domain Applications <ul> <li>Visualization of cross-border (cross-country, cross-biddingzone, cross-macroregion) variables (flows, spreads, capacities, ...)</li> </ul> <p>Attributes:</p> Name Type Description <code>area_model_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame with area polygon geometries</p> <code>non_crossing_path_finder</code> <code>NonCrossingPathFinder</code> <p>Path optimization engine</p> <code>_centroid_cache</code> <code>dict</code> <p>Cached representative points for performance</p> <code>_line_cache</code> <code>dict</code> <p>Cached border lines for repeated calculations</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; from shapely.geometry import box\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Setup area geometries\n&gt;&gt;&gt; areas = gpd.GeoDataFrame({\n...     'geometry': [box(0, 0, 1, 1), box(2, 0, 3, 1)]  # Two separate areas\n... }, index=['Area_A', 'Area_B'])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Calculate border geometry\n&gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas)\n&gt;&gt;&gt; border_info = calculator.calculate_border_geometry('Area_A', 'Area_B')\n&gt;&gt;&gt; print(f\"Border type: {'Physical' if border_info['is_physical'] else 'Logical'}\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>class AreaBorderGeometryCalculator(GeoModelGeneratorBase):\n    \"\"\"Advanced geometric calculator for energy system area border analysis.\n\n    This class provides sophisticated geometric calculations for borders between\n    energy system areas, handling both physical borders (adjacent areas sharing\n    geographic boundaries) and logical borders (non-adjacent areas requiring\n    connection paths, e.g. through the sea). It's specifically designed to generate\n    properties for energy market cross-border visualizations.\n\n    The calculator combines multiple geometric algorithms:\n    - Physical border extraction using geometric intersection\n    - Logical geo-line-border path finding with obstacle avoidance\n    - Representative point computation using pole of inaccessibility for label placements on maps\n    - Azimuth angle calculation for flow icon (arrow) visualization\n    - Geometric validation and optimization\n\n    Key Features:\n        - Automatic detection of physical vs logical borders\n        - Optimal path finding for non-crossing connections\n        - Representative point calculation for label placement\n        - Directional angle computation for arrow orientation\n        - Performance optimization with geometric caching\n        - Integration with MESQUAL area accounting workflows\n\n    Energy Domain Applications:\n        - Visualization of cross-border (cross-country, cross-biddingzone, cross-macroregion) variables (flows, spreads, capacities, ...)\n\n    Attributes:\n        area_model_gdf (gpd.GeoDataFrame): GeoDataFrame with area polygon geometries\n        non_crossing_path_finder (NonCrossingPathFinder): Path optimization engine\n        _centroid_cache (dict): Cached representative points for performance\n        _line_cache (dict): Cached border lines for repeated calculations\n\n    Example:\n\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; from shapely.geometry import box\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Setup area geometries\n        &gt;&gt;&gt; areas = gpd.GeoDataFrame({\n        ...     'geometry': [box(0, 0, 1, 1), box(2, 0, 3, 1)]  # Two separate areas\n        ... }, index=['Area_A', 'Area_B'])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Calculate border geometry\n        &gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas)\n        &gt;&gt;&gt; border_info = calculator.calculate_border_geometry('Area_A', 'Area_B')\n        &gt;&gt;&gt; print(f\"Border type: {'Physical' if border_info['is_physical'] else 'Logical'}\")\n    \"\"\"\n\n    PROJECTION_POINT_IDENTIFIER = 'projection_point'\n    AZIMUTH_ANGLE_IDENTIFIER = 'azimuth_angle'\n    BORDER_IS_PHYSICAL_IDENTIFIER = 'is_physical'\n    BORDER_LINE_STRING_IDENTIFIER = 'geo_line_string'\n\n    def __init__(self, area_model_gdf: gpd.GeoDataFrame, non_crossing_path_finder: 'NonCrossingPathFinder' = None):\n        \"\"\"Initialize the border geometry calculator.\n\n        Args:\n            area_model_gdf: GeoDataFrame containing area geometries with polygon\n                boundaries. Index should contain area identifiers (e.g., country codes,\n                bidding zone names). Must contain valid polygon geometries in 'geometry' column.\n            non_crossing_path_finder: Optional custom path finder for logical borders.\n                If None, creates default NonCrossingPathFinder with standard parameters.\n\n        Raises:\n            ValueError: If geometries are invalid or area_model_gdf lacks required structure\n\n        Example:\n\n            &gt;&gt;&gt; areas_gdf = gpd.read_file('countries.geojson').set_index('ISO_A2')\n            &gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas_gdf)\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Custom path finder for specific requirements\n            &gt;&gt;&gt; custom_finder = NonCrossingPathFinder(num_points=200, min_clearance=10000)\n            &gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas_gdf, custom_finder)\n\n        Note:\n            Invalid geometries are automatically cleaned using buffer(0) operation.\n            Large area datasets benefit from using projected coordinate systems\n            for accurate geometric calculations.\n        \"\"\"\n        self.area_model_gdf = area_model_gdf\n        self.non_crossing_path_finder = non_crossing_path_finder or NonCrossingPathFinder()\n        self._validate_geometries()\n\n        self._centroid_cache: dict[str, Point] = {}\n        self._line_cache: dict[Tuple[str, str], LineString] = {}\n\n    def _validate_geometries(self):\n        \"\"\"Validate and clean area geometries for reliable calculations.\n\n        Applies buffer(0) operation to fix invalid geometries (self-intersections,\n        unclosed rings, etc.) that could cause calculation failures. This is\n        particularly important for real-world geographic data that may have\n        topology issues.\n\n        Note:\n            The buffer(0) operation is a common technique for fixing invalid\n            polygon geometries without changing their fundamental shape.\n        \"\"\"\n        self.area_model_gdf['geometry'] = self.area_model_gdf['geometry'].apply(\n            lambda geom: geom if geom.is_valid else geom.buffer(0)\n        )\n\n    def calculate_border_geometry(\n        self, \n        area_from: str, \n        area_to: str\n    ) -&gt; dict[str, Union[Point, float, LineString, bool]]:\n        \"\"\"Calculate comprehensive geometric properties for an area border.\n\n        This is the main interface method that computes all geometric properties\n        needed for border visualization and analysis. It automatically detects\n        whether areas are physically adjacent or logically connected and applies\n        appropriate geometric algorithms.\n\n        Processing Logic:\n            1. Detect if areas share physical boundary (touching/intersecting)\n            2. For physical borders: extract shared boundary line\n            3. For logical borders: compute optimal connection path\n            4. Calculate representative point for label/arrow placement\n            5. Compute azimuth angle for arrow icon visualization\n\n        Args:\n            area_from: Source area identifier (must exist in area_model_gdf index)\n            area_to: Target area identifier (must exist in area_model_gdf index)\n\n        Returns:\n            dict: Comprehensive border geometry information containing:\n                - 'projection_point' (Point): Optimal point for label/arrow placement\n                - 'azimuth_angle' (float): Directional angle in degrees (0-360)\n                - 'geo_line_string' (LineString): Border line geometry\n                - 'is_physical' (bool): True for touching areas, False for logical borders\n\n        Raises:\n            KeyError: If area_from or area_to not found in area_model_gdf\n            ValueError: If geometric calculations fail\n\n        Example:\n\n            &gt;&gt;&gt; border_info = calculator.calculate_border_geometry('DE', 'FR')\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Use for visualization\n            &gt;&gt;&gt; point = border_info['projection_point']\n            &gt;&gt;&gt; angle = border_info['azimuth_angle']\n            &gt;&gt;&gt; is_physical = border_info['is_physical']\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; print(f\"Border DE\u2192FR: {point} at {angle}\u00b0 ({'physical' if is_physical else 'logical'})\")\n        \"\"\"\n        midpoint, angle = self.get_area_border_midpoint_and_angle(area_from, area_to)\n\n        if self.areas_touch(area_from, area_to) or self.areas_intersect(area_from, area_to):\n            geom_from = self.get_area_geometry(area_from)\n            geom_to = self.get_area_geometry(area_to)\n            border_line = self._get_continuous_border_line(geom_from, geom_to)\n            is_physical = True\n        else:\n            border_line = self.get_straight_line_between_areas(area_from, area_to)\n            is_physical = False\n\n        return {\n            self.PROJECTION_POINT_IDENTIFIER: midpoint,\n            self.AZIMUTH_ANGLE_IDENTIFIER: angle,\n            self.BORDER_LINE_STRING_IDENTIFIER: border_line,\n            self.BORDER_IS_PHYSICAL_IDENTIFIER: is_physical\n        }\n\n    def areas_touch(self, area_from: str, area_to: str) -&gt; bool:\n        \"\"\"Check if two areas share a common physical (geographic) border.\n\n        Uses Shapely's touches() method to determine if area boundaries\n        intersect without overlapping. This is the standard definition\n        of physical adjacency for energy market regions.\n\n        Args:\n            area_from: Source area identifier\n            area_to: Target area identifier\n\n        Returns:\n            bool: True if areas share a common boundary, False otherwise\n\n        Example:\n\n            &gt;&gt;&gt; touching = calculator.areas_touch('DE', 'FR')  # True for neighboring countries\n            &gt;&gt;&gt; separated = calculator.areas_touch('DE', 'GB')  # False for non-adjacent countries\n        \"\"\"\n        geom_from = self.get_area_geometry(area_from)\n        geom_to = self.get_area_geometry(area_to)\n        return geom_from.touches(geom_to)\n\n    def areas_intersect(self, area_from: str, area_to: str) -&gt; bool:\n        \"\"\"Check if two areas have any geometric intersection.\n\n        Uses Shapely's intersects() method to check for any form of geometric\n        intersection, including touching, overlapping, or containment. This is\n        broader than the touches() check and handles edge cases in geographic data.\n\n        Args:\n            area_from: Source area identifier\n            area_to: Target area identifier\n\n        Returns:\n            bool: True if areas have any geometric intersection, False otherwise\n\n        Note:\n            This method is used as a fallback for areas_touch() to handle\n            geographic data with small overlaps or slight topology inconsistencies\n            that are common in real-world boundary datasets.\n        \"\"\"\n        geom_from = self.get_area_geometry(area_from)\n        geom_to = self.get_area_geometry(area_to)\n        return geom_from.intersects(geom_to)\n\n    def get_area_border_midpoint_and_angle(\n        self, \n        area_from: str, \n        area_to: str\n    ) -&gt; tuple[Point, float]:\n        \"\"\"Calculate representative point and directional angle for border.\n\n        Computes the optimal point for placing directional indicators (arrows,\n        labels) and the corresponding angle for proper orientation. The algorithm\n        adapts to both physical and logical borders to ensure optimal placement.\n\n        For Physical Borders:\n            - Uses midpoint of shared boundary line\n            - Angle is perpendicular to boundary, pointing toward target area\n\n        For Logical Borders:\n            - Uses midpoint of optimal connection line\n            - Angle follows connection direction from source to target\n\n        Args:\n            area_from: Source area identifier\n            area_to: Target area identifier\n\n        Returns:\n            tuple[Point, float]: Representative point and directional angle in degrees.\n                Angle range: 0-360 degrees, where 0\u00b0 is North, 90\u00b0 is East.\n\n        Example:\n\n            &gt;&gt;&gt; point, angle = calculator.get_area_border_midpoint_and_angle('DE', 'FR')\n            &gt;&gt;&gt; print(f\"Place arrow at {point} oriented at {angle}\u00b0 for DE\u2192FR flow\")\n        \"\"\"\n        geom_from = self.get_area_geometry(area_from)\n        geom_to = self.get_area_geometry(area_to)\n\n        if self.areas_touch(area_from, area_to) or self.areas_intersect(area_from, area_to):\n            midpoint, angle = self._get_midpoint_and_angle_for_touching_areas(geom_from, geom_to)\n        else:\n            straight_line = self.get_straight_line_between_areas(area_from, area_to)\n            midpoint, angle = self._get_midpoint_and_angle_from_line(straight_line)\n\n        # Ensure angle points from area_from to area_to\n        if not self._angle_points_to_target(geom_from, geom_to, midpoint, angle):\n            angle = (angle + 180) % 360\n\n        return midpoint, angle\n\n    def get_area_geometry(self, area: str) -&gt; Union[Polygon, MultiPolygon]:\n        \"\"\"Retrieve and validate geometry for a specified area.\n\n        Args:\n            area: Area identifier that must exist in area_model_gdf index\n\n        Returns:\n            Union[Polygon, MultiPolygon]: Cleaned geometry with buffer(0) applied\n                to ensure validity for geometric operations\n\n        Raises:\n            KeyError: If area is not found in area_model_gdf\n\n        Note:\n            The buffer(0) operation ensures geometric validity for complex\n            calculations, which is essential for reliable border analysis.\n        \"\"\"\n        return self.area_model_gdf.loc[area].geometry.buffer(0)\n\n    def get_straight_line_between_areas(self, area_from: str, area_to: str) -&gt; LineString:\n        \"\"\"Compute optimal straight-line connection between non-adjacent areas.\n\n        Creates a direct line connection between area boundaries, with intelligent\n        path optimization to avoid crossing other areas when possible. This is\n        particularly important for non-physical borders.\n\n        Algorithm:\n            1. Find representative points for both areas\n            2. Create line connecting area centroids\n            3. Calculate intersection points with area boundaries  \n            4. Check for conflicts with other areas\n            5. Apply non-crossing path optimization if needed\n\n        Args:\n            area_from: Source area identifier\n            area_to: Target area identifier\n\n        Returns:\n            LineString: Optimized connection line between area boundaries.\n                Line endpoints touch the area boundaries, not the centroids.\n\n        Raises:\n            ValueError: If areas are touching (should use physical border instead)\n\n        Example:\n\n            &gt;&gt;&gt; # Connect non-adjacent areas (e.g., Germany to UK)\n            &gt;&gt;&gt; line = calculator.get_straight_line_between_areas('DE', 'GB')\n            &gt;&gt;&gt; print(f\"Connection length: {line.length:.0f} km\")\n\n        Performance Note:\n            Results are cached to improve performance for repeated calculations.\n            Path optimization can be computationally intensive for complex geometries.\n        \"\"\"\n        key = tuple(sorted((area_from, area_to)))\n        if key in self._line_cache:\n            return self._line_cache[key]\n\n        if self.areas_touch(area_from, area_to):\n            raise ValueError(f\"Areas {area_from} and {area_to} touch - use border line instead\")\n\n        geom_from = self._get_largest_polygon(self.get_area_geometry(area_from))\n        geom_to = self._get_largest_polygon(self.get_area_geometry(area_to))\n\n        centroid_from = self.get_representative_area_point(geom_from)\n        centroid_to = self.get_representative_area_point(geom_to)\n\n        line_full = LineString([centroid_from, centroid_to])\n\n        # Find intersection points with area boundaries\n        intersection_from = self._get_boundary_intersection(geom_from, line_full, centroid_to)\n        intersection_to = self._get_boundary_intersection(geom_to, line_full, centroid_from)\n\n        straight_line = LineString([intersection_from, intersection_to])\n\n        # Check if line crosses other areas\n        if self._line_crosses_other_areas(straight_line, area_from, area_to):\n            # Try to find alternative path\n            better_line = self._find_non_crossing_line(area_from, area_to)\n            if better_line is not None:\n                straight_line = better_line\n\n        self._line_cache[key] = straight_line\n        return straight_line\n\n    def _get_midpoint_and_angle_for_touching_areas(\n        self,\n        geom_from: Union[Polygon, MultiPolygon],\n        geom_to: Union[Polygon, MultiPolygon]\n    ) -&gt; tuple[Point, float]:\n        \"\"\"Calculate midpoint and angle for physically adjacent areas.\n\n        For areas that share a physical boundary, this method extracts the\n        shared border line and computes the optimal point and angle for\n        directional indicators.\n\n        Args:\n            geom_from: Source area geometry\n            geom_to: Target area geometry\n\n        Returns:\n            tuple[Point, float]: Midpoint of shared border and perpendicular angle\n                pointing from source toward target area\n\n        Algorithm:\n            1. Extract continuous border line from geometric intersection\n            2. Find midpoint along border line (50% interpolation)\n            3. Calculate border bearing and perpendicular angle\n            4. Ensure angle points from source to target area\n        \"\"\"\n        border_line = self._get_continuous_border_line(geom_from, geom_to)\n        midpoint = border_line.interpolate(0.5, normalized=True)\n\n        # Get angle perpendicular to border\n        start_to_end = self._get_straight_line_from_endpoints(border_line)\n        border_bearing = self._calculate_bearing(start_to_end)\n        perpendicular_angle = (border_bearing + 90) % 360\n\n        return midpoint, perpendicular_angle\n\n    def _get_midpoint_and_angle_from_line(self, line: LineString) -&gt; tuple[Point, float]:\n        \"\"\"Extract midpoint and directional angle from a LineString.\n\n        Args:\n            line: Input LineString geometry\n\n        Returns:\n            tuple[Point, float]: Midpoint and bearing angle in degrees\n\n        Note:\n            Uses 50% interpolation to find the midpoint, ensuring consistent\n            positioning regardless of coordinate density along the line.\n        \"\"\"\n        midpoint = line.interpolate(0.5, normalized=True)\n        angle = self._calculate_bearing(line)\n        return midpoint, angle\n\n    def _angle_points_to_target(\n        self,\n        geom_from: Union[Polygon, MultiPolygon],\n        geom_to: Union[Polygon, MultiPolygon],\n        midpoint: Point,\n        angle: float\n    ) -&gt; bool:\n        \"\"\"Validate that computed angle points from source toward target area.\n\n        Ensures directional consistency by checking if the angle points closer\n        to the target area than to the source area. This is essential for\n        correct arrow orientation in energy flow visualization.\n\n        Args:\n            geom_from: Source area geometry\n            geom_to: Target area geometry  \n            midpoint: Reference point for angle measurement\n            angle: Angle to validate (in degrees)\n\n        Returns:\n            bool: True if angle points toward target, False if it points toward source\n\n        Algorithm:\n            1. Calculate bearings from midpoint to both area centroids\n            2. Compute angular differences between proposed angle and both bearings\n            3. Return True if angle is closer to target bearing than source bearing\n        \"\"\"\n        centroid_from = self.get_representative_area_point(geom_from)\n        centroid_to = self.get_representative_area_point(geom_to)\n\n        bearing_to_from = self._calculate_bearing(LineString([midpoint, centroid_from]))\n        bearing_to_to = self._calculate_bearing(LineString([midpoint, centroid_to]))\n\n        angle_diff_from = self._angular_difference(bearing_to_from, angle)\n        angle_diff_to = self._angular_difference(bearing_to_to, angle)\n\n        return angle_diff_to &lt; angle_diff_from\n\n    def _get_continuous_border_line(\n        self,\n        geom_a: Union[Polygon, MultiPolygon],\n        geom_b: Union[Polygon, MultiPolygon]\n    ) -&gt; LineString:\n        \"\"\"Extract shared boundary line between touching geometries.\n\n        Computes the geometric intersection between two touching areas and\n        converts the result into a continuous LineString representing the\n        shared border. Handles complex intersection geometries including\n        multiple segments and mixed geometry types.\n\n        Args:\n            geom_a: First area geometry\n            geom_b: Second area geometry\n\n        Returns:\n            LineString: Continuous line representing the shared boundary\n\n        Raises:\n            ValueError: If geometries don't touch or intersect\n            TypeError: If intersection cannot be converted to LineString\n\n        Algorithm:\n            1. Compute geometric intersection of the two areas\n            2. Handle GeometryCollection by extracting line components\n            3. Convert Polygon boundaries to LineString if needed\n            4. Merge multiple LineStrings into continuous representation\n            5. Handle MultiLineString by connecting segments optimally\n\n        Note:\n            This method handles the complexity of real-world geographic boundaries\n            which may result in complex intersection geometries.\n        \"\"\"\n        \"\"\"Get the shared border between two touching geometries.\"\"\"\n        if not (geom_a.touches(geom_b) or geom_a.intersects(geom_b)):\n            raise ValueError(\"Geometries do not touch or intersect\")\n\n        border = geom_a.intersection(geom_b)\n\n        if isinstance(border, GeometryCollection):\n            extracted_lines = []\n\n            for g in border.geoms:\n                if isinstance(g, LineString):\n                    extracted_lines.append(g)\n                elif isinstance(g, Polygon):\n                    extracted_lines.append(g.boundary)\n                elif isinstance(g, MultiLineString):\n                    extracted_lines.extend(g.geoms)\n                elif isinstance(g, MultiPolygon):\n                    extracted_lines.extend([p.boundary for p in g.geoms])\n\n            if not extracted_lines:\n                raise TypeError(f\"GeometryCollection could not be converted into line: {type(border)}\")\n\n            border = linemerge(extracted_lines)\n\n        if isinstance(border, MultiPolygon):\n            border = linemerge([p.boundary for p in border.geoms])\n\n        if isinstance(border, Polygon):\n            border = border.boundary\n\n        if isinstance(border, MultiLineString):\n            border = self._merge_multilinestring(border)\n\n        if isinstance(border, LineString):\n            return border\n\n        raise TypeError(f\"Unexpected border type: {type(border)}\")\n\n    def _get_boundary_intersection(\n        self,\n        geom: Polygon,\n        line: LineString,\n        target_point: Point\n    ) -&gt; Point:\n        \"\"\"Find optimal intersection point between line and polygon boundary.\n\n        When a line intersects a polygon boundary at multiple points, this method\n        selects the point that is closest to a specified target point. This is\n        essential for creating clean border connections.\n\n        Args:\n            geom: Polygon whose boundary to intersect with\n            line: LineString to intersect with polygon boundary\n            target_point: Reference point for choosing among multiple intersections\n\n        Returns:\n            Point: Intersection point closest to target_point\n\n        Raises:\n            TypeError: If intersection geometry type is unexpected\n\n        Algorithm:\n            1. Compute intersection between line and polygon boundary\n            2. Handle different intersection geometry types (Point, MultiPoint, LineString)\n            3. For multiple options, select point closest to target\n            4. Extract coordinates and create Point geometry\n        \"\"\"\n        \"\"\"Find where a line intersects a polygon boundary, choosing the point closest to target.\"\"\"\n        intersection = geom.boundary.intersection(line)\n\n        if isinstance(intersection, Point):\n            return intersection\n        elif isinstance(intersection, MultiPoint):\n            return min(intersection.geoms, key=lambda p: p.distance(target_point))\n        elif isinstance(intersection, (LineString, MultiLineString)):\n            # Get all coordinates and find closest\n            coords = []\n            if isinstance(intersection, LineString):\n                coords = list(intersection.coords)\n            else:\n                for line in intersection.geoms:\n                    coords.extend(list(line.coords))\n            return Point(min(coords, key=lambda c: Point(c).distance(target_point)))\n        else:\n            raise TypeError(f\"Unexpected intersection type: {type(intersection)}\")\n\n    def _line_crosses_other_areas(\n        self,\n        line: LineString,\n        *exclude_areas: str\n    ) -&gt; bool:\n        \"\"\"Check if a line crosses through any areas except specified exclusions.\n\n        This method is crucial for validating logical border connections to ensure\n        they don't inappropriately cross through other energy market areas, which\n        would be misleading in visualization.\n\n        Args:\n            line: LineString to test for crossings\n            *exclude_areas: Area identifiers to exclude from crossing check\n                (typically the source and target areas of the line)\n\n        Returns:\n            bool: True if line crosses any non-excluded areas, False otherwise\n        \"\"\"\n        other_areas = self.area_model_gdf.drop(list(exclude_areas))\n        return other_areas.geometry.crosses(line).any()\n\n    def _find_non_crossing_line(\n        self,\n        area_from: str,\n        area_to: str\n    ) -&gt; Union[LineString, None]:\n        \"\"\"Find optimal connection path that avoids crossing other areas.\n\n        Uses the NonCrossingPathFinder to compute the shortest connection between\n        two areas that maintains minimum clearance from other areas. This creates\n        clean visualization paths for logical borders.\n\n        Args:\n            area_from: Source area identifier\n            area_to: Target area identifier\n\n        Returns:\n            LineString or None: Optimal non-crossing path, or None if no suitable\n                path found within the configured constraints\n\n        Algorithm:\n            1. Extract largest polygons from MultiPolygon geometries\n            2. Create exclusion set of all other areas\n            3. Apply NonCrossingPathFinder algorithm\n            4. Return shortest valid path or None if impossible\n\n        Performance Note:\n            This operation can be computationally intensive for complex geometries\n            and large numbers of areas. Results are cached for efficiency.\n        \"\"\"\n        poly_from = self._get_largest_polygon(self.get_area_geometry(area_from))\n        poly_to = self._get_largest_polygon(self.get_area_geometry(area_to))\n\n        return self.non_crossing_path_finder.find_shortest_path(\n            poly_from,\n            poly_to,\n            self.area_model_gdf.drop([area_from, area_to]),\n            f\"{area_from} to {area_to}\"\n        )\n\n    def _get_largest_polygon(self, geom: Union[Polygon, MultiPolygon]) -&gt; Polygon:\n        \"\"\"Extract largest polygon component from MultiPolygon geometry.\n\n        For MultiPolygon geometries (e.g., countries with islands), this method\n        returns the largest polygon by area, which is typically the main landmass.\n        This simplifies calculations while focusing on the most significant\n        geographic component.\n\n        Args:\n            geom: Input geometry (Polygon returned as-is, MultiPolygon simplified)\n\n        Returns:\n            Polygon: Largest polygon component by area\n        \"\"\"\n        if isinstance(geom, Polygon):\n            return geom\n        return max(geom.geoms, key=lambda p: p.area)\n\n    def _merge_multilinestring(self, mls: MultiLineString) -&gt; LineString:\n        \"\"\"Merge disconnected LineString segments into continuous line.\n\n        Converts a MultiLineString into a single continuous LineString by\n        intelligently connecting segments to minimize total length while\n        preserving the overall geometric relationship.\n\n        Args:\n            mls: MultiLineString with potentially disconnected segments\n\n        Returns:\n            LineString: Continuous line connecting all segments optimally\n\n        Algorithm:\n            1. Attempt automatic merge using Shapely's linemerge\n            2. If segments remain disconnected, apply iterative connection:\n               - Find closest pair of segment endpoints\n               - Connect segments with optimal orientation\n               - Repeat until single continuous line achieved\n\n        Note:\n            This method is particularly important for complex international\n            borders that may be represented as multiple disconnected segments\n            in geographic datasets.\n        \"\"\"\n        merged = linemerge(list(mls.geoms))\n\n        if isinstance(merged, LineString):\n            return merged\n\n        # Connect disconnected segments\n        lines = list(merged.geoms)\n\n        while len(lines) &gt; 1:\n            # Find closest pair\n            min_dist = float('inf')\n            closest_pair = None\n\n            for i, line1 in enumerate(lines):\n                for j, line2 in enumerate(lines[i+1:], i+1):\n                    p1, p2 = nearest_points(line1, line2)\n                    dist = p1.distance(p2)\n                    if dist &lt; min_dist:\n                        min_dist = dist\n                        closest_pair = (i, j)\n\n            # Connect closest pair\n            i, j = closest_pair\n            line1, line2 = lines[i], lines[j]\n\n            # Create connected line\n            coords1 = list(line1.coords)\n            coords2 = list(line2.coords)\n\n            # Find best connection direction\n            connections = [\n                (coords1 + coords2, LineString(coords1 + coords2).length),\n                (coords1 + coords2[::-1], LineString(coords1 + coords2[::-1]).length),\n                (coords1[::-1] + coords2, LineString(coords1[::-1] + coords2).length),\n                (coords1[::-1] + coords2[::-1], LineString(coords1[::-1] + coords2[::-1]).length)\n            ]\n\n            best_coords = min(connections, key=lambda x: x[1])[0]\n            new_line = LineString(best_coords)\n\n            # Update lines list\n            lines = [l for k, l in enumerate(lines) if k not in (i, j)]\n            lines.append(new_line)\n\n        return lines[0]\n\n    def _get_straight_line_from_endpoints(self, line: LineString) -&gt; LineString:\n        \"\"\"Create straight line connecting first and last coordinates.\n\n        Args:\n            line: Input LineString with potentially complex path\n\n        Returns:\n            LineString: Simplified straight line from start to end\n\n        Use Case:\n            Useful for computing bearing angles from complex border geometries\n            by simplifying to the fundamental start-to-end direction.\n        \"\"\"\n        return LineString([line.coords[0], line.coords[-1]])\n\n    def _calculate_bearing(self, line: LineString) -&gt; float:\n        \"\"\"Calculate compass bearing (Azimuth angle) for a LineString.\n\n        Args:\n            line: LineString from which to calculate bearing\n\n        Returns:\n            float: Compass bearing (Azimuth angle) in degrees (0-360)\n                - 0\u00b0 = North\n                - 90\u00b0 = East  \n                - 180\u00b0 = South\n                - 270\u00b0 = West\n\n        Algorithm:\n            Uses the forward azimuth formula from geodetic calculations:\n            1. Convert coordinates to radians\n            2. Apply spherical trigonometry formulas\n            3. Convert result to compass bearing (0-360\u00b0)\n\n        Note:\n            This implementation assumes coordinates are in geographic (lat/lon)\n            format. For projected coordinates, results approximate true bearings\n            within reasonable accuracy for visualization purposes.\n        \"\"\"\n        start = line.coords[0]\n        end = line.coords[-1]\n\n        lat1 = math.radians(start[1])\n        lat2 = math.radians(end[1])\n\n        diff_lon = math.radians(end[0] - start[0])\n\n        x = math.sin(diff_lon) * math.cos(lat2)\n        y = (math.cos(lat1) * math.sin(lat2) - \n             math.sin(lat1) * math.cos(lat2) * math.cos(diff_lon))\n\n        bearing = math.atan2(x, y)\n        bearing = math.degrees(bearing)\n        compass_bearing = (bearing + 360) % 360\n\n        return compass_bearing\n\n    def _angular_difference(self, angle1: float, angle2: float) -&gt; float:\n        \"\"\"Calculate minimum angular difference between two compass bearings.\n\n        Handles the circular nature of compass bearings to find the shortest\n        angular distance between two directions, accounting for the 0\u00b0/360\u00b0 wraparound.\n\n        Args:\n            angle1: First angle in degrees (0-360)\n            angle2: Second angle in degrees (0-360)\n\n        Returns:\n            float: Minimum angular difference in degrees (0-180)\n\n        Example:\n\n            &gt;&gt;&gt; diff = calculator._angular_difference(10, 350)  # Returns 20, not 340\n            &gt;&gt;&gt; diff = calculator._angular_difference(90, 270)  # Returns 180\n        \"\"\"\n        diff = abs(angle1 - angle2) % 360\n        return min(diff, 360 - diff)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator.__init__","title":"__init__","text":"<pre><code>__init__(area_model_gdf: GeoDataFrame, non_crossing_path_finder: NonCrossingPathFinder = None)\n</code></pre> <p>Initialize the border geometry calculator.</p> <p>Parameters:</p> Name Type Description Default <code>area_model_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing area geometries with polygon boundaries. Index should contain area identifiers (e.g., country codes, bidding zone names). Must contain valid polygon geometries in 'geometry' column.</p> required <code>non_crossing_path_finder</code> <code>NonCrossingPathFinder</code> <p>Optional custom path finder for logical borders. If None, creates default NonCrossingPathFinder with standard parameters.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If geometries are invalid or area_model_gdf lacks required structure</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; areas_gdf = gpd.read_file('countries.geojson').set_index('ISO_A2')\n&gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas_gdf)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Custom path finder for specific requirements\n&gt;&gt;&gt; custom_finder = NonCrossingPathFinder(num_points=200, min_clearance=10000)\n&gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas_gdf, custom_finder)\n</code></pre> Note <p>Invalid geometries are automatically cleaned using buffer(0) operation. Large area datasets benefit from using projected coordinate systems for accurate geometric calculations.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def __init__(self, area_model_gdf: gpd.GeoDataFrame, non_crossing_path_finder: 'NonCrossingPathFinder' = None):\n    \"\"\"Initialize the border geometry calculator.\n\n    Args:\n        area_model_gdf: GeoDataFrame containing area geometries with polygon\n            boundaries. Index should contain area identifiers (e.g., country codes,\n            bidding zone names). Must contain valid polygon geometries in 'geometry' column.\n        non_crossing_path_finder: Optional custom path finder for logical borders.\n            If None, creates default NonCrossingPathFinder with standard parameters.\n\n    Raises:\n        ValueError: If geometries are invalid or area_model_gdf lacks required structure\n\n    Example:\n\n        &gt;&gt;&gt; areas_gdf = gpd.read_file('countries.geojson').set_index('ISO_A2')\n        &gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas_gdf)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Custom path finder for specific requirements\n        &gt;&gt;&gt; custom_finder = NonCrossingPathFinder(num_points=200, min_clearance=10000)\n        &gt;&gt;&gt; calculator = AreaBorderGeometryCalculator(areas_gdf, custom_finder)\n\n    Note:\n        Invalid geometries are automatically cleaned using buffer(0) operation.\n        Large area datasets benefit from using projected coordinate systems\n        for accurate geometric calculations.\n    \"\"\"\n    self.area_model_gdf = area_model_gdf\n    self.non_crossing_path_finder = non_crossing_path_finder or NonCrossingPathFinder()\n    self._validate_geometries()\n\n    self._centroid_cache: dict[str, Point] = {}\n    self._line_cache: dict[Tuple[str, str], LineString] = {}\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator.calculate_border_geometry","title":"calculate_border_geometry","text":"<pre><code>calculate_border_geometry(area_from: str, area_to: str) -&gt; dict[str, Union[Point, float, LineString, bool]]\n</code></pre> <p>Calculate comprehensive geometric properties for an area border.</p> <p>This is the main interface method that computes all geometric properties needed for border visualization and analysis. It automatically detects whether areas are physically adjacent or logically connected and applies appropriate geometric algorithms.</p> Processing Logic <ol> <li>Detect if areas share physical boundary (touching/intersecting)</li> <li>For physical borders: extract shared boundary line</li> <li>For logical borders: compute optimal connection path</li> <li>Calculate representative point for label/arrow placement</li> <li>Compute azimuth angle for arrow icon visualization</li> </ol> <p>Parameters:</p> Name Type Description Default <code>area_from</code> <code>str</code> <p>Source area identifier (must exist in area_model_gdf index)</p> required <code>area_to</code> <code>str</code> <p>Target area identifier (must exist in area_model_gdf index)</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Union[Point, float, LineString, bool]]</code> <p>Comprehensive border geometry information containing: - 'projection_point' (Point): Optimal point for label/arrow placement - 'azimuth_angle' (float): Directional angle in degrees (0-360) - 'geo_line_string' (LineString): Border line geometry - 'is_physical' (bool): True for touching areas, False for logical borders</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If area_from or area_to not found in area_model_gdf</p> <code>ValueError</code> <p>If geometric calculations fail</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; border_info = calculator.calculate_border_geometry('DE', 'FR')\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Use for visualization\n&gt;&gt;&gt; point = border_info['projection_point']\n&gt;&gt;&gt; angle = border_info['azimuth_angle']\n&gt;&gt;&gt; is_physical = border_info['is_physical']\n&gt;&gt;&gt; \n&gt;&gt;&gt; print(f\"Border DE\u2192FR: {point} at {angle}\u00b0 ({'physical' if is_physical else 'logical'})\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def calculate_border_geometry(\n    self, \n    area_from: str, \n    area_to: str\n) -&gt; dict[str, Union[Point, float, LineString, bool]]:\n    \"\"\"Calculate comprehensive geometric properties for an area border.\n\n    This is the main interface method that computes all geometric properties\n    needed for border visualization and analysis. It automatically detects\n    whether areas are physically adjacent or logically connected and applies\n    appropriate geometric algorithms.\n\n    Processing Logic:\n        1. Detect if areas share physical boundary (touching/intersecting)\n        2. For physical borders: extract shared boundary line\n        3. For logical borders: compute optimal connection path\n        4. Calculate representative point for label/arrow placement\n        5. Compute azimuth angle for arrow icon visualization\n\n    Args:\n        area_from: Source area identifier (must exist in area_model_gdf index)\n        area_to: Target area identifier (must exist in area_model_gdf index)\n\n    Returns:\n        dict: Comprehensive border geometry information containing:\n            - 'projection_point' (Point): Optimal point for label/arrow placement\n            - 'azimuth_angle' (float): Directional angle in degrees (0-360)\n            - 'geo_line_string' (LineString): Border line geometry\n            - 'is_physical' (bool): True for touching areas, False for logical borders\n\n    Raises:\n        KeyError: If area_from or area_to not found in area_model_gdf\n        ValueError: If geometric calculations fail\n\n    Example:\n\n        &gt;&gt;&gt; border_info = calculator.calculate_border_geometry('DE', 'FR')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Use for visualization\n        &gt;&gt;&gt; point = border_info['projection_point']\n        &gt;&gt;&gt; angle = border_info['azimuth_angle']\n        &gt;&gt;&gt; is_physical = border_info['is_physical']\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; print(f\"Border DE\u2192FR: {point} at {angle}\u00b0 ({'physical' if is_physical else 'logical'})\")\n    \"\"\"\n    midpoint, angle = self.get_area_border_midpoint_and_angle(area_from, area_to)\n\n    if self.areas_touch(area_from, area_to) or self.areas_intersect(area_from, area_to):\n        geom_from = self.get_area_geometry(area_from)\n        geom_to = self.get_area_geometry(area_to)\n        border_line = self._get_continuous_border_line(geom_from, geom_to)\n        is_physical = True\n    else:\n        border_line = self.get_straight_line_between_areas(area_from, area_to)\n        is_physical = False\n\n    return {\n        self.PROJECTION_POINT_IDENTIFIER: midpoint,\n        self.AZIMUTH_ANGLE_IDENTIFIER: angle,\n        self.BORDER_LINE_STRING_IDENTIFIER: border_line,\n        self.BORDER_IS_PHYSICAL_IDENTIFIER: is_physical\n    }\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator.areas_touch","title":"areas_touch","text":"<pre><code>areas_touch(area_from: str, area_to: str) -&gt; bool\n</code></pre> <p>Check if two areas share a common physical (geographic) border.</p> <p>Uses Shapely's touches() method to determine if area boundaries intersect without overlapping. This is the standard definition of physical adjacency for energy market regions.</p> <p>Parameters:</p> Name Type Description Default <code>area_from</code> <code>str</code> <p>Source area identifier</p> required <code>area_to</code> <code>str</code> <p>Target area identifier</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if areas share a common boundary, False otherwise</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; touching = calculator.areas_touch('DE', 'FR')  # True for neighboring countries\n&gt;&gt;&gt; separated = calculator.areas_touch('DE', 'GB')  # False for non-adjacent countries\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def areas_touch(self, area_from: str, area_to: str) -&gt; bool:\n    \"\"\"Check if two areas share a common physical (geographic) border.\n\n    Uses Shapely's touches() method to determine if area boundaries\n    intersect without overlapping. This is the standard definition\n    of physical adjacency for energy market regions.\n\n    Args:\n        area_from: Source area identifier\n        area_to: Target area identifier\n\n    Returns:\n        bool: True if areas share a common boundary, False otherwise\n\n    Example:\n\n        &gt;&gt;&gt; touching = calculator.areas_touch('DE', 'FR')  # True for neighboring countries\n        &gt;&gt;&gt; separated = calculator.areas_touch('DE', 'GB')  # False for non-adjacent countries\n    \"\"\"\n    geom_from = self.get_area_geometry(area_from)\n    geom_to = self.get_area_geometry(area_to)\n    return geom_from.touches(geom_to)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator.areas_intersect","title":"areas_intersect","text":"<pre><code>areas_intersect(area_from: str, area_to: str) -&gt; bool\n</code></pre> <p>Check if two areas have any geometric intersection.</p> <p>Uses Shapely's intersects() method to check for any form of geometric intersection, including touching, overlapping, or containment. This is broader than the touches() check and handles edge cases in geographic data.</p> <p>Parameters:</p> Name Type Description Default <code>area_from</code> <code>str</code> <p>Source area identifier</p> required <code>area_to</code> <code>str</code> <p>Target area identifier</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if areas have any geometric intersection, False otherwise</p> Note <p>This method is used as a fallback for areas_touch() to handle geographic data with small overlaps or slight topology inconsistencies that are common in real-world boundary datasets.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def areas_intersect(self, area_from: str, area_to: str) -&gt; bool:\n    \"\"\"Check if two areas have any geometric intersection.\n\n    Uses Shapely's intersects() method to check for any form of geometric\n    intersection, including touching, overlapping, or containment. This is\n    broader than the touches() check and handles edge cases in geographic data.\n\n    Args:\n        area_from: Source area identifier\n        area_to: Target area identifier\n\n    Returns:\n        bool: True if areas have any geometric intersection, False otherwise\n\n    Note:\n        This method is used as a fallback for areas_touch() to handle\n        geographic data with small overlaps or slight topology inconsistencies\n        that are common in real-world boundary datasets.\n    \"\"\"\n    geom_from = self.get_area_geometry(area_from)\n    geom_to = self.get_area_geometry(area_to)\n    return geom_from.intersects(geom_to)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator.get_area_border_midpoint_and_angle","title":"get_area_border_midpoint_and_angle","text":"<pre><code>get_area_border_midpoint_and_angle(area_from: str, area_to: str) -&gt; tuple[Point, float]\n</code></pre> <p>Calculate representative point and directional angle for border.</p> <p>Computes the optimal point for placing directional indicators (arrows, labels) and the corresponding angle for proper orientation. The algorithm adapts to both physical and logical borders to ensure optimal placement.</p> For Physical Borders <ul> <li>Uses midpoint of shared boundary line</li> <li>Angle is perpendicular to boundary, pointing toward target area</li> </ul> For Logical Borders <ul> <li>Uses midpoint of optimal connection line</li> <li>Angle follows connection direction from source to target</li> </ul> <p>Parameters:</p> Name Type Description Default <code>area_from</code> <code>str</code> <p>Source area identifier</p> required <code>area_to</code> <code>str</code> <p>Target area identifier</p> required <p>Returns:</p> Type Description <code>tuple[Point, float]</code> <p>tuple[Point, float]: Representative point and directional angle in degrees. Angle range: 0-360 degrees, where 0\u00b0 is North, 90\u00b0 is East.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; point, angle = calculator.get_area_border_midpoint_and_angle('DE', 'FR')\n&gt;&gt;&gt; print(f\"Place arrow at {point} oriented at {angle}\u00b0 for DE\u2192FR flow\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def get_area_border_midpoint_and_angle(\n    self, \n    area_from: str, \n    area_to: str\n) -&gt; tuple[Point, float]:\n    \"\"\"Calculate representative point and directional angle for border.\n\n    Computes the optimal point for placing directional indicators (arrows,\n    labels) and the corresponding angle for proper orientation. The algorithm\n    adapts to both physical and logical borders to ensure optimal placement.\n\n    For Physical Borders:\n        - Uses midpoint of shared boundary line\n        - Angle is perpendicular to boundary, pointing toward target area\n\n    For Logical Borders:\n        - Uses midpoint of optimal connection line\n        - Angle follows connection direction from source to target\n\n    Args:\n        area_from: Source area identifier\n        area_to: Target area identifier\n\n    Returns:\n        tuple[Point, float]: Representative point and directional angle in degrees.\n            Angle range: 0-360 degrees, where 0\u00b0 is North, 90\u00b0 is East.\n\n    Example:\n\n        &gt;&gt;&gt; point, angle = calculator.get_area_border_midpoint_and_angle('DE', 'FR')\n        &gt;&gt;&gt; print(f\"Place arrow at {point} oriented at {angle}\u00b0 for DE\u2192FR flow\")\n    \"\"\"\n    geom_from = self.get_area_geometry(area_from)\n    geom_to = self.get_area_geometry(area_to)\n\n    if self.areas_touch(area_from, area_to) or self.areas_intersect(area_from, area_to):\n        midpoint, angle = self._get_midpoint_and_angle_for_touching_areas(geom_from, geom_to)\n    else:\n        straight_line = self.get_straight_line_between_areas(area_from, area_to)\n        midpoint, angle = self._get_midpoint_and_angle_from_line(straight_line)\n\n    # Ensure angle points from area_from to area_to\n    if not self._angle_points_to_target(geom_from, geom_to, midpoint, angle):\n        angle = (angle + 180) % 360\n\n    return midpoint, angle\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator.get_area_geometry","title":"get_area_geometry","text":"<pre><code>get_area_geometry(area: str) -&gt; Union[Polygon, MultiPolygon]\n</code></pre> <p>Retrieve and validate geometry for a specified area.</p> <p>Parameters:</p> Name Type Description Default <code>area</code> <code>str</code> <p>Area identifier that must exist in area_model_gdf index</p> required <p>Returns:</p> Type Description <code>Union[Polygon, MultiPolygon]</code> <p>Union[Polygon, MultiPolygon]: Cleaned geometry with buffer(0) applied to ensure validity for geometric operations</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If area is not found in area_model_gdf</p> Note <p>The buffer(0) operation ensures geometric validity for complex calculations, which is essential for reliable border analysis.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def get_area_geometry(self, area: str) -&gt; Union[Polygon, MultiPolygon]:\n    \"\"\"Retrieve and validate geometry for a specified area.\n\n    Args:\n        area: Area identifier that must exist in area_model_gdf index\n\n    Returns:\n        Union[Polygon, MultiPolygon]: Cleaned geometry with buffer(0) applied\n            to ensure validity for geometric operations\n\n    Raises:\n        KeyError: If area is not found in area_model_gdf\n\n    Note:\n        The buffer(0) operation ensures geometric validity for complex\n        calculations, which is essential for reliable border analysis.\n    \"\"\"\n    return self.area_model_gdf.loc[area].geometry.buffer(0)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_geometry_calculator.AreaBorderGeometryCalculator.get_straight_line_between_areas","title":"get_straight_line_between_areas","text":"<pre><code>get_straight_line_between_areas(area_from: str, area_to: str) -&gt; LineString\n</code></pre> <p>Compute optimal straight-line connection between non-adjacent areas.</p> <p>Creates a direct line connection between area boundaries, with intelligent path optimization to avoid crossing other areas when possible. This is particularly important for non-physical borders.</p> Algorithm <ol> <li>Find representative points for both areas</li> <li>Create line connecting area centroids</li> <li>Calculate intersection points with area boundaries  </li> <li>Check for conflicts with other areas</li> <li>Apply non-crossing path optimization if needed</li> </ol> <p>Parameters:</p> Name Type Description Default <code>area_from</code> <code>str</code> <p>Source area identifier</p> required <code>area_to</code> <code>str</code> <p>Target area identifier</p> required <p>Returns:</p> Name Type Description <code>LineString</code> <code>LineString</code> <p>Optimized connection line between area boundaries. Line endpoints touch the area boundaries, not the centroids.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If areas are touching (should use physical border instead)</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Connect non-adjacent areas (e.g., Germany to UK)\n&gt;&gt;&gt; line = calculator.get_straight_line_between_areas('DE', 'GB')\n&gt;&gt;&gt; print(f\"Connection length: {line.length:.0f} km\")\n</code></pre> Performance Note <p>Results are cached to improve performance for repeated calculations. Path optimization can be computationally intensive for complex geometries.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def get_straight_line_between_areas(self, area_from: str, area_to: str) -&gt; LineString:\n    \"\"\"Compute optimal straight-line connection between non-adjacent areas.\n\n    Creates a direct line connection between area boundaries, with intelligent\n    path optimization to avoid crossing other areas when possible. This is\n    particularly important for non-physical borders.\n\n    Algorithm:\n        1. Find representative points for both areas\n        2. Create line connecting area centroids\n        3. Calculate intersection points with area boundaries  \n        4. Check for conflicts with other areas\n        5. Apply non-crossing path optimization if needed\n\n    Args:\n        area_from: Source area identifier\n        area_to: Target area identifier\n\n    Returns:\n        LineString: Optimized connection line between area boundaries.\n            Line endpoints touch the area boundaries, not the centroids.\n\n    Raises:\n        ValueError: If areas are touching (should use physical border instead)\n\n    Example:\n\n        &gt;&gt;&gt; # Connect non-adjacent areas (e.g., Germany to UK)\n        &gt;&gt;&gt; line = calculator.get_straight_line_between_areas('DE', 'GB')\n        &gt;&gt;&gt; print(f\"Connection length: {line.length:.0f} km\")\n\n    Performance Note:\n        Results are cached to improve performance for repeated calculations.\n        Path optimization can be computationally intensive for complex geometries.\n    \"\"\"\n    key = tuple(sorted((area_from, area_to)))\n    if key in self._line_cache:\n        return self._line_cache[key]\n\n    if self.areas_touch(area_from, area_to):\n        raise ValueError(f\"Areas {area_from} and {area_to} touch - use border line instead\")\n\n    geom_from = self._get_largest_polygon(self.get_area_geometry(area_from))\n    geom_to = self._get_largest_polygon(self.get_area_geometry(area_to))\n\n    centroid_from = self.get_representative_area_point(geom_from)\n    centroid_to = self.get_representative_area_point(geom_to)\n\n    line_full = LineString([centroid_from, centroid_to])\n\n    # Find intersection points with area boundaries\n    intersection_from = self._get_boundary_intersection(geom_from, line_full, centroid_to)\n    intersection_to = self._get_boundary_intersection(geom_to, line_full, centroid_from)\n\n    straight_line = LineString([intersection_from, intersection_to])\n\n    # Check if line crosses other areas\n    if self._line_crosses_other_areas(straight_line, area_from, area_to):\n        # Try to find alternative path\n        better_line = self._find_non_crossing_line(area_from, area_to)\n        if better_line is not None:\n            straight_line = better_line\n\n    self._line_cache[key] = straight_line\n    return straight_line\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_geometry_calculator.NonCrossingPathFinder","title":"NonCrossingPathFinder","text":"<p>Optimized path finder for non-crossing connections between areas.</p> <p>This class implements an algorithm for finding the shortest path between two polygon areas while maintaining specified clearance from other areas. It's specifically designed for energy system visualization where geographic border line representations should not misleadingly cross through other market areas.</p> <p>The algorithm uses a brute-force approach to test multiple potential paths and select the optimal solution.</p> Key Features <ul> <li>Configurable boundary point sampling density</li> <li>Adjustable minimum clearance distances</li> <li>Progress tracking for long-running operations</li> <li>Optimization for common geometric scenarios</li> </ul> Performance Characteristics <ul> <li>Time complexity: O(n\u00b2 \u00d7 m) where n=num_points, m=number of other areas</li> <li>Memory usage scales with point sampling density</li> <li>Results improve with higher point sampling but at computational cost</li> </ul> <p>Attributes:</p> Name Type Description <code>num_points</code> <code>int</code> <p>Number of boundary points to sample per area</p> <code>min_clearance</code> <code>float</code> <p>Minimum distance from other areas (in CRS units)</p> <code>show_progress</code> <code>bool</code> <p>Whether to display progress bars for long operations</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # High-precision path finding\n&gt;&gt;&gt; finder = NonCrossingPathFinder(num_points=500, min_clearance=50000)\n&gt;&gt;&gt; path = finder.find_shortest_path(area1_poly, area2_poly, other_areas_gdf)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>class NonCrossingPathFinder:\n    \"\"\"Optimized path finder for non-crossing connections between areas.\n\n    This class implements an algorithm for finding the shortest\n    path between two polygon areas while maintaining specified clearance from\n    other areas. It's specifically designed for energy system visualization\n    where geographic border line representations should not misleadingly\n    cross through other market areas.\n\n    The algorithm uses a brute-force approach to test multiple potential paths\n    and select the optimal solution.\n\n    Key Features:\n        - Configurable boundary point sampling density\n        - Adjustable minimum clearance distances\n        - Progress tracking for long-running operations\n        - Optimization for common geometric scenarios\n\n    Performance Characteristics:\n        - Time complexity: O(n\u00b2 \u00d7 m) where n=num_points, m=number of other areas\n        - Memory usage scales with point sampling density\n        - Results improve with higher point sampling but at computational cost\n\n    Attributes:\n        num_points (int): Number of boundary points to sample per area\n        min_clearance (float): Minimum distance from other areas (in CRS units)\n        show_progress (bool): Whether to display progress bars for long operations\n\n    Example:\n\n        &gt;&gt;&gt; # High-precision path finding\n        &gt;&gt;&gt; finder = NonCrossingPathFinder(num_points=500, min_clearance=50000)\n        &gt;&gt;&gt; path = finder.find_shortest_path(area1_poly, area2_poly, other_areas_gdf)\n    \"\"\"\n\n    def __init__(\n        self,\n        num_points: int = 100,\n        min_clearance: float = 50000,\n        show_progress: bool = True\n    ):\n        \"\"\"Initialize the non-crossing path finder.\n\n        Args:\n            num_points: Number of boundary points to sample per polygon.\n                Higher values improve path quality but increase computation time.\n                Typical range: 50-500 depending on precision requirements.\n            min_clearance: Minimum clearance distance from other areas in\n                coordinate reference system units. For geographic coordinates,\n                this is typically in meters when using projected CRS.\n            show_progress: Whether to display progress bars during computation.\n                Useful for long-running operations with high num_points values.\n\n        Example:\n\n            &gt;&gt;&gt; # High-precision finder for detailed analysis\n            &gt;&gt;&gt; finder = NonCrossingPathFinder(\n            ...     num_points=300,      # High sampling density\n            ...     min_clearance=25000, # 25km minimum clearance\n            ...     show_progress=True   # Show progress for long operations\n            ... )\n        \"\"\"\n        self.num_points = num_points\n        self.min_clearance = min_clearance\n        self.show_progress = show_progress\n\n    def find_shortest_path(\n        self,\n        polygon1: Polygon,\n        polygon2: Polygon,\n        other_areas: gpd.GeoDataFrame,\n        name: str = None\n    ) -&gt; Union[LineString, None]:\n        \"\"\"Find shortest non-crossing path between two polygons.\n\n        Tests all combinations of boundary points between two polygons to find\n        the shortest connection that maintains minimum clearance from other areas.\n        If the algorithm succeedes and finds a non-crossing LineString, it ensures\n        clean visualization paths for energy market border analysis.\n\n        Args:\n            polygon1: Source polygon geometry\n            polygon2: Target polygon geometry\n            other_areas: GeoDataFrame of areas to avoid crossing through\n            name: Optional name for progress tracking display\n\n        Returns:\n            LineString or None: Shortest valid path between polygons, or None\n                if no path meets clearance requirements\n\n        Algorithm:\n            1. Sample boundary points for both polygons\n            2. Buffer other areas by minimum clearance distance\n            3. Test all point-to-point connections\n            4. Filter out paths that cross buffered areas\n            5. Return shortest valid path\n\n        Performance Scaling:\n            - Total paths tested: num_points\u00b2 \n            - With default num_points=100: tests 10,000 potential paths\n            - Computation time scales roughly O(n\u00b2 \u00d7 m) where m = number of other areas\n\n        Example:\n\n            &gt;&gt;&gt; path = finder.find_shortest_path(\n            ...     source_area, target_area, obstacles_gdf, \"Germany to UK\"\n            ... )\n            &gt;&gt;&gt; if path:\n            ...     print(f\"Found path with length: {path.length:.0f} meters\")\n            ... else:\n            ...     print(\"No valid path found with current clearance settings\")\n        \"\"\"\n        buffered_areas = self._buffer_areas(other_areas, self.min_clearance)\n        points1 = self._get_boundary_points(polygon1, self.num_points)\n        points2 = self._get_boundary_points(polygon2, self.num_points)\n\n        lines = [LineString([p1, p2]) for p1, p2 in itertools.product(points1, points2)]\n        shortest_line = None\n        min_length = float('inf')\n\n        iterator = tqdm(lines, desc=f\"Finding path for {name or 'path'}\") if self.show_progress else lines\n\n        for line in iterator:\n            if not buffered_areas.geometry.crosses(line).any():\n                if line.length &lt; min_length:\n                    shortest_line = line\n                    min_length = line.length\n\n        return shortest_line\n\n    def _buffer_areas(self, areas: gpd.GeoDataFrame, buffer_distance: float) -&gt; gpd.GeoDataFrame:\n        \"\"\"Apply buffer operation to create clearance zones around areas.\n\n        Creates expanded geometries around areas to enforce minimum clearance\n        distances. Handles coordinate system transformations to ensure accurate\n        distance-based buffering regardless of input CRS.\n\n        Args:\n            areas: GeoDataFrame containing areas to buffer\n            buffer_distance: Buffer distance in meters\n\n        Returns:\n            gpd.GeoDataFrame: Areas with buffered geometries in original CRS\n\n        Raises:\n            ValueError: If GeoDataFrame lacks valid CRS definition\n\n        Algorithm:\n            1. Check if CRS is geographic (lat/lon)\n            2. If geographic, temporarily project to Web Mercator (EPSG:3857)\n            3. Apply buffer operation in projected coordinates\n            4. Transform back to original CRS\n            5. If already projected, buffer directly\n        \"\"\"\n        areas_copy = areas.copy()\n        original_crs = areas_copy.crs\n\n        if original_crs is None:\n            raise ValueError(\"GeoDataFrame must have a valid CRS defined.\")\n\n        if original_crs.is_geographic:\n            # Use Web Mercator for accurate distance-based buffering\n            projected_crs = \"EPSG:3857\"\n            areas_copy = areas_copy.to_crs(projected_crs)\n            areas_copy['geometry'] = areas_copy.buffer(buffer_distance)\n            areas_copy = areas_copy.to_crs(original_crs)\n        else:\n            # Already in projected coordinates\n            areas_copy['geometry'] = areas_copy.buffer(buffer_distance)\n\n        return areas_copy\n\n    def _get_boundary_points(self, polygon: Polygon, num_points: int) -&gt; list[Point]:\n        \"\"\"Sample evenly distributed points along polygon boundary.\n\n        Creates a uniform sampling of points along the polygon perimeter using\n        interpolation. This provides comprehensive coverage for path-finding\n        while maintaining computational efficiency.\n\n        Args:\n            polygon: Input polygon to sample\n            num_points: Number of points to sample along boundary\n\n        Returns:\n            list[Point]: List of evenly spaced boundary points\n\n        Algorithm:\n            1. Calculate total boundary length\n            2. Divide into equal segments\n            3. Interpolate points at regular intervals\n            4. Return as list of Point geometries\n\n        Note:\n            Points are distributed proportionally to boundary length,\n            ensuring uniform density regardless of polygon complexity.\n        \"\"\"\n        boundary = polygon.boundary\n        total_length = boundary.length\n        # Generate evenly spaced points along boundary\n        return [boundary.interpolate((i / num_points) * total_length) for i in range(num_points)]\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_geometry_calculator.NonCrossingPathFinder.__init__","title":"__init__","text":"<pre><code>__init__(num_points: int = 100, min_clearance: float = 50000, show_progress: bool = True)\n</code></pre> <p>Initialize the non-crossing path finder.</p> <p>Parameters:</p> Name Type Description Default <code>num_points</code> <code>int</code> <p>Number of boundary points to sample per polygon. Higher values improve path quality but increase computation time. Typical range: 50-500 depending on precision requirements.</p> <code>100</code> <code>min_clearance</code> <code>float</code> <p>Minimum clearance distance from other areas in coordinate reference system units. For geographic coordinates, this is typically in meters when using projected CRS.</p> <code>50000</code> <code>show_progress</code> <code>bool</code> <p>Whether to display progress bars during computation. Useful for long-running operations with high num_points values.</p> <code>True</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; # High-precision finder for detailed analysis\n&gt;&gt;&gt; finder = NonCrossingPathFinder(\n...     num_points=300,      # High sampling density\n...     min_clearance=25000, # 25km minimum clearance\n...     show_progress=True   # Show progress for long operations\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def __init__(\n    self,\n    num_points: int = 100,\n    min_clearance: float = 50000,\n    show_progress: bool = True\n):\n    \"\"\"Initialize the non-crossing path finder.\n\n    Args:\n        num_points: Number of boundary points to sample per polygon.\n            Higher values improve path quality but increase computation time.\n            Typical range: 50-500 depending on precision requirements.\n        min_clearance: Minimum clearance distance from other areas in\n            coordinate reference system units. For geographic coordinates,\n            this is typically in meters when using projected CRS.\n        show_progress: Whether to display progress bars during computation.\n            Useful for long-running operations with high num_points values.\n\n    Example:\n\n        &gt;&gt;&gt; # High-precision finder for detailed analysis\n        &gt;&gt;&gt; finder = NonCrossingPathFinder(\n        ...     num_points=300,      # High sampling density\n        ...     min_clearance=25000, # 25km minimum clearance\n        ...     show_progress=True   # Show progress for long operations\n        ... )\n    \"\"\"\n    self.num_points = num_points\n    self.min_clearance = min_clearance\n    self.show_progress = show_progress\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.border_model_geometry_calculator.NonCrossingPathFinder.find_shortest_path","title":"find_shortest_path","text":"<pre><code>find_shortest_path(polygon1: Polygon, polygon2: Polygon, other_areas: GeoDataFrame, name: str = None) -&gt; Union[LineString, None]\n</code></pre> <p>Find shortest non-crossing path between two polygons.</p> <p>Tests all combinations of boundary points between two polygons to find the shortest connection that maintains minimum clearance from other areas. If the algorithm succeedes and finds a non-crossing LineString, it ensures clean visualization paths for energy market border analysis.</p> <p>Parameters:</p> Name Type Description Default <code>polygon1</code> <code>Polygon</code> <p>Source polygon geometry</p> required <code>polygon2</code> <code>Polygon</code> <p>Target polygon geometry</p> required <code>other_areas</code> <code>GeoDataFrame</code> <p>GeoDataFrame of areas to avoid crossing through</p> required <code>name</code> <code>str</code> <p>Optional name for progress tracking display</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[LineString, None]</code> <p>LineString or None: Shortest valid path between polygons, or None if no path meets clearance requirements</p> Algorithm <ol> <li>Sample boundary points for both polygons</li> <li>Buffer other areas by minimum clearance distance</li> <li>Test all point-to-point connections</li> <li>Filter out paths that cross buffered areas</li> <li>Return shortest valid path</li> </ol> Performance Scaling <ul> <li>Total paths tested: num_points\u00b2 </li> <li>With default num_points=100: tests 10,000 potential paths</li> <li>Computation time scales roughly O(n\u00b2 \u00d7 m) where m = number of other areas</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; path = finder.find_shortest_path(\n...     source_area, target_area, obstacles_gdf, \"Germany to UK\"\n... )\n&gt;&gt;&gt; if path:\n...     print(f\"Found path with length: {path.length:.0f} meters\")\n... else:\n...     print(\"No valid path found with current clearance settings\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/border_model_geometry_calculator.py</code> <pre><code>def find_shortest_path(\n    self,\n    polygon1: Polygon,\n    polygon2: Polygon,\n    other_areas: gpd.GeoDataFrame,\n    name: str = None\n) -&gt; Union[LineString, None]:\n    \"\"\"Find shortest non-crossing path between two polygons.\n\n    Tests all combinations of boundary points between two polygons to find\n    the shortest connection that maintains minimum clearance from other areas.\n    If the algorithm succeedes and finds a non-crossing LineString, it ensures\n    clean visualization paths for energy market border analysis.\n\n    Args:\n        polygon1: Source polygon geometry\n        polygon2: Target polygon geometry\n        other_areas: GeoDataFrame of areas to avoid crossing through\n        name: Optional name for progress tracking display\n\n    Returns:\n        LineString or None: Shortest valid path between polygons, or None\n            if no path meets clearance requirements\n\n    Algorithm:\n        1. Sample boundary points for both polygons\n        2. Buffer other areas by minimum clearance distance\n        3. Test all point-to-point connections\n        4. Filter out paths that cross buffered areas\n        5. Return shortest valid path\n\n    Performance Scaling:\n        - Total paths tested: num_points\u00b2 \n        - With default num_points=100: tests 10,000 potential paths\n        - Computation time scales roughly O(n\u00b2 \u00d7 m) where m = number of other areas\n\n    Example:\n\n        &gt;&gt;&gt; path = finder.find_shortest_path(\n        ...     source_area, target_area, obstacles_gdf, \"Germany to UK\"\n        ... )\n        &gt;&gt;&gt; if path:\n        ...     print(f\"Found path with length: {path.length:.0f} meters\")\n        ... else:\n        ...     print(\"No valid path found with current clearance settings\")\n    \"\"\"\n    buffered_areas = self._buffer_areas(other_areas, self.min_clearance)\n    points1 = self._get_boundary_points(polygon1, self.num_points)\n    points2 = self._get_boundary_points(polygon2, self.num_points)\n\n    lines = [LineString([p1, p2]) for p1, p2 in itertools.product(points1, points2)]\n    shortest_line = None\n    min_length = float('inf')\n\n    iterator = tqdm(lines, desc=f\"Finding path for {name or 'path'}\") if self.show_progress else lines\n\n    for line in iterator:\n        if not buffered_areas.geometry.crosses(line).any():\n            if line.length &lt; min_length:\n                shortest_line = line\n                min_length = line.length\n\n    return shortest_line\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.model_generator_base.GeoModelGeneratorBase","title":"GeoModelGeneratorBase","text":"<p>Base class for generating geometric models with representative points.</p> <p>This class provides common functionality for working with geometric representations of energy system areas, including methods for computing representative points within polygons and multipolygons. It's designed to support energy market analysis where spatial aggregation of nodes into areas is required.</p> <p>The class supports two methods for computing representative points: - 'polylabel': Uses pole of inaccessibility algorithm for optimal label placement - 'representative_point': Uses Shapely's built-in representative point method</p> <p>Attributes:</p> Name Type Description <code>REPRESENTATIVE_POINT_METHOD</code> <code>str</code> <p>Method used for computing representative points ('polylabel' or 'representative_point')</p> <code>_polylabel_cache</code> <code>dict</code> <p>Cache for expensive polylabel calculations</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/model_generator_base.py</code> <pre><code>class GeoModelGeneratorBase:\n    \"\"\"Base class for generating geometric models with representative points.\n\n    This class provides common functionality for working with geometric representations\n    of energy system areas, including methods for computing representative points\n    within polygons and multipolygons. It's designed to support energy market\n    analysis where spatial aggregation of nodes into areas is required.\n\n    The class supports two methods for computing representative points:\n    - 'polylabel': Uses pole of inaccessibility algorithm for optimal label placement\n    - 'representative_point': Uses Shapely's built-in representative point method\n\n    Attributes:\n        REPRESENTATIVE_POINT_METHOD (str): Method used for computing representative\n            points ('polylabel' or 'representative_point')\n        _polylabel_cache (dict): Cache for expensive polylabel calculations\n    \"\"\"\n    REPRESENTATIVE_POINT_METHOD: Literal['polylabel', 'representative_point'] = 'polylabel'\n    _polylabel_cache = {}\n\n    def get_representative_area_point(self, geom: Union[Polygon, MultiPolygon]) -&gt; Point:\n        \"\"\"Get a representative point for a polygon or multipolygon geometry.\n\n        This method computes a point that is guaranteed to be inside the geometry\n        and is suitable for label placement or other visualization purposes in\n        energy system maps. For MultiPolygons, it operates on the largest constituent.\n\n        Args:\n            geom: A Shapely Polygon or MultiPolygon geometry representing an\n                energy system area (e.g., bidding zone, market region)\n\n        Returns:\n            Point: A Shapely Point guaranteed to be inside the input geometry,\n                suitable for map labels or representative location analysis\n\n        Raises:\n            ValueError: If REPRESENTATIVE_POINT_METHOD is not supported\n\n        Example:\n\n            &gt;&gt;&gt; from shapely.geometry import Polygon, Point\n            &gt;&gt;&gt; generator = GeoModelGeneratorBase()\n            &gt;&gt;&gt; area_polygon = Polygon([(0, 0), (10, 0), (10, 10), (0, 10)])\n            &gt;&gt;&gt; rep_point = generator.get_representative_area_point(area_polygon)\n            &gt;&gt;&gt; print(f\"Representative point: {rep_point.x:.2f}, {rep_point.y:.2f}\")\n        \"\"\"\n        if self.REPRESENTATIVE_POINT_METHOD == 'polylabel':\n            return self._get_polylabel_point(geom)\n        elif self.REPRESENTATIVE_POINT_METHOD == 'representative_point':\n            return geom.representative_point()\n        else:\n            raise ValueError(f'REPRESENTATIVE_POINT_METHOD {self.REPRESENTATIVE_POINT_METHOD} not supported')\n\n    def _get_polylabel_point(self, geom: Union[Polygon, MultiPolygon]) -&gt; Point:\n        \"\"\"Compute representative point using the polylabel algorithm.\n\n        The polylabel algorithm finds the pole of inaccessibility - the most distant\n        internal point from the polygon outline. This is particularly useful for\n        placing labels on complex energy system area geometries.\n\n        For MultiPolygons, operates on the largest polygon by area, which is\n        typically the main landmass for country/region representations.\n\n        Args:\n            geom: A Shapely Polygon or MultiPolygon geometry\n\n        Returns:\n            Point: The pole of inaccessibility point, cached for performance\n\n        Note:\n            Results are cached using the geometry's WKT representation as key.\n            The precision parameter (1.0) provides good balance between accuracy\n            and performance for typical energy system area sizes.\n        \"\"\"\n        key = geom.wkt\n        if key in self._polylabel_cache:\n            return self._polylabel_cache[key]\n\n        if isinstance(geom, MultiPolygon):\n            geom = max(geom.geoms, key=lambda g: g.area)\n\n        exterior = list(geom.exterior.coords)\n        holes = [list(ring.coords) for ring in geom.interiors]\n        rings = [exterior] + holes\n\n        point = Point(polylabel(rings, 1.0))\n        self._polylabel_cache[key] = point\n        return point\n\n    @staticmethod\n    def _compute_representative_point_from_cloud_of_2d_points(points: List[Point]) -&gt; Point:\n        \"\"\"Compute geometric centroid from a collection of 2D points.\n\n        This method is particularly useful in energy systems analysis for computing\n        representative locations of energy assets (e.g., power plants, substations)\n        that belong to the same area or region.\n\n        The algorithm adapts based on the number of input points:\n        - 1 point: Returns the point itself\n        - 2 points: Returns the midpoint\n        - \u22653 points: Computes convex hull and returns polygon centroid\n\n        Args:\n            points: List of Shapely Point objects representing energy asset\n                locations or other spatial features within an area\n\n        Returns:\n            Point: Representative point for the collection of input points\n\n        Raises:\n            ValueError: If the input list is empty\n\n        Example:\n\n            &gt;&gt;&gt; from shapely.geometry import Point\n            &gt;&gt;&gt; power_plants = [Point(1, 1), Point(3, 2), Point(2, 4)]\n            &gt;&gt;&gt; centroid = GeoModelGeneratorBase._compute_representative_point_from_cloud_of_2d_points(power_plants)\n            &gt;&gt;&gt; print(f\"Regional centroid: {centroid.x:.2f}, {centroid.y:.2f}\")\n        \"\"\"\n        from scipy.spatial import ConvexHull\n\n        n = len(points)\n        if n == 0:\n            raise ValueError(\"Empty point list provided - cannot compute representative point\")\n        if n == 1:\n            return points[0]\n        if n == 2:\n            return Point((points[0].x + points[1].x) / 2, (points[0].y + points[1].y) / 2)\n\n        # For 3+ points, compute convex hull and return centroid\n        coords = [(p.x, p.y) for p in points]\n        hull = ConvexHull(coords)\n        hull_coords = [coords[i] for i in hull.vertices]\n        polygon = Polygon(hull_coords)\n        return polygon.representative_point()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/area_accounting/model_gen/#mesqual.energy_data_handling.area_accounting.model_generator_base.GeoModelGeneratorBase.get_representative_area_point","title":"get_representative_area_point","text":"<pre><code>get_representative_area_point(geom: Union[Polygon, MultiPolygon]) -&gt; Point\n</code></pre> <p>Get a representative point for a polygon or multipolygon geometry.</p> <p>This method computes a point that is guaranteed to be inside the geometry and is suitable for label placement or other visualization purposes in energy system maps. For MultiPolygons, it operates on the largest constituent.</p> <p>Parameters:</p> Name Type Description Default <code>geom</code> <code>Union[Polygon, MultiPolygon]</code> <p>A Shapely Polygon or MultiPolygon geometry representing an energy system area (e.g., bidding zone, market region)</p> required <p>Returns:</p> Name Type Description <code>Point</code> <code>Point</code> <p>A Shapely Point guaranteed to be inside the input geometry, suitable for map labels or representative location analysis</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If REPRESENTATIVE_POINT_METHOD is not supported</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from shapely.geometry import Polygon, Point\n&gt;&gt;&gt; generator = GeoModelGeneratorBase()\n&gt;&gt;&gt; area_polygon = Polygon([(0, 0), (10, 0), (10, 10), (0, 10)])\n&gt;&gt;&gt; rep_point = generator.get_representative_area_point(area_polygon)\n&gt;&gt;&gt; print(f\"Representative point: {rep_point.x:.2f}, {rep_point.y:.2f}\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/area_accounting/model_generator_base.py</code> <pre><code>def get_representative_area_point(self, geom: Union[Polygon, MultiPolygon]) -&gt; Point:\n    \"\"\"Get a representative point for a polygon or multipolygon geometry.\n\n    This method computes a point that is guaranteed to be inside the geometry\n    and is suitable for label placement or other visualization purposes in\n    energy system maps. For MultiPolygons, it operates on the largest constituent.\n\n    Args:\n        geom: A Shapely Polygon or MultiPolygon geometry representing an\n            energy system area (e.g., bidding zone, market region)\n\n    Returns:\n        Point: A Shapely Point guaranteed to be inside the input geometry,\n            suitable for map labels or representative location analysis\n\n    Raises:\n        ValueError: If REPRESENTATIVE_POINT_METHOD is not supported\n\n    Example:\n\n        &gt;&gt;&gt; from shapely.geometry import Polygon, Point\n        &gt;&gt;&gt; generator = GeoModelGeneratorBase()\n        &gt;&gt;&gt; area_polygon = Polygon([(0, 0), (10, 0), (10, 10), (0, 10)])\n        &gt;&gt;&gt; rep_point = generator.get_representative_area_point(area_polygon)\n        &gt;&gt;&gt; print(f\"Representative point: {rep_point.x:.2f}, {rep_point.y:.2f}\")\n    \"\"\"\n    if self.REPRESENTATIVE_POINT_METHOD == 'polylabel':\n        return self._get_polylabel_point(geom)\n    elif self.REPRESENTATIVE_POINT_METHOD == 'representative_point':\n        return geom.representative_point()\n    else:\n        raise ValueError(f'REPRESENTATIVE_POINT_METHOD {self.REPRESENTATIVE_POINT_METHOD} not supported')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/","title":"MESQUAL Membership Model Handling","text":""},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/#mesqual.energy_data_handling.model_handling","title":"model_handling","text":"<p>Model handling utilities for MESQUAL energy data processing.</p> <p>This package provides tools for enriching energy system DataFrames with properties from related model objects and creating combination columns for paired relationships.</p> Key Components <ul> <li>Membership property enrichers for adding related object properties to DataFrames</li> <li>Directional relationship handling for from/to column pairs in network data</li> <li>Membership pairs appenders for creating combination identifiers from paired columns</li> </ul> Example use cases <ul> <li>Enriching generator data with node properties   (e.g. generator_model_df then has node_voltage, node_country, ...)</li> <li>Enriching line_model_df with node_from and node_to characteristics   (e.g. region_from, region_to, voltage_from, voltage_to, ...)</li> </ul>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/","title":"MESQUAL Membership Pairs Appender","text":""},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/#mesqual.energy_data_handling.model_handling.membership_pairs_appender.BaseMembershipPairsAppender","title":"BaseMembershipPairsAppender","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for creating combination identifiers from paired energy system relationships.</p> <p>In energy system modeling, many entities have directional relationships that require unique identification for analysis. This class provides a unified framework for creating combination identifiers from paired columns, particularly useful for:</p> <ul> <li>Transmission lines connecting nodes (node_from/node_to combinations)</li> <li>Regional trade flows (region_from/region_to pairs)</li> <li>Pipeline connections (hub_from/hub_to relationships)</li> <li>Market interconnections (market_from/market_to links)</li> </ul> <p>The class supports three distinct combination strategies:</p> <ol> <li> <p>Directional: Preserves relationship direction (A\u2192B \u2260 B\u2192A)</p> <ul> <li>Essential for analyzing flow directions, capacity constraints, and directional costs</li> </ul> </li> <li> <p>Sorted: Creates bidirectional identifiers (A\u2192B = B\u2192A becomes A-B)</p> <ul> <li>Useful for identifying unique connections regardless of direction</li> </ul> </li> <li> <p>Opposite: Reverses relationship direction (A\u2192B becomes B\u2192A)</p> <ul> <li>Enables reverse flow analysis and bidirectional modeling</li> </ul> </li> </ol> <p>This abstraction enables different implementation strategies (string concatenation, tuple creation, etc.) while maintaining consistent naming patterns across MESQUAL energy data models.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>str</code> <p>Suffix/prefix identifying source/origin columns. Defaults to '_from'.</p> <code>'_from'</code> <code>to_identifier</code> <code>str</code> <p>Suffix/prefix identifying destination/target columns. Defaults to '_to'.</p> <code>'_to'</code> <code>combo_col_suffix</code> <code>str</code> <p>Suffix for directional combination column names. Defaults to '_combo'.</p> <code>'_combo'</code> <code>combo_col_prefix</code> <code>str</code> <p>Prefix for directional combination column names. Defaults to None.</p> <code>None</code> <code>sorted_combo_col_suffix</code> <code>str</code> <p>Suffix for sorted combination column names. Defaults to '_combo_sorted'.</p> <code>'_combo_sorted'</code> <code>sorted_combo_col_prefix</code> <code>str</code> <p>Prefix for sorted combination column names. Defaults to None.</p> <code>None</code> <code>opposite_combo_col_suffix</code> <code>str</code> <p>Suffix for opposite combination column names. Defaults to '_combo_opposite'.</p> <code>'_combo_opposite'</code> <code>opposite_combo_col_prefix</code> <code>str</code> <p>Prefix for opposite combination column names. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither suffix nor prefix is provided for any combination type.</p> Note <p>Either suffix or prefix must be specified for each combination type to ensure proper column naming conventions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # For transmission line analysis\n&gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2192 ')\n&gt;&gt;&gt; lines_df = appender.append_combo_columns(transmission_df)\n&gt;&gt;&gt; # Creates 'node_combo' column: 'NodeA \u2192 NodeB'\n\n&gt;&gt;&gt; # For bidirectional connections\n&gt;&gt;&gt; lines_df = appender.append_sorted_combo_columns(transmission_df)\n&gt;&gt;&gt; # Creates 'node_combo_sorted' column: 'NodeA \u2192 NodeB' (alphabetical)\n&gt;&gt;&gt; lines_df = appender.append_opposite_combo_columns(transmission_df)\n&gt;&gt;&gt; # Creates 'node_combo_opposite' column: 'NodeB \u2192 NodeA' (alphabetical)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_pairs_appender.py</code> <pre><code>class BaseMembershipPairsAppender(ABC):\n    \"\"\"Abstract base class for creating combination identifiers from paired energy system relationships.\n\n    In energy system modeling, many entities have directional relationships that require\n    unique identification for analysis. This class provides a unified framework for creating\n    combination identifiers from paired columns, particularly useful for:\n\n    - Transmission lines connecting nodes (node_from/node_to combinations)\n    - Regional trade flows (region_from/region_to pairs)\n    - Pipeline connections (hub_from/hub_to relationships)\n    - Market interconnections (market_from/market_to links)\n\n    The class supports three distinct combination strategies:\n\n    1. **Directional**: Preserves relationship direction (A\u2192B \u2260 B\u2192A)\n        - Essential for analyzing flow directions, capacity constraints, and directional costs\n\n    2. **Sorted**: Creates bidirectional identifiers (A\u2192B = B\u2192A becomes A-B)\n        - Useful for identifying unique connections regardless of direction\n\n    3. **Opposite**: Reverses relationship direction (A\u2192B becomes B\u2192A)\n        - Enables reverse flow analysis and bidirectional modeling\n\n    This abstraction enables different implementation strategies (string concatenation,\n    tuple creation, etc.) while maintaining consistent naming patterns across MESQUAL\n    energy data models.\n\n    Args:\n        from_identifier: Suffix/prefix identifying source/origin columns. Defaults to '_from'.\n        to_identifier: Suffix/prefix identifying destination/target columns. Defaults to '_to'.\n        combo_col_suffix: Suffix for directional combination column names. Defaults to '_combo'.\n        combo_col_prefix: Prefix for directional combination column names. Defaults to None.\n        sorted_combo_col_suffix: Suffix for sorted combination column names. Defaults to '_combo_sorted'.\n        sorted_combo_col_prefix: Prefix for sorted combination column names. Defaults to None.\n        opposite_combo_col_suffix: Suffix for opposite combination column names. Defaults to '_combo_opposite'.\n        opposite_combo_col_prefix: Prefix for opposite combination column names. Defaults to None.\n\n    Raises:\n        ValueError: If neither suffix nor prefix is provided for any combination type.\n\n    Note:\n        Either suffix or prefix must be specified for each combination type to ensure\n        proper column naming conventions.\n\n    Examples:\n\n        &gt;&gt;&gt; # For transmission line analysis\n        &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2192 ')\n        &gt;&gt;&gt; lines_df = appender.append_combo_columns(transmission_df)\n        &gt;&gt;&gt; # Creates 'node_combo' column: 'NodeA \u2192 NodeB'\n\n        &gt;&gt;&gt; # For bidirectional connections\n        &gt;&gt;&gt; lines_df = appender.append_sorted_combo_columns(transmission_df)\n        &gt;&gt;&gt; # Creates 'node_combo_sorted' column: 'NodeA \u2192 NodeB' (alphabetical)\n        &gt;&gt;&gt; lines_df = appender.append_opposite_combo_columns(transmission_df)\n        &gt;&gt;&gt; # Creates 'node_combo_opposite' column: 'NodeB \u2192 NodeA' (alphabetical)\n    \"\"\"\n    def __init__(\n            self,\n            from_identifier: str = '_from',\n            to_identifier: str = '_to',\n            combo_col_suffix: str = '_combo',\n            combo_col_prefix: str = None,\n            sorted_combo_col_suffix: str = '_combo_sorted',\n            sorted_combo_col_prefix: str = None,\n            opposite_combo_col_suffix: str = '_combo_opposite',\n            opposite_combo_col_prefix: str = None,\n    ):\n        self._from_identifier = from_identifier\n        self._to_identifier = to_identifier\n\n        self._combo_col_suffix = combo_col_suffix or ''\n        self._combo_col_prefix = combo_col_prefix or ''\n        if not any([self._combo_col_suffix, self._combo_col_prefix]):\n            raise ValueError(\"Must specify either suffix or prefix for combo columns\")\n\n        self._sorted_combo_col_suffix = sorted_combo_col_suffix or ''\n        self._sorted_combo_col_prefix = sorted_combo_col_prefix or ''\n        if not any([self._sorted_combo_col_suffix, self._sorted_combo_col_prefix]):\n            raise ValueError(\"Must specify either suffix or prefix for sorted combo columns\")\n\n        self._opposite_combo_col_suffix = opposite_combo_col_suffix or ''\n        self._opposite_combo_col_prefix = opposite_combo_col_prefix or ''\n        if not any([self._opposite_combo_col_suffix, self._opposite_combo_col_prefix]):\n            raise ValueError(\"Must specify either suffix or prefix for opposite combo columns\")\n\n        self._common_base_key_finder = CommonBaseKeyFinder(from_identifier, to_identifier)\n\n    def append_combo_columns(\n            self,\n            df_with_from_to_columns: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Creates directional combination columns preserving relationship direction.\n\n        Generates combination identifiers that maintain the original direction of\n        relationships, essential for energy system analysis where flow direction,\n        capacity constraints, and directional costs matter.\n\n        This method automatically identifies all paired columns (those with matching\n        base names plus from/to identifiers) and creates directional combinations\n        using the configured naming strategy.\n\n        Args:\n            df_with_from_to_columns: DataFrame containing paired relationship columns\n                                   (e.g., 'node_from'/'node_to', 'region_from'/'region_to')\n\n        Returns:\n            Enhanced DataFrame with directional combination columns added.\n            Original data preserved, new columns follow configured naming pattern.\n\n        Examples:\n            Transmission line directional analysis:\n\n            &gt;&gt;&gt; # DataFrame with node connections\n            &gt;&gt;&gt; lines_df = pd.DataFrame({\n            ...     'line_id': ['L1', 'L2', 'L3'],\n            ...     'node_from': ['NodeA', 'NodeB', 'NodeC'],\n            ...     'node_to': ['NodeB', 'NodeA', 'NodeA'],\n            ...     'capacity_mw': [1000, 800, 600]\n            ... })\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2192 ')\n            &gt;&gt;&gt; result = appender.append_combo_columns(lines_df)\n            &gt;&gt;&gt; # Result includes 'node_combo': ['NodeA \u2192 NodeB', 'NodeB \u2192 NodeA', 'NodeC \u2192 NodeA']\n        \"\"\"\n        return self._append_combo_columns(df_with_from_to_columns, 'directional')\n\n    def append_sorted_combo_columns(\n            self,\n            df_with_from_to_columns: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Creates bidirectional combination columns with alphabetical ordering.\n\n        Generates combination identifiers that treat relationships as bidirectional\n        by sorting the paired values alphabetically. This is particularly useful for\n        identifying unique connections regardless of direction, such as:\n\n        - Transmission line corridors (same physical connection)\n        - Regional trade partnerships (bidirectional trade agreements)\n        - Pipeline systems (flow can be reversed)\n        - Market coupling arrangements (mutual price influence)\n\n        Args:\n            df_with_from_to_columns: DataFrame containing paired relationship columns\n                                   with potential bidirectional connections\n\n        Returns:\n            Enhanced DataFrame with sorted combination columns added.\n            Bidirectional relationships receive identical identifiers.\n\n        Examples:\n            Bidirectional connection analysis:\n\n            &gt;&gt;&gt; # DataFrame with potentially bidirectional connections\n            &gt;&gt;&gt; connections_df = pd.DataFrame({\n            ...     'connection_id': ['C1', 'C2', 'C3'],\n            ...     'region_from': ['DE', 'FR', 'DE'],\n            ...     'region_to': ['FR', 'DE', 'NL'],\n            ...     'trade_capacity': [2000, 2000, 1500]\n            ... })\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator='-')\n            &gt;&gt;&gt; result = appender.append_sorted_combo_columns(connections_df)\n            &gt;&gt;&gt; # Result includes 'region_combo_sorted': ['DE-FR', 'DE-FR', 'DE-NL']\n            &gt;&gt;&gt; # Note: 'DE\u2192FR' and 'FR\u2192DE' both become 'DE-FR'\n        \"\"\"\n        return self._append_combo_columns(df_with_from_to_columns, 'sorted')\n\n    def append_opposite_combo_columns(\n            self,\n            df_with_from_to_columns: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Creates reverse-direction combination columns for opposite flow analysis.\n\n        Generates combination identifiers with reversed direction, enabling analysis\n        of reverse flows, return paths, and bidirectional modeling scenarios. This\n        is particularly valuable for:\n\n        - Reverse power flows in transmission networks\n        - Return commodity flows in pipeline systems\n        - Opposite direction trade flows\n        - Backup routing analysis\n\n        The method swaps the from/to values before creating combinations, effectively\n        creating the opposite directional identifier for each relationship.\n\n        Args:\n            df_with_from_to_columns: DataFrame containing directional relationships\n                                   where reverse analysis is needed\n\n        Returns:\n            Enhanced DataFrame with opposite-direction combination columns added.\n            Each relationship receives its reverse-direction identifier.\n\n        Examples:\n            Reverse flow analysis:\n\n            &gt;&gt;&gt; # DataFrame with primary flow directions\n            &gt;&gt;&gt; flows_df = pd.DataFrame({\n            ...     'flow_id': ['F1', 'F2', 'F3'],\n            ...     'hub_from': ['HubA', 'HubB', 'HubC'],\n            ...     'hub_to': ['HubB', 'HubC', 'HubA'],\n            ...     'primary_flow': [100, 150, 80]\n            ... })\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2190 ')\n            &gt;&gt;&gt; result = appender.append_opposite_combo_columns(flows_df)\n            &gt;&gt;&gt; # Result includes 'hub_combo_opposite': ['HubB \u2190 HubA', 'HubC \u2190 HubB', 'HubA \u2190 HubC']\n            &gt;&gt;&gt; # Useful for modeling reverse flow scenarios\n        \"\"\"\n        return self._append_combo_columns(df_with_from_to_columns, 'opposite')\n\n    def _append_combo_columns(\n            self,\n            df_with_from_to_columns: pd.DataFrame,\n            which_combo: Literal['directional', 'sorted', 'opposite']\n    ) -&gt; pd.DataFrame:\n        from_id = self._from_identifier\n        to_id = self._to_identifier\n\n        _col_names = df_with_from_to_columns.columns\n        base_columns = self._common_base_key_finder.get_keys_for_which_all_association_tags_appear(_col_names)\n\n        for base in base_columns:\n            from_col = f'{base}{from_id}'\n            to_col = f'{base}{to_id}'\n\n            _from_values = df_with_from_to_columns[from_col]\n            _to_values = df_with_from_to_columns[to_col]\n\n            if which_combo == 'directional':\n                new_col = f'{self._combo_col_prefix}{base}{self._combo_col_suffix}'\n                new_values = self._combine_values(_from_values, _to_values)\n            elif which_combo == 'sorted':\n                new_col = f'{self._sorted_combo_col_prefix}{base}{self._sorted_combo_col_suffix}'\n                new_values = self._combine_sorted_values(_from_values, _to_values)\n            elif which_combo == 'opposite':\n                new_col = f'{self._opposite_combo_col_prefix}{base}{self._opposite_combo_col_suffix}'\n                new_values = self._combine_values(_to_values, _from_values)\n            else:\n                raise NotImplementedError\n\n            df_with_from_to_columns = set_column(df_with_from_to_columns, new_col, new_values)\n\n        return df_with_from_to_columns\n\n    @abstractmethod\n    def _combine_values(self, a: pd.Series, b: pd.Series) -&gt; pd.Series:\n        pass\n\n    @abstractmethod\n    def _combine_sorted_values(self, a: pd.Series, b: pd.Series) -&gt; pd.Series:\n        pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/#mesqual.energy_data_handling.model_handling.membership_pairs_appender.BaseMembershipPairsAppender.append_combo_columns","title":"append_combo_columns","text":"<pre><code>append_combo_columns(df_with_from_to_columns: DataFrame) -&gt; DataFrame\n</code></pre> <p>Creates directional combination columns preserving relationship direction.</p> <p>Generates combination identifiers that maintain the original direction of relationships, essential for energy system analysis where flow direction, capacity constraints, and directional costs matter.</p> <p>This method automatically identifies all paired columns (those with matching base names plus from/to identifiers) and creates directional combinations using the configured naming strategy.</p> <p>Parameters:</p> Name Type Description Default <code>df_with_from_to_columns</code> <code>DataFrame</code> <p>DataFrame containing paired relationship columns                    (e.g., 'node_from'/'node_to', 'region_from'/'region_to')</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Enhanced DataFrame with directional combination columns added.</p> <code>DataFrame</code> <p>Original data preserved, new columns follow configured naming pattern.</p> <p>Examples:</p> <p>Transmission line directional analysis:</p> <pre><code>&gt;&gt;&gt; # DataFrame with node connections\n&gt;&gt;&gt; lines_df = pd.DataFrame({\n...     'line_id': ['L1', 'L2', 'L3'],\n...     'node_from': ['NodeA', 'NodeB', 'NodeC'],\n...     'node_to': ['NodeB', 'NodeA', 'NodeA'],\n...     'capacity_mw': [1000, 800, 600]\n... })\n&gt;&gt;&gt; \n&gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2192 ')\n&gt;&gt;&gt; result = appender.append_combo_columns(lines_df)\n&gt;&gt;&gt; # Result includes 'node_combo': ['NodeA \u2192 NodeB', 'NodeB \u2192 NodeA', 'NodeC \u2192 NodeA']\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_pairs_appender.py</code> <pre><code>def append_combo_columns(\n        self,\n        df_with_from_to_columns: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Creates directional combination columns preserving relationship direction.\n\n    Generates combination identifiers that maintain the original direction of\n    relationships, essential for energy system analysis where flow direction,\n    capacity constraints, and directional costs matter.\n\n    This method automatically identifies all paired columns (those with matching\n    base names plus from/to identifiers) and creates directional combinations\n    using the configured naming strategy.\n\n    Args:\n        df_with_from_to_columns: DataFrame containing paired relationship columns\n                               (e.g., 'node_from'/'node_to', 'region_from'/'region_to')\n\n    Returns:\n        Enhanced DataFrame with directional combination columns added.\n        Original data preserved, new columns follow configured naming pattern.\n\n    Examples:\n        Transmission line directional analysis:\n\n        &gt;&gt;&gt; # DataFrame with node connections\n        &gt;&gt;&gt; lines_df = pd.DataFrame({\n        ...     'line_id': ['L1', 'L2', 'L3'],\n        ...     'node_from': ['NodeA', 'NodeB', 'NodeC'],\n        ...     'node_to': ['NodeB', 'NodeA', 'NodeA'],\n        ...     'capacity_mw': [1000, 800, 600]\n        ... })\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2192 ')\n        &gt;&gt;&gt; result = appender.append_combo_columns(lines_df)\n        &gt;&gt;&gt; # Result includes 'node_combo': ['NodeA \u2192 NodeB', 'NodeB \u2192 NodeA', 'NodeC \u2192 NodeA']\n    \"\"\"\n    return self._append_combo_columns(df_with_from_to_columns, 'directional')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/#mesqual.energy_data_handling.model_handling.membership_pairs_appender.BaseMembershipPairsAppender.append_sorted_combo_columns","title":"append_sorted_combo_columns","text":"<pre><code>append_sorted_combo_columns(df_with_from_to_columns: DataFrame) -&gt; DataFrame\n</code></pre> <p>Creates bidirectional combination columns with alphabetical ordering.</p> <p>Generates combination identifiers that treat relationships as bidirectional by sorting the paired values alphabetically. This is particularly useful for identifying unique connections regardless of direction, such as:</p> <ul> <li>Transmission line corridors (same physical connection)</li> <li>Regional trade partnerships (bidirectional trade agreements)</li> <li>Pipeline systems (flow can be reversed)</li> <li>Market coupling arrangements (mutual price influence)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>df_with_from_to_columns</code> <code>DataFrame</code> <p>DataFrame containing paired relationship columns                    with potential bidirectional connections</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Enhanced DataFrame with sorted combination columns added.</p> <code>DataFrame</code> <p>Bidirectional relationships receive identical identifiers.</p> <p>Examples:</p> <p>Bidirectional connection analysis:</p> <pre><code>&gt;&gt;&gt; # DataFrame with potentially bidirectional connections\n&gt;&gt;&gt; connections_df = pd.DataFrame({\n...     'connection_id': ['C1', 'C2', 'C3'],\n...     'region_from': ['DE', 'FR', 'DE'],\n...     'region_to': ['FR', 'DE', 'NL'],\n...     'trade_capacity': [2000, 2000, 1500]\n... })\n&gt;&gt;&gt; \n&gt;&gt;&gt; appender = StringMembershipPairsAppender(separator='-')\n&gt;&gt;&gt; result = appender.append_sorted_combo_columns(connections_df)\n&gt;&gt;&gt; # Result includes 'region_combo_sorted': ['DE-FR', 'DE-FR', 'DE-NL']\n&gt;&gt;&gt; # Note: 'DE\u2192FR' and 'FR\u2192DE' both become 'DE-FR'\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_pairs_appender.py</code> <pre><code>def append_sorted_combo_columns(\n        self,\n        df_with_from_to_columns: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Creates bidirectional combination columns with alphabetical ordering.\n\n    Generates combination identifiers that treat relationships as bidirectional\n    by sorting the paired values alphabetically. This is particularly useful for\n    identifying unique connections regardless of direction, such as:\n\n    - Transmission line corridors (same physical connection)\n    - Regional trade partnerships (bidirectional trade agreements)\n    - Pipeline systems (flow can be reversed)\n    - Market coupling arrangements (mutual price influence)\n\n    Args:\n        df_with_from_to_columns: DataFrame containing paired relationship columns\n                               with potential bidirectional connections\n\n    Returns:\n        Enhanced DataFrame with sorted combination columns added.\n        Bidirectional relationships receive identical identifiers.\n\n    Examples:\n        Bidirectional connection analysis:\n\n        &gt;&gt;&gt; # DataFrame with potentially bidirectional connections\n        &gt;&gt;&gt; connections_df = pd.DataFrame({\n        ...     'connection_id': ['C1', 'C2', 'C3'],\n        ...     'region_from': ['DE', 'FR', 'DE'],\n        ...     'region_to': ['FR', 'DE', 'NL'],\n        ...     'trade_capacity': [2000, 2000, 1500]\n        ... })\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator='-')\n        &gt;&gt;&gt; result = appender.append_sorted_combo_columns(connections_df)\n        &gt;&gt;&gt; # Result includes 'region_combo_sorted': ['DE-FR', 'DE-FR', 'DE-NL']\n        &gt;&gt;&gt; # Note: 'DE\u2192FR' and 'FR\u2192DE' both become 'DE-FR'\n    \"\"\"\n    return self._append_combo_columns(df_with_from_to_columns, 'sorted')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/#mesqual.energy_data_handling.model_handling.membership_pairs_appender.BaseMembershipPairsAppender.append_opposite_combo_columns","title":"append_opposite_combo_columns","text":"<pre><code>append_opposite_combo_columns(df_with_from_to_columns: DataFrame) -&gt; DataFrame\n</code></pre> <p>Creates reverse-direction combination columns for opposite flow analysis.</p> <p>Generates combination identifiers with reversed direction, enabling analysis of reverse flows, return paths, and bidirectional modeling scenarios. This is particularly valuable for:</p> <ul> <li>Reverse power flows in transmission networks</li> <li>Return commodity flows in pipeline systems</li> <li>Opposite direction trade flows</li> <li>Backup routing analysis</li> </ul> <p>The method swaps the from/to values before creating combinations, effectively creating the opposite directional identifier for each relationship.</p> <p>Parameters:</p> Name Type Description Default <code>df_with_from_to_columns</code> <code>DataFrame</code> <p>DataFrame containing directional relationships                    where reverse analysis is needed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Enhanced DataFrame with opposite-direction combination columns added.</p> <code>DataFrame</code> <p>Each relationship receives its reverse-direction identifier.</p> <p>Examples:</p> <p>Reverse flow analysis:</p> <pre><code>&gt;&gt;&gt; # DataFrame with primary flow directions\n&gt;&gt;&gt; flows_df = pd.DataFrame({\n...     'flow_id': ['F1', 'F2', 'F3'],\n...     'hub_from': ['HubA', 'HubB', 'HubC'],\n...     'hub_to': ['HubB', 'HubC', 'HubA'],\n...     'primary_flow': [100, 150, 80]\n... })\n&gt;&gt;&gt; \n&gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2190 ')\n&gt;&gt;&gt; result = appender.append_opposite_combo_columns(flows_df)\n&gt;&gt;&gt; # Result includes 'hub_combo_opposite': ['HubB \u2190 HubA', 'HubC \u2190 HubB', 'HubA \u2190 HubC']\n&gt;&gt;&gt; # Useful for modeling reverse flow scenarios\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_pairs_appender.py</code> <pre><code>def append_opposite_combo_columns(\n        self,\n        df_with_from_to_columns: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Creates reverse-direction combination columns for opposite flow analysis.\n\n    Generates combination identifiers with reversed direction, enabling analysis\n    of reverse flows, return paths, and bidirectional modeling scenarios. This\n    is particularly valuable for:\n\n    - Reverse power flows in transmission networks\n    - Return commodity flows in pipeline systems\n    - Opposite direction trade flows\n    - Backup routing analysis\n\n    The method swaps the from/to values before creating combinations, effectively\n    creating the opposite directional identifier for each relationship.\n\n    Args:\n        df_with_from_to_columns: DataFrame containing directional relationships\n                               where reverse analysis is needed\n\n    Returns:\n        Enhanced DataFrame with opposite-direction combination columns added.\n        Each relationship receives its reverse-direction identifier.\n\n    Examples:\n        Reverse flow analysis:\n\n        &gt;&gt;&gt; # DataFrame with primary flow directions\n        &gt;&gt;&gt; flows_df = pd.DataFrame({\n        ...     'flow_id': ['F1', 'F2', 'F3'],\n        ...     'hub_from': ['HubA', 'HubB', 'HubC'],\n        ...     'hub_to': ['HubB', 'HubC', 'HubA'],\n        ...     'primary_flow': [100, 150, 80]\n        ... })\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2190 ')\n        &gt;&gt;&gt; result = appender.append_opposite_combo_columns(flows_df)\n        &gt;&gt;&gt; # Result includes 'hub_combo_opposite': ['HubB \u2190 HubA', 'HubC \u2190 HubB', 'HubA \u2190 HubC']\n        &gt;&gt;&gt; # Useful for modeling reverse flow scenarios\n    \"\"\"\n    return self._append_combo_columns(df_with_from_to_columns, 'opposite')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/#mesqual.energy_data_handling.model_handling.membership_pairs_appender.StringMembershipPairsAppender","title":"StringMembershipPairsAppender","text":"<p>               Bases: <code>BaseMembershipPairsAppender</code></p> <p>String-based implementation for creating combination identifiers from energy system relationships.</p> <p>This concrete implementation creates human-readable string combinations from paired relationships using configurable separators. Particularly well-suited for:</p> <ul> <li>Data visualization and reporting (readable connection labels)</li> <li>User interface displays (network connection names)</li> <li>Export formats requiring string identifiers</li> <li>Debugging and data exploration</li> </ul> <p>The class inherits all combination strategies from the base class while implementing string-specific combination logic with customizable separators for different use cases.</p> <p>Parameters:</p> Name Type Description Default <code>separator</code> <code>str</code> <p>String used to join paired values in combinations. Defaults to ' - '.       Common patterns: ' \u2192 ' (directional), '-' (neutral), ' &lt;-&gt; ' (bidirectional)</p> <code>' - '</code> <code>**kwargs</code> <p>All arguments from BaseMembershipPairsAppender for column naming configuration</p> required <p>Examples:</p> <p>Energy system string combinations:</p> <pre><code>&gt;&gt;&gt; # Transmission line connections with directional separator\n&gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2192 ')\n&gt;&gt;&gt; lines_with_combos = appender.append_combo_columns(transmission_df)\n&gt;&gt;&gt; # Creates readable labels: 'NodeA \u2192 NodeB', 'NodeB \u2192 NodeC'\n</code></pre> <pre><code>&gt;&gt;&gt; # Regional trade connections with neutral separator\n&gt;&gt;&gt; trade_appender = StringMembershipPairsAppender(separator='-')\n&gt;&gt;&gt; trade_with_combos = trade_appender.append_sorted_combo_columns(trade_df)\n&gt;&gt;&gt; # Creates trade corridor labels: 'DE-FR', 'FR-NL'\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_pairs_appender.py</code> <pre><code>class StringMembershipPairsAppender(BaseMembershipPairsAppender):\n    \"\"\"String-based implementation for creating combination identifiers from energy system relationships.\n\n    This concrete implementation creates human-readable string combinations from paired\n    relationships using configurable separators. Particularly well-suited for:\n\n    - Data visualization and reporting (readable connection labels)\n    - User interface displays (network connection names)\n    - Export formats requiring string identifiers\n    - Debugging and data exploration\n\n    The class inherits all combination strategies from the base class while implementing\n    string-specific combination logic with customizable separators for different use cases.\n\n    Args:\n        separator: String used to join paired values in combinations. Defaults to ' - '.\n                  Common patterns: ' \u2192 ' (directional), '-' (neutral), ' &lt;-&gt; ' (bidirectional)\n        **kwargs: All arguments from BaseMembershipPairsAppender for column naming configuration\n\n    Examples:\n        Energy system string combinations:\n\n        &gt;&gt;&gt; # Transmission line connections with directional separator\n        &gt;&gt;&gt; appender = StringMembershipPairsAppender(separator=' \u2192 ')\n        &gt;&gt;&gt; lines_with_combos = appender.append_combo_columns(transmission_df)\n        &gt;&gt;&gt; # Creates readable labels: 'NodeA \u2192 NodeB', 'NodeB \u2192 NodeC'\n\n        &gt;&gt;&gt; # Regional trade connections with neutral separator\n        &gt;&gt;&gt; trade_appender = StringMembershipPairsAppender(separator='-')\n        &gt;&gt;&gt; trade_with_combos = trade_appender.append_sorted_combo_columns(trade_df)\n        &gt;&gt;&gt; # Creates trade corridor labels: 'DE-FR', 'FR-NL'\n    \"\"\"\n    def __init__(\n            self,\n            from_identifier: str = '_from',\n            to_identifier: str = '_to',\n            combo_col_suffix: str = '_combo',\n            combo_col_prefix: str = None,\n            sorted_combo_col_suffix: str = '_combo_sorted',\n            sorted_combo_col_prefix: str = None,\n            opposite_combo_col_suffix: str = '_combo_opposite',\n            opposite_combo_col_prefix: str = None,\n            separator: str = ' - ',\n    ):\n        \"\"\"Initialize the string-based membership pairs appender.\n\n        Args:\n            from_identifier: Suffix/prefix identifying source/origin columns. Defaults to '_from'.\n            to_identifier: Suffix/prefix identifying destination/target columns. Defaults to '_to'.\n            combo_col_suffix: Suffix for directional combination column names. Defaults to '_combo'.\n            combo_col_prefix: Prefix for directional combination column names. Defaults to None.\n            sorted_combo_col_suffix: Suffix for sorted combination column names. Defaults to '_combo_sorted'.\n            sorted_combo_col_prefix: Prefix for sorted combination column names. Defaults to None.\n            opposite_combo_col_suffix: Suffix for opposite combination column names. Defaults to '_combo_opposite'.\n            opposite_combo_col_prefix: Prefix for opposite combination column names. Defaults to None.\n            separator: String separator for joining paired values. Defaults to ' - '.\n                      Use ' \u2192 ' for directional flows, '-' for neutral connections,\n                      ' &lt;-&gt; ' for bidirectional relationships.\n        \"\"\"\n        super().__init__(\n            from_identifier=from_identifier,\n            to_identifier=to_identifier,\n            combo_col_suffix=combo_col_suffix,\n            combo_col_prefix=combo_col_prefix,\n            sorted_combo_col_suffix=sorted_combo_col_suffix,\n            sorted_combo_col_prefix=sorted_combo_col_prefix,\n            opposite_combo_col_suffix=opposite_combo_col_suffix,\n            opposite_combo_col_prefix=opposite_combo_col_prefix,\n        )\n        self._separator = separator\n\n    def _combine_values(self, a: pd.Series, b: pd.Series) -&gt; pd.Series:\n        \"\"\"Combines two series into directional string combinations.\n\n        Args:\n            a: Source/origin values (from column)\n            b: Destination/target values (to column)\n\n        Returns:\n            Series with string combinations preserving a\u2192b direction\n        \"\"\"\n        return a.astype(str) + self._separator + b.astype(str)\n\n    def _combine_sorted_values(self, a: pd.Series, b: pd.Series) -&gt; pd.Series:\n        \"\"\"Combines two series into bidirectional string combinations with alphabetical ordering.\n\n        Args:\n            a: First set of relationship values\n            b: Second set of relationship values\n\n        Returns:\n            Series with alphabetically sorted string combinations (bidirectional)\n        \"\"\"\n        if a.empty and b.empty:\n            return pd.Series()\n        return pd.DataFrame({\n            'a': a.astype(str),\n            'b': b.astype(str)\n        }).apply(lambda x: self._separator.join(sorted([x['a'], x['b']])), axis=1)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/#mesqual.energy_data_handling.model_handling.membership_pairs_appender.StringMembershipPairsAppender.__init__","title":"__init__","text":"<pre><code>__init__(from_identifier: str = '_from', to_identifier: str = '_to', combo_col_suffix: str = '_combo', combo_col_prefix: str = None, sorted_combo_col_suffix: str = '_combo_sorted', sorted_combo_col_prefix: str = None, opposite_combo_col_suffix: str = '_combo_opposite', opposite_combo_col_prefix: str = None, separator: str = ' - ')\n</code></pre> <p>Initialize the string-based membership pairs appender.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>str</code> <p>Suffix/prefix identifying source/origin columns. Defaults to '_from'.</p> <code>'_from'</code> <code>to_identifier</code> <code>str</code> <p>Suffix/prefix identifying destination/target columns. Defaults to '_to'.</p> <code>'_to'</code> <code>combo_col_suffix</code> <code>str</code> <p>Suffix for directional combination column names. Defaults to '_combo'.</p> <code>'_combo'</code> <code>combo_col_prefix</code> <code>str</code> <p>Prefix for directional combination column names. Defaults to None.</p> <code>None</code> <code>sorted_combo_col_suffix</code> <code>str</code> <p>Suffix for sorted combination column names. Defaults to '_combo_sorted'.</p> <code>'_combo_sorted'</code> <code>sorted_combo_col_prefix</code> <code>str</code> <p>Prefix for sorted combination column names. Defaults to None.</p> <code>None</code> <code>opposite_combo_col_suffix</code> <code>str</code> <p>Suffix for opposite combination column names. Defaults to '_combo_opposite'.</p> <code>'_combo_opposite'</code> <code>opposite_combo_col_prefix</code> <code>str</code> <p>Prefix for opposite combination column names. Defaults to None.</p> <code>None</code> <code>separator</code> <code>str</code> <p>String separator for joining paired values. Defaults to ' - '.       Use ' \u2192 ' for directional flows, '-' for neutral connections,       ' &lt;-&gt; ' for bidirectional relationships.</p> <code>' - '</code> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_pairs_appender.py</code> <pre><code>def __init__(\n        self,\n        from_identifier: str = '_from',\n        to_identifier: str = '_to',\n        combo_col_suffix: str = '_combo',\n        combo_col_prefix: str = None,\n        sorted_combo_col_suffix: str = '_combo_sorted',\n        sorted_combo_col_prefix: str = None,\n        opposite_combo_col_suffix: str = '_combo_opposite',\n        opposite_combo_col_prefix: str = None,\n        separator: str = ' - ',\n):\n    \"\"\"Initialize the string-based membership pairs appender.\n\n    Args:\n        from_identifier: Suffix/prefix identifying source/origin columns. Defaults to '_from'.\n        to_identifier: Suffix/prefix identifying destination/target columns. Defaults to '_to'.\n        combo_col_suffix: Suffix for directional combination column names. Defaults to '_combo'.\n        combo_col_prefix: Prefix for directional combination column names. Defaults to None.\n        sorted_combo_col_suffix: Suffix for sorted combination column names. Defaults to '_combo_sorted'.\n        sorted_combo_col_prefix: Prefix for sorted combination column names. Defaults to None.\n        opposite_combo_col_suffix: Suffix for opposite combination column names. Defaults to '_combo_opposite'.\n        opposite_combo_col_prefix: Prefix for opposite combination column names. Defaults to None.\n        separator: String separator for joining paired values. Defaults to ' - '.\n                  Use ' \u2192 ' for directional flows, '-' for neutral connections,\n                  ' &lt;-&gt; ' for bidirectional relationships.\n    \"\"\"\n    super().__init__(\n        from_identifier=from_identifier,\n        to_identifier=to_identifier,\n        combo_col_suffix=combo_col_suffix,\n        combo_col_prefix=combo_col_prefix,\n        sorted_combo_col_suffix=sorted_combo_col_suffix,\n        sorted_combo_col_prefix=sorted_combo_col_prefix,\n        opposite_combo_col_suffix=opposite_combo_col_suffix,\n        opposite_combo_col_prefix=opposite_combo_col_prefix,\n    )\n    self._separator = separator\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/pairs_appender/#mesqual.energy_data_handling.model_handling.membership_pairs_appender.TupleMembershipPairsAppender","title":"TupleMembershipPairsAppender","text":"<p>               Bases: <code>BaseMembershipPairsAppender</code></p> <p>Tuple-based implementation for creating combination identifiers from energy system relationships.</p> <p>This concrete implementation creates tuple combinations from paired relationships, offering several advantages for programmatic use:</p> <ul> <li>Memory efficiency (no string concatenation overhead)</li> <li>Fast equality comparisons and set operations</li> <li>Preservation of original data types</li> <li>Direct use as dictionary keys or index values</li> <li>Integration with pandas MultiIndex structures</li> </ul> <p>Particularly valuable for:</p> <ul> <li>High-performance energy system simulations</li> <li>Large-scale network analysis</li> <li>Optimization model formulations</li> <li>Internal data processing pipelines</li> </ul> <p>The tuple format maintains the exact relationship structure while enabling efficient programmatic manipulation of connection data.</p> <p>Examples:</p> <p>Energy system tuple combinations:</p> <pre><code>&gt;&gt;&gt; # Transmission network with tuple identifiers\n&gt;&gt;&gt; appender = TupleMembershipPairsAppender()\n&gt;&gt;&gt; lines_with_tuples = appender.append_combo_columns(transmission_df)\n&gt;&gt;&gt; # Creates tuple identifiers: ('NodeA', 'NodeB'), ('NodeB', 'NodeC')\n</code></pre> <pre><code>&gt;&gt;&gt; # Bidirectional connections for optimization\n&gt;&gt;&gt; lines_bidirectional = appender.append_sorted_combo_columns(transmission_df)\n&gt;&gt;&gt; # Creates sorted tuples: ('NodeA', 'NodeB'), ('NodeB', 'NodeC')\n&gt;&gt;&gt; # Useful as keys in optimization constraints\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_pairs_appender.py</code> <pre><code>class TupleMembershipPairsAppender(BaseMembershipPairsAppender):\n    \"\"\"Tuple-based implementation for creating combination identifiers from energy system relationships.\n\n    This concrete implementation creates tuple combinations from paired relationships,\n    offering several advantages for programmatic use:\n\n    - Memory efficiency (no string concatenation overhead)\n    - Fast equality comparisons and set operations\n    - Preservation of original data types\n    - Direct use as dictionary keys or index values\n    - Integration with pandas MultiIndex structures\n\n    Particularly valuable for:\n\n    - High-performance energy system simulations\n    - Large-scale network analysis\n    - Optimization model formulations\n    - Internal data processing pipelines\n\n    The tuple format maintains the exact relationship structure while enabling\n    efficient programmatic manipulation of connection data.\n\n    Examples:\n        Energy system tuple combinations:\n\n        &gt;&gt;&gt; # Transmission network with tuple identifiers\n        &gt;&gt;&gt; appender = TupleMembershipPairsAppender()\n        &gt;&gt;&gt; lines_with_tuples = appender.append_combo_columns(transmission_df)\n        &gt;&gt;&gt; # Creates tuple identifiers: ('NodeA', 'NodeB'), ('NodeB', 'NodeC')\n\n        &gt;&gt;&gt; # Bidirectional connections for optimization\n        &gt;&gt;&gt; lines_bidirectional = appender.append_sorted_combo_columns(transmission_df)\n        &gt;&gt;&gt; # Creates sorted tuples: ('NodeA', 'NodeB'), ('NodeB', 'NodeC')\n        &gt;&gt;&gt; # Useful as keys in optimization constraints\n    \"\"\"\n    def _combine_values(self, a: pd.Series, b: pd.Series) -&gt; pd.Series:\n        \"\"\"Combines two series into directional tuple combinations.\n\n        Args:\n            a: Source/origin values (from column)\n            b: Destination/target values (to column)\n\n        Returns:\n            Series with tuple combinations preserving (a, b) direction\n        \"\"\"\n        return pd.Series([tuple(x) for x in zip(a, b)], index=a.index)\n\n    def _combine_sorted_values(self, a: pd.Series, b: pd.Series) -&gt; pd.Series:\n        \"\"\"Combines two series into bidirectional tuple combinations with alphabetical ordering.\n\n        Args:\n            a: First set of relationship values\n            b: Second set of relationship values\n\n        Returns:\n            Series with alphabetically sorted tuple combinations (bidirectional)\n        \"\"\"\n        return pd.Series([tuple(sorted([x, y])) for x, y in zip(a, b)], index=a.index)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/","title":"MESQUAL Membership Property Enrichers","text":""},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mesqual.energy_data_handling.model_handling.membership_property_enrichers.MembershipTagging","title":"MembershipTagging","text":"<p>               Bases: <code>Enum</code></p> <p>Controls how enriched property names are tagged when added to target DataFrames.</p> <p>In energy system modeling, objects often have relationships to other model components (e.g., generators belong to nodes, lines connect nodes). When enriching a DataFrame with properties from related objects, this enum controls naming conventions to avoid column name conflicts and maintain clarity about property origins.</p> Values <ul> <li>NONE: Property names remain unchanged (may cause conflicts with existing columns)</li> <li>PREFIX: Property names get membership name as prefix (e.g., 'node_voltage' from 'voltage')</li> <li>SUFFIX: Property names get membership name as suffix (e.g., 'voltage_node' from 'voltage')</li> </ul> <p>Examples:</p> <p>For a generator DataFrame (target_df) with 'node' membership, enriching with node properties:</p> <pre><code>&gt;&gt;&gt; MembershipPropertyEnricher().append_properties(target_df, dataset, MembershipTagging.NONE)\n&gt;&gt;&gt; # Returns target_df with new columns ['voltage', 'load'] (original names from node DataFrame)\n&gt;&gt;&gt;\n&gt;&gt;&gt; MembershipPropertyEnricher().append_properties(target_df, dataset, MembershipTagging.PREFIX)\n&gt;&gt;&gt; # Returns target_df with new columns ['node_voltage', 'node_load']\n&gt;&gt;&gt;\n&gt;&gt;&gt; MembershipPropertyEnricher().append_properties(target_df, dataset, MembershipTagging.SUFFIX)\n&gt;&gt;&gt; # Returns target_df with new columns ['voltage_node', 'load_node']\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>class MembershipTagging(Enum):\n    \"\"\"\n    Controls how enriched property names are tagged when added to target DataFrames.\n\n    In energy system modeling, objects often have relationships to other model components\n    (e.g., generators belong to nodes, lines connect nodes). When enriching a DataFrame\n    with properties from related objects, this enum controls naming conventions to avoid\n    column name conflicts and maintain clarity about property origins.\n\n    Values:\n        - NONE: Property names remain unchanged (may cause conflicts with existing columns)\n        - PREFIX: Property names get membership name as prefix (e.g., 'node_voltage' from 'voltage')\n        - SUFFIX: Property names get membership name as suffix (e.g., 'voltage_node' from 'voltage')\n\n    Examples:\n        For a generator DataFrame (target_df) with 'node' membership, enriching with node properties:\n        &gt;&gt;&gt; MembershipPropertyEnricher().append_properties(target_df, dataset, MembershipTagging.NONE)\n        &gt;&gt;&gt; # Returns target_df with new columns ['voltage', 'load'] (original names from node DataFrame)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; MembershipPropertyEnricher().append_properties(target_df, dataset, MembershipTagging.PREFIX)\n        &gt;&gt;&gt; # Returns target_df with new columns ['node_voltage', 'node_load']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; MembershipPropertyEnricher().append_properties(target_df, dataset, MembershipTagging.SUFFIX)\n        &gt;&gt;&gt; # Returns target_df with new columns ['voltage_node', 'load_node']\n    \"\"\"\n    NONE = \"none\"\n    PREFIX = \"prefix\"\n    SUFFIX = \"suffix\"\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mesqual.energy_data_handling.model_handling.membership_property_enrichers.MembershipPropertyEnricher","title":"MembershipPropertyEnricher","text":"<p>Enriches energy system DataFrames with properties from related model objects.</p> <p>In energy system modeling, entities often have membership relationships to other model components. For example: - Generators belong to nodes and have fuel types - Lines connect between nodes - Storage units are located at nodes and have technology types</p> <p>This enricher automatically identifies membership columns in a target DataFrame and adds all properties from the corresponding model DataFrames. This enables comprehensive analysis by combining object properties with their relationships.</p> <p>Key Features: - Automatic identification of membership columns using MESQUAL's flag index system - Support for multiple simultaneous memberships (node, fuel_type, company, etc.) - Preservation of NaN memberships in enriched data - Configurable property naming to avoid column conflicts - Integration with MESQUAL Dataset architecture</p> <p>Parameters:</p> Name Type Description Default <code>membership_tag_separator</code> <code>str</code> <p>Separator used between membership name and property                       when PREFIX or SUFFIX tagging is applied</p> <code>'_'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; enricher = MembershipPropertyEnricher()\n&gt;&gt;&gt; # Generator DataFrame with 'node' column linking to node objects\n&gt;&gt;&gt; enriched_gen_df = enricher.append_properties(\n...     generator_df, dataset, MembershipTagging.PREFIX\n... )\n&gt;&gt;&gt; # enriched_gen_df now includes 'node_voltage', 'node_area' columns from node properties\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>class MembershipPropertyEnricher:\n    \"\"\"\n    Enriches energy system DataFrames with properties from related model objects.\n\n    In energy system modeling, entities often have membership relationships to other\n    model components. For example:\n    - Generators belong to nodes and have fuel types\n    - Lines connect between nodes\n    - Storage units are located at nodes and have technology types\n\n    This enricher automatically identifies membership columns in a target DataFrame\n    and adds all properties from the corresponding model DataFrames. This enables\n    comprehensive analysis by combining object properties with their relationships.\n\n    Key Features:\n    - Automatic identification of membership columns using MESQUAL's flag index system\n    - Support for multiple simultaneous memberships (node, fuel_type, company, etc.)\n    - Preservation of NaN memberships in enriched data\n    - Configurable property naming to avoid column conflicts\n    - Integration with MESQUAL Dataset architecture\n\n    Args:\n        membership_tag_separator: Separator used between membership name and property\n                                  when PREFIX or SUFFIX tagging is applied\n\n    Examples:\n        &gt;&gt;&gt; enricher = MembershipPropertyEnricher()\n        &gt;&gt;&gt; # Generator DataFrame with 'node' column linking to node objects\n        &gt;&gt;&gt; enriched_gen_df = enricher.append_properties(\n        ...     generator_df, dataset, MembershipTagging.PREFIX\n        ... )\n        &gt;&gt;&gt; # enriched_gen_df now includes 'node_voltage', 'node_area' columns from node properties\n    \"\"\"\n    def __init__(self, membership_tag_separator: str = '_'):\n        \"\"\"\n        Initialize the membership property enricher.\n\n        Args:\n            membership_tag_separator: Character(s) used to separate membership names\n                                     from property names in PREFIX/SUFFIX modes\n        \"\"\"\n        self._membership_tag_separator = membership_tag_separator\n\n    def identify_membership_columns(self, column_names: list[str], dataset: Dataset) -&gt; list[str]:\n        \"\"\"\n        Identifies columns that represent memberships to other model objects.\n\n        Uses MESQUAL's flag index system to determine which columns in the target\n        DataFrame represent relationships to other model components. This enables\n        automatic discovery of enrichment opportunities without manual specification.\n\n        Args:\n            column_names: List of column names from the target DataFrame\n            dataset: MESQUAL Dataset containing model definitions and flag mappings\n\n        Returns:\n            List of column names that represent memberships to other model objects\n\n        Examples:\n            For a generator DataFrame with columns ['name', 'capacity', 'node', 'fuel_type']:\n            &gt;&gt;&gt; membership_cols = enricher.identify_membership_columns(\n            ...     generator_df.columns, dataset\n            ... )\n            &gt;&gt;&gt; print(membership_cols)  # ['node', 'fuel_type']\n        \"\"\"\n        return [\n            col for col in column_names\n            if dataset.flag_index.column_name_in_model_describes_membership(col)\n        ]\n\n    def append_properties(\n            self,\n            target_df: pd.DataFrame,\n            dataset: Dataset,\n            membership_tagging: MembershipTagging = MembershipTagging.NONE\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Enriches target DataFrame with properties from all linked model objects.\n\n        Performs comprehensive enrichment by automatically identifying all membership\n        relationships and adding corresponding properties. This is the primary method\n        for energy system DataFrame enrichment, enabling complex multi-dimensional\n        analysis by combining object properties with their relationships.\n\n        The method preserves all original data while adding new property columns.\n        Missing relationships (NaN memberships) are handled gracefully by preserving\n        NaN values in the enriched properties.\n\n        Args:\n            target_df: DataFrame to enrich (e.g., generator, line, storage data)\n            dataset: MESQUAL Dataset containing linked model DataFrames with properties\n            membership_tagging: Strategy for naming enriched properties to avoid conflicts\n\n        Returns:\n            Enhanced DataFrame with all properties from linked model objects added.\n            Original columns are preserved, new columns added based on memberships.\n\n        Raises:\n            Warning: Logged when membership objects are missing from source DataFrames\n\n        Examples:\n            Energy system use cases:\n\n            &gt;&gt;&gt; # Enrich generator data with node and fuel properties\n            &gt;&gt;&gt; enriched_generators = enricher.append_properties(\n            ...     generators_df, dataset, MembershipTagging.PREFIX\n            ... )\n            &gt;&gt;&gt; # Result: original columns + 'node_voltage', 'node_area', 'fuel_co2_rate', etc.\n\n            &gt;&gt;&gt; # Enrich transmission data with node characteristics\n            &gt;&gt;&gt; enriched_lines = enricher.append_properties(\n            ...     transmission_df, dataset, MembershipTagging.SUFFIX\n            ... )\n            &gt;&gt;&gt; # Result: line properties + node properties with '_node' suffix\n        \"\"\"\n        membership_columns = self.identify_membership_columns(target_df.columns, dataset)\n        result_df = target_df.copy()\n\n        for column in membership_columns:\n            result_df = self.append_single_membership_properties(\n                result_df,\n                dataset,\n                column,\n                membership_tagging\n            )\n\n        return result_df\n\n    def append_single_membership_properties(\n            self,\n            target_df: pd.DataFrame,\n            dataset: Dataset,\n            membership_column: str,\n            membership_tagging: MembershipTagging = MembershipTagging.NONE\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Enriches target DataFrame with properties from a specific membership relationship.\n\n        This method provides fine-grained control over property enrichment by handling\n        a single membership column. Useful when custom logic is needed for specific\n        relationships or when processing memberships sequentially with different\n        tagging strategies.\n\n        The method uses MESQUAL's flag index to determine the source model DataFrame\n        for the membership column, then performs a left join to preserve all target\n        records while adding available properties.\n\n        Args:\n            target_df: DataFrame to enrich (must contain the membership column)\n            dataset: MESQUAL Dataset with access to linked model DataFrames\n            membership_column: Name of column containing object references (e.g., 'node', 'fuel_type')\n            membership_tagging: Strategy for naming enriched properties\n\n        Returns:\n            DataFrame with properties from the linked model objects added.\n            All original rows preserved; NaN memberships result in NaN properties.\n\n        Raises:\n            Warning: Logged when referenced objects are missing from the source DataFrame\n\n        Examples:\n            Targeted enrichment scenarios:\n\n            &gt;&gt;&gt; # Add only node properties to generators\n            &gt;&gt;&gt; gen_with_nodes = enricher.append_single_membership_properties(\n            ...     generators_df, dataset, 'node', MembershipTagging.PREFIX\n            ... )\n            &gt;&gt;&gt; # Result: generators + 'node_voltage', 'node_area', etc.\n\n            &gt;&gt;&gt; # Sequential enrichment with different tagging\n            &gt;&gt;&gt; result = generators_df.copy()\n            &gt;&gt;&gt; result = enricher.append_single_membership_properties(\n            ...     result, dataset, 'node', MembershipTagging.PREFIX\n            ... )\n            &gt;&gt;&gt; result = enricher.append_single_membership_properties(\n            ...     result, dataset, 'fuel_type', MembershipTagging.SUFFIX\n            ... )\n        \"\"\"\n        source_flag = dataset.flag_index.get_linked_model_flag_for_membership_column(membership_column)\n        source_df = dataset.fetch(source_flag)\n\n        membership_objects = target_df[membership_column].dropna().unique()\n        missing_objects = set(membership_objects) - set(source_df.index)\n\n        if missing_objects:\n            self._log_missing_objects_warning(missing_objects, membership_column)\n\n        source_properties = source_df.copy()\n\n        match membership_tagging:\n            case MembershipTagging.PREFIX:\n                source_properties = source_properties.add_prefix(f\"{membership_column}{self._membership_tag_separator}\")\n            case MembershipTagging.SUFFIX:\n                source_properties = source_properties.add_suffix(f\"{self._membership_tag_separator}{membership_column}\")\n\n        result_df = target_df.merge(\n            source_properties,\n            left_on=membership_column,\n            right_index=True,\n            how=\"left\"\n        )\n\n        return result_df\n\n    def _log_missing_objects_warning(\n            self,\n            missing_objects: set,\n            membership_column: str,\n            max_show: int = 5\n    ):\n        \"\"\"\n        Logs warning about missing objects in the source DataFrame.\n\n        In energy system modeling, missing references can indicate data quality\n        issues, model inconsistencies, or incomplete datasets. This method provides\n        informative warnings to help identify and resolve such issues.\n\n        Args:\n            missing_objects: Set of object identifiers missing from source DataFrame\n            membership_column: Name of the membership column being processed\n            max_show: Maximum number of missing objects to display in the warning\n        \"\"\"\n        num_missing = len(missing_objects)\n        warning_suffix = \", and more\" if num_missing &gt; max_show else \"\"\n        logger.warning(\n            f\"{num_missing} objects missing in source dataframe for {membership_column}: \"\n            f\"{list(missing_objects)[:max_show]}{warning_suffix}.\"\n        )\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mesqual.energy_data_handling.model_handling.membership_property_enrichers.MembershipPropertyEnricher.__init__","title":"__init__","text":"<pre><code>__init__(membership_tag_separator: str = '_')\n</code></pre> <p>Initialize the membership property enricher.</p> <p>Parameters:</p> Name Type Description Default <code>membership_tag_separator</code> <code>str</code> <p>Character(s) used to separate membership names                      from property names in PREFIX/SUFFIX modes</p> <code>'_'</code> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def __init__(self, membership_tag_separator: str = '_'):\n    \"\"\"\n    Initialize the membership property enricher.\n\n    Args:\n        membership_tag_separator: Character(s) used to separate membership names\n                                 from property names in PREFIX/SUFFIX modes\n    \"\"\"\n    self._membership_tag_separator = membership_tag_separator\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mesqual.energy_data_handling.model_handling.membership_property_enrichers.MembershipPropertyEnricher.identify_membership_columns","title":"identify_membership_columns","text":"<pre><code>identify_membership_columns(column_names: list[str], dataset: Dataset) -&gt; list[str]\n</code></pre> <p>Identifies columns that represent memberships to other model objects.</p> <p>Uses MESQUAL's flag index system to determine which columns in the target DataFrame represent relationships to other model components. This enables automatic discovery of enrichment opportunities without manual specification.</p> <p>Parameters:</p> Name Type Description Default <code>column_names</code> <code>list[str]</code> <p>List of column names from the target DataFrame</p> required <code>dataset</code> <code>Dataset</code> <p>MESQUAL Dataset containing model definitions and flag mappings</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of column names that represent memberships to other model objects</p> <p>Examples:</p> <p>For a generator DataFrame with columns ['name', 'capacity', 'node', 'fuel_type']:</p> <pre><code>&gt;&gt;&gt; membership_cols = enricher.identify_membership_columns(\n...     generator_df.columns, dataset\n... )\n&gt;&gt;&gt; print(membership_cols)  # ['node', 'fuel_type']\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def identify_membership_columns(self, column_names: list[str], dataset: Dataset) -&gt; list[str]:\n    \"\"\"\n    Identifies columns that represent memberships to other model objects.\n\n    Uses MESQUAL's flag index system to determine which columns in the target\n    DataFrame represent relationships to other model components. This enables\n    automatic discovery of enrichment opportunities without manual specification.\n\n    Args:\n        column_names: List of column names from the target DataFrame\n        dataset: MESQUAL Dataset containing model definitions and flag mappings\n\n    Returns:\n        List of column names that represent memberships to other model objects\n\n    Examples:\n        For a generator DataFrame with columns ['name', 'capacity', 'node', 'fuel_type']:\n        &gt;&gt;&gt; membership_cols = enricher.identify_membership_columns(\n        ...     generator_df.columns, dataset\n        ... )\n        &gt;&gt;&gt; print(membership_cols)  # ['node', 'fuel_type']\n    \"\"\"\n    return [\n        col for col in column_names\n        if dataset.flag_index.column_name_in_model_describes_membership(col)\n    ]\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mesqual.energy_data_handling.model_handling.membership_property_enrichers.MembershipPropertyEnricher.append_properties","title":"append_properties","text":"<pre><code>append_properties(target_df: DataFrame, dataset: Dataset, membership_tagging: MembershipTagging = NONE) -&gt; DataFrame\n</code></pre> <p>Enriches target DataFrame with properties from all linked model objects.</p> <p>Performs comprehensive enrichment by automatically identifying all membership relationships and adding corresponding properties. This is the primary method for energy system DataFrame enrichment, enabling complex multi-dimensional analysis by combining object properties with their relationships.</p> <p>The method preserves all original data while adding new property columns. Missing relationships (NaN memberships) are handled gracefully by preserving NaN values in the enriched properties.</p> <p>Parameters:</p> Name Type Description Default <code>target_df</code> <code>DataFrame</code> <p>DataFrame to enrich (e.g., generator, line, storage data)</p> required <code>dataset</code> <code>Dataset</code> <p>MESQUAL Dataset containing linked model DataFrames with properties</p> required <code>membership_tagging</code> <code>MembershipTagging</code> <p>Strategy for naming enriched properties to avoid conflicts</p> <code>NONE</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Enhanced DataFrame with all properties from linked model objects added.</p> <code>DataFrame</code> <p>Original columns are preserved, new columns added based on memberships.</p> <p>Raises:</p> Type Description <code>Warning</code> <p>Logged when membership objects are missing from source DataFrames</p> <p>Examples:</p> <p>Energy system use cases:</p> <pre><code>&gt;&gt;&gt; # Enrich generator data with node and fuel properties\n&gt;&gt;&gt; enriched_generators = enricher.append_properties(\n...     generators_df, dataset, MembershipTagging.PREFIX\n... )\n&gt;&gt;&gt; # Result: original columns + 'node_voltage', 'node_area', 'fuel_co2_rate', etc.\n</code></pre> <pre><code>&gt;&gt;&gt; # Enrich transmission data with node characteristics\n&gt;&gt;&gt; enriched_lines = enricher.append_properties(\n...     transmission_df, dataset, MembershipTagging.SUFFIX\n... )\n&gt;&gt;&gt; # Result: line properties + node properties with '_node' suffix\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def append_properties(\n        self,\n        target_df: pd.DataFrame,\n        dataset: Dataset,\n        membership_tagging: MembershipTagging = MembershipTagging.NONE\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Enriches target DataFrame with properties from all linked model objects.\n\n    Performs comprehensive enrichment by automatically identifying all membership\n    relationships and adding corresponding properties. This is the primary method\n    for energy system DataFrame enrichment, enabling complex multi-dimensional\n    analysis by combining object properties with their relationships.\n\n    The method preserves all original data while adding new property columns.\n    Missing relationships (NaN memberships) are handled gracefully by preserving\n    NaN values in the enriched properties.\n\n    Args:\n        target_df: DataFrame to enrich (e.g., generator, line, storage data)\n        dataset: MESQUAL Dataset containing linked model DataFrames with properties\n        membership_tagging: Strategy for naming enriched properties to avoid conflicts\n\n    Returns:\n        Enhanced DataFrame with all properties from linked model objects added.\n        Original columns are preserved, new columns added based on memberships.\n\n    Raises:\n        Warning: Logged when membership objects are missing from source DataFrames\n\n    Examples:\n        Energy system use cases:\n\n        &gt;&gt;&gt; # Enrich generator data with node and fuel properties\n        &gt;&gt;&gt; enriched_generators = enricher.append_properties(\n        ...     generators_df, dataset, MembershipTagging.PREFIX\n        ... )\n        &gt;&gt;&gt; # Result: original columns + 'node_voltage', 'node_area', 'fuel_co2_rate', etc.\n\n        &gt;&gt;&gt; # Enrich transmission data with node characteristics\n        &gt;&gt;&gt; enriched_lines = enricher.append_properties(\n        ...     transmission_df, dataset, MembershipTagging.SUFFIX\n        ... )\n        &gt;&gt;&gt; # Result: line properties + node properties with '_node' suffix\n    \"\"\"\n    membership_columns = self.identify_membership_columns(target_df.columns, dataset)\n    result_df = target_df.copy()\n\n    for column in membership_columns:\n        result_df = self.append_single_membership_properties(\n            result_df,\n            dataset,\n            column,\n            membership_tagging\n        )\n\n    return result_df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mesqual.energy_data_handling.model_handling.membership_property_enrichers.MembershipPropertyEnricher.append_single_membership_properties","title":"append_single_membership_properties","text":"<pre><code>append_single_membership_properties(target_df: DataFrame, dataset: Dataset, membership_column: str, membership_tagging: MembershipTagging = NONE) -&gt; DataFrame\n</code></pre> <p>Enriches target DataFrame with properties from a specific membership relationship.</p> <p>This method provides fine-grained control over property enrichment by handling a single membership column. Useful when custom logic is needed for specific relationships or when processing memberships sequentially with different tagging strategies.</p> <p>The method uses MESQUAL's flag index to determine the source model DataFrame for the membership column, then performs a left join to preserve all target records while adding available properties.</p> <p>Parameters:</p> Name Type Description Default <code>target_df</code> <code>DataFrame</code> <p>DataFrame to enrich (must contain the membership column)</p> required <code>dataset</code> <code>Dataset</code> <p>MESQUAL Dataset with access to linked model DataFrames</p> required <code>membership_column</code> <code>str</code> <p>Name of column containing object references (e.g., 'node', 'fuel_type')</p> required <code>membership_tagging</code> <code>MembershipTagging</code> <p>Strategy for naming enriched properties</p> <code>NONE</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with properties from the linked model objects added.</p> <code>DataFrame</code> <p>All original rows preserved; NaN memberships result in NaN properties.</p> <p>Raises:</p> Type Description <code>Warning</code> <p>Logged when referenced objects are missing from the source DataFrame</p> <p>Examples:</p> <p>Targeted enrichment scenarios:</p> <pre><code>&gt;&gt;&gt; # Add only node properties to generators\n&gt;&gt;&gt; gen_with_nodes = enricher.append_single_membership_properties(\n...     generators_df, dataset, 'node', MembershipTagging.PREFIX\n... )\n&gt;&gt;&gt; # Result: generators + 'node_voltage', 'node_area', etc.\n</code></pre> <pre><code>&gt;&gt;&gt; # Sequential enrichment with different tagging\n&gt;&gt;&gt; result = generators_df.copy()\n&gt;&gt;&gt; result = enricher.append_single_membership_properties(\n...     result, dataset, 'node', MembershipTagging.PREFIX\n... )\n&gt;&gt;&gt; result = enricher.append_single_membership_properties(\n...     result, dataset, 'fuel_type', MembershipTagging.SUFFIX\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def append_single_membership_properties(\n        self,\n        target_df: pd.DataFrame,\n        dataset: Dataset,\n        membership_column: str,\n        membership_tagging: MembershipTagging = MembershipTagging.NONE\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Enriches target DataFrame with properties from a specific membership relationship.\n\n    This method provides fine-grained control over property enrichment by handling\n    a single membership column. Useful when custom logic is needed for specific\n    relationships or when processing memberships sequentially with different\n    tagging strategies.\n\n    The method uses MESQUAL's flag index to determine the source model DataFrame\n    for the membership column, then performs a left join to preserve all target\n    records while adding available properties.\n\n    Args:\n        target_df: DataFrame to enrich (must contain the membership column)\n        dataset: MESQUAL Dataset with access to linked model DataFrames\n        membership_column: Name of column containing object references (e.g., 'node', 'fuel_type')\n        membership_tagging: Strategy for naming enriched properties\n\n    Returns:\n        DataFrame with properties from the linked model objects added.\n        All original rows preserved; NaN memberships result in NaN properties.\n\n    Raises:\n        Warning: Logged when referenced objects are missing from the source DataFrame\n\n    Examples:\n        Targeted enrichment scenarios:\n\n        &gt;&gt;&gt; # Add only node properties to generators\n        &gt;&gt;&gt; gen_with_nodes = enricher.append_single_membership_properties(\n        ...     generators_df, dataset, 'node', MembershipTagging.PREFIX\n        ... )\n        &gt;&gt;&gt; # Result: generators + 'node_voltage', 'node_area', etc.\n\n        &gt;&gt;&gt; # Sequential enrichment with different tagging\n        &gt;&gt;&gt; result = generators_df.copy()\n        &gt;&gt;&gt; result = enricher.append_single_membership_properties(\n        ...     result, dataset, 'node', MembershipTagging.PREFIX\n        ... )\n        &gt;&gt;&gt; result = enricher.append_single_membership_properties(\n        ...     result, dataset, 'fuel_type', MembershipTagging.SUFFIX\n        ... )\n    \"\"\"\n    source_flag = dataset.flag_index.get_linked_model_flag_for_membership_column(membership_column)\n    source_df = dataset.fetch(source_flag)\n\n    membership_objects = target_df[membership_column].dropna().unique()\n    missing_objects = set(membership_objects) - set(source_df.index)\n\n    if missing_objects:\n        self._log_missing_objects_warning(missing_objects, membership_column)\n\n    source_properties = source_df.copy()\n\n    match membership_tagging:\n        case MembershipTagging.PREFIX:\n            source_properties = source_properties.add_prefix(f\"{membership_column}{self._membership_tag_separator}\")\n        case MembershipTagging.SUFFIX:\n            source_properties = source_properties.add_suffix(f\"{self._membership_tag_separator}{membership_column}\")\n\n    result_df = target_df.merge(\n        source_properties,\n        left_on=membership_column,\n        right_index=True,\n        how=\"left\"\n    )\n\n    return result_df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mesqual.energy_data_handling.model_handling.membership_property_enrichers.DirectionalMembershipPropertyEnricher","title":"DirectionalMembershipPropertyEnricher","text":"<p>Enriches energy system DataFrames with properties for directional relationships.</p> <p>Energy networks inherently contain directional relationships - transmission lines connect from one node to another, flows have origins and destinations, and trade occurs between regions. This enricher handles such bidirectional memberships by identifying from/to column pairs and enriching with appropriate directional tags.</p> <p>Common energy system applications: - Transmission lines: 'node_from' and 'node_to' linking to node properties - Inter-regional flows: 'region_from' and 'region_to' for trade analysis - Pipeline systems: 'hub_from' and 'hub_to' for gas network modeling - Market connections: 'market_from' and 'market_to' for price analysis</p> <p>The enricher automatically identifies directional column pairs using configurable identifiers (default: '_from' and '_to') and adds properties from the linked model objects with appropriate directional suffixes.</p> Key Features <ul> <li>Automatic identification of from/to column pairs</li> <li>Flexible directional identifiers (customizable beyond '_from'/'_to')</li> <li>Support for multiple directional relationships in one DataFrame</li> <li>Preservation of NaN relationships</li> <li>Integration with MESQUAL's model flag system</li> </ul> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>str</code> <p>Suffix/prefix identifying 'from' direction (default: '_from')</p> <code>'_from'</code> <code>to_identifier</code> <code>str</code> <p>Suffix/prefix identifying 'to' direction (default: '_to')</p> <code>'_to'</code> <code>membership_tag_separator</code> <code>str</code> <p>Separator for property name construction</p> <code>'_'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; enricher = DirectionalMembershipPropertyEnricher()\n&gt;&gt;&gt; # Line DataFrame with 'node_from', 'node_to' columns\n&gt;&gt;&gt; enriched_lines = enricher.append_properties(\n...     line_df, dataset, MembershipTagging.NONE\n... )\n&gt;&gt;&gt; # Result includes node properties with '_from' and '_to' suffixes\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>class DirectionalMembershipPropertyEnricher:\n    \"\"\"\n    Enriches energy system DataFrames with properties for directional relationships.\n\n    Energy networks inherently contain directional relationships - transmission lines\n    connect from one node to another, flows have origins and destinations, and trade\n    occurs between regions. This enricher handles such bidirectional memberships by\n    identifying from/to column pairs and enriching with appropriate directional tags.\n\n    Common energy system applications:\n    - Transmission lines: 'node_from' and 'node_to' linking to node properties\n    - Inter-regional flows: 'region_from' and 'region_to' for trade analysis\n    - Pipeline systems: 'hub_from' and 'hub_to' for gas network modeling\n    - Market connections: 'market_from' and 'market_to' for price analysis\n\n    The enricher automatically identifies directional column pairs using configurable\n    identifiers (default: '_from' and '_to') and adds properties from the linked\n    model objects with appropriate directional suffixes.\n\n    Key Features:\n        - Automatic identification of from/to column pairs\n        - Flexible directional identifiers (customizable beyond '_from'/'_to')\n        - Support for multiple directional relationships in one DataFrame\n        - Preservation of NaN relationships\n        - Integration with MESQUAL's model flag system\n\n    Args:\n        from_identifier: Suffix/prefix identifying 'from' direction (default: '_from')\n        to_identifier: Suffix/prefix identifying 'to' direction (default: '_to')\n        membership_tag_separator: Separator for property name construction\n\n    Examples:\n\n        &gt;&gt;&gt; enricher = DirectionalMembershipPropertyEnricher()\n        &gt;&gt;&gt; # Line DataFrame with 'node_from', 'node_to' columns\n        &gt;&gt;&gt; enriched_lines = enricher.append_properties(\n        ...     line_df, dataset, MembershipTagging.NONE\n        ... )\n        &gt;&gt;&gt; # Result includes node properties with '_from' and '_to' suffixes\n    \"\"\"\n    def __init__(\n            self,\n            from_identifier: str = \"_from\",\n            to_identifier: str = \"_to\",\n            membership_tag_separator: str = '_',\n    ):\n        \"\"\"\n        Initialize the directional membership property enricher.\n\n        Args:\n            from_identifier: String identifying source/origin columns (e.g., '_from', 'source_')\n            to_identifier: String identifying destination/target columns (e.g., '_to', 'dest_')\n            membership_tag_separator: Character(s) separating membership names from properties\n        \"\"\"\n        self._from_identifier = from_identifier\n        self._to_identifier = to_identifier\n        self._membership_tag_separator = membership_tag_separator\n        self._tag_finder = CommonBaseKeyFinder(from_identifier, to_identifier)\n\n    def identify_from_to_columns(self, column_names: list[str], dataset: Dataset) -&gt; list[str]:\n        \"\"\"\n        Identifies base names for directional membership column pairs.\n\n        Analyzes column names to find base membership types that have both 'from'\n        and 'to' variants. For example, identifies 'node' as a base when both\n        'node_from' and 'node_to' columns exist and represent valid memberships.\n\n        Args:\n            column_names: List of column names from the target DataFrame\n            dataset: MESQUAL Dataset for membership validation\n\n        Returns:\n            List of base column names that have both from/to variants\n\n        Examples:\n            For a line DataFrame with ['name', 'capacity', 'node_from', 'node_to']:\n            &gt;&gt;&gt; base_columns = enricher.identify_from_to_columns(\n            ...     line_df.columns, dataset\n            ... )\n            &gt;&gt;&gt; print(base_columns)  # ['node']\n\n            For inter-regional trade with ['flow', 'region_from', 'region_to', 'market_from']:\n            &gt;&gt;&gt; base_columns = enricher.identify_from_to_columns(\n            ...     trade_df.columns, dataset  \n            ... )\n            &gt;&gt;&gt; print(base_columns)  # ['region'] (market missing 'market_to')\n        \"\"\"\n        potential_columns = self._tag_finder.get_keys_for_which_all_association_tags_appear(column_names)\n        return [\n            col for col in potential_columns\n            if dataset.flag_index.column_name_in_model_describes_membership(col)\n        ]\n\n    def append_properties(\n            self,\n            target_df: pd.DataFrame,\n            dataset: Dataset,\n            membership_tagging: MembershipTagging = MembershipTagging.NONE\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Enriches DataFrame with properties from all directional relationships.\n\n        Performs comprehensive directional enrichment by identifying all from/to\n        column pairs and adding properties from both directions. Essential for\n        network analysis where understanding characteristics of connected nodes,\n        regions, or components is crucial for energy system modeling.\n\n        Each directional relationship results in two sets of enriched properties:\n        one for the 'from' direction and one for the 'to' direction, clearly\n        distinguished by directional suffixes.\n\n        Args:\n            target_df: DataFrame with directional relationships (e.g., transmission lines)\n            dataset: MESQUAL Dataset containing model objects and their properties\n            membership_tagging: Strategy for property naming (applied before directional tags)\n\n        Returns:\n            Enhanced DataFrame with directional properties added. Original data preserved,\n            new columns follow pattern: [prefix_]property_name[_suffix]_direction\n\n        Raises:\n            Warning: Logged when referenced objects missing from source DataFrames\n\n        Examples:\n            Network transmission analysis:\n\n            &gt;&gt;&gt; # Transmission lines with node endpoints\n            &gt;&gt;&gt; enriched_lines = enricher.append_properties(\n            ...     transmission_df, dataset, MembershipTagging.NONE\n            ... )\n            &gt;&gt;&gt; # Result: original columns + 'voltage_from', 'voltage_to', \n            &gt;&gt;&gt; #         'area_from', 'area_to', etc.\n\n            &gt;&gt;&gt; # Inter-regional trade flows\n            &gt;&gt;&gt; enriched_trade = enricher.append_properties(\n            ...     trade_df, dataset, MembershipTagging.PREFIX\n            ... )\n            &gt;&gt;&gt; # Result: trade data + 'region_gdp_from', 'region_gdp_to', etc.\n        \"\"\"\n        membership_base_columns = self.identify_from_to_columns(target_df.columns, dataset)\n        result_df = target_df.copy()\n\n        for base_column in membership_base_columns:\n            result_df = self.append_directional_properties(\n                result_df,\n                dataset,\n                base_column,\n                membership_tagging\n            )\n\n        return result_df\n\n    def append_directional_properties(\n            self,\n            target_df: pd.DataFrame,\n            dataset: Dataset,\n            base_column: str,\n            membership_tagging: MembershipTagging = MembershipTagging.NONE\n    ) -&gt; pd.DataFrame:\n        source_flag = dataset.flag_index.get_linked_model_flag_for_membership_column(base_column)\n        source_df = dataset.fetch(source_flag)\n        return self.append_directional_properties_in_source_to_target_df(\n            target_df,\n            source_df,\n            base_column,\n            membership_tagging,\n        )\n\n    def append_directional_properties_in_source_to_target_df(\n            self,\n            target_df: pd.DataFrame,\n            source_df: pd.DataFrame,\n            base_column: str,\n            membership_tagging: MembershipTagging = MembershipTagging.NONE\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Enriches DataFrame with properties from a directional relationship.\n\n        Handles a single from/to membership pair by adding the corresponding model\n        DataFrame's properties with directional tags.\n\n        The method processes both directions (from/to) for the specified base column,\n        adding properties with appropriate directional suffixes. Missing references\n        are handled gracefully with NaN preservation.\n\n        Args:\n            target_df: DataFrame containing the directional columns\n            source_df: DataFrame containing the properties\n            base_column: Base membership name (e.g., 'node' for 'node_from'/'node_to')\n            membership_tagging: Property naming strategy (applied before directional tags)\n\n        Returns:\n            DataFrame with directional properties added for the specified relationship.\n            Properties follow naming pattern: [prefix_]property[_suffix]_direction\n\n        Raises:\n            Warning: Logged when referenced objects are missing from source DataFrame\n\n        Examples:\n            Targeted directional enrichment:\n\n            &gt;&gt;&gt; # Add only node properties to transmission lines\n            &gt;&gt;&gt; lines_with_nodes = enricher.append_directional_properties(\n            ...     line_df, node_df, 'node', MembershipTagging.NONE\n            ... )\n            &gt;&gt;&gt; # Result: lines + 'voltage_from', 'voltage_to', 'area_from', 'area_to'\n        \"\"\"\n        result_df = target_df.copy()\n        for tag in [self._from_identifier, self._to_identifier]:\n            membership_column = self._get_full_column_name(base_column, tag, target_df.columns)\n\n            if membership_column not in target_df.columns:\n                continue\n\n            membership_objects = target_df[membership_column].dropna().unique()\n            missing_objects = set(membership_objects) - set(source_df.index)\n\n            if missing_objects:\n                self._log_missing_objects_warning(missing_objects, membership_column)\n\n            source_properties = source_df.copy()\n\n            match membership_tagging:\n                case MembershipTagging.PREFIX:\n                    source_properties = source_properties.add_prefix(f\"{base_column}{self._membership_tag_separator}\")\n                case MembershipTagging.SUFFIX:\n                    source_properties = source_properties.add_suffix(f\"{self._membership_tag_separator}{base_column}\")\n\n            source_properties = source_properties.add_suffix(tag)\n\n            result_df = result_df.merge(\n                source_properties,\n                left_on=membership_column,\n                right_index=True,\n                how=\"left\"\n            )\n        return result_df\n\n    def _get_full_column_name(self, base_column: str, tag: str, df_columns: list[str]) -&gt; str:\n        \"\"\"\n        Determines the actual column name for a directional membership.\n\n        Handles flexibility in directional column naming by testing both\n        suffix and prefix patterns. Supports various naming conventions\n        used across different energy modeling platforms.\n\n        Args:\n            base_column: Base membership name (e.g., 'node')\n            tag: Directional identifier (e.g., '_from', '_to')\n            df_columns: List of actual column names in the DataFrame\n\n        Returns:\n            Actual column name found in the DataFrame\n\n        Examples:\n            &gt;&gt;&gt; # For base_column='node', tag='_from'\n            &gt;&gt;&gt; # Tests 'node_from' first, then 'from_node'\n            &gt;&gt;&gt; name = enricher._get_full_column_name('node', '_from', df.columns)\n        \"\"\"\n        test_suffix = f\"{base_column}{tag}\"\n        return test_suffix if test_suffix in df_columns else f\"{tag}{base_column}\"\n\n    def _log_missing_objects_warning(\n            self,\n            missing_objects: set,\n            membership_column: str,\n            max_show: int = 5\n    ):\n        num_missing = len(missing_objects)\n        warning_suffix = \", and more\" if num_missing &gt; max_show else \"\"\n        logger.warning(\n            f\"{num_missing} objects missing in source dataframe for {membership_column}: \"\n            f\"{list(missing_objects)[:max_show]}{warning_suffix}.\"\n        )\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mesqual.energy_data_handling.model_handling.membership_property_enrichers.DirectionalMembershipPropertyEnricher.__init__","title":"__init__","text":"<pre><code>__init__(from_identifier: str = '_from', to_identifier: str = '_to', membership_tag_separator: str = '_')\n</code></pre> <p>Initialize the directional membership property enricher.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>str</code> <p>String identifying source/origin columns (e.g., 'from', 'source')</p> <code>'_from'</code> <code>to_identifier</code> <code>str</code> <p>String identifying destination/target columns (e.g., 'to', 'dest')</p> <code>'_to'</code> <code>membership_tag_separator</code> <code>str</code> <p>Character(s) separating membership names from properties</p> <code>'_'</code> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def __init__(\n        self,\n        from_identifier: str = \"_from\",\n        to_identifier: str = \"_to\",\n        membership_tag_separator: str = '_',\n):\n    \"\"\"\n    Initialize the directional membership property enricher.\n\n    Args:\n        from_identifier: String identifying source/origin columns (e.g., '_from', 'source_')\n        to_identifier: String identifying destination/target columns (e.g., '_to', 'dest_')\n        membership_tag_separator: Character(s) separating membership names from properties\n    \"\"\"\n    self._from_identifier = from_identifier\n    self._to_identifier = to_identifier\n    self._membership_tag_separator = membership_tag_separator\n    self._tag_finder = CommonBaseKeyFinder(from_identifier, to_identifier)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mesqual.energy_data_handling.model_handling.membership_property_enrichers.DirectionalMembershipPropertyEnricher.identify_from_to_columns","title":"identify_from_to_columns","text":"<pre><code>identify_from_to_columns(column_names: list[str], dataset: Dataset) -&gt; list[str]\n</code></pre> <p>Identifies base names for directional membership column pairs.</p> <p>Analyzes column names to find base membership types that have both 'from' and 'to' variants. For example, identifies 'node' as a base when both 'node_from' and 'node_to' columns exist and represent valid memberships.</p> <p>Parameters:</p> Name Type Description Default <code>column_names</code> <code>list[str]</code> <p>List of column names from the target DataFrame</p> required <code>dataset</code> <code>Dataset</code> <p>MESQUAL Dataset for membership validation</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of base column names that have both from/to variants</p> <p>Examples:</p> <p>For a line DataFrame with ['name', 'capacity', 'node_from', 'node_to']:</p> <pre><code>&gt;&gt;&gt; base_columns = enricher.identify_from_to_columns(\n...     line_df.columns, dataset\n... )\n&gt;&gt;&gt; print(base_columns)  # ['node']\n</code></pre> <p>For inter-regional trade with ['flow', 'region_from', 'region_to', 'market_from']:</p> <pre><code>&gt;&gt;&gt; base_columns = enricher.identify_from_to_columns(\n...     trade_df.columns, dataset  \n... )\n&gt;&gt;&gt; print(base_columns)  # ['region'] (market missing 'market_to')\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def identify_from_to_columns(self, column_names: list[str], dataset: Dataset) -&gt; list[str]:\n    \"\"\"\n    Identifies base names for directional membership column pairs.\n\n    Analyzes column names to find base membership types that have both 'from'\n    and 'to' variants. For example, identifies 'node' as a base when both\n    'node_from' and 'node_to' columns exist and represent valid memberships.\n\n    Args:\n        column_names: List of column names from the target DataFrame\n        dataset: MESQUAL Dataset for membership validation\n\n    Returns:\n        List of base column names that have both from/to variants\n\n    Examples:\n        For a line DataFrame with ['name', 'capacity', 'node_from', 'node_to']:\n        &gt;&gt;&gt; base_columns = enricher.identify_from_to_columns(\n        ...     line_df.columns, dataset\n        ... )\n        &gt;&gt;&gt; print(base_columns)  # ['node']\n\n        For inter-regional trade with ['flow', 'region_from', 'region_to', 'market_from']:\n        &gt;&gt;&gt; base_columns = enricher.identify_from_to_columns(\n        ...     trade_df.columns, dataset  \n        ... )\n        &gt;&gt;&gt; print(base_columns)  # ['region'] (market missing 'market_to')\n    \"\"\"\n    potential_columns = self._tag_finder.get_keys_for_which_all_association_tags_appear(column_names)\n    return [\n        col for col in potential_columns\n        if dataset.flag_index.column_name_in_model_describes_membership(col)\n    ]\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mesqual.energy_data_handling.model_handling.membership_property_enrichers.DirectionalMembershipPropertyEnricher.append_properties","title":"append_properties","text":"<pre><code>append_properties(target_df: DataFrame, dataset: Dataset, membership_tagging: MembershipTagging = NONE) -&gt; DataFrame\n</code></pre> <p>Enriches DataFrame with properties from all directional relationships.</p> <p>Performs comprehensive directional enrichment by identifying all from/to column pairs and adding properties from both directions. Essential for network analysis where understanding characteristics of connected nodes, regions, or components is crucial for energy system modeling.</p> <p>Each directional relationship results in two sets of enriched properties: one for the 'from' direction and one for the 'to' direction, clearly distinguished by directional suffixes.</p> <p>Parameters:</p> Name Type Description Default <code>target_df</code> <code>DataFrame</code> <p>DataFrame with directional relationships (e.g., transmission lines)</p> required <code>dataset</code> <code>Dataset</code> <p>MESQUAL Dataset containing model objects and their properties</p> required <code>membership_tagging</code> <code>MembershipTagging</code> <p>Strategy for property naming (applied before directional tags)</p> <code>NONE</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Enhanced DataFrame with directional properties added. Original data preserved,</p> <code>DataFrame</code> <p>new columns follow pattern: [prefix_]property_name[_suffix]_direction</p> <p>Raises:</p> Type Description <code>Warning</code> <p>Logged when referenced objects missing from source DataFrames</p> <p>Examples:</p> <p>Network transmission analysis:</p> <pre><code>&gt;&gt;&gt; # Transmission lines with node endpoints\n&gt;&gt;&gt; enriched_lines = enricher.append_properties(\n...     transmission_df, dataset, MembershipTagging.NONE\n... )\n&gt;&gt;&gt; # Result: original columns + 'voltage_from', 'voltage_to', \n&gt;&gt;&gt; #         'area_from', 'area_to', etc.\n</code></pre> <pre><code>&gt;&gt;&gt; # Inter-regional trade flows\n&gt;&gt;&gt; enriched_trade = enricher.append_properties(\n...     trade_df, dataset, MembershipTagging.PREFIX\n... )\n&gt;&gt;&gt; # Result: trade data + 'region_gdp_from', 'region_gdp_to', etc.\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def append_properties(\n        self,\n        target_df: pd.DataFrame,\n        dataset: Dataset,\n        membership_tagging: MembershipTagging = MembershipTagging.NONE\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Enriches DataFrame with properties from all directional relationships.\n\n    Performs comprehensive directional enrichment by identifying all from/to\n    column pairs and adding properties from both directions. Essential for\n    network analysis where understanding characteristics of connected nodes,\n    regions, or components is crucial for energy system modeling.\n\n    Each directional relationship results in two sets of enriched properties:\n    one for the 'from' direction and one for the 'to' direction, clearly\n    distinguished by directional suffixes.\n\n    Args:\n        target_df: DataFrame with directional relationships (e.g., transmission lines)\n        dataset: MESQUAL Dataset containing model objects and their properties\n        membership_tagging: Strategy for property naming (applied before directional tags)\n\n    Returns:\n        Enhanced DataFrame with directional properties added. Original data preserved,\n        new columns follow pattern: [prefix_]property_name[_suffix]_direction\n\n    Raises:\n        Warning: Logged when referenced objects missing from source DataFrames\n\n    Examples:\n        Network transmission analysis:\n\n        &gt;&gt;&gt; # Transmission lines with node endpoints\n        &gt;&gt;&gt; enriched_lines = enricher.append_properties(\n        ...     transmission_df, dataset, MembershipTagging.NONE\n        ... )\n        &gt;&gt;&gt; # Result: original columns + 'voltage_from', 'voltage_to', \n        &gt;&gt;&gt; #         'area_from', 'area_to', etc.\n\n        &gt;&gt;&gt; # Inter-regional trade flows\n        &gt;&gt;&gt; enriched_trade = enricher.append_properties(\n        ...     trade_df, dataset, MembershipTagging.PREFIX\n        ... )\n        &gt;&gt;&gt; # Result: trade data + 'region_gdp_from', 'region_gdp_to', etc.\n    \"\"\"\n    membership_base_columns = self.identify_from_to_columns(target_df.columns, dataset)\n    result_df = target_df.copy()\n\n    for base_column in membership_base_columns:\n        result_df = self.append_directional_properties(\n            result_df,\n            dataset,\n            base_column,\n            membership_tagging\n        )\n\n    return result_df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/model_handling/property_enricher/#mesqual.energy_data_handling.model_handling.membership_property_enrichers.DirectionalMembershipPropertyEnricher.append_directional_properties_in_source_to_target_df","title":"append_directional_properties_in_source_to_target_df","text":"<pre><code>append_directional_properties_in_source_to_target_df(target_df: DataFrame, source_df: DataFrame, base_column: str, membership_tagging: MembershipTagging = NONE) -&gt; DataFrame\n</code></pre> <p>Enriches DataFrame with properties from a directional relationship.</p> <p>Handles a single from/to membership pair by adding the corresponding model DataFrame's properties with directional tags.</p> <p>The method processes both directions (from/to) for the specified base column, adding properties with appropriate directional suffixes. Missing references are handled gracefully with NaN preservation.</p> <p>Parameters:</p> Name Type Description Default <code>target_df</code> <code>DataFrame</code> <p>DataFrame containing the directional columns</p> required <code>source_df</code> <code>DataFrame</code> <p>DataFrame containing the properties</p> required <code>base_column</code> <code>str</code> <p>Base membership name (e.g., 'node' for 'node_from'/'node_to')</p> required <code>membership_tagging</code> <code>MembershipTagging</code> <p>Property naming strategy (applied before directional tags)</p> <code>NONE</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with directional properties added for the specified relationship.</p> <code>DataFrame</code> <p>Properties follow naming pattern: [prefix_]property[_suffix]_direction</p> <p>Raises:</p> Type Description <code>Warning</code> <p>Logged when referenced objects are missing from source DataFrame</p> <p>Examples:</p> <p>Targeted directional enrichment:</p> <pre><code>&gt;&gt;&gt; # Add only node properties to transmission lines\n&gt;&gt;&gt; lines_with_nodes = enricher.append_directional_properties(\n...     line_df, node_df, 'node', MembershipTagging.NONE\n... )\n&gt;&gt;&gt; # Result: lines + 'voltage_from', 'voltage_to', 'area_from', 'area_to'\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/model_handling/membership_property_enrichers.py</code> <pre><code>def append_directional_properties_in_source_to_target_df(\n        self,\n        target_df: pd.DataFrame,\n        source_df: pd.DataFrame,\n        base_column: str,\n        membership_tagging: MembershipTagging = MembershipTagging.NONE\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Enriches DataFrame with properties from a directional relationship.\n\n    Handles a single from/to membership pair by adding the corresponding model\n    DataFrame's properties with directional tags.\n\n    The method processes both directions (from/to) for the specified base column,\n    adding properties with appropriate directional suffixes. Missing references\n    are handled gracefully with NaN preservation.\n\n    Args:\n        target_df: DataFrame containing the directional columns\n        source_df: DataFrame containing the properties\n        base_column: Base membership name (e.g., 'node' for 'node_from'/'node_to')\n        membership_tagging: Property naming strategy (applied before directional tags)\n\n    Returns:\n        DataFrame with directional properties added for the specified relationship.\n        Properties follow naming pattern: [prefix_]property[_suffix]_direction\n\n    Raises:\n        Warning: Logged when referenced objects are missing from source DataFrame\n\n    Examples:\n        Targeted directional enrichment:\n\n        &gt;&gt;&gt; # Add only node properties to transmission lines\n        &gt;&gt;&gt; lines_with_nodes = enricher.append_directional_properties(\n        ...     line_df, node_df, 'node', MembershipTagging.NONE\n        ... )\n        &gt;&gt;&gt; # Result: lines + 'voltage_from', 'voltage_to', 'area_from', 'area_to'\n    \"\"\"\n    result_df = target_df.copy()\n    for tag in [self._from_identifier, self._to_identifier]:\n        membership_column = self._get_full_column_name(base_column, tag, target_df.columns)\n\n        if membership_column not in target_df.columns:\n            continue\n\n        membership_objects = target_df[membership_column].dropna().unique()\n        missing_objects = set(membership_objects) - set(source_df.index)\n\n        if missing_objects:\n            self._log_missing_objects_warning(missing_objects, membership_column)\n\n        source_properties = source_df.copy()\n\n        match membership_tagging:\n            case MembershipTagging.PREFIX:\n                source_properties = source_properties.add_prefix(f\"{base_column}{self._membership_tag_separator}\")\n            case MembershipTagging.SUFFIX:\n                source_properties = source_properties.add_suffix(f\"{self._membership_tag_separator}{base_column}\")\n\n        source_properties = source_properties.add_suffix(tag)\n\n        result_df = result_df.merge(\n            source_properties,\n            left_on=membership_column,\n            right_index=True,\n            how=\"left\"\n        )\n    return result_df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/","title":"MESQUAL Energy Variable Utils","text":""},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/#mesqual.energy_data_handling.variable_utils","title":"variable_utils","text":"<p>MESQUAL Energy Data Variable Utilities</p> <p>This package provides specialized utilities for processing and transforming energy system variables in MESQUAL (Modular Energy Scenario Comparison Analysis Library). These utilities handle common operations on energy data including flow aggregations, price calculations, congestion analysis, and bidirectional data processing.</p> Key Components <ul> <li>RegionalTradeBalanceCalculator: Aggregates bidirectional power flows between regions</li> <li>CongestionRentCalculator: Calculates congestion rents for transmission lines</li> <li>AggregatedColumnAppender: Aggregates columns by common identifiers (e.g., technology types)</li> <li>UpDownNetAppender: Processes bidirectional data to create net and total columns</li> </ul>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/","title":"MESQUAL Congestion Rent Calculator","text":""},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mesqual.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator","title":"CongestionRentCalculator  <code>dataclass</code>","text":"<p>Calculates congestion rents for electricity transmission lines.</p> <p>Congestion rent represents the economic value captured by transmission assets due to price differences between nodes. It's calculated as the product of power flow, price spread, and time granularity.</p> <p>The calculator handles bidirectional flows (up/down) and accounts for transmission losses by using separate sent and received quantities. This provides accurate congestion rent calculations that reflect actual market conditions and physical constraints.</p> <p>Mathematical formulation: - Congestion rent (up) = granularity \u00d7 (received_up \u00d7 price_to - sent_up \u00d7 price_from) - Congestion rent (down) = granularity \u00d7 (received_down \u00d7 price_from - sent_down \u00d7 price_to) - Total congestion rent = congestion_rent_up + congestion_rent_down</p> <p>Attributes:</p> Name Type Description <code>sent_up</code> <code>Series</code> <p>Power sent in up direction (MW or MWh)</p> <code>received_up</code> <code>Series</code> <p>Power received in up direction after losses (MW or MWh)  </p> <code>sent_down</code> <code>Series</code> <p>Power sent in down direction (MW or MWh)</p> <code>received_down</code> <code>Series</code> <p>Power received in down direction after losses (MW or MWh)</p> <code>price_node_from</code> <code>Series</code> <p>Price at sending node (\u20ac/MWh)</p> <code>price_node_to</code> <code>Series</code> <p>Price at receiving node (\u20ac/MWh)</p> <code>granularity_hrs</code> <code>Series | float</code> <p>Time granularity in hours (auto-detected if None)</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # Time series data\n&gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=3, freq='h')\n&gt;&gt;&gt; # Flow and price data\n&gt;&gt;&gt; calc = CongestionRentCalculator(\n...     sent_up=pd.Series([100, 150, 200], index=index),\n...     received_up=pd.Series([95, 142, 190], index=index),  # 5% losses\n...     sent_down=pd.Series([50, 75, 100], index=index),\n...     received_down=pd.Series([48, 71, 95], index=index),  # 4% losses\n...     price_node_from=pd.Series([45, 50, 55], index=index),\n...     price_node_to=pd.Series([65, 70, 75], index=index)\n... )\n&gt;&gt;&gt; total_rent = calc.calculate()\n&gt;&gt;&gt; print(total_rent)  # Congestion rent in \u20ac\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/variable_utils/congestion_rent.py</code> <pre><code>@dataclass\nclass CongestionRentCalculator:\n    \"\"\"Calculates congestion rents for electricity transmission lines.\n\n    Congestion rent represents the economic value captured by transmission\n    assets due to price differences between nodes. It's calculated as the\n    product of power flow, price spread, and time granularity.\n\n    The calculator handles bidirectional flows (up/down) and accounts for\n    transmission losses by using separate sent and received quantities.\n    This provides accurate congestion rent calculations that reflect actual\n    market conditions and physical constraints.\n\n    Mathematical formulation:\n    - Congestion rent (up) = granularity \u00d7 (received_up \u00d7 price_to - sent_up \u00d7 price_from)\n    - Congestion rent (down) = granularity \u00d7 (received_down \u00d7 price_from - sent_down \u00d7 price_to)\n    - Total congestion rent = congestion_rent_up + congestion_rent_down\n\n    Attributes:\n        sent_up: Power sent in up direction (MW or MWh)\n        received_up: Power received in up direction after losses (MW or MWh)  \n        sent_down: Power sent in down direction (MW or MWh)\n        received_down: Power received in down direction after losses (MW or MWh)\n        price_node_from: Price at sending node (\u20ac/MWh)\n        price_node_to: Price at receiving node (\u20ac/MWh)\n        granularity_hrs: Time granularity in hours (auto-detected if None)\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; # Time series data\n        &gt;&gt;&gt; index = pd.date_range('2024-01-01', periods=3, freq='h')\n        &gt;&gt;&gt; # Flow and price data\n        &gt;&gt;&gt; calc = CongestionRentCalculator(\n        ...     sent_up=pd.Series([100, 150, 200], index=index),\n        ...     received_up=pd.Series([95, 142, 190], index=index),  # 5% losses\n        ...     sent_down=pd.Series([50, 75, 100], index=index),\n        ...     received_down=pd.Series([48, 71, 95], index=index),  # 4% losses\n        ...     price_node_from=pd.Series([45, 50, 55], index=index),\n        ...     price_node_to=pd.Series([65, 70, 75], index=index)\n        ... )\n        &gt;&gt;&gt; total_rent = calc.calculate()\n        &gt;&gt;&gt; print(total_rent)  # Congestion rent in \u20ac\n    \"\"\"\n    sent_up: pd.Series\n    received_up: pd.Series\n    sent_down: pd.Series\n    received_down: pd.Series\n    price_node_from: pd.Series\n    price_node_to: pd.Series\n    granularity_hrs: pd.Series | float = None\n\n    def __post_init__(self):\n        \"\"\"Initialize granularity and validate input consistency.\"\"\"\n        if self.granularity_hrs is None:\n            if isinstance(self.sent_up.index, pd.DatetimeIndex):\n                if len(self.sent_up.index) &gt; 0:\n                    from mesqual.energy_data_handling.granularity_analyzer import TimeSeriesGranularityAnalyzer\n                    analyzer = TimeSeriesGranularityAnalyzer(strict_mode=False)\n                    self.granularity_hrs = analyzer.get_granularity_as_series_of_hours(self.sent_up.index)\n                else:\n                    self.granularity_hrs = 0\n            else:\n                logger.warning(f'Granularity for CongestionRentCalculator is defaulting back to 1 hrs.')\n                self.granularity_hrs = 1\n        if isinstance(self.granularity_hrs, (float, int)):\n            self.granularity_hrs = pd.Series(self.granularity_hrs, index=self.sent_up.index)\n        self.__check_indices()\n\n    def __check_indices(self):\n        \"\"\"Validate that all input Series have matching indices.\n\n        Raises:\n            ValueError: If any Series has mismatched index with sent_up\n        \"\"\"\n        ref_index = self.sent_up.index\n        to_check = [\n            self.received_up,\n            self.sent_down,\n            self.received_down,\n            self.price_node_from,\n            self.price_node_to\n        ]\n        for v in to_check:\n            if not ref_index.equals(v.index):\n                raise ValueError(f'All indices of provided series must be equal.')\n\n    @property\n    def congestion_rent_up(self) -&gt; pd.Series:\n        \"\"\"Calculate congestion rent for up direction flows.\n\n        Returns:\n            Series with congestion rents in up direction (\u20ac)\n        \"\"\"\n        return self.granularity_hrs * (\n                self.received_up * self.price_node_to -\n                self.sent_up * self.price_node_from\n        )\n\n    @property\n    def congestion_rent_down(self) -&gt; pd.Series:\n        \"\"\"Calculate congestion rent for down direction flows.\n\n        Returns:\n            Series with congestion rents in down direction (\u20ac)\n        \"\"\"\n        return self.granularity_hrs * (\n                self.received_down * self.price_node_from -\n                self.sent_down * self.price_node_to\n        )\n\n    @property\n    def congestion_rent_total(self) -&gt; pd.Series:\n        \"\"\"Calculate total congestion rent (sum of up and down directions).\n\n        Returns:\n            Series with total congestion rents (\u20ac)\n        \"\"\"\n        return self.congestion_rent_up + self.congestion_rent_down\n\n    def calculate(self) -&gt; pd.Series:\n        \"\"\"Calculate total congestion rent (convenience method).\n\n        Returns:\n            Series with total congestion rents in \u20ac (same as congestion_rent_total property)\n        \"\"\"\n        return self.congestion_rent_total\n\n    @classmethod\n    def from_net_flow_without_losses(\n            cls,\n            net_flow: pd.Series,\n            price_node_from: pd.Series,\n            price_node_to: pd.Series,\n            granularity_hrs: float = None\n    ) -&gt; pd.Series:\n        \"\"\"Calculate congestion rent from net flow data assuming no losses.\n\n        Convenience method for cases where transmission losses are negligible\n        or not available. Splits net flow into unidirectional components and\n        assumes sent equals received for each direction.\n\n        Args:\n            net_flow: Net power flow (positive = up direction, negative = down direction) in MW or MWh\n            price_node_from: Price at sending node (\u20ac/MWh)\n            price_node_to: Price at receiving node (\u20ac/MWh)\n            granularity_hrs: Time granularity in hours (auto-detected if None)\n\n        Returns:\n            Series with total congestion rents in \u20ac\n\n        Example:\n\n            &gt;&gt;&gt; # Net flow with price data\n            &gt;&gt;&gt; net_flow = pd.Series([100, -50, 75], index=time_index)  \n            &gt;&gt;&gt; rent = CongestionRentCalculator.from_net_flow_without_losses(\n            ...     net_flow, price_from, price_to\n            ... )\n        \"\"\"\n        sent_up = net_flow.clip(lower=0)\n        sent_down = (-net_flow).clip(lower=0)\n        return cls(\n            sent_up=sent_up,\n            received_up=sent_up,\n            sent_down=sent_down,\n            received_down=sent_down,\n            price_node_from=price_node_from,\n            price_node_to=price_node_to,\n            granularity_hrs=granularity_hrs\n        ).calculate()\n\n    @classmethod\n    def from_up_and_down_flow_without_losses(\n            cls,\n            flow_up: pd.Series,\n            flow_down: pd.Series,\n            price_node_from: pd.Series,\n            price_node_to: pd.Series,\n            granularity_hrs: float = None\n    ) -&gt; pd.Series:\n        \"\"\"Calculate congestion rent from bidirectional flow data assuming no losses.\n\n        Convenience method for cases where flows are already separated into up/down\n        directions and transmission losses are negligible. Assumes sent equals\n        received for each direction.\n\n        Args:\n            flow_up: Power flow in up direction (MW or MWh, non-negative)\n            flow_down: Power flow in down direction (MW or MWh, non-negative)  \n            price_node_from: Price at sending node (\u20ac/MWh)\n            price_node_to: Price at receiving node (\u20ac/MWh)\n            granularity_hrs: Time granularity in hours (auto-detected if None)\n\n        Returns:\n            Series with total congestion rents in \u20ac\n\n        Example:\n\n            &gt;&gt;&gt; # Separate up/down flows\n            &gt;&gt;&gt; flow_up = pd.Series([100, 0, 75], index=time_index)\n            &gt;&gt;&gt; flow_down = pd.Series([0, 50, 0], index=time_index)\n            &gt;&gt;&gt; rent = CongestionRentCalculator.from_up_and_down_flow_without_losses(\n            ...     flow_up, flow_down, price_from, price_to\n            ... )\n        \"\"\"\n        return cls(\n            sent_up=flow_up,\n            received_up=flow_up,\n            sent_down=flow_down,\n            received_down=flow_down,\n            price_node_from=price_node_from,\n            price_node_to=price_node_to,\n            granularity_hrs=granularity_hrs\n        ).calculate()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mesqual.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.congestion_rent_up","title":"congestion_rent_up  <code>property</code>","text":"<pre><code>congestion_rent_up: Series\n</code></pre> <p>Calculate congestion rent for up direction flows.</p> <p>Returns:</p> Type Description <code>Series</code> <p>Series with congestion rents in up direction (\u20ac)</p>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mesqual.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.congestion_rent_down","title":"congestion_rent_down  <code>property</code>","text":"<pre><code>congestion_rent_down: Series\n</code></pre> <p>Calculate congestion rent for down direction flows.</p> <p>Returns:</p> Type Description <code>Series</code> <p>Series with congestion rents in down direction (\u20ac)</p>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mesqual.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.congestion_rent_total","title":"congestion_rent_total  <code>property</code>","text":"<pre><code>congestion_rent_total: Series\n</code></pre> <p>Calculate total congestion rent (sum of up and down directions).</p> <p>Returns:</p> Type Description <code>Series</code> <p>Series with total congestion rents (\u20ac)</p>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mesqual.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Initialize granularity and validate input consistency.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/variable_utils/congestion_rent.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize granularity and validate input consistency.\"\"\"\n    if self.granularity_hrs is None:\n        if isinstance(self.sent_up.index, pd.DatetimeIndex):\n            if len(self.sent_up.index) &gt; 0:\n                from mesqual.energy_data_handling.granularity_analyzer import TimeSeriesGranularityAnalyzer\n                analyzer = TimeSeriesGranularityAnalyzer(strict_mode=False)\n                self.granularity_hrs = analyzer.get_granularity_as_series_of_hours(self.sent_up.index)\n            else:\n                self.granularity_hrs = 0\n        else:\n            logger.warning(f'Granularity for CongestionRentCalculator is defaulting back to 1 hrs.')\n            self.granularity_hrs = 1\n    if isinstance(self.granularity_hrs, (float, int)):\n        self.granularity_hrs = pd.Series(self.granularity_hrs, index=self.sent_up.index)\n    self.__check_indices()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mesqual.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.__check_indices","title":"__check_indices","text":"<pre><code>__check_indices()\n</code></pre> <p>Validate that all input Series have matching indices.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any Series has mismatched index with sent_up</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/variable_utils/congestion_rent.py</code> <pre><code>def __check_indices(self):\n    \"\"\"Validate that all input Series have matching indices.\n\n    Raises:\n        ValueError: If any Series has mismatched index with sent_up\n    \"\"\"\n    ref_index = self.sent_up.index\n    to_check = [\n        self.received_up,\n        self.sent_down,\n        self.received_down,\n        self.price_node_from,\n        self.price_node_to\n    ]\n    for v in to_check:\n        if not ref_index.equals(v.index):\n            raise ValueError(f'All indices of provided series must be equal.')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mesqual.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.calculate","title":"calculate","text":"<pre><code>calculate() -&gt; Series\n</code></pre> <p>Calculate total congestion rent (convenience method).</p> <p>Returns:</p> Type Description <code>Series</code> <p>Series with total congestion rents in \u20ac (same as congestion_rent_total property)</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/variable_utils/congestion_rent.py</code> <pre><code>def calculate(self) -&gt; pd.Series:\n    \"\"\"Calculate total congestion rent (convenience method).\n\n    Returns:\n        Series with total congestion rents in \u20ac (same as congestion_rent_total property)\n    \"\"\"\n    return self.congestion_rent_total\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mesqual.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.from_net_flow_without_losses","title":"from_net_flow_without_losses  <code>classmethod</code>","text":"<pre><code>from_net_flow_without_losses(net_flow: Series, price_node_from: Series, price_node_to: Series, granularity_hrs: float = None) -&gt; Series\n</code></pre> <p>Calculate congestion rent from net flow data assuming no losses.</p> <p>Convenience method for cases where transmission losses are negligible or not available. Splits net flow into unidirectional components and assumes sent equals received for each direction.</p> <p>Parameters:</p> Name Type Description Default <code>net_flow</code> <code>Series</code> <p>Net power flow (positive = up direction, negative = down direction) in MW or MWh</p> required <code>price_node_from</code> <code>Series</code> <p>Price at sending node (\u20ac/MWh)</p> required <code>price_node_to</code> <code>Series</code> <p>Price at receiving node (\u20ac/MWh)</p> required <code>granularity_hrs</code> <code>float</code> <p>Time granularity in hours (auto-detected if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series with total congestion rents in \u20ac</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Net flow with price data\n&gt;&gt;&gt; net_flow = pd.Series([100, -50, 75], index=time_index)  \n&gt;&gt;&gt; rent = CongestionRentCalculator.from_net_flow_without_losses(\n...     net_flow, price_from, price_to\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/variable_utils/congestion_rent.py</code> <pre><code>@classmethod\ndef from_net_flow_without_losses(\n        cls,\n        net_flow: pd.Series,\n        price_node_from: pd.Series,\n        price_node_to: pd.Series,\n        granularity_hrs: float = None\n) -&gt; pd.Series:\n    \"\"\"Calculate congestion rent from net flow data assuming no losses.\n\n    Convenience method for cases where transmission losses are negligible\n    or not available. Splits net flow into unidirectional components and\n    assumes sent equals received for each direction.\n\n    Args:\n        net_flow: Net power flow (positive = up direction, negative = down direction) in MW or MWh\n        price_node_from: Price at sending node (\u20ac/MWh)\n        price_node_to: Price at receiving node (\u20ac/MWh)\n        granularity_hrs: Time granularity in hours (auto-detected if None)\n\n    Returns:\n        Series with total congestion rents in \u20ac\n\n    Example:\n\n        &gt;&gt;&gt; # Net flow with price data\n        &gt;&gt;&gt; net_flow = pd.Series([100, -50, 75], index=time_index)  \n        &gt;&gt;&gt; rent = CongestionRentCalculator.from_net_flow_without_losses(\n        ...     net_flow, price_from, price_to\n        ... )\n    \"\"\"\n    sent_up = net_flow.clip(lower=0)\n    sent_down = (-net_flow).clip(lower=0)\n    return cls(\n        sent_up=sent_up,\n        received_up=sent_up,\n        sent_down=sent_down,\n        received_down=sent_down,\n        price_node_from=price_node_from,\n        price_node_to=price_node_to,\n        granularity_hrs=granularity_hrs\n    ).calculate()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/congestion_rent/#mesqual.energy_data_handling.variable_utils.congestion_rent.CongestionRentCalculator.from_up_and_down_flow_without_losses","title":"from_up_and_down_flow_without_losses  <code>classmethod</code>","text":"<pre><code>from_up_and_down_flow_without_losses(flow_up: Series, flow_down: Series, price_node_from: Series, price_node_to: Series, granularity_hrs: float = None) -&gt; Series\n</code></pre> <p>Calculate congestion rent from bidirectional flow data assuming no losses.</p> <p>Convenience method for cases where flows are already separated into up/down directions and transmission losses are negligible. Assumes sent equals received for each direction.</p> <p>Parameters:</p> Name Type Description Default <code>flow_up</code> <code>Series</code> <p>Power flow in up direction (MW or MWh, non-negative)</p> required <code>flow_down</code> <code>Series</code> <p>Power flow in down direction (MW or MWh, non-negative)  </p> required <code>price_node_from</code> <code>Series</code> <p>Price at sending node (\u20ac/MWh)</p> required <code>price_node_to</code> <code>Series</code> <p>Price at receiving node (\u20ac/MWh)</p> required <code>granularity_hrs</code> <code>float</code> <p>Time granularity in hours (auto-detected if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series with total congestion rents in \u20ac</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Separate up/down flows\n&gt;&gt;&gt; flow_up = pd.Series([100, 0, 75], index=time_index)\n&gt;&gt;&gt; flow_down = pd.Series([0, 50, 0], index=time_index)\n&gt;&gt;&gt; rent = CongestionRentCalculator.from_up_and_down_flow_without_losses(\n...     flow_up, flow_down, price_from, price_to\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/variable_utils/congestion_rent.py</code> <pre><code>@classmethod\ndef from_up_and_down_flow_without_losses(\n        cls,\n        flow_up: pd.Series,\n        flow_down: pd.Series,\n        price_node_from: pd.Series,\n        price_node_to: pd.Series,\n        granularity_hrs: float = None\n) -&gt; pd.Series:\n    \"\"\"Calculate congestion rent from bidirectional flow data assuming no losses.\n\n    Convenience method for cases where flows are already separated into up/down\n    directions and transmission losses are negligible. Assumes sent equals\n    received for each direction.\n\n    Args:\n        flow_up: Power flow in up direction (MW or MWh, non-negative)\n        flow_down: Power flow in down direction (MW or MWh, non-negative)  \n        price_node_from: Price at sending node (\u20ac/MWh)\n        price_node_to: Price at receiving node (\u20ac/MWh)\n        granularity_hrs: Time granularity in hours (auto-detected if None)\n\n    Returns:\n        Series with total congestion rents in \u20ac\n\n    Example:\n\n        &gt;&gt;&gt; # Separate up/down flows\n        &gt;&gt;&gt; flow_up = pd.Series([100, 0, 75], index=time_index)\n        &gt;&gt;&gt; flow_down = pd.Series([0, 50, 0], index=time_index)\n        &gt;&gt;&gt; rent = CongestionRentCalculator.from_up_and_down_flow_without_losses(\n        ...     flow_up, flow_down, price_from, price_to\n        ... )\n    \"\"\"\n    return cls(\n        sent_up=flow_up,\n        received_up=flow_up,\n        sent_down=flow_down,\n        received_down=flow_down,\n        price_node_from=price_node_from,\n        price_node_to=price_node_to,\n        granularity_hrs=granularity_hrs\n    ).calculate()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/direction_aggs/","title":"MESQUAL Aggregate Directional (Up- / Down-) to Net-Variables","text":""},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/direction_aggs/#mesqual.energy_data_handling.variable_utils.aggregate_up_down_directions_to_net_column.UpDownNetAppender","title":"UpDownNetAppender","text":"<p>Computes and appends net and total columns from bidirectional data.</p> <p>This utility class processes DataFrames containing bidirectional energy data (e.g., power flows, trade volumes) and creates derived columns representing net flows (up - down) and total flows (up + down). It automatically identifies column pairs based on configurable directional identifiers.</p> <p>Energy market context: Bidirectional energy data is common in electricity markets where flows, trades, or capacities can occur in both directions between nodes or areas. Net calculations show the overall direction and magnitude of transfers, while total calculations show overall activity levels.</p> <p>Common use cases: - Power flow analysis: Converting line flows to net flows - Trade balance: Net imports/exports from bilateral trade data</p> <p>The class uses CommonBaseKeyFinder to identify related column pairs and supports flexible naming conventions for input and output columns.</p> <p>Parameters:</p> Name Type Description Default <code>up_identifier</code> <code>str</code> <p>String identifier for \"up\" direction columns (default: '_up')</p> <code>'_up'</code> <code>down_identifier</code> <code>str</code> <p>String identifier for \"down\" direction columns (default: '_down')</p> <code>'_down'</code> <code>net_col_suffix</code> <code>str</code> <p>Suffix for generated net columns (default: '_net')</p> <code>'_net'</code> <code>net_col_prefix</code> <code>str</code> <p>Prefix for generated net columns (default: None)</p> <code>None</code> <code>total_col_suffix</code> <code>str</code> <p>Suffix for generated total columns (default: '_total')</p> <code>'_total'</code> <code>total_col_prefix</code> <code>str</code> <p>Prefix for generated total columns (default: None)</p> <code>None</code> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If neither suffix nor prefix provided for net or total columns</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # Data with bidirectional flows\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'DE_FR_up': [100, 200, 300],\n...     'DE_FR_down': [30, 50, 100],\n...     'FR_BE_up': [400, 500, 600],\n...     'FR_BE_down': [150, 200, 300]\n... })\n&gt;&gt;&gt; \n&gt;&gt;&gt; appender = UpDownNetAppender()\n&gt;&gt;&gt; result = appender.append_net_columns_from_up_down_columns(data)\n&gt;&gt;&gt; print(result.columns)  # Includes DE_FR_net, FR_BE_net\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/variable_utils/aggregate_up_down_directions_to_net_column.py</code> <pre><code>class UpDownNetAppender:\n    \"\"\"Computes and appends net and total columns from bidirectional data.\n\n    This utility class processes DataFrames containing bidirectional energy data\n    (e.g., power flows, trade volumes) and creates derived columns representing\n    net flows (up - down) and total flows (up + down). It automatically identifies\n    column pairs based on configurable directional identifiers.\n\n    Energy market context:\n    Bidirectional energy data is common in electricity markets where flows,\n    trades, or capacities can occur in both directions between nodes or areas.\n    Net calculations show the overall direction and magnitude of transfers,\n    while total calculations show overall activity levels.\n\n    Common use cases:\n    - Power flow analysis: Converting line flows to net flows\n    - Trade balance: Net imports/exports from bilateral trade data\n\n    The class uses CommonBaseKeyFinder to identify related column pairs and\n    supports flexible naming conventions for input and output columns.\n\n    Args:\n        up_identifier: String identifier for \"up\" direction columns (default: '_up')\n        down_identifier: String identifier for \"down\" direction columns (default: '_down')\n        net_col_suffix: Suffix for generated net columns (default: '_net')\n        net_col_prefix: Prefix for generated net columns (default: None)\n        total_col_suffix: Suffix for generated total columns (default: '_total')\n        total_col_prefix: Prefix for generated total columns (default: None)\n\n    Raises:\n        AttributeError: If neither suffix nor prefix provided for net or total columns\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; # Data with bidirectional flows\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'DE_FR_up': [100, 200, 300],\n        ...     'DE_FR_down': [30, 50, 100],\n        ...     'FR_BE_up': [400, 500, 600],\n        ...     'FR_BE_down': [150, 200, 300]\n        ... })\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; appender = UpDownNetAppender()\n        &gt;&gt;&gt; result = appender.append_net_columns_from_up_down_columns(data)\n        &gt;&gt;&gt; print(result.columns)  # Includes DE_FR_net, FR_BE_net\n    \"\"\"\n\n    def __init__(\n            self,\n            up_identifier: str = '_up',\n            down_identifier: str = '_down',\n            net_col_suffix: str = '_net',\n            net_col_prefix: str = None,\n            total_col_suffix: str = '_total',\n            total_col_prefix: str = None,\n    ):\n        \"\"\"Initialize the UpDownNetAppender with naming conventions.\n\n        Args:\n            up_identifier: String identifier for \"up\" direction columns\n            down_identifier: String identifier for \"down\" direction columns  \n            net_col_suffix: Suffix for generated net columns\n            net_col_prefix: Prefix for generated net columns\n            total_col_suffix: Suffix for generated total columns\n            total_col_prefix: Prefix for generated total columns\n\n        Raises:\n            AttributeError: If neither suffix nor prefix provided for net or total columns\n        \"\"\"\n        self._up_identifier = up_identifier\n        self._down_identifier = down_identifier\n\n        self._net_col_suffix = net_col_suffix or ''\n        self._net_col_prefix = net_col_prefix or ''\n        if not any([self._net_col_suffix, self._net_col_prefix]):\n            raise AttributeError(\"Either net_col_suffix or net_col_prefix must be provided\")\n\n        self._total_col_suffix = total_col_suffix or ''\n        self._total_col_prefix = total_col_prefix or ''\n        if not any([self._total_col_suffix, self._total_col_prefix]):\n            raise AttributeError(\"Either total_col_suffix or total_col_prefix must be provided\")\n\n        self._common_base_key_finder = CommonBaseKeyFinder(up_identifier, down_identifier)\n\n    def append_net_columns_from_up_down_columns(\n            self,\n            ts_df_with_up_down_columns: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Append net columns (up - down) to the DataFrame.\n\n        Identifies all column pairs with up/down directional identifiers and\n        creates corresponding net columns by subtracting down values from up values.\n        Net columns represent the overall direction and magnitude of flows.\n\n        Args:\n            ts_df_with_up_down_columns: DataFrame containing bidirectional columns\n                with up/down identifiers. Can have single or multi-level column index.\n\n        Returns:\n            DataFrame with original columns plus new net columns. Net values are\n            positive when up &gt; down, negative when down &gt; up.\n\n        Example:\n\n            &gt;&gt;&gt; data = pd.DataFrame({\n            ...     'flow_up': [100, 200], 'flow_down': [30, 50]\n            ... })\n            &gt;&gt;&gt; result = appender.append_net_columns_from_up_down_columns(data)\n            &gt;&gt;&gt; print(result['flow_net'])  # [70, 150]\n        \"\"\"\n        return self._append_columns_from_up_down_columns(ts_df_with_up_down_columns, 'net')\n\n    def append_total_columns_from_up_down_columns(\n            self,\n            ts_df_with_up_down_columns: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Append total columns (up + down) to the DataFrame.\n\n        Identifies all column pairs with up/down directional identifiers and\n        creates corresponding total columns by adding up and down values.\n        Total columns represent the overall activity level regardless of direction.\n\n        Args:\n            ts_df_with_up_down_columns: DataFrame containing bidirectional columns\n                with up/down identifiers. Can have single or multi-level column index.\n\n        Returns:\n            DataFrame with original columns plus new total columns. Total values\n            represent the sum of absolute flows in both directions.\n\n        Example:\n\n            &gt;&gt;&gt; data = pd.DataFrame({\n            ...     'flow_up': [100, 200], 'flow_down': [30, 50]  \n            ... })\n            &gt;&gt;&gt; result = appender.append_total_columns_from_up_down_columns(data)\n            &gt;&gt;&gt; print(result['flow_total'])  # [130, 250]\n        \"\"\"\n        return self._append_columns_from_up_down_columns(ts_df_with_up_down_columns, 'total')\n\n    def _append_columns_from_up_down_columns(\n            self,\n            ts_df_with_up_down_columns: pd.DataFrame,\n            which_agg: Literal['net', 'total']\n    ) -&gt; pd.DataFrame:\n        \"\"\"Internal method to append either net or total columns.\n\n        Args:\n            ts_df_with_up_down_columns: DataFrame with bidirectional data\n            which_agg: Type of aggregation to perform ('net' or 'total')\n\n        Returns:\n            DataFrame with new aggregated columns appended\n\n        Raises:\n            NotImplementedError: If which_agg is not 'net' or 'total'\n        \"\"\"\n\n        up_id = self._up_identifier\n        down_id = self._down_identifier\n\n        _col_names = ts_df_with_up_down_columns.columns.get_level_values(0).unique()\n        up_down_columns = self._common_base_key_finder.get_keys_for_which_all_association_tags_appear(_col_names)\n\n        for c in up_down_columns:\n            up_col = f'{c}{up_id}'\n            down_col = f'{c}{down_id}'\n            if which_agg == 'net':\n                new_col = f'{self._net_col_prefix}{c}{self._net_col_suffix}'\n                new_values = ts_df_with_up_down_columns[up_col].subtract(\n                    ts_df_with_up_down_columns[down_col], fill_value=0,\n                )\n            elif which_agg == 'total':\n                new_col = f'{self._total_col_prefix}{c}{self._total_col_suffix}'\n                new_values = ts_df_with_up_down_columns[up_col].add(\n                    ts_df_with_up_down_columns[down_col], fill_value=0,\n                )\n            else:\n                raise NotImplementedError\n            ts_df_with_up_down_columns = set_column(ts_df_with_up_down_columns, new_col, new_values)\n        return ts_df_with_up_down_columns\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/direction_aggs/#mesqual.energy_data_handling.variable_utils.aggregate_up_down_directions_to_net_column.UpDownNetAppender.__init__","title":"__init__","text":"<pre><code>__init__(up_identifier: str = '_up', down_identifier: str = '_down', net_col_suffix: str = '_net', net_col_prefix: str = None, total_col_suffix: str = '_total', total_col_prefix: str = None)\n</code></pre> <p>Initialize the UpDownNetAppender with naming conventions.</p> <p>Parameters:</p> Name Type Description Default <code>up_identifier</code> <code>str</code> <p>String identifier for \"up\" direction columns</p> <code>'_up'</code> <code>down_identifier</code> <code>str</code> <p>String identifier for \"down\" direction columns  </p> <code>'_down'</code> <code>net_col_suffix</code> <code>str</code> <p>Suffix for generated net columns</p> <code>'_net'</code> <code>net_col_prefix</code> <code>str</code> <p>Prefix for generated net columns</p> <code>None</code> <code>total_col_suffix</code> <code>str</code> <p>Suffix for generated total columns</p> <code>'_total'</code> <code>total_col_prefix</code> <code>str</code> <p>Prefix for generated total columns</p> <code>None</code> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If neither suffix nor prefix provided for net or total columns</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/variable_utils/aggregate_up_down_directions_to_net_column.py</code> <pre><code>def __init__(\n        self,\n        up_identifier: str = '_up',\n        down_identifier: str = '_down',\n        net_col_suffix: str = '_net',\n        net_col_prefix: str = None,\n        total_col_suffix: str = '_total',\n        total_col_prefix: str = None,\n):\n    \"\"\"Initialize the UpDownNetAppender with naming conventions.\n\n    Args:\n        up_identifier: String identifier for \"up\" direction columns\n        down_identifier: String identifier for \"down\" direction columns  \n        net_col_suffix: Suffix for generated net columns\n        net_col_prefix: Prefix for generated net columns\n        total_col_suffix: Suffix for generated total columns\n        total_col_prefix: Prefix for generated total columns\n\n    Raises:\n        AttributeError: If neither suffix nor prefix provided for net or total columns\n    \"\"\"\n    self._up_identifier = up_identifier\n    self._down_identifier = down_identifier\n\n    self._net_col_suffix = net_col_suffix or ''\n    self._net_col_prefix = net_col_prefix or ''\n    if not any([self._net_col_suffix, self._net_col_prefix]):\n        raise AttributeError(\"Either net_col_suffix or net_col_prefix must be provided\")\n\n    self._total_col_suffix = total_col_suffix or ''\n    self._total_col_prefix = total_col_prefix or ''\n    if not any([self._total_col_suffix, self._total_col_prefix]):\n        raise AttributeError(\"Either total_col_suffix or total_col_prefix must be provided\")\n\n    self._common_base_key_finder = CommonBaseKeyFinder(up_identifier, down_identifier)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/direction_aggs/#mesqual.energy_data_handling.variable_utils.aggregate_up_down_directions_to_net_column.UpDownNetAppender.append_net_columns_from_up_down_columns","title":"append_net_columns_from_up_down_columns","text":"<pre><code>append_net_columns_from_up_down_columns(ts_df_with_up_down_columns: DataFrame) -&gt; DataFrame\n</code></pre> <p>Append net columns (up - down) to the DataFrame.</p> <p>Identifies all column pairs with up/down directional identifiers and creates corresponding net columns by subtracting down values from up values. Net columns represent the overall direction and magnitude of flows.</p> <p>Parameters:</p> Name Type Description Default <code>ts_df_with_up_down_columns</code> <code>DataFrame</code> <p>DataFrame containing bidirectional columns with up/down identifiers. Can have single or multi-level column index.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with original columns plus new net columns. Net values are</p> <code>DataFrame</code> <p>positive when up &gt; down, negative when down &gt; up.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; data = pd.DataFrame({\n...     'flow_up': [100, 200], 'flow_down': [30, 50]\n... })\n&gt;&gt;&gt; result = appender.append_net_columns_from_up_down_columns(data)\n&gt;&gt;&gt; print(result['flow_net'])  # [70, 150]\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/variable_utils/aggregate_up_down_directions_to_net_column.py</code> <pre><code>def append_net_columns_from_up_down_columns(\n        self,\n        ts_df_with_up_down_columns: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Append net columns (up - down) to the DataFrame.\n\n    Identifies all column pairs with up/down directional identifiers and\n    creates corresponding net columns by subtracting down values from up values.\n    Net columns represent the overall direction and magnitude of flows.\n\n    Args:\n        ts_df_with_up_down_columns: DataFrame containing bidirectional columns\n            with up/down identifiers. Can have single or multi-level column index.\n\n    Returns:\n        DataFrame with original columns plus new net columns. Net values are\n        positive when up &gt; down, negative when down &gt; up.\n\n    Example:\n\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'flow_up': [100, 200], 'flow_down': [30, 50]\n        ... })\n        &gt;&gt;&gt; result = appender.append_net_columns_from_up_down_columns(data)\n        &gt;&gt;&gt; print(result['flow_net'])  # [70, 150]\n    \"\"\"\n    return self._append_columns_from_up_down_columns(ts_df_with_up_down_columns, 'net')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/direction_aggs/#mesqual.energy_data_handling.variable_utils.aggregate_up_down_directions_to_net_column.UpDownNetAppender.append_total_columns_from_up_down_columns","title":"append_total_columns_from_up_down_columns","text":"<pre><code>append_total_columns_from_up_down_columns(ts_df_with_up_down_columns: DataFrame) -&gt; DataFrame\n</code></pre> <p>Append total columns (up + down) to the DataFrame.</p> <p>Identifies all column pairs with up/down directional identifiers and creates corresponding total columns by adding up and down values. Total columns represent the overall activity level regardless of direction.</p> <p>Parameters:</p> Name Type Description Default <code>ts_df_with_up_down_columns</code> <code>DataFrame</code> <p>DataFrame containing bidirectional columns with up/down identifiers. Can have single or multi-level column index.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with original columns plus new total columns. Total values</p> <code>DataFrame</code> <p>represent the sum of absolute flows in both directions.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; data = pd.DataFrame({\n...     'flow_up': [100, 200], 'flow_down': [30, 50]  \n... })\n&gt;&gt;&gt; result = appender.append_total_columns_from_up_down_columns(data)\n&gt;&gt;&gt; print(result['flow_total'])  # [130, 250]\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/variable_utils/aggregate_up_down_directions_to_net_column.py</code> <pre><code>def append_total_columns_from_up_down_columns(\n        self,\n        ts_df_with_up_down_columns: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Append total columns (up + down) to the DataFrame.\n\n    Identifies all column pairs with up/down directional identifiers and\n    creates corresponding total columns by adding up and down values.\n    Total columns represent the overall activity level regardless of direction.\n\n    Args:\n        ts_df_with_up_down_columns: DataFrame containing bidirectional columns\n            with up/down identifiers. Can have single or multi-level column index.\n\n    Returns:\n        DataFrame with original columns plus new total columns. Total values\n        represent the sum of absolute flows in both directions.\n\n    Example:\n\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'flow_up': [100, 200], 'flow_down': [30, 50]  \n        ... })\n        &gt;&gt;&gt; result = appender.append_total_columns_from_up_down_columns(data)\n        &gt;&gt;&gt; print(result['flow_total'])  # [130, 250]\n    \"\"\"\n    return self._append_columns_from_up_down_columns(ts_df_with_up_down_columns, 'total')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/in_common_aggs/","title":"MESQUAL Aggregate Columns with Part in Common","text":""},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/in_common_aggs/#mesqual.energy_data_handling.variable_utils.aggregate_cols_with_part_in_common.AggregatedColumnAppender","title":"AggregatedColumnAppender","text":"<p>Adds aggregated columns to pandas DataFrames by summing columns that share a common identifier.</p> <p>This utility is particularly useful in energy systems modeling where multiple variables of the same type (e.g., different wind generation sources, various demand components, multiple storage units) need to be aggregated into totals for analysis or visualization.</p> <p>The class handles both single-level and multi-level DataFrame columns, making it suitable for MESQUAL's energy variable structures that often include hierarchical indexing (time, regions, technologies, etc.).</p> Energy Domain Context <ul> <li>Aggregates extensive quantities (volumes, energy, capacities) by summation</li> <li>Preserves NaN when all quantities in an aggregation are NaNs</li> <li>Suitable for technology groupings, regional aggregations, and fuel type summations</li> </ul> <p>Parameters:</p> Name Type Description Default <code>in_common_part</code> <code>str</code> <p>The common substring to search for in column names. All columns containing this substring will be summed together.</p> required <code>agg_col_name_prefix</code> <code>str</code> <p>Prefix to add to the aggregated column name. Defaults to empty string.</p> <code>None</code> <code>agg_col_name_suffix</code> <code>str</code> <p>Suffix to add to the aggregated column name. Defaults to empty string.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic energy variable aggregation\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'wind_onshore_gen': [100, 200, 300],\n...     'wind_offshore_gen': [50, 70, 100], \n...     'solar_pv_gen': [30, 40, 50]\n... })\n&gt;&gt;&gt; appender = AggregatedColumnAppender('wind', agg_col_name_suffix='_total')\n&gt;&gt;&gt; result = appender.add_aggregated_column(data)\n&gt;&gt;&gt; # Creates 'wind_total' column with sum of wind_onshore_gen and wind_offshore_gen\n\n&gt;&gt;&gt; # Multi-level columns for scenario analysis\n&gt;&gt;&gt; columns = pd.MultiIndex.from_tuples([\n...     ('wind_onshore', 'scenario_1'), ('wind_onshore', 'scenario_2'),\n...     ('wind_offshore', 'scenario_1'), ('wind_offshore', 'scenario_2')\n... ])\n&gt;&gt;&gt; data = pd.DataFrame(np.random.rand(24, 4), columns=columns)\n&gt;&gt;&gt; appender = AggregatedColumnAppender('wind', agg_col_name_prefix='total_')\n&gt;&gt;&gt; result = appender.add_aggregated_column(data)\n&gt;&gt;&gt; # Creates 'total_wind' with aggregated values for each scenario\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/variable_utils/aggregate_cols_with_part_in_common.py</code> <pre><code>class AggregatedColumnAppender:\n    \"\"\"\n    Adds aggregated columns to pandas DataFrames by summing columns that share a common identifier.\n\n    This utility is particularly useful in energy systems modeling where multiple variables of the same\n    type (e.g., different wind generation sources, various demand components, multiple storage units)\n    need to be aggregated into totals for analysis or visualization.\n\n    The class handles both single-level and multi-level DataFrame columns, making it suitable for\n    MESQUAL's energy variable structures that often include hierarchical indexing (time,\n    regions, technologies, etc.).\n\n    Energy Domain Context:\n        - Aggregates extensive quantities (volumes, energy, capacities) by summation\n        - Preserves NaN when all quantities in an aggregation are NaNs\n        - Suitable for technology groupings, regional aggregations, and fuel type summations\n\n    Args:\n        in_common_part (str): The common substring to search for in column names. All columns\n            containing this substring will be summed together.\n        agg_col_name_prefix (str, optional): Prefix to add to the aggregated column name.\n            Defaults to empty string.\n        agg_col_name_suffix (str, optional): Suffix to add to the aggregated column name.\n            Defaults to empty string.\n\n    Examples:\n\n        &gt;&gt;&gt; # Basic energy variable aggregation\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'wind_onshore_gen': [100, 200, 300],\n        ...     'wind_offshore_gen': [50, 70, 100], \n        ...     'solar_pv_gen': [30, 40, 50]\n        ... })\n        &gt;&gt;&gt; appender = AggregatedColumnAppender('wind', agg_col_name_suffix='_total')\n        &gt;&gt;&gt; result = appender.add_aggregated_column(data)\n        &gt;&gt;&gt; # Creates 'wind_total' column with sum of wind_onshore_gen and wind_offshore_gen\n\n        &gt;&gt;&gt; # Multi-level columns for scenario analysis\n        &gt;&gt;&gt; columns = pd.MultiIndex.from_tuples([\n        ...     ('wind_onshore', 'scenario_1'), ('wind_onshore', 'scenario_2'),\n        ...     ('wind_offshore', 'scenario_1'), ('wind_offshore', 'scenario_2')\n        ... ])\n        &gt;&gt;&gt; data = pd.DataFrame(np.random.rand(24, 4), columns=columns)\n        &gt;&gt;&gt; appender = AggregatedColumnAppender('wind', agg_col_name_prefix='total_')\n        &gt;&gt;&gt; result = appender.add_aggregated_column(data)\n        &gt;&gt;&gt; # Creates 'total_wind' with aggregated values for each scenario\n    \"\"\"\n\n    def __init__(\n            self,\n            in_common_part: str,\n            agg_col_name_prefix: str = None,\n            agg_col_name_suffix: str = None,\n    ):\n        self._in_common_part = in_common_part\n        self._agg_col_name_prefix = agg_col_name_prefix or ''\n        self._agg_col_name_suffix = agg_col_name_suffix or ''\n\n    def add_aggregated_column(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Add an aggregated column to the DataFrame by summing matching columns.\n\n        This method identifies all columns containing the specified common part and sums them\n        into a new aggregated column. The aggregation preserves the DataFrame's structure\n        and handles both single-level and multi-level column indices.\n\n        For energy data, this is typically used to:\n        - Aggregate different technology types (e.g., all wind sources)\n        - Sum regional contributions (e.g., all demand in a country)\n        - Combine fuel types (e.g., all fossil fuel generators)\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing energy variables. Can have single-level\n                or multi-level column structure. For multi-level columns, aggregation is performed\n                at the appropriate level while preserving the hierarchy.\n\n        Returns:\n            pd.DataFrame: Original DataFrame with added aggregated column. The new column name\n                follows the pattern: {prefix}{in_common_part}{suffix}\n\n        Raises:\n            ValueError: If no columns contain the specified common part.\n            TypeError: If input is not a pandas DataFrame.\n\n        Examples:\n            Single-level columns - technology aggregation:\n            &gt;&gt;&gt; gen_data = pd.DataFrame({\n            ...     'wind_onshore_MW': [120, 150, 180],\n            ...     'wind_offshore_MW': [80, 90, 100],\n            ...     'solar_pv_MW': [200, 250, 300],\n            ...     'demand_MW': [400, 490, 580]\n            ... })\n            &gt;&gt;&gt; appender = AggregatedColumnAppender('wind', agg_col_name_suffix='_total_MW')\n            &gt;&gt;&gt; result = appender.add_aggregated_column(gen_data)\n            &gt;&gt;&gt; result['wind_total_MW']  # [200, 240, 280]\n\n            Multi-level columns - scenario and regional analysis\n            &gt;&gt;&gt; idx = pd.date_range('2024-01-01', periods=3, freq='h')\n            &gt;&gt;&gt; cols = pd.MultiIndex.from_tuples([\n            ...     ('storage_battery', 'DE', 'scenario_1'), \n            ...     ('storage_battery', 'FR', 'scenario_1'),\n            ...     ('storage_pumped', 'DE', 'scenario_1'),\n            ...     ('storage_pumped', 'FR', 'scenario_1')\n            ... ], names=['technology', 'region', 'scenario'])\n            &gt;&gt;&gt; data = pd.DataFrame(np.random.rand(3, 4), index=idx, columns=cols)\n            &gt;&gt;&gt; appender = AggregatedColumnAppender('storage', agg_col_name_prefix='total_')\n            &gt;&gt;&gt; result = appender.add_aggregated_column(data)\n            &gt;&gt;&gt; # Creates 'total_storage' with sums grouped by (region, scenario)\n\n            Handling NaN values in energy time series\n            &gt;&gt;&gt; price_data = pd.DataFrame({\n            ...     'price_day_ahead': [50.0, np.nan, 45.0],\n            ...     'price_intraday': [np.nan, np.nan, 47.0],  \n            ...     'demand_forecast': [1000, 1100, 1200]\n            ... })\n            &gt;&gt;&gt; appender = AggregatedColumnAppender('price', agg_col_name_suffix='_avg')\n            &gt;&gt;&gt; result = appender.add_aggregated_column(price_data)\n            &gt;&gt;&gt; # 'price_avg': [50.0, NaN, 92.0] - preserves NaN when all inputs are NaN\n\n        Note:\n            For multi-level DataFrames, the aggregation respects the hierarchical structure.\n            If columns have levels beyond the first (technology level), the aggregation\n            groups by those additional levels, creating separate totals for each combination.\n\n            NaN handling follows pandas summation rules: NaN values are ignored unless all\n            values in a row are NaN, in which case the result is NaN.\n        \"\"\"\n        cols = df.columns.get_level_values(0).unique()\n        cols_with_common_part = [x for x in cols if self._in_common_part in x]\n\n        df_in_common = df[cols_with_common_part]\n        if df.columns.nlevels == 1:\n            dff = df_in_common.sum(axis=1)\n            dff.loc[df_in_common.isna().all(axis=1)] = np.nan\n        else:\n            _groupby = list(range(1, df.columns.nlevels))\n            dff = df_in_common.T.groupby(level=_groupby).sum().T\n            _all_na = df_in_common.isna().T.groupby(level=_groupby).all().T\n            if _all_na.any().any():\n                for c in _all_na.columns:\n                    dff.loc[_all_na[c], c] = np.nan\n\n        new_col_name = f'{self._agg_col_name_prefix}{self._in_common_part}{self._agg_col_name_suffix}'\n        df = set_column(df, new_col_name, dff)\n        return df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/in_common_aggs/#mesqual.energy_data_handling.variable_utils.aggregate_cols_with_part_in_common.AggregatedColumnAppender.add_aggregated_column","title":"add_aggregated_column","text":"<pre><code>add_aggregated_column(df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Add an aggregated column to the DataFrame by summing matching columns.</p> <p>This method identifies all columns containing the specified common part and sums them into a new aggregated column. The aggregation preserves the DataFrame's structure and handles both single-level and multi-level column indices.</p> <p>For energy data, this is typically used to: - Aggregate different technology types (e.g., all wind sources) - Sum regional contributions (e.g., all demand in a country) - Combine fuel types (e.g., all fossil fuel generators)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing energy variables. Can have single-level or multi-level column structure. For multi-level columns, aggregation is performed at the appropriate level while preserving the hierarchy.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Original DataFrame with added aggregated column. The new column name follows the pattern: {prefix}{in_common_part}{suffix}</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no columns contain the specified common part.</p> <code>TypeError</code> <p>If input is not a pandas DataFrame.</p> <p>Examples:</p> <p>Single-level columns - technology aggregation:</p> <pre><code>&gt;&gt;&gt; gen_data = pd.DataFrame({\n...     'wind_onshore_MW': [120, 150, 180],\n...     'wind_offshore_MW': [80, 90, 100],\n...     'solar_pv_MW': [200, 250, 300],\n...     'demand_MW': [400, 490, 580]\n... })\n&gt;&gt;&gt; appender = AggregatedColumnAppender('wind', agg_col_name_suffix='_total_MW')\n&gt;&gt;&gt; result = appender.add_aggregated_column(gen_data)\n&gt;&gt;&gt; result['wind_total_MW']  # [200, 240, 280]\n</code></pre> <p>Multi-level columns - scenario and regional analysis</p> <pre><code>&gt;&gt;&gt; idx = pd.date_range('2024-01-01', periods=3, freq='h')\n&gt;&gt;&gt; cols = pd.MultiIndex.from_tuples([\n...     ('storage_battery', 'DE', 'scenario_1'), \n...     ('storage_battery', 'FR', 'scenario_1'),\n...     ('storage_pumped', 'DE', 'scenario_1'),\n...     ('storage_pumped', 'FR', 'scenario_1')\n... ], names=['technology', 'region', 'scenario'])\n&gt;&gt;&gt; data = pd.DataFrame(np.random.rand(3, 4), index=idx, columns=cols)\n&gt;&gt;&gt; appender = AggregatedColumnAppender('storage', agg_col_name_prefix='total_')\n&gt;&gt;&gt; result = appender.add_aggregated_column(data)\n&gt;&gt;&gt; # Creates 'total_storage' with sums grouped by (region, scenario)\n</code></pre> <p>Handling NaN values in energy time series</p> <pre><code>&gt;&gt;&gt; price_data = pd.DataFrame({\n...     'price_day_ahead': [50.0, np.nan, 45.0],\n...     'price_intraday': [np.nan, np.nan, 47.0],  \n...     'demand_forecast': [1000, 1100, 1200]\n... })\n&gt;&gt;&gt; appender = AggregatedColumnAppender('price', agg_col_name_suffix='_avg')\n&gt;&gt;&gt; result = appender.add_aggregated_column(price_data)\n&gt;&gt;&gt; # 'price_avg': [50.0, NaN, 92.0] - preserves NaN when all inputs are NaN\n</code></pre> Note <p>For multi-level DataFrames, the aggregation respects the hierarchical structure. If columns have levels beyond the first (technology level), the aggregation groups by those additional levels, creating separate totals for each combination.</p> <p>NaN handling follows pandas summation rules: NaN values are ignored unless all values in a row are NaN, in which case the result is NaN.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/variable_utils/aggregate_cols_with_part_in_common.py</code> <pre><code>def add_aggregated_column(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Add an aggregated column to the DataFrame by summing matching columns.\n\n    This method identifies all columns containing the specified common part and sums them\n    into a new aggregated column. The aggregation preserves the DataFrame's structure\n    and handles both single-level and multi-level column indices.\n\n    For energy data, this is typically used to:\n    - Aggregate different technology types (e.g., all wind sources)\n    - Sum regional contributions (e.g., all demand in a country)\n    - Combine fuel types (e.g., all fossil fuel generators)\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing energy variables. Can have single-level\n            or multi-level column structure. For multi-level columns, aggregation is performed\n            at the appropriate level while preserving the hierarchy.\n\n    Returns:\n        pd.DataFrame: Original DataFrame with added aggregated column. The new column name\n            follows the pattern: {prefix}{in_common_part}{suffix}\n\n    Raises:\n        ValueError: If no columns contain the specified common part.\n        TypeError: If input is not a pandas DataFrame.\n\n    Examples:\n        Single-level columns - technology aggregation:\n        &gt;&gt;&gt; gen_data = pd.DataFrame({\n        ...     'wind_onshore_MW': [120, 150, 180],\n        ...     'wind_offshore_MW': [80, 90, 100],\n        ...     'solar_pv_MW': [200, 250, 300],\n        ...     'demand_MW': [400, 490, 580]\n        ... })\n        &gt;&gt;&gt; appender = AggregatedColumnAppender('wind', agg_col_name_suffix='_total_MW')\n        &gt;&gt;&gt; result = appender.add_aggregated_column(gen_data)\n        &gt;&gt;&gt; result['wind_total_MW']  # [200, 240, 280]\n\n        Multi-level columns - scenario and regional analysis\n        &gt;&gt;&gt; idx = pd.date_range('2024-01-01', periods=3, freq='h')\n        &gt;&gt;&gt; cols = pd.MultiIndex.from_tuples([\n        ...     ('storage_battery', 'DE', 'scenario_1'), \n        ...     ('storage_battery', 'FR', 'scenario_1'),\n        ...     ('storage_pumped', 'DE', 'scenario_1'),\n        ...     ('storage_pumped', 'FR', 'scenario_1')\n        ... ], names=['technology', 'region', 'scenario'])\n        &gt;&gt;&gt; data = pd.DataFrame(np.random.rand(3, 4), index=idx, columns=cols)\n        &gt;&gt;&gt; appender = AggregatedColumnAppender('storage', agg_col_name_prefix='total_')\n        &gt;&gt;&gt; result = appender.add_aggregated_column(data)\n        &gt;&gt;&gt; # Creates 'total_storage' with sums grouped by (region, scenario)\n\n        Handling NaN values in energy time series\n        &gt;&gt;&gt; price_data = pd.DataFrame({\n        ...     'price_day_ahead': [50.0, np.nan, 45.0],\n        ...     'price_intraday': [np.nan, np.nan, 47.0],  \n        ...     'demand_forecast': [1000, 1100, 1200]\n        ... })\n        &gt;&gt;&gt; appender = AggregatedColumnAppender('price', agg_col_name_suffix='_avg')\n        &gt;&gt;&gt; result = appender.add_aggregated_column(price_data)\n        &gt;&gt;&gt; # 'price_avg': [50.0, NaN, 92.0] - preserves NaN when all inputs are NaN\n\n    Note:\n        For multi-level DataFrames, the aggregation respects the hierarchical structure.\n        If columns have levels beyond the first (technology level), the aggregation\n        groups by those additional levels, creating separate totals for each combination.\n\n        NaN handling follows pandas summation rules: NaN values are ignored unless all\n        values in a row are NaN, in which case the result is NaN.\n    \"\"\"\n    cols = df.columns.get_level_values(0).unique()\n    cols_with_common_part = [x for x in cols if self._in_common_part in x]\n\n    df_in_common = df[cols_with_common_part]\n    if df.columns.nlevels == 1:\n        dff = df_in_common.sum(axis=1)\n        dff.loc[df_in_common.isna().all(axis=1)] = np.nan\n    else:\n        _groupby = list(range(1, df.columns.nlevels))\n        dff = df_in_common.T.groupby(level=_groupby).sum().T\n        _all_na = df_in_common.isna().T.groupby(level=_groupby).all().T\n        if _all_na.any().any():\n            for c in _all_na.columns:\n                dff.loc[_all_na[c], c] = np.nan\n\n    new_col_name = f'{self._agg_col_name_prefix}{self._in_common_part}{self._agg_col_name_suffix}'\n    df = set_column(df, new_col_name, dff)\n    return df\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/regional_trade_balance/","title":"MESQUAL Regional Trade Balance Calculator","text":""},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/regional_trade_balance/#mesqual.energy_data_handling.variable_utils.regional_trade_balance_calculator.RegionalTradeBalanceCalculator","title":"RegionalTradeBalanceCalculator","text":"<p>Aggregates bidirectional power flows between regions based on node-level flow data.</p> <p>Takes line-level flow data with bidirectional flows (up/down) and losses, and aggregates them to a higher regional level (e.g., countries, market areas) or keeps them at node level. Uses networkx for identifying region/node connections and handles multiple lines between the same region/node pairs.</p> <p>When agg_region_column is None, each node is treated as its own region, allowing for node-to-node trade balance analysis without aggregation.</p> <p>Example for regional aggregation:</p> <pre><code>&gt;&gt;&gt; line_model_df = pd.DataFrame({\n...     \"node_from\": [\"DE1\", \"FR1\"],\n...     \"node_to\": [\"FR1\", \"BE1\"]\n... })\n&gt;&gt;&gt; node_model_df = pd.DataFrame({\n...     \"country\": [\"DE\", \"FR\", \"BE\"]\n... }, index=[\"DE1\", \"FR1\", \"BE1\"])\n&gt;&gt;&gt; aggregator = RegionalTradeBalanceCalculator(\n...     line_model_df=line_model_df,\n...     node_model_df=node_model_df,\n...     agg_region_column=\"country\"\n... )\n</code></pre> <p>Example for node-level flows:</p> <pre><code>&gt;&gt;&gt; aggregator = RegionalTradeBalanceCalculator(\n...     line_model_df=line_model_df,\n...     node_model_df=node_model_df,\n...     agg_region_column=None  # Keep flows at node level\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/variable_utils/regional_trade_balance_calculator.py</code> <pre><code>class RegionalTradeBalanceCalculator:\n    \"\"\"Aggregates bidirectional power flows between regions based on node-level flow data.\n\n    Takes line-level flow data with bidirectional flows (up/down) and losses, and aggregates\n    them to a higher regional level (e.g., countries, market areas) or keeps them at node level.\n    Uses networkx for identifying region/node connections and handles multiple lines between\n    the same region/node pairs.\n\n    When agg_region_column is None, each node is treated as its own region, allowing for\n    node-to-node trade balance analysis without aggregation.\n\n    Example for regional aggregation:\n\n        &gt;&gt;&gt; line_model_df = pd.DataFrame({\n        ...     \"node_from\": [\"DE1\", \"FR1\"],\n        ...     \"node_to\": [\"FR1\", \"BE1\"]\n        ... })\n        &gt;&gt;&gt; node_model_df = pd.DataFrame({\n        ...     \"country\": [\"DE\", \"FR\", \"BE\"]\n        ... }, index=[\"DE1\", \"FR1\", \"BE1\"])\n        &gt;&gt;&gt; aggregator = RegionalTradeBalanceCalculator(\n        ...     line_model_df=line_model_df,\n        ...     node_model_df=node_model_df,\n        ...     agg_region_column=\"country\"\n        ... )\n\n    Example for node-level flows:\n\n        &gt;&gt;&gt; aggregator = RegionalTradeBalanceCalculator(\n        ...     line_model_df=line_model_df,\n        ...     node_model_df=node_model_df,\n        ...     agg_region_column=None  # Keep flows at node level\n        ... )\n    \"\"\"\n    EXP_VAR = \"exp\"\n    IMP_VAR = \"imp\"\n    NET_EXP_VAR = \"net_exp\"\n    ALL_VARS = [EXP_VAR, IMP_VAR, NET_EXP_VAR]\n\n    def __init__(\n            self,\n            line_model_df: pd.DataFrame,\n            node_model_df: pd.DataFrame,\n            agg_region_column: str | None = \"country\",\n            node_from_col: str = \"node_from\",\n            node_to_col: str = \"node_to\"\n    ):\n        self.line_model_df = line_model_df\n        self.node_model_df = node_model_df\n        self.agg_region_column = agg_region_column\n        self.node_from_col = node_from_col\n        self.node_to_col = node_to_col\n        self.node_to_agg_region_map = self._create_node_to_region_map()\n        self.agg_region_graph = self._create_region_graph()\n\n    def _create_node_to_region_map(self) -&gt; dict:\n        if self.agg_region_column is None:\n            return {node: node for node in self.node_model_df.index}\n        return self.node_model_df[self.agg_region_column].to_dict()\n\n    def _create_region_graph(self) -&gt; nx.Graph:\n        graph = nx.Graph()\n\n        for _, line in self.line_model_df.iterrows():\n            region_from = self.node_to_agg_region_map[line[self.node_from_col]]\n            region_to = self.node_to_agg_region_map[line[self.node_to_col]]\n\n            if region_from != region_to:\n                if not graph.has_edge(region_from, region_to):\n                    graph.add_edge(region_from, region_to)\n\n        return graph\n\n    def _get_net_exp_for_couple(self, primary, secondary, flow_data: NetworkLineFlowsData, flow_type: FlowType) -&gt; pd.Series:\n        mask_forward = (\n                (self.line_model_df[self.node_from_col].map(self.node_to_agg_region_map) == primary) &amp;\n                (self.line_model_df[self.node_to_col].map(self.node_to_agg_region_map) == secondary)\n        )\n        mask_backward = (\n                (self.line_model_df[self.node_from_col].map(self.node_to_agg_region_map) == secondary) &amp;\n                (self.line_model_df[self.node_to_col].map(self.node_to_agg_region_map) == primary)\n        )\n\n        lines_forward = self.line_model_df[mask_forward].index\n        lines_backward = self.line_model_df[mask_backward].index\n\n        if flow_type == FlowType.PRE_LOSS:\n            return (\n                    flow_data.sent_up[lines_forward].sum(axis=1) -\n                    flow_data.sent_down[lines_forward].sum(axis=1) +\n                    flow_data.sent_down[lines_backward].sum(axis=1) -\n                    flow_data.sent_up[lines_backward].sum(axis=1)\n            )\n        else:  # POST_LOSS\n            return (\n                    flow_data.sent_up[lines_forward].sum(axis=1) -\n                    flow_data.received_down[lines_forward].sum(axis=1) +\n                    flow_data.sent_down[lines_backward].sum(axis=1) -\n                    flow_data.received_up[lines_backward].sum(axis=1)\n            )\n\n    def get_trade_balance(\n            self,\n            flow_data: NetworkLineFlowsData,\n            flow_type: FlowType = FlowType.POST_LOSS\n    ) -&gt; pd.DataFrame:\n        flows_list = []\n        column_level_names = [self.primary_name, self.partner_name, \"variable\"]\n\n        for primary in self.get_all_regions():\n            for secondary in self.get_region_neighbors(primary):\n                net_exp = self._get_net_exp_for_couple(primary, secondary, flow_data, flow_type)\n                df = pd.concat(\n                    {\n                        (primary, secondary, self.NET_EXP_VAR): net_exp,\n                        (primary, secondary, self.EXP_VAR): net_exp.clip(0),\n                        (primary, secondary, self.IMP_VAR): net_exp.clip(None, 0).abs(),\n                    },\n                    axis=1,\n                    names=column_level_names,\n                )\n                flows_list.append(df)\n\n        if not flows_list:\n            return pd.DataFrame(\n                index=flow_data.sent_up.index,\n                columns=pd.MultiIndex.from_tuples([], names=column_level_names)\n            )\n\n        return pd.concat(flows_list, axis=1)\n\n    def get_region_neighbors(self, region: str) -&gt; set:\n        return set(self.agg_region_graph.neighbors(region))\n\n    def get_all_regions(self) -&gt; set:\n        return set(self.agg_region_graph.nodes())\n\n    @property\n    def primary_name(self) -&gt; str:\n        return f\"primary_{self.agg_region_column}\"\n\n    @property\n    def partner_name(self) -&gt; str:\n        return f\"partner_{self.agg_region_column}\"\n\n    def aggregate_trade_balance_to_primary_level(self, trade_balance_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Reduces three-level trade balance DataFrame to primary region and variable only.\"\"\"\n        if trade_balance_df.columns.nlevels != 3:\n            raise ValueError(\"Input DataFrame must have three column levels\")\n        if trade_balance_df.columns.names != [self.primary_name, self.partner_name, \"variable\"]:\n            raise ValueError(\"Input DataFrame must be in format from aggregate_flows\")\n\n        return trade_balance_df.T.groupby(level=[self.primary_name, \"variable\"]).sum().T\n\n    def get_net_position_per_primary_level(self, trade_balance_df: pd.DataFrame) -&gt; pd.DataFrame:\n        return self.aggregate_trade_balance_to_primary_level(trade_balance_df).xs(self.NET_EXP_VAR, level=-1, axis=1)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/energy_data_handling/variable_utils/regional_trade_balance/#mesqual.energy_data_handling.variable_utils.regional_trade_balance_calculator.RegionalTradeBalanceCalculator.aggregate_trade_balance_to_primary_level","title":"aggregate_trade_balance_to_primary_level","text":"<pre><code>aggregate_trade_balance_to_primary_level(trade_balance_df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Reduces three-level trade balance DataFrame to primary region and variable only.</p> Source code in <code>submodules/mesqual/mesqual/energy_data_handling/variable_utils/regional_trade_balance_calculator.py</code> <pre><code>def aggregate_trade_balance_to_primary_level(self, trade_balance_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Reduces three-level trade balance DataFrame to primary region and variable only.\"\"\"\n    if trade_balance_df.columns.nlevels != 3:\n        raise ValueError(\"Input DataFrame must have three column levels\")\n    if trade_balance_df.columns.names != [self.primary_name, self.partner_name, \"variable\"]:\n        raise ValueError(\"Input DataFrame must be in format from aggregate_flows\")\n\n    return trade_balance_df.T.groupby(level=[self.primary_name, \"variable\"]).sum().T\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/","title":"MESQUAL KPI System","text":"<p>MESQUAL KPI System V2 (kpis)</p> <p>A high-performance, attribute-rich KPI system for energy systems analysis.</p>"},{"location":"mesqual-package-documentation/api_reference/kpis/#mesqual.kpis--when-to-use-the-kpi-system-vs-direct-dataframe-processing","title":"When to Use the KPI System vs Direct DataFrame Processing:","text":"<p>The KPI framework is NOT required for basic metric extraction. For quick analysis, you can simply fetch time-series data and process it directly with pandas:</p> <pre><code>&gt;&gt;&gt; # Quick approach: Direct dataframe processing\n&gt;&gt;&gt; df = study.scen.fetch('BiddingZone.Results.market_price')\n&gt;&gt;&gt; mean_price = df.mean().unstack('dataset')  # Fast and straightforward\n</code></pre> <p>However, the KPI system provides unique advantages for complex workflows:</p> <pre><code>1. **Model Object Integration**: KPIs retain links to model object metadata,\n   enabling property-based filtering and queries (e.g., filter KPIs by bus\n   voltage level, generator carrier type, or custom model properties)\n\n2. **Folium Visualization Integration**: KPIs work seamlessly with MESQUAL's\n   map visualization system, automatically providing geographic context and\n   tooltips with related metrics\n\n3. **Related KPI Discovery**: Within a KPICollection, easily find related metrics\n   (e.g., same object/scenario but different aggregation, time period, or unit)\n\n4. **Advanced Unit Handling**: Automatic unit conversion and validation when\n   working with collections of heterogeneous KPIs (critical for multi-physics\n   analysis with mixed energy/power/price metrics)\n\n5. **Multi-Scenario Workflows**: Built-in support for comparison KPIs and\n   bulk computation across scenario collections with progress tracking\n</code></pre> <p>Use the KPI system when you need these features; use direct dataframe processing for quick, one-off calculations that stay within the pandas framework.</p> <p>Core Components:</p> <pre><code>- KPI: Single computed metric with rich metadata\n- KPIAttributes: Metadata container for filtering and grouping\n- KPICollection: Container with advanced filtering and export\n- KPIDefinition: Abstract base API for KPI specifications\n- KPIBuilder: Abstract base API for creating KPIDefinitions in bulk\n</code></pre> <p>Architecture and High Level Workflow:</p> <pre><code>KPIBuilder (instructions on how to create a set of definitions)\n    \u2193\nKPIDefinition (what to compute)\n    \u2193\nKPI (computed result + metadata)\n    \u2193\nKPICollection (filtering, querying, visualization)\n</code></pre> <p>Definitions:</p> <pre><code>- FlagAggKPIDefinition: Standard flag + aggregation KPIs (most common)\n- CustomKPIDefinition: Abstract base for study-specific computation logic\n- ComparisonKPIDefinition: Delta calculations between scenarios\n</code></pre> <p>Aggregations:</p> <pre><code>- Aggregations: Standard aggregation functions (Mean, Sum, Max, etc.)\n- ValueComparisons: Comparison operations (Increase, PercentageIncrease, etc.)\n- ArithmeticValueOperations: Arithmetic operations (Product, Division, etc.)\n</code></pre> <p>Example Usage - Standard KPIs:</p> <pre><code>&gt;&gt;&gt; from mesqual.kpis import (\n...     KPI, KPICollection, FlagAggKPIDefinition, Aggregations\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create definition for standard aggregation\n&gt;&gt;&gt; definition = FlagAggKPIDefinition(\n...     flag='BZ.Results.market_price',\n...     aggregation=Aggregations.Mean\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Generate and add KPIs to a dataset\n&gt;&gt;&gt; dataset: Dataset\n&gt;&gt;&gt; dataset.add_kpis_from_definitions(definition)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Access collection with filtering and unit handling\n&gt;&gt;&gt; collection = dataset.kpi_collection\n&gt;&gt;&gt; german_kpis = collection.filter_by_model_properties(properties={'country': 'DE'})\n&gt;&gt;&gt; df = german_kpis.to_dataframe(unit_handling='auto_convert')\n</code></pre> <p>Example Usage - Custom KPIs:</p> <p>For study-specific calculations, subclass CustomKPIDefinition and implement either compute_for_object() or compute_batch() plus get_unit():</p> <pre><code>&gt;&gt;&gt; from mesqual.kpis import CustomKPIDefinition\n&gt;&gt;&gt; from mesqual.units import Units\n&gt;&gt;&gt;\n&gt;&gt;&gt; class GeneratorCapacityFactor(CustomKPIDefinition):\n...     '''Custom KPI calculating system efficiency per generator.'''\n...\n...     def compute_for_object(self, dataset, object_name):\n...         # Fetch data for specific generator\n...         generation = dataset.fetch('generators_t.p')[object_name].sum()\n...         capacity = dataset.fetch('generators').loc[object_name, 'p_nom']\n...         return (generation / (capacity * 8760)) * 100\n...\n...     def get_unit(self):\n...         return Units.percent\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use like standard definitions\n&gt;&gt;&gt; custom_def = SystemEfficiencyKPI(\n...     flag='generators_t.p',\n...     aggregation=None  # Optional metadata\n... )\n&gt;&gt;&gt; study.scen.add_kpis_from_definitions_to_all_child_datasets(custom_def)\n</code></pre> <p>Note: For many custom metrics, it's often cleaner to first create a study-specific flag (variable) via a custom flag interpreter, then apply standard FlagAggKPIDefinition aggregations to that flag. This keeps computation logic in the interpreter layer and enables you to fetch the flag also as a time-series or outside of the KPI framework.</p>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/","title":"Aggregations","text":""},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations","title":"Aggregations","text":"<p>Standard aggregations for batch KPI computation.</p> <p>All aggregations operate column-wise on DataFrames and return a Series with one value per column.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({'obj_1': [10, 20, 30], 'obj_2': [15, 25, 35]})\n&gt;&gt;&gt; Aggregations.Mean(df)\n    pd.Series({'obj_1': 20.0, 'obj_2': 25.0})\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>class Aggregations:\n    \"\"\"\n    Standard aggregations for batch KPI computation.\n\n    All aggregations operate column-wise on DataFrames and return\n    a Series with one value per column.\n\n    Example:\n\n        &gt;&gt;&gt; df = pd.DataFrame({'obj_1': [10, 20, 30], 'obj_2': [15, 25, 35]})\n        &gt;&gt;&gt; Aggregations.Mean(df)\n            pd.Series({'obj_1': 20.0, 'obj_2': 25.0})\n    \"\"\"\n\n    # Basic statistical aggregations\n    Sum = Aggregation(\n        'Sum',\n        lambda df: _column_wise_sum(_ensure_frame_format(df))\n    )\n    Total = Aggregation(\n        'Total',\n        lambda df: _column_wise_sum(_ensure_frame_format(df))\n    )\n    Mean = Aggregation(\n        'Mean',\n        lambda df: _ensure_frame_format(df).mean(axis=0)\n    )\n    Max = Aggregation(\n        'Max',\n        lambda df: _ensure_frame_format(df).max(axis=0)\n    )\n    Min = Aggregation(\n        'Min',\n        lambda df: _ensure_frame_format(df).min(axis=0)\n    )\n\n    # Time-based aggregations\n    AnnualizedSum = Aggregation('AnnualizedSum', _annualized_sum_batch)\n    DailySum = Aggregation('DailySum', _daily_sum_batch)\n\n    # Absolute value aggregations\n    AbsSum = Aggregation(\n        'AbsSum',\n        lambda df: _ensure_frame_format(df).abs().sum(axis=0)\n    )\n    AbsMax = Aggregation(\n        'AbsMax',\n        lambda df: _ensure_frame_format(df).abs().max(axis=0)\n    )\n    AbsMean = Aggregation(\n        'AbsMean',\n        lambda df: _ensure_frame_format(df).abs().mean(axis=0)\n    )\n    AbsMin = Aggregation(\n        'AbsMin',\n        lambda df: _ensure_frame_format(df).abs().min(axis=0)\n    )\n\n    # Clipped aggregations (positive/negative only)\n    SumGeqZero = Aggregation(\n        'SumGeqZero',\n        lambda df: _ensure_frame_format(df).clip(0, None).sum(axis=0)\n    )\n    SumLeqZero = Aggregation(\n        'SumLeqZero',\n        lambda df: _ensure_frame_format(df).clip(None, 0).sum(axis=0)\n    )\n    MeanGeqZero = Aggregation(\n        'MeanGeqZero',\n        lambda df: _ensure_frame_format(df).clip(0, None).mean(axis=0)\n    )\n    MeanLeqZero = Aggregation(\n        'MeanLeqZero',\n        lambda df: _ensure_frame_format(df).clip(None, 0).mean(axis=0)\n    )\n\n    # MTU (Market Time Unit) aggregations - count timesteps\n    MTUsWithNaN = Aggregation(\n        'MTUsWithNaN',\n        lambda df: _ensure_frame_format(df).isna().sum(axis=0),\n        Units.MTU\n    )\n    MTUsNonZero = Aggregation(\n        'MTUsNonZero',\n        lambda df: ((_ensure_frame_format(df) != 0) &amp; ~_ensure_frame_format(df).isna()).sum(axis=0),\n        Units.MTU\n    )\n    MTUsEqZero = Aggregation(\n        'MTUsEqZero',\n        lambda df: (_ensure_frame_format(df) == 0).sum(axis=0),\n        Units.MTU\n    )\n    MTUsAboveZero = Aggregation(\n        'MTUsAboveZero',\n        lambda df: (_ensure_frame_format(df) &gt; 0).sum(axis=0),\n        Units.MTU\n    )\n    MTUsBelowZero = Aggregation(\n        'MTUsBelowZero',\n        lambda df: (_ensure_frame_format(df) &lt; 0).sum(axis=0),\n        Units.MTU\n    )\n\n    # Parameterized MTU aggregations\n    @staticmethod\n    def MTUsAboveX(x: float) -&gt; Aggregation:\n        \"\"\"\n        Count timesteps where value is above threshold.\n\n        Args:\n            x: Threshold value\n\n        Returns:\n            Aggregation that counts timesteps &gt; x per column\n        \"\"\"\n        return Aggregation(\n            f'MTUsAbove{x}',\n            lambda df: (_ensure_frame_format(df) &gt; x).sum(axis=0),\n            Units.MTU\n        )\n\n    @staticmethod\n    def MTUsBelowX(x: float) -&gt; Aggregation:\n        \"\"\"\n        Count timesteps where value is below threshold.\n\n        Args:\n            x: Threshold value\n\n        Returns:\n            Aggregation that counts timesteps &lt; x per column\n        \"\"\"\n        return Aggregation(\n            f'MTUsBelow{x}',\n            lambda df: (_ensure_frame_format(df) &lt; x).sum(axis=0),\n            Units.MTU\n        )\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.Sum","title":"Sum  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Sum = Aggregation('Sum', lambda df: _column_wise_sum(_ensure_frame_format(df)))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.Total","title":"Total  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Total = Aggregation('Total', lambda df: _column_wise_sum(_ensure_frame_format(df)))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.Mean","title":"Mean  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Mean = Aggregation('Mean', lambda df: mean(axis=0))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.Max","title":"Max  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Max = Aggregation('Max', lambda df: max(axis=0))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.Min","title":"Min  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Min = Aggregation('Min', lambda df: min(axis=0))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.AnnualizedSum","title":"AnnualizedSum  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AnnualizedSum = Aggregation('AnnualizedSum', _annualized_sum_batch)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.DailySum","title":"DailySum  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DailySum = Aggregation('DailySum', _daily_sum_batch)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.AbsSum","title":"AbsSum  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AbsSum = Aggregation('AbsSum', lambda df: sum(axis=0))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.AbsMax","title":"AbsMax  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AbsMax = Aggregation('AbsMax', lambda df: max(axis=0))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.AbsMean","title":"AbsMean  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AbsMean = Aggregation('AbsMean', lambda df: mean(axis=0))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.AbsMin","title":"AbsMin  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AbsMin = Aggregation('AbsMin', lambda df: min(axis=0))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.SumGeqZero","title":"SumGeqZero  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SumGeqZero = Aggregation('SumGeqZero', lambda df: sum(axis=0))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.SumLeqZero","title":"SumLeqZero  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SumLeqZero = Aggregation('SumLeqZero', lambda df: sum(axis=0))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.MeanGeqZero","title":"MeanGeqZero  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MeanGeqZero = Aggregation('MeanGeqZero', lambda df: mean(axis=0))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.MeanLeqZero","title":"MeanLeqZero  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MeanLeqZero = Aggregation('MeanLeqZero', lambda df: mean(axis=0))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.MTUsWithNaN","title":"MTUsWithNaN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MTUsWithNaN = Aggregation('MTUsWithNaN', lambda df: sum(axis=0), MTU)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.MTUsNonZero","title":"MTUsNonZero  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MTUsNonZero = Aggregation('MTUsNonZero', lambda df: sum(axis=0), MTU)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.MTUsEqZero","title":"MTUsEqZero  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MTUsEqZero = Aggregation('MTUsEqZero', lambda df: sum(axis=0), MTU)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.MTUsAboveZero","title":"MTUsAboveZero  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MTUsAboveZero = Aggregation('MTUsAboveZero', lambda df: sum(axis=0), MTU)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.MTUsBelowZero","title":"MTUsBelowZero  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MTUsBelowZero = Aggregation('MTUsBelowZero', lambda df: sum(axis=0), MTU)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.MTUsAboveX","title":"MTUsAboveX  <code>staticmethod</code>","text":"<pre><code>MTUsAboveX(x: float) -&gt; Aggregation\n</code></pre> <p>Count timesteps where value is above threshold.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>Threshold value</p> required <p>Returns:</p> Type Description <code>Aggregation</code> <p>Aggregation that counts timesteps &gt; x per column</p> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>@staticmethod\ndef MTUsAboveX(x: float) -&gt; Aggregation:\n    \"\"\"\n    Count timesteps where value is above threshold.\n\n    Args:\n        x: Threshold value\n\n    Returns:\n        Aggregation that counts timesteps &gt; x per column\n    \"\"\"\n    return Aggregation(\n        f'MTUsAbove{x}',\n        lambda df: (_ensure_frame_format(df) &gt; x).sum(axis=0),\n        Units.MTU\n    )\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregations.MTUsBelowX","title":"MTUsBelowX  <code>staticmethod</code>","text":"<pre><code>MTUsBelowX(x: float) -&gt; Aggregation\n</code></pre> <p>Count timesteps where value is below threshold.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>Threshold value</p> required <p>Returns:</p> Type Description <code>Aggregation</code> <p>Aggregation that counts timesteps &lt; x per column</p> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>@staticmethod\ndef MTUsBelowX(x: float) -&gt; Aggregation:\n    \"\"\"\n    Count timesteps where value is below threshold.\n\n    Args:\n        x: Threshold value\n\n    Returns:\n        Aggregation that counts timesteps &lt; x per column\n    \"\"\"\n    return Aggregation(\n        f'MTUsBelow{x}',\n        lambda df: (_ensure_frame_format(df) &lt; x).sum(axis=0),\n        Units.MTU\n    )\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ValueComparisons","title":"ValueComparisons","text":"<p>Standard comparison operations for KPIs.</p> <p>Used to compare variation vs reference scenarios.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; ref_value = 100\n&gt;&gt;&gt; var_value = 120\n&gt;&gt;&gt; ValueComparisons.Increase(var_value, ref_value)  # 20\n&gt;&gt;&gt; ValueComparisons.PercentageIncrease(var_value, ref_value)  # 20.0\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>class ValueComparisons:\n    \"\"\"\n    Standard comparison operations for KPIs.\n\n    Used to compare variation vs reference scenarios.\n\n    Example:\n\n        &gt;&gt;&gt; ref_value = 100\n        &gt;&gt;&gt; var_value = 120\n        &gt;&gt;&gt; ValueComparisons.Increase(var_value, ref_value)  # 20\n        &gt;&gt;&gt; ValueComparisons.PercentageIncrease(var_value, ref_value)  # 20.0\n    \"\"\"\n\n    Increase = ValueComparison(\"Increase\", lambda var, ref: var - ref)\n    Decrease = ValueComparison(\"Decrease\", lambda var, ref: ref - var)\n    PercentageIncrease = ValueComparison(\n        \"PercentageIncrease\",\n        lambda var, ref: np.inf * np.sign(var) if ref == 0 else (var - ref) / ref * 100,\n        Units.percent\n    )\n    PercentageDecrease = ValueComparison(\n        \"PercentageDecrease\",\n        lambda var, ref: -1 * np.inf * np.sign(var) if ref == 0 else (ref - var) / ref * 100,\n        Units.percent\n    )\n    Share = ValueComparison(\n        \"Share\",\n        lambda var, ref: np.inf * np.sign(var) if ref == 0 else var / ref * 100,\n        Units.percent\n    )\n    Delta = ValueComparison(\"Delta\", lambda var, ref: var - ref)\n    Diff = ValueComparison(\"Diff\", lambda var, ref: var - ref)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ValueComparisons.Increase","title":"Increase  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Increase = ValueComparison('Increase', lambda var, ref: var - ref)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ValueComparisons.Decrease","title":"Decrease  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Decrease = ValueComparison('Decrease', lambda var, ref: ref - var)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ValueComparisons.PercentageIncrease","title":"PercentageIncrease  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PercentageIncrease = ValueComparison('PercentageIncrease', lambda var, ref: inf * sign(var) if ref == 0 else (var - ref) / ref * 100, percent)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ValueComparisons.PercentageDecrease","title":"PercentageDecrease  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PercentageDecrease = ValueComparison('PercentageDecrease', lambda var, ref: -1 * inf * sign(var) if ref == 0 else (ref - var) / ref * 100, percent)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ValueComparisons.Share","title":"Share  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Share = ValueComparison('Share', lambda var, ref: inf * sign(var) if ref == 0 else var / ref * 100, percent)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ValueComparisons.Delta","title":"Delta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Delta = ValueComparison('Delta', lambda var, ref: var - ref)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ValueComparisons.Diff","title":"Diff  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Diff = ValueComparison('Diff', lambda var, ref: var - ref)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ArithmeticValueOperations","title":"ArithmeticValueOperations","text":"<p>Standard arithmetic operations for KPIs.</p> <p>Used to derive new KPI values from existing ones.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; value1 = 100\n&gt;&gt;&gt; value2 = 50\n&gt;&gt;&gt; ArithmeticValueOperations.Sum(value1, value2)  # 150\n&gt;&gt;&gt; ArithmeticValueOperations.Division(value1, value2)  # 2.0\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>class ArithmeticValueOperations:\n    \"\"\"\n    Standard arithmetic operations for KPIs.\n\n    Used to derive new KPI values from existing ones.\n\n    Example:\n\n        &gt;&gt;&gt; value1 = 100\n        &gt;&gt;&gt; value2 = 50\n        &gt;&gt;&gt; ArithmeticValueOperations.Sum(value1, value2)  # 150\n        &gt;&gt;&gt; ArithmeticValueOperations.Division(value1, value2)  # 2.0\n    \"\"\"\n\n    Product = ArithmeticValueOperation(\"Product\", lambda var, ref: var * ref)\n    Division = ArithmeticValueOperation(\n        \"Division\",\n        lambda var, ref: np.inf * np.sign(var) if ref == 0 else var / ref\n    )\n    Share = ArithmeticValueOperation(\n        \"Share\",\n        lambda var, ref: np.inf * np.sign(var) if ref == 0 else var / ref * 100,\n        Units.percent\n    )\n    Sum = ArithmeticValueOperation(\"Sum\", lambda var, ref: var + ref)\n    Diff = ArithmeticValueOperation(\"Diff\", lambda var, ref: var - ref)\n    Delta = ArithmeticValueOperation(\"Delta\", lambda var, ref: var - ref)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ArithmeticValueOperations.Product","title":"Product  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Product = ArithmeticValueOperation('Product', lambda var, ref: var * ref)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ArithmeticValueOperations.Division","title":"Division  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Division = ArithmeticValueOperation('Division', lambda var, ref: inf * sign(var) if ref == 0 else var / ref)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ArithmeticValueOperations.Share","title":"Share  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Share = ArithmeticValueOperation('Share', lambda var, ref: inf * sign(var) if ref == 0 else var / ref * 100, percent)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ArithmeticValueOperations.Sum","title":"Sum  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Sum = ArithmeticValueOperation('Sum', lambda var, ref: var + ref)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ArithmeticValueOperations.Diff","title":"Diff  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Diff = ArithmeticValueOperation('Diff', lambda var, ref: var - ref)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ArithmeticValueOperations.Delta","title":"Delta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Delta = ArithmeticValueOperation('Delta', lambda var, ref: var - ref)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregation","title":"Aggregation  <code>dataclass</code>","text":"<p>Aggregation function for batch KPI computation.</p> <p>Unlike the old system, these aggregations:</p> <pre><code>- Operate on entire DataFrames\n- Return Series with one value per column (one per object)\n- Enable batch computation across all objects\n</code></pre> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Human-readable name of the aggregation</p> <code>agg</code> <code>Callable[[DataFrame], Series]</code> <p>Function that takes DataFrame and returns Series</p> <code>unit</code> <code>Unit</code> <p>Optional unit override (e.g., MTU for time-based counts)</p> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>@dataclass\nclass Aggregation:\n    \"\"\"\n    Aggregation function for batch KPI computation.\n\n    Unlike the old system, these aggregations:\n\n        - Operate on entire DataFrames\n        - Return Series with one value per column (one per object)\n        - Enable batch computation across all objects\n\n    Attributes:\n        name: Human-readable name of the aggregation\n        agg: Function that takes DataFrame and returns Series\n        unit: Optional unit override (e.g., MTU for time-based counts)\n    \"\"\"\n    name: str\n    agg: Callable[[pd.DataFrame], pd.Series]\n    unit: Units.Unit = None\n\n    def __call__(self, df: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Apply aggregation to DataFrame, return Series with one value per column.\"\"\"\n        return self.agg(df)\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return hash(self.name)\n\n    def __eq__(self, other) -&gt; bool:\n        return isinstance(other, Aggregation) and self.name == other.name\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregation.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregation.agg","title":"agg  <code>instance-attribute</code>","text":"<pre><code>agg: Callable[[DataFrame], Series]\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregation.unit","title":"unit  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>unit: Unit = None\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregation.__init__","title":"__init__","text":"<pre><code>__init__(name: str, agg: Callable[[DataFrame], Series], unit: Unit = None) -&gt; None\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregation.__call__","title":"__call__","text":"<pre><code>__call__(df: DataFrame) -&gt; Series\n</code></pre> <p>Apply aggregation to DataFrame, return Series with one value per column.</p> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>def __call__(self, df: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Apply aggregation to DataFrame, return Series with one value per column.\"\"\"\n    return self.agg(df)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregation.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>def __str__(self):\n    return self.name\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregation.__hash__","title":"__hash__","text":"<pre><code>__hash__()\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>def __hash__(self):\n    return hash(self.name)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.Aggregation.__eq__","title":"__eq__","text":"<pre><code>__eq__(other) -&gt; bool\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>def __eq__(self, other) -&gt; bool:\n    return isinstance(other, Aggregation) and self.name == other.name\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.OperationOfTwoValues","title":"OperationOfTwoValues  <code>dataclass</code>","text":"<p>Operation that combines two scalar values.</p> <p>Used for comparison KPIs (e.g., difference between scenarios) and arithmetic operations between KPI values.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Human-readable name of the operation</p> <code>agg</code> <code>Callable[[float | int, float | int], float | int]</code> <p>Function that takes two scalars and returns a scalar</p> <code>unit</code> <code>Unit</code> <p>Optional unit override</p> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>@dataclass\nclass OperationOfTwoValues:\n    \"\"\"\n    Operation that combines two scalar values.\n\n    Used for comparison KPIs (e.g., difference between scenarios)\n    and arithmetic operations between KPI values.\n\n    Attributes:\n        name: Human-readable name of the operation\n        agg: Function that takes two scalars and returns a scalar\n        unit: Optional unit override\n    \"\"\"\n    name: str\n    agg: Callable[[float | int, float | int], float | int]\n    unit: Units.Unit = None\n\n    def __call__(self, variation_value: float | int, reference_value: float | int) -&gt; float | int:\n        \"\"\"Apply operation to two values.\"\"\"\n        return self.agg(variation_value, reference_value)\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return hash(self.name)\n\n    def __eq__(self, other) -&gt; bool:\n        return isinstance(other, OperationOfTwoValues) and self.name == other.name\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.OperationOfTwoValues.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.OperationOfTwoValues.agg","title":"agg  <code>instance-attribute</code>","text":"<pre><code>agg: Callable[[float | int, float | int], float | int]\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.OperationOfTwoValues.unit","title":"unit  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>unit: Unit = None\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.OperationOfTwoValues.__init__","title":"__init__","text":"<pre><code>__init__(name: str, agg: Callable[[float | int, float | int], float | int], unit: Unit = None) -&gt; None\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.OperationOfTwoValues.__call__","title":"__call__","text":"<pre><code>__call__(variation_value: float | int, reference_value: float | int) -&gt; float | int\n</code></pre> <p>Apply operation to two values.</p> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>def __call__(self, variation_value: float | int, reference_value: float | int) -&gt; float | int:\n    \"\"\"Apply operation to two values.\"\"\"\n    return self.agg(variation_value, reference_value)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.OperationOfTwoValues.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>def __str__(self):\n    return self.name\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.OperationOfTwoValues.__hash__","title":"__hash__","text":"<pre><code>__hash__()\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>def __hash__(self):\n    return hash(self.name)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.OperationOfTwoValues.__eq__","title":"__eq__","text":"<pre><code>__eq__(other) -&gt; bool\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>def __eq__(self, other) -&gt; bool:\n    return isinstance(other, OperationOfTwoValues) and self.name == other.name\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ValueComparison","title":"ValueComparison  <code>dataclass</code>","text":"<p>               Bases: <code>OperationOfTwoValues</code></p> <p>Comparison operation between two KPI values (e.g., difference, percentage change).</p> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>class ValueComparison(OperationOfTwoValues):\n    \"\"\"Comparison operation between two KPI values (e.g., difference, percentage change).\"\"\"\n    pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/aggregations/#mesqual.kpis.aggregations.ArithmeticValueOperation","title":"ArithmeticValueOperation  <code>dataclass</code>","text":"<p>               Bases: <code>OperationOfTwoValues</code></p> <p>Arithmetic operation between two KPI values (e.g., sum, product).</p> Source code in <code>submodules/mesqual/mesqual/kpis/aggregations.py</code> <pre><code>class ArithmeticValueOperation(OperationOfTwoValues):\n    \"\"\"Arithmetic operation between two KPI values (e.g., sum, product).\"\"\"\n    pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/","title":"KPI Collection","text":""},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection","title":"KPICollection","text":"<p>Collection of KPIs with filtering, grouping, and export capabilities.</p> <p>Provides rich query interface for finding related KPIs and organizing KPIs for visualization and reporting.</p> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>class KPICollection:\n    \"\"\"\n    Collection of KPIs with filtering, grouping, and export capabilities.\n\n    Provides rich query interface for finding related KPIs and organizing\n    KPIs for visualization and reporting.\n    \"\"\"\n\n    def __init__(self, kpis: Iterable[KPI] | None = None):\n        \"\"\"\n        Initialize KPI collection.\n\n        Args:\n            kpis: Optional list of KPIs to initialize with\n        \"\"\"\n        self._kpis: list[KPI] = list(kpis) if kpis is not None else []\n\n    def add(self, kpi: KPI) -&gt; None:\n        \"\"\"Add single KPI to collection.\"\"\"\n        self._kpis.append(kpi)\n\n    def extend(self, kpis: list[KPI]) -&gt; None:\n        \"\"\"Add multiple KPIs to collection.\"\"\"\n        self._kpis.extend(kpis)\n\n    def clear(self) -&gt; None:\n        \"\"\"Remove all KPIs from collection.\"\"\"\n        self._kpis.clear()\n\n    def filter(self, **attribute_filters) -&gt; KPICollection:\n        \"\"\"\n        Filter KPIs by exact attribute matches.\n\n        Args:\n            **attribute_filters: Attribute name-value pairs to filter by\n\n        Returns:\n            New KPICollection with filtered KPIs\n\n        Examples:\n\n            collection.filter(flag='BZ.Results.market_price')\n            collection.filter(object_name='DE-LU', aggregation=Aggregations.Mean)\n            collection.filter(dataset_type='scenario')\n        \"\"\"\n        filtered = []\n        for kpi in self._kpis:\n            primitive = kpi.to_dict(primitive_values=True)\n            non_primitive = kpi.to_dict(primitive_values=False)\n            match = all(\n                primitive.get(attr, None) == value\n                or non_primitive.get(attr, None) == value\n                for attr, value in attribute_filters.items()\n            )\n            if match:\n                filtered.append(kpi)\n        return KPICollection(filtered)\n\n    def filter_by_model_properties(\n            self,\n            properties: dict[str, Any] | None = None,\n            query_expr: str | None = None,\n            filter_funcs: dict[str, Callable[[Any], bool]] | None = None\n    ) -&gt; KPICollection:\n        \"\"\"\n        Filter KPIs by properties from their model objects.\n\n        Three modes of operation (can be combined with AND logic):\n            1. Property filters: Exact match or list membership\n            2. Query expression: Pandas query string\n            3. Filter functions: Custom functions applied to properties\n\n        All conditions across all modes are combined with AND logic.\n\n        Args:\n            properties: Dict of property names to values. Scalars for exact match,\n                lists for membership checks.\n            query_expr: Pandas query expression (uses engine=\"python\")\n            filter_funcs: Dict of property names to filter functions\n\n        Returns:\n            New KPICollection with filtered KPIs\n\n        Examples:\n\n            # Property filter - exact match and list membership\n            collection.filter_by_model_properties(\n                properties={'country': 'DE', 'type': ['wind', 'solar']}\n            )\n\n            # Query expression\n            collection.filter_by_model_properties(\n                query_expr='country == \"DE\" and voltage_kV &gt; 200'\n            )\n\n            # Custom filter function\n            collection.filter_by_model_properties(\n                filter_funcs={'voltage_kV': lambda x: x &gt; 200}\n            )\n\n            # Combined - properties AND query\n            collection.filter_by_model_properties(\n                properties={'country': 'DE'},\n                query_expr='voltage_kV &gt; 200'\n            )\n\n            # Combined - properties AND filter functions\n            collection.filter_by_model_properties(\n                properties={'type': ['wind', 'solar']},\n                filter_funcs={'voltage_kV': lambda x: x &gt; 200}\n            )\n        \"\"\"\n        filtered = []\n\n        for kpi in self._kpis:\n            if kpi.attributes.object_name is None:\n                continue\n\n            obj_info = kpi.get_object_info_from_model()\n\n            if self._series_passes_property_filters(obj_info, properties, query_expr, filter_funcs):\n                filtered.append(kpi)\n\n        return KPICollection(filtered)\n\n    def filter_by_kpi_attributes(\n            self,\n            attributes: dict[str, Any] | None = None,\n            query_expr: str | None = None,\n            filter_funcs: dict[str, Callable[[Any], bool]] | None = None\n    ) -&gt; KPICollection:\n        \"\"\"\n        Filter KPIs by their attributes.\n\n        Three modes of operation (can be combined with AND logic):\n            1. Property filters: Exact match or list membership\n            2. Query expression: Pandas query string\n            3. Filter functions: Custom functions applied to attributes\n\n        All conditions across all modes are combined with AND logic.\n\n        Args:\n            attributes: Dict of attribute names to values. Scalars for exact match,\n                lists for membership checks.\n            query_expr: Pandas query expression (uses engine=\"python\")\n            filter_funcs: Dict of attribute names to filter functions\n\n        Returns:\n            New KPICollection with filtered KPIs\n\n        Examples:\n\n            # Property filter - exact match and list membership\n            collection.filter_by_kpi_attributes(\n                attributes={'flag': 'BZ.Results.price'}\n            )\n\n            # Query expression\n            collection.filter_by_kpi_attributes(\n                query_expr='flag.str.contains(\"price\") and value &gt; 100'\n            )\n\n            # Custom filter function\n            collection.filter_by_kpi_attributes(\n                filter_funcs={'value': lambda x: x &gt; 100}\n            )\n\n            # Combined - properties AND query\n            collection.filter_by_kpi_attributes(\n                attributes={'flag': 'BZ.Results.price'},\n                query_expr='value &gt; 100'\n            )\n        \"\"\"\n        filtered = []\n\n        for kpi in self._kpis:\n            kpi_attrs = pd.Series(kpi.to_dict(primitive_values=True))\n\n            if self._series_passes_property_filters(kpi_attrs, attributes, query_expr, filter_funcs):\n                filtered.append(kpi)\n\n        return KPICollection(filtered)\n\n    @staticmethod\n    def _series_passes_property_filters(\n            data: pd.Series,\n            properties: dict[str, Any] | None = None,\n            query_expr: str | None = None,\n            filter_funcs: dict[str, Callable[[Any], bool]] | None = None\n    ) -&gt; bool:\n        \"\"\"\n        Apply property filters to a data Series.\n\n        Private method used by both filter_by_model_properties and filter_by_kpi_attributes.\n\n        Args:\n            data: Series containing property/attribute data\n            properties: Dict of property names to values for exact match or list membership\n            query_expr: Pandas query expression\n            filter_funcs: Dict of property names to filter functions\n\n        Returns:\n            True if data passes all filters, False otherwise\n        \"\"\"\n        if properties is not None:\n            for prop_name, expected_value in properties.items():\n                if prop_name not in data.index:\n                    return False\n\n                actual_value = data[prop_name]\n\n                if isinstance(expected_value, list):\n                    if actual_value not in expected_value:\n                        return False\n                else:\n                    if actual_value != expected_value:\n                        return False\n\n        if query_expr is not None:\n            temp_df = pd.DataFrame({k: [v] for k, v in data.items()})\n            try:\n                result = temp_df.query(query_expr, engine=\"python\")\n                if result.empty:\n                    return False\n            except:\n                return False\n\n        if filter_funcs is not None:\n            for prop_name, filter_function in filter_funcs.items():\n                if prop_name not in data.index:\n                    return False\n\n                try:\n                    if not filter_function(data[prop_name]):\n                        return False\n                except:\n                    return False\n\n        return True\n\n    def group_by(self, *attributes: str) -&gt; dict[tuple, KPICollection]:\n        \"\"\"\n        Group KPIs by attribute values.\n\n        Args:\n            *attributes: Attribute names to group by\n\n        Returns:\n            Dictionary mapping attribute value tuples to KPICollections\n\n        Example:\n\n            &gt;&gt;&gt; groups = collection.group_by('flag', 'aggregation')\n                {('BZ.Results.market_price', Aggregations.Mean): KPICollection(...), ...}\n        \"\"\"\n        groups = defaultdict(list)\n\n        for kpi in self._kpis:\n            key = tuple(\n                getattr(kpi.attributes, attr, None)\n                or kpi.attributes.dataset_attributes.get(attr)\n                for attr in attributes\n            )\n            groups[key].append(kpi)\n\n        return {k: KPICollection(v) for k, v in groups.items()}\n\n    def get_related(\n        self,\n        reference_kpi: KPI,\n        vary_attributes: list[str],\n        exclude_attributes: list[str] | None = None\n    ) -&gt; 'KPICollection':\n        \"\"\"\n        Find KPIs related to reference, varying only specified attributes.\n\n        Args:\n            reference_kpi: The KPI to find relatives for\n            vary_attributes: Attributes that can differ (e.g., ['aggregation'])\n            exclude_attributes: Attributes to ignore in comparison (e.g., ['name_prefix'])\n\n        Returns:\n            New KPICollection with related KPIs\n\n        Examples:\n\n            # Same object/flag, different aggregations\n            collection.get_related(my_kpi, vary_attributes=['aggregation'])\n\n            # Same object/flag/agg, different datasets\n            collection.get_related(my_kpi, vary_attributes=['dataset_name'])\n\n            # Same flag/agg, different objects\n            collection.get_related(my_kpi, vary_attributes=['object_name'])\n        \"\"\"\n        exclude_attributes = exclude_attributes or ['name_prefix', 'name_suffix', 'custom_name']\n        ref_attrs = reference_kpi.attributes.as_dict()\n        related = []\n\n        for kpi in self._kpis:\n            if kpi is reference_kpi:\n                continue\n\n            kpi_attrs = kpi.attributes.as_dict()\n            match = True\n\n            for attr_name, ref_value in ref_attrs.items():\n                # Skip attributes we want to vary or exclude\n                if attr_name in vary_attributes or attr_name in exclude_attributes:\n                    continue\n\n                if kpi_attrs.get(attr_name) != ref_value:\n                    match = False\n                    break\n\n            if match:\n                related.append(kpi)\n\n        return KPICollection(related)\n\n    def get_all_attribute_values(self, attribute: str) -&gt; set:\n        \"\"\"\n        Get all unique values for a specific attribute across collection.\n\n        Args:\n            attribute: Attribute name to get values for\n\n        Returns:\n            Set of unique values for the attribute\n        \"\"\"\n        values = set()\n        for kpi in self._kpis:\n            val = getattr(kpi.attributes, attribute, None)\n            if val is None:\n                val = kpi.attributes.dataset_attributes.get(attribute)\n            if val is not None:\n                values.add(val)\n        return values\n\n    def get_all_kpi_attributes_and_value_sets(self, primitive_values: bool = False) -&gt; dict[str, set]:\n        \"\"\"\n        Get all attribute names and their unique value sets.\n\n        Used by KPIGroupingManager for intelligent grouping.\n\n        Args:\n            primitive_values: If True, convert values to primitives\n\n        Returns:\n            Dictionary mapping attribute names to sets of values\n        \"\"\"\n        attribute_sets = defaultdict(set)\n\n        for kpi in self._kpis:\n            attrs = kpi.attributes.as_dict(primitive_values=primitive_values)\n            for attr_name, attr_value in attrs.items():\n                if attr_value is not None:\n                    attribute_sets[attr_name].add(attr_value)\n\n        return dict(attribute_sets)\n\n    def get_in_common_kpi_attributes(self, primitive_values: bool = False) -&gt; dict:\n        \"\"\"\n        Get attributes that have same value across all KPIs in collection.\n\n        Args:\n            primitive_values: If True, convert values to primitives\n\n        Returns:\n            Dictionary of common attributes\n        \"\"\"\n        if self.empty:\n            return {}\n\n        # Start with first KPI's attributes\n        common = self._kpis[0].attributes.as_dict(primitive_values=primitive_values)\n\n        # Remove any that differ in subsequent KPIs\n        for kpi in self._kpis[1:]:\n            attrs = kpi.attributes.as_dict(primitive_values=primitive_values)\n            common = {\n                k: v for k, v in common.items()\n                if attrs.get(k) == v\n            }\n\n        return common\n\n    def to_dataframe(\n        self,\n        unit_handling: Literal['original', 'auto_convert', 'target', 'custom'] = 'original',\n        target_unit: Units.Unit | None = None,\n        target_units_by_group: dict[tuple, Units.Unit] | None = None,\n        group_by_attributes: list[str] | None = None,\n        normalize_to_collection: bool = False\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Export KPIs as DataFrame with flexible unit handling.\n\n        Args:\n            unit_handling: Strategy for unit conversion\n                - 'original': Keep original units\n                - 'auto_convert': Convert to pretty units per KPI\n                - 'target': Use single target_unit for all\n                - 'custom': Use target_units_by_group mapping\n            target_unit: Single target unit (when unit_handling='target')\n            target_units_by_group: Dict mapping group key \u2192 target unit\n            group_by_attributes: Attributes to group by for 'custom' mode\n            normalize_to_collection: If True, find common \"pretty\" unit across entire collection\n\n        Returns:\n            DataFrame with KPI data\n\n        Examples:\n            # Keep original units\n            df = collection.to_dataframe()\n\n            # Auto-convert each KPI to its own pretty unit\n            df = collection.to_dataframe(unit_handling='auto_convert')\n\n            # Convert all to single unit\n            df = collection.to_dataframe(\n                unit_handling='target',\n                target_unit=Units.MEUR\n            )\n\n            # Custom units per group\n            df = collection.to_dataframe(\n                unit_handling='custom',\n                group_by_attributes=['flag', 'aggregation'],\n                target_units_by_group={\n                    ('consumer_surplus', 'Sum'): Units.BEUR,\n                    ('producer_surplus', 'Sum'): Units.MEUR,\n                }\n            )\n\n            # Normalize to common pretty unit for collection\n            df = collection.to_dataframe(normalize_to_collection=True)\n        \"\"\"\n        data = []\n\n        # Normalize to collection: find common pretty unit\n        common_unit = None\n        if normalize_to_collection:\n            quantities = [kpi.quantity for kpi in self._kpis]\n            try:\n                common_unit = Units.get_common_pretty_unit_for_quantities(quantities)\n            except ValueError:\n                # Quantities have different dimensionalities, fall back to auto_convert\n                pass\n\n        for kpi in self._kpis:\n            quantity = kpi.quantity\n\n            # Apply unit conversion based on strategy\n            if normalize_to_collection and common_unit:\n                quantity = quantity.to(common_unit)\n\n            elif unit_handling == 'auto_convert':\n                quantity = Units.get_quantity_in_pretty_unit(quantity)\n\n            elif unit_handling == 'target' and target_unit:\n                quantity = quantity.to(target_unit)\n\n            elif unit_handling == 'custom' and target_units_by_group and group_by_attributes:\n                # Build group key\n                group_key = tuple(\n                    getattr(kpi.attributes, attr, None)\n                    for attr in group_by_attributes\n                )\n                if group_key in target_units_by_group:\n                    quantity = quantity.to(target_units_by_group[group_key])\n\n            row = {\n                'name': kpi.name,\n                **kpi.attributes.as_dict(primitive_values=True),\n                'value': quantity.magnitude,\n                'unit': str(quantity.units),  # Set after attributes to avoid being overwritten\n            }\n            data.append(row)\n\n        return pd.DataFrame(data)\n\n    @property\n    def kpis(self) -&gt; dict[str, KPI]:\n        return {kpi.name: kpi for kpi in self._kpis}\n\n    def __iter__(self):\n        \"\"\"Iterate over KPIs in collection.\"\"\"\n        return iter(self._kpis)\n\n    def __len__(self):\n        \"\"\"Number of KPIs in collection.\"\"\"\n        return len(self._kpis)\n\n    def __getitem__(self, index):\n        \"\"\"Get KPI by index.\"\"\"\n        return self._kpis[index]\n\n    def __contains__(self, item):\n        kpi_name = item.name\n        return any(kpi.name == kpi_name for kpi in self._kpis)\n\n    @property\n    def empty(self) -&gt; bool:\n        \"\"\"Check if collection is empty.\"\"\"\n        return len(self._kpis) == 0\n\n    @property\n    def size(self) -&gt; int:\n        \"\"\"Get number of KPIs in collection.\"\"\"\n        return len(self._kpis)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of collection.\"\"\"\n        return f\"KPICollection({self.size} KPIs)\"\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.empty","title":"empty  <code>property</code>","text":"<pre><code>empty: bool\n</code></pre> <p>Check if collection is empty.</p>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.size","title":"size  <code>property</code>","text":"<pre><code>size: int\n</code></pre> <p>Get number of KPIs in collection.</p>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.__init__","title":"__init__","text":"<pre><code>__init__(kpis: Iterable[KPI] | None = None)\n</code></pre> <p>Initialize KPI collection.</p> <p>Parameters:</p> Name Type Description Default <code>kpis</code> <code>Iterable[KPI] | None</code> <p>Optional list of KPIs to initialize with</p> <code>None</code> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def __init__(self, kpis: Iterable[KPI] | None = None):\n    \"\"\"\n    Initialize KPI collection.\n\n    Args:\n        kpis: Optional list of KPIs to initialize with\n    \"\"\"\n    self._kpis: list[KPI] = list(kpis) if kpis is not None else []\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.add","title":"add","text":"<pre><code>add(kpi: KPI) -&gt; None\n</code></pre> <p>Add single KPI to collection.</p> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def add(self, kpi: KPI) -&gt; None:\n    \"\"\"Add single KPI to collection.\"\"\"\n    self._kpis.append(kpi)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.extend","title":"extend","text":"<pre><code>extend(kpis: list[KPI]) -&gt; None\n</code></pre> <p>Add multiple KPIs to collection.</p> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def extend(self, kpis: list[KPI]) -&gt; None:\n    \"\"\"Add multiple KPIs to collection.\"\"\"\n    self._kpis.extend(kpis)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Remove all KPIs from collection.</p> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Remove all KPIs from collection.\"\"\"\n    self._kpis.clear()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.filter","title":"filter","text":"<pre><code>filter(**attribute_filters) -&gt; KPICollection\n</code></pre> <p>Filter KPIs by exact attribute matches.</p> <p>Parameters:</p> Name Type Description Default <code>**attribute_filters</code> <p>Attribute name-value pairs to filter by</p> <code>{}</code> <p>Returns:</p> Type Description <code>KPICollection</code> <p>New KPICollection with filtered KPIs</p> <p>Examples:</p> <pre><code>collection.filter(flag='BZ.Results.market_price')\ncollection.filter(object_name='DE-LU', aggregation=Aggregations.Mean)\ncollection.filter(dataset_type='scenario')\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def filter(self, **attribute_filters) -&gt; KPICollection:\n    \"\"\"\n    Filter KPIs by exact attribute matches.\n\n    Args:\n        **attribute_filters: Attribute name-value pairs to filter by\n\n    Returns:\n        New KPICollection with filtered KPIs\n\n    Examples:\n\n        collection.filter(flag='BZ.Results.market_price')\n        collection.filter(object_name='DE-LU', aggregation=Aggregations.Mean)\n        collection.filter(dataset_type='scenario')\n    \"\"\"\n    filtered = []\n    for kpi in self._kpis:\n        primitive = kpi.to_dict(primitive_values=True)\n        non_primitive = kpi.to_dict(primitive_values=False)\n        match = all(\n            primitive.get(attr, None) == value\n            or non_primitive.get(attr, None) == value\n            for attr, value in attribute_filters.items()\n        )\n        if match:\n            filtered.append(kpi)\n    return KPICollection(filtered)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.filter_by_model_properties","title":"filter_by_model_properties","text":"<pre><code>filter_by_model_properties(properties: dict[str, Any] | None = None, query_expr: str | None = None, filter_funcs: dict[str, Callable[[Any], bool]] | None = None) -&gt; KPICollection\n</code></pre> <p>Filter KPIs by properties from their model objects.</p> <p>Three modes of operation (can be combined with AND logic):     1. Property filters: Exact match or list membership     2. Query expression: Pandas query string     3. Filter functions: Custom functions applied to properties</p> <p>All conditions across all modes are combined with AND logic.</p> <p>Parameters:</p> Name Type Description Default <code>properties</code> <code>dict[str, Any] | None</code> <p>Dict of property names to values. Scalars for exact match, lists for membership checks.</p> <code>None</code> <code>query_expr</code> <code>str | None</code> <p>Pandas query expression (uses engine=\"python\")</p> <code>None</code> <code>filter_funcs</code> <code>dict[str, Callable[[Any], bool]] | None</code> <p>Dict of property names to filter functions</p> <code>None</code> <p>Returns:</p> Type Description <code>KPICollection</code> <p>New KPICollection with filtered KPIs</p> <p>Examples:</p> <pre><code># Property filter - exact match and list membership\ncollection.filter_by_model_properties(\n    properties={'country': 'DE', 'type': ['wind', 'solar']}\n)\n\n# Query expression\ncollection.filter_by_model_properties(\n    query_expr='country == \"DE\" and voltage_kV &gt; 200'\n)\n\n# Custom filter function\ncollection.filter_by_model_properties(\n    filter_funcs={'voltage_kV': lambda x: x &gt; 200}\n)\n\n# Combined - properties AND query\ncollection.filter_by_model_properties(\n    properties={'country': 'DE'},\n    query_expr='voltage_kV &gt; 200'\n)\n\n# Combined - properties AND filter functions\ncollection.filter_by_model_properties(\n    properties={'type': ['wind', 'solar']},\n    filter_funcs={'voltage_kV': lambda x: x &gt; 200}\n)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def filter_by_model_properties(\n        self,\n        properties: dict[str, Any] | None = None,\n        query_expr: str | None = None,\n        filter_funcs: dict[str, Callable[[Any], bool]] | None = None\n) -&gt; KPICollection:\n    \"\"\"\n    Filter KPIs by properties from their model objects.\n\n    Three modes of operation (can be combined with AND logic):\n        1. Property filters: Exact match or list membership\n        2. Query expression: Pandas query string\n        3. Filter functions: Custom functions applied to properties\n\n    All conditions across all modes are combined with AND logic.\n\n    Args:\n        properties: Dict of property names to values. Scalars for exact match,\n            lists for membership checks.\n        query_expr: Pandas query expression (uses engine=\"python\")\n        filter_funcs: Dict of property names to filter functions\n\n    Returns:\n        New KPICollection with filtered KPIs\n\n    Examples:\n\n        # Property filter - exact match and list membership\n        collection.filter_by_model_properties(\n            properties={'country': 'DE', 'type': ['wind', 'solar']}\n        )\n\n        # Query expression\n        collection.filter_by_model_properties(\n            query_expr='country == \"DE\" and voltage_kV &gt; 200'\n        )\n\n        # Custom filter function\n        collection.filter_by_model_properties(\n            filter_funcs={'voltage_kV': lambda x: x &gt; 200}\n        )\n\n        # Combined - properties AND query\n        collection.filter_by_model_properties(\n            properties={'country': 'DE'},\n            query_expr='voltage_kV &gt; 200'\n        )\n\n        # Combined - properties AND filter functions\n        collection.filter_by_model_properties(\n            properties={'type': ['wind', 'solar']},\n            filter_funcs={'voltage_kV': lambda x: x &gt; 200}\n        )\n    \"\"\"\n    filtered = []\n\n    for kpi in self._kpis:\n        if kpi.attributes.object_name is None:\n            continue\n\n        obj_info = kpi.get_object_info_from_model()\n\n        if self._series_passes_property_filters(obj_info, properties, query_expr, filter_funcs):\n            filtered.append(kpi)\n\n    return KPICollection(filtered)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.filter_by_kpi_attributes","title":"filter_by_kpi_attributes","text":"<pre><code>filter_by_kpi_attributes(attributes: dict[str, Any] | None = None, query_expr: str | None = None, filter_funcs: dict[str, Callable[[Any], bool]] | None = None) -&gt; KPICollection\n</code></pre> <p>Filter KPIs by their attributes.</p> <p>Three modes of operation (can be combined with AND logic):     1. Property filters: Exact match or list membership     2. Query expression: Pandas query string     3. Filter functions: Custom functions applied to attributes</p> <p>All conditions across all modes are combined with AND logic.</p> <p>Parameters:</p> Name Type Description Default <code>attributes</code> <code>dict[str, Any] | None</code> <p>Dict of attribute names to values. Scalars for exact match, lists for membership checks.</p> <code>None</code> <code>query_expr</code> <code>str | None</code> <p>Pandas query expression (uses engine=\"python\")</p> <code>None</code> <code>filter_funcs</code> <code>dict[str, Callable[[Any], bool]] | None</code> <p>Dict of attribute names to filter functions</p> <code>None</code> <p>Returns:</p> Type Description <code>KPICollection</code> <p>New KPICollection with filtered KPIs</p> <p>Examples:</p> <pre><code># Property filter - exact match and list membership\ncollection.filter_by_kpi_attributes(\n    attributes={'flag': 'BZ.Results.price'}\n)\n\n# Query expression\ncollection.filter_by_kpi_attributes(\n    query_expr='flag.str.contains(\"price\") and value &gt; 100'\n)\n\n# Custom filter function\ncollection.filter_by_kpi_attributes(\n    filter_funcs={'value': lambda x: x &gt; 100}\n)\n\n# Combined - properties AND query\ncollection.filter_by_kpi_attributes(\n    attributes={'flag': 'BZ.Results.price'},\n    query_expr='value &gt; 100'\n)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def filter_by_kpi_attributes(\n        self,\n        attributes: dict[str, Any] | None = None,\n        query_expr: str | None = None,\n        filter_funcs: dict[str, Callable[[Any], bool]] | None = None\n) -&gt; KPICollection:\n    \"\"\"\n    Filter KPIs by their attributes.\n\n    Three modes of operation (can be combined with AND logic):\n        1. Property filters: Exact match or list membership\n        2. Query expression: Pandas query string\n        3. Filter functions: Custom functions applied to attributes\n\n    All conditions across all modes are combined with AND logic.\n\n    Args:\n        attributes: Dict of attribute names to values. Scalars for exact match,\n            lists for membership checks.\n        query_expr: Pandas query expression (uses engine=\"python\")\n        filter_funcs: Dict of attribute names to filter functions\n\n    Returns:\n        New KPICollection with filtered KPIs\n\n    Examples:\n\n        # Property filter - exact match and list membership\n        collection.filter_by_kpi_attributes(\n            attributes={'flag': 'BZ.Results.price'}\n        )\n\n        # Query expression\n        collection.filter_by_kpi_attributes(\n            query_expr='flag.str.contains(\"price\") and value &gt; 100'\n        )\n\n        # Custom filter function\n        collection.filter_by_kpi_attributes(\n            filter_funcs={'value': lambda x: x &gt; 100}\n        )\n\n        # Combined - properties AND query\n        collection.filter_by_kpi_attributes(\n            attributes={'flag': 'BZ.Results.price'},\n            query_expr='value &gt; 100'\n        )\n    \"\"\"\n    filtered = []\n\n    for kpi in self._kpis:\n        kpi_attrs = pd.Series(kpi.to_dict(primitive_values=True))\n\n        if self._series_passes_property_filters(kpi_attrs, attributes, query_expr, filter_funcs):\n            filtered.append(kpi)\n\n    return KPICollection(filtered)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.group_by","title":"group_by","text":"<pre><code>group_by(*attributes: str) -&gt; dict[tuple, KPICollection]\n</code></pre> <p>Group KPIs by attribute values.</p> <p>Parameters:</p> Name Type Description Default <code>*attributes</code> <code>str</code> <p>Attribute names to group by</p> <code>()</code> <p>Returns:</p> Type Description <code>dict[tuple, KPICollection]</code> <p>Dictionary mapping attribute value tuples to KPICollections</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; groups = collection.group_by('flag', 'aggregation')\n    {('BZ.Results.market_price', Aggregations.Mean): KPICollection(...), ...}\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def group_by(self, *attributes: str) -&gt; dict[tuple, KPICollection]:\n    \"\"\"\n    Group KPIs by attribute values.\n\n    Args:\n        *attributes: Attribute names to group by\n\n    Returns:\n        Dictionary mapping attribute value tuples to KPICollections\n\n    Example:\n\n        &gt;&gt;&gt; groups = collection.group_by('flag', 'aggregation')\n            {('BZ.Results.market_price', Aggregations.Mean): KPICollection(...), ...}\n    \"\"\"\n    groups = defaultdict(list)\n\n    for kpi in self._kpis:\n        key = tuple(\n            getattr(kpi.attributes, attr, None)\n            or kpi.attributes.dataset_attributes.get(attr)\n            for attr in attributes\n        )\n        groups[key].append(kpi)\n\n    return {k: KPICollection(v) for k, v in groups.items()}\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.get_related","title":"get_related","text":"<pre><code>get_related(reference_kpi: KPI, vary_attributes: list[str], exclude_attributes: list[str] | None = None) -&gt; 'KPICollection'\n</code></pre> <p>Find KPIs related to reference, varying only specified attributes.</p> <p>Parameters:</p> Name Type Description Default <code>reference_kpi</code> <code>KPI</code> <p>The KPI to find relatives for</p> required <code>vary_attributes</code> <code>list[str]</code> <p>Attributes that can differ (e.g., ['aggregation'])</p> required <code>exclude_attributes</code> <code>list[str] | None</code> <p>Attributes to ignore in comparison (e.g., ['name_prefix'])</p> <code>None</code> <p>Returns:</p> Type Description <code>'KPICollection'</code> <p>New KPICollection with related KPIs</p> <p>Examples:</p> <pre><code># Same object/flag, different aggregations\ncollection.get_related(my_kpi, vary_attributes=['aggregation'])\n\n# Same object/flag/agg, different datasets\ncollection.get_related(my_kpi, vary_attributes=['dataset_name'])\n\n# Same flag/agg, different objects\ncollection.get_related(my_kpi, vary_attributes=['object_name'])\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def get_related(\n    self,\n    reference_kpi: KPI,\n    vary_attributes: list[str],\n    exclude_attributes: list[str] | None = None\n) -&gt; 'KPICollection':\n    \"\"\"\n    Find KPIs related to reference, varying only specified attributes.\n\n    Args:\n        reference_kpi: The KPI to find relatives for\n        vary_attributes: Attributes that can differ (e.g., ['aggregation'])\n        exclude_attributes: Attributes to ignore in comparison (e.g., ['name_prefix'])\n\n    Returns:\n        New KPICollection with related KPIs\n\n    Examples:\n\n        # Same object/flag, different aggregations\n        collection.get_related(my_kpi, vary_attributes=['aggregation'])\n\n        # Same object/flag/agg, different datasets\n        collection.get_related(my_kpi, vary_attributes=['dataset_name'])\n\n        # Same flag/agg, different objects\n        collection.get_related(my_kpi, vary_attributes=['object_name'])\n    \"\"\"\n    exclude_attributes = exclude_attributes or ['name_prefix', 'name_suffix', 'custom_name']\n    ref_attrs = reference_kpi.attributes.as_dict()\n    related = []\n\n    for kpi in self._kpis:\n        if kpi is reference_kpi:\n            continue\n\n        kpi_attrs = kpi.attributes.as_dict()\n        match = True\n\n        for attr_name, ref_value in ref_attrs.items():\n            # Skip attributes we want to vary or exclude\n            if attr_name in vary_attributes or attr_name in exclude_attributes:\n                continue\n\n            if kpi_attrs.get(attr_name) != ref_value:\n                match = False\n                break\n\n        if match:\n            related.append(kpi)\n\n    return KPICollection(related)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.get_all_attribute_values","title":"get_all_attribute_values","text":"<pre><code>get_all_attribute_values(attribute: str) -&gt; set\n</code></pre> <p>Get all unique values for a specific attribute across collection.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>Attribute name to get values for</p> required <p>Returns:</p> Type Description <code>set</code> <p>Set of unique values for the attribute</p> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def get_all_attribute_values(self, attribute: str) -&gt; set:\n    \"\"\"\n    Get all unique values for a specific attribute across collection.\n\n    Args:\n        attribute: Attribute name to get values for\n\n    Returns:\n        Set of unique values for the attribute\n    \"\"\"\n    values = set()\n    for kpi in self._kpis:\n        val = getattr(kpi.attributes, attribute, None)\n        if val is None:\n            val = kpi.attributes.dataset_attributes.get(attribute)\n        if val is not None:\n            values.add(val)\n    return values\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.get_all_kpi_attributes_and_value_sets","title":"get_all_kpi_attributes_and_value_sets","text":"<pre><code>get_all_kpi_attributes_and_value_sets(primitive_values: bool = False) -&gt; dict[str, set]\n</code></pre> <p>Get all attribute names and their unique value sets.</p> <p>Used by KPIGroupingManager for intelligent grouping.</p> <p>Parameters:</p> Name Type Description Default <code>primitive_values</code> <code>bool</code> <p>If True, convert values to primitives</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, set]</code> <p>Dictionary mapping attribute names to sets of values</p> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def get_all_kpi_attributes_and_value_sets(self, primitive_values: bool = False) -&gt; dict[str, set]:\n    \"\"\"\n    Get all attribute names and their unique value sets.\n\n    Used by KPIGroupingManager for intelligent grouping.\n\n    Args:\n        primitive_values: If True, convert values to primitives\n\n    Returns:\n        Dictionary mapping attribute names to sets of values\n    \"\"\"\n    attribute_sets = defaultdict(set)\n\n    for kpi in self._kpis:\n        attrs = kpi.attributes.as_dict(primitive_values=primitive_values)\n        for attr_name, attr_value in attrs.items():\n            if attr_value is not None:\n                attribute_sets[attr_name].add(attr_value)\n\n    return dict(attribute_sets)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.get_in_common_kpi_attributes","title":"get_in_common_kpi_attributes","text":"<pre><code>get_in_common_kpi_attributes(primitive_values: bool = False) -&gt; dict\n</code></pre> <p>Get attributes that have same value across all KPIs in collection.</p> <p>Parameters:</p> Name Type Description Default <code>primitive_values</code> <code>bool</code> <p>If True, convert values to primitives</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of common attributes</p> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def get_in_common_kpi_attributes(self, primitive_values: bool = False) -&gt; dict:\n    \"\"\"\n    Get attributes that have same value across all KPIs in collection.\n\n    Args:\n        primitive_values: If True, convert values to primitives\n\n    Returns:\n        Dictionary of common attributes\n    \"\"\"\n    if self.empty:\n        return {}\n\n    # Start with first KPI's attributes\n    common = self._kpis[0].attributes.as_dict(primitive_values=primitive_values)\n\n    # Remove any that differ in subsequent KPIs\n    for kpi in self._kpis[1:]:\n        attrs = kpi.attributes.as_dict(primitive_values=primitive_values)\n        common = {\n            k: v for k, v in common.items()\n            if attrs.get(k) == v\n        }\n\n    return common\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe(unit_handling: Literal['original', 'auto_convert', 'target', 'custom'] = 'original', target_unit: Unit | None = None, target_units_by_group: dict[tuple, Unit] | None = None, group_by_attributes: list[str] | None = None, normalize_to_collection: bool = False) -&gt; DataFrame\n</code></pre> <p>Export KPIs as DataFrame with flexible unit handling.</p> <p>Parameters:</p> Name Type Description Default <code>unit_handling</code> <code>Literal['original', 'auto_convert', 'target', 'custom']</code> <p>Strategy for unit conversion - 'original': Keep original units - 'auto_convert': Convert to pretty units per KPI - 'target': Use single target_unit for all - 'custom': Use target_units_by_group mapping</p> <code>'original'</code> <code>target_unit</code> <code>Unit | None</code> <p>Single target unit (when unit_handling='target')</p> <code>None</code> <code>target_units_by_group</code> <code>dict[tuple, Unit] | None</code> <p>Dict mapping group key \u2192 target unit</p> <code>None</code> <code>group_by_attributes</code> <code>list[str] | None</code> <p>Attributes to group by for 'custom' mode</p> <code>None</code> <code>normalize_to_collection</code> <code>bool</code> <p>If True, find common \"pretty\" unit across entire collection</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with KPI data</p> <p>Examples:</p>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.to_dataframe--keep-original-units","title":"Keep original units","text":"<p>df = collection.to_dataframe()</p>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.to_dataframe--auto-convert-each-kpi-to-its-own-pretty-unit","title":"Auto-convert each KPI to its own pretty unit","text":"<p>df = collection.to_dataframe(unit_handling='auto_convert')</p>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.to_dataframe--convert-all-to-single-unit","title":"Convert all to single unit","text":"<p>df = collection.to_dataframe(     unit_handling='target',     target_unit=Units.MEUR )</p>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.to_dataframe--custom-units-per-group","title":"Custom units per group","text":"<p>df = collection.to_dataframe(     unit_handling='custom',     group_by_attributes=['flag', 'aggregation'],     target_units_by_group={         ('consumer_surplus', 'Sum'): Units.BEUR,         ('producer_surplus', 'Sum'): Units.MEUR,     } )</p>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.to_dataframe--normalize-to-common-pretty-unit-for-collection","title":"Normalize to common pretty unit for collection","text":"<p>df = collection.to_dataframe(normalize_to_collection=True)</p> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def to_dataframe(\n    self,\n    unit_handling: Literal['original', 'auto_convert', 'target', 'custom'] = 'original',\n    target_unit: Units.Unit | None = None,\n    target_units_by_group: dict[tuple, Units.Unit] | None = None,\n    group_by_attributes: list[str] | None = None,\n    normalize_to_collection: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Export KPIs as DataFrame with flexible unit handling.\n\n    Args:\n        unit_handling: Strategy for unit conversion\n            - 'original': Keep original units\n            - 'auto_convert': Convert to pretty units per KPI\n            - 'target': Use single target_unit for all\n            - 'custom': Use target_units_by_group mapping\n        target_unit: Single target unit (when unit_handling='target')\n        target_units_by_group: Dict mapping group key \u2192 target unit\n        group_by_attributes: Attributes to group by for 'custom' mode\n        normalize_to_collection: If True, find common \"pretty\" unit across entire collection\n\n    Returns:\n        DataFrame with KPI data\n\n    Examples:\n        # Keep original units\n        df = collection.to_dataframe()\n\n        # Auto-convert each KPI to its own pretty unit\n        df = collection.to_dataframe(unit_handling='auto_convert')\n\n        # Convert all to single unit\n        df = collection.to_dataframe(\n            unit_handling='target',\n            target_unit=Units.MEUR\n        )\n\n        # Custom units per group\n        df = collection.to_dataframe(\n            unit_handling='custom',\n            group_by_attributes=['flag', 'aggregation'],\n            target_units_by_group={\n                ('consumer_surplus', 'Sum'): Units.BEUR,\n                ('producer_surplus', 'Sum'): Units.MEUR,\n            }\n        )\n\n        # Normalize to common pretty unit for collection\n        df = collection.to_dataframe(normalize_to_collection=True)\n    \"\"\"\n    data = []\n\n    # Normalize to collection: find common pretty unit\n    common_unit = None\n    if normalize_to_collection:\n        quantities = [kpi.quantity for kpi in self._kpis]\n        try:\n            common_unit = Units.get_common_pretty_unit_for_quantities(quantities)\n        except ValueError:\n            # Quantities have different dimensionalities, fall back to auto_convert\n            pass\n\n    for kpi in self._kpis:\n        quantity = kpi.quantity\n\n        # Apply unit conversion based on strategy\n        if normalize_to_collection and common_unit:\n            quantity = quantity.to(common_unit)\n\n        elif unit_handling == 'auto_convert':\n            quantity = Units.get_quantity_in_pretty_unit(quantity)\n\n        elif unit_handling == 'target' and target_unit:\n            quantity = quantity.to(target_unit)\n\n        elif unit_handling == 'custom' and target_units_by_group and group_by_attributes:\n            # Build group key\n            group_key = tuple(\n                getattr(kpi.attributes, attr, None)\n                for attr in group_by_attributes\n            )\n            if group_key in target_units_by_group:\n                quantity = quantity.to(target_units_by_group[group_key])\n\n        row = {\n            'name': kpi.name,\n            **kpi.attributes.as_dict(primitive_values=True),\n            'value': quantity.magnitude,\n            'unit': str(quantity.units),  # Set after attributes to avoid being overwritten\n        }\n        data.append(row)\n\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Iterate over KPIs in collection.</p> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate over KPIs in collection.\"\"\"\n    return iter(self._kpis)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Number of KPIs in collection.</p> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def __len__(self):\n    \"\"\"Number of KPIs in collection.\"\"\"\n    return len(self._kpis)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index)\n</code></pre> <p>Get KPI by index.</p> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def __getitem__(self, index):\n    \"\"\"Get KPI by index.\"\"\"\n    return self._kpis[index]\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/collection/#mesqual.kpis.collection.KPICollection.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>String representation of collection.</p> Source code in <code>submodules/mesqual/mesqual/kpis/collection.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of collection.\"\"\"\n    return f\"KPICollection({self.size} KPIs)\"\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/kpi/","title":"KPI &amp; Attributes","text":""},{"location":"mesqual-package-documentation/api_reference/kpis/kpi/#mesqual.kpis.kpi.KPI","title":"KPI  <code>dataclass</code>","text":"<p>Single computed KPI with value and rich metadata.</p> <p>Represents a scalar metric (e.g., \"Mean market price for BZ DE-LU in scenario base\") with all context needed for filtering, visualization, and unit conversion.</p> <p>Attributes:</p> Name Type Description <code>value</code> <code>float | int</code> <p>The computed scalar value</p> <code>attributes</code> <code>KPIAttributes</code> <p>Rich metadata container</p> <code>dataset</code> <code>Dataset</code> <p>Reference to source dataset for lazy model access</p> Source code in <code>submodules/mesqual/mesqual/kpis/kpi.py</code> <pre><code>@dataclass\nclass KPI:\n    \"\"\"\n    Single computed KPI with value and rich metadata.\n\n    Represents a scalar metric (e.g., \"Mean market price for BZ DE-LU in scenario base\")\n    with all context needed for filtering, visualization, and unit conversion.\n\n    Attributes:\n        value: The computed scalar value\n        attributes: Rich metadata container\n        dataset: Reference to source dataset for lazy model access\n    \"\"\"\n\n    value: float | int\n    attributes: KPIAttributes\n    dataset: Dataset\n\n    _object_info: pd.Series | None = None\n    _quantity: Units.Quantity | None = None\n\n    @property\n    def quantity(self) -&gt; Units.Quantity:\n        \"\"\"\n        Return value as pint Quantity with unit.\n\n        Returns:\n            Quantity with value and unit\n        \"\"\"\n        if self._quantity is None:\n            unit = self.attributes.unit or self.dataset.flag_index.get_unit(self.attributes.flag)\n            self._quantity = self.value * unit\n        return self._quantity\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        Generate human-readable unique name.\n\n        Returns:\n            KPI name (custom or auto-generated)\n        \"\"\"\n        if self.attributes.custom_name:\n            return self.attributes.custom_name\n\n        return self._generate_automatic_name()\n\n    def _generate_automatic_name(self) -&gt; str:\n        \"\"\"\n        Auto-generate name from attributes.\n\n        Format: [prefix] flag aggregation object [comparison/operation] [suffix]\n\n        Returns:\n            Generated name string\n        \"\"\"\n        parts = []\n\n        if self.attributes.name_prefix:\n            parts.append(self.attributes.name_prefix)\n\n        # Core components\n        flag_short = str(self.attributes.flag)\n        parts.append(flag_short)\n\n        if self.attributes.aggregation:\n            parts.append(str(self.attributes.aggregation))\n\n        if self.attributes.object_name:\n            parts.append(str(self.attributes.object_name))\n\n        if self.attributes.value_comparison:\n            parts.append(str(self.attributes.value_comparison))\n\n        if self.attributes.arithmetic_operation:\n            parts.append(str(self.attributes.arithmetic_operation))\n\n        if self.attributes.name_suffix:\n            parts.append(self.attributes.name_suffix)\n\n        return ' '.join(parts)\n\n    def get_object_info_from_model(self) -&gt; pd.Series:\n        \"\"\"\n        Lazy fetch of object metadata from Model flag.\n\n        Returns:\n            Series with object properties from model DataFrame\n        \"\"\"\n        if self._object_info is None:\n            if self.attributes.model_flag and self.attributes.object_name:\n                if isinstance(self.dataset, DatasetComparison):\n                    sets = [self.dataset.variation_dataset, self.dataset.reference_dataset]\n                else:\n                    sets = [self.dataset]\n                for ds in sets:\n                    if ds.flag_is_accepted(self.attributes.model_flag):\n                        model_df = ds.fetch(self.attributes.model_flag)\n                        if self.attributes.object_name in model_df.index:\n                            self._object_info = model_df.loc[self.attributes.object_name]\n                            break\n                if self._object_info is None:\n                    self._object_info = pd.Series()\n        return self._object_info\n\n    def get_kpi_name_with_dataset_name(self) -&gt; str:\n        \"\"\"\n        Get KPI name with dataset name for visualization tooltips.\n\n        Returns:\n            Name including dataset (e.g., \"market_price Mean [base_scenario]\")\n        \"\"\"\n        base_name = self.name\n        if self.attributes.dataset_name:\n            return f\"{base_name} [{self.attributes.dataset_name}]\"\n        return base_name\n\n    def to_dict(self, primitive_values: bool = True) -&gt; dict:\n        \"\"\"\n        Export KPI as flat dictionary for tables.\n\n        Args:\n            primitive_values: If True, convert objects to strings for serialization\n\n        Returns:\n            Dictionary with all KPI data\n        \"\"\"\n        return {\n            'name': self.name,\n            'value': self.value,\n            'quantity': str(self.quantity) if primitive_values else self.quantity,\n            **self.attributes.as_dict(primitive_values=primitive_values),\n        }\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of KPI.\"\"\"\n        return f\"KPI(name='{self.name}', value={self.value}, unit={self.quantity.units})\"\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/kpi/#mesqual.kpis.kpi.KPI.quantity","title":"quantity  <code>property</code>","text":"<pre><code>quantity: Quantity\n</code></pre> <p>Return value as pint Quantity with unit.</p> <p>Returns:</p> Type Description <code>Quantity</code> <p>Quantity with value and unit</p>"},{"location":"mesqual-package-documentation/api_reference/kpis/kpi/#mesqual.kpis.kpi.KPI.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Generate human-readable unique name.</p> <p>Returns:</p> Type Description <code>str</code> <p>KPI name (custom or auto-generated)</p>"},{"location":"mesqual-package-documentation/api_reference/kpis/kpi/#mesqual.kpis.kpi.KPI.get_object_info_from_model","title":"get_object_info_from_model","text":"<pre><code>get_object_info_from_model() -&gt; Series\n</code></pre> <p>Lazy fetch of object metadata from Model flag.</p> <p>Returns:</p> Type Description <code>Series</code> <p>Series with object properties from model DataFrame</p> Source code in <code>submodules/mesqual/mesqual/kpis/kpi.py</code> <pre><code>def get_object_info_from_model(self) -&gt; pd.Series:\n    \"\"\"\n    Lazy fetch of object metadata from Model flag.\n\n    Returns:\n        Series with object properties from model DataFrame\n    \"\"\"\n    if self._object_info is None:\n        if self.attributes.model_flag and self.attributes.object_name:\n            if isinstance(self.dataset, DatasetComparison):\n                sets = [self.dataset.variation_dataset, self.dataset.reference_dataset]\n            else:\n                sets = [self.dataset]\n            for ds in sets:\n                if ds.flag_is_accepted(self.attributes.model_flag):\n                    model_df = ds.fetch(self.attributes.model_flag)\n                    if self.attributes.object_name in model_df.index:\n                        self._object_info = model_df.loc[self.attributes.object_name]\n                        break\n            if self._object_info is None:\n                self._object_info = pd.Series()\n    return self._object_info\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/kpi/#mesqual.kpis.kpi.KPI.get_kpi_name_with_dataset_name","title":"get_kpi_name_with_dataset_name","text":"<pre><code>get_kpi_name_with_dataset_name() -&gt; str\n</code></pre> <p>Get KPI name with dataset name for visualization tooltips.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name including dataset (e.g., \"market_price Mean [base_scenario]\")</p> Source code in <code>submodules/mesqual/mesqual/kpis/kpi.py</code> <pre><code>def get_kpi_name_with_dataset_name(self) -&gt; str:\n    \"\"\"\n    Get KPI name with dataset name for visualization tooltips.\n\n    Returns:\n        Name including dataset (e.g., \"market_price Mean [base_scenario]\")\n    \"\"\"\n    base_name = self.name\n    if self.attributes.dataset_name:\n        return f\"{base_name} [{self.attributes.dataset_name}]\"\n    return base_name\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/kpi/#mesqual.kpis.kpi.KPI.to_dict","title":"to_dict","text":"<pre><code>to_dict(primitive_values: bool = True) -&gt; dict\n</code></pre> <p>Export KPI as flat dictionary for tables.</p> <p>Parameters:</p> Name Type Description Default <code>primitive_values</code> <code>bool</code> <p>If True, convert objects to strings for serialization</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with all KPI data</p> Source code in <code>submodules/mesqual/mesqual/kpis/kpi.py</code> <pre><code>def to_dict(self, primitive_values: bool = True) -&gt; dict:\n    \"\"\"\n    Export KPI as flat dictionary for tables.\n\n    Args:\n        primitive_values: If True, convert objects to strings for serialization\n\n    Returns:\n        Dictionary with all KPI data\n    \"\"\"\n    return {\n        'name': self.name,\n        'value': self.value,\n        'quantity': str(self.quantity) if primitive_values else self.quantity,\n        **self.attributes.as_dict(primitive_values=primitive_values),\n    }\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/kpi/#mesqual.kpis.kpi.KPI.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>String representation of KPI.</p> Source code in <code>submodules/mesqual/mesqual/kpis/kpi.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of KPI.\"\"\"\n    return f\"KPI(name='{self.name}', value={self.value}, unit={self.quantity.units})\"\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/kpi/#mesqual.kpis.attributes.KPIAttributes","title":"KPIAttributes  <code>dataclass</code>","text":"<p>Rich metadata container for KPI instances.</p> <p>Stores all context needed for filtering, grouping, visualization, and unit conversion of KPI values.</p> <p>Attributes:</p> Name Type Description <code>flag</code> <code>FlagTypeProtocol</code> <p>Variable flag (e.g., 'BZ.Results.market_price')</p> <code>model_flag</code> <code>FlagTypeProtocol | None</code> <p>Associated model flag (e.g., 'BZ.Model')</p> <code>object_name</code> <code>Hashable | None</code> <p>Specific object identifier (e.g., 'DE-LU')</p> <code>aggregation</code> <code>Aggregation | None</code> <p>Aggregation applied (e.g., Aggregations.Mean)</p> <code>dataset_name</code> <code>str</code> <p>Name of the dataset</p> <code>dataset_type</code> <code>Type[Dataset]</code> <p>Type of dataset ('scenario', 'comparison', etc.)</p> <code>value_comparison</code> <code>ValueComparison | None</code> <p>Comparison operation for comparison KPIs</p> <code>arithmetic_operation</code> <code>ArithmeticValueOperation | None</code> <p>Arithmetic operation for derived KPIs</p> <code>reference_dataset_name</code> <code>str | None</code> <p>Reference dataset for comparisons</p> <code>variation_dataset_name</code> <code>str | None</code> <p>Variation dataset for comparisons</p> <code>name_prefix</code> <code>str</code> <p>Custom prefix for KPI name</p> <code>name_suffix</code> <code>str</code> <p>Custom suffix for KPI name</p> <code>custom_name</code> <code>str | None</code> <p>Complete custom name override</p> <code>unit</code> <code>Unit | None</code> <p>Physical unit of the KPI value</p> <code>target_unit</code> <code>Unit | None</code> <p>Target unit for conversion</p> <code>dataset_attributes</code> <code>dict[str, Any]</code> <p>Additional attributes from dataset (e.g., scenario attributes)</p> <code>extra_attributes</code> <code>dict[str, Any]</code> <p>Extra attributes set by user (e.g. for filtering / grouping purposes)</p> Source code in <code>submodules/mesqual/mesqual/kpis/attributes.py</code> <pre><code>@dataclass\nclass KPIAttributes:\n    \"\"\"\n    Rich metadata container for KPI instances.\n\n    Stores all context needed for filtering, grouping, visualization,\n    and unit conversion of KPI values.\n\n    Attributes:\n        flag: Variable flag (e.g., 'BZ.Results.market_price')\n        model_flag: Associated model flag (e.g., 'BZ.Model')\n        object_name: Specific object identifier (e.g., 'DE-LU')\n        aggregation: Aggregation applied (e.g., Aggregations.Mean)\n        dataset_name: Name of the dataset\n        dataset_type: Type of dataset ('scenario', 'comparison', etc.)\n        value_comparison: Comparison operation for comparison KPIs\n        arithmetic_operation: Arithmetic operation for derived KPIs\n        reference_dataset_name: Reference dataset for comparisons\n        variation_dataset_name: Variation dataset for comparisons\n        name_prefix: Custom prefix for KPI name\n        name_suffix: Custom suffix for KPI name\n        custom_name: Complete custom name override\n        unit: Physical unit of the KPI value\n        target_unit: Target unit for conversion\n        dataset_attributes: Additional attributes from dataset (e.g., scenario attributes)\n        extra_attributes: Extra attributes set by user (e.g. for filtering / grouping purposes)\n    \"\"\"\n\n    # Core identifiers\n    flag: FlagTypeProtocol\n    model_flag: FlagTypeProtocol | None = None\n    object_name: Hashable | None = None\n    aggregation: Aggregation | None = None\n\n    # Dataset context\n    dataset_name: str = ''\n    dataset_type: Type[Dataset] = None\n\n    # Comparison-specific\n    value_comparison: ValueComparison | None = None\n    arithmetic_operation: ArithmeticValueOperation | None = None\n    reference_dataset_name: str | None = None\n    variation_dataset_name: str | None = None\n\n    # Naming\n    name_prefix: str = ''\n    name_suffix: str = ''\n    custom_name: str | None = None\n\n    # Unit handling\n    unit: Units.Unit | None = None\n\n    # Additional attributes\n    dataset_attributes: dict[str, Any] = field(default_factory=dict)\n    extra_attributes: dict[str, Any] = field(default_factory=dict)\n\n    def as_dict(self, primitive_values: bool = True) -&gt; dict[str, Any]:\n        \"\"\"\n        Export attributes as dictionary for filtering.\n\n        Args:\n            primitive_values: If True, convert objects to strings for serialization\n\n        Returns:\n            Dictionary representation of attributes\n        \"\"\"\n        d = {\n            'flag': self.flag,\n            'model_flag': self.model_flag,\n            'object_name': self.object_name,\n            'aggregation': self.aggregation,\n            'dataset_name': self.dataset_name,\n            'dataset_type': self.dataset_type,\n            'value_comparison': self.value_comparison,\n            'arithmetic_operation': self.arithmetic_operation,\n            'reference_dataset_name': self.reference_dataset_name,\n            'variation_dataset_name': self.variation_dataset_name,\n            'name_prefix': self.name_prefix,\n            'name_suffix': self.name_suffix,\n            'custom_name': self.custom_name,\n            'unit': self.unit,\n            **self.dataset_attributes,\n            **self.extra_attributes\n        }\n        if primitive_values:\n            d = {k: _to_primitive(v) for k, v in d.items()}\n        return d\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"\n        Dict-like get interface for compatibility.\n\n        Args:\n            key: Attribute key to retrieve\n            default: Default value if key not found\n\n        Returns:\n            Attribute value or default\n        \"\"\"\n        return self.as_dict().get(key, default)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/kpi/#mesqual.kpis.attributes.KPIAttributes.as_dict","title":"as_dict","text":"<pre><code>as_dict(primitive_values: bool = True) -&gt; dict[str, Any]\n</code></pre> <p>Export attributes as dictionary for filtering.</p> <p>Parameters:</p> Name Type Description Default <code>primitive_values</code> <code>bool</code> <p>If True, convert objects to strings for serialization</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary representation of attributes</p> Source code in <code>submodules/mesqual/mesqual/kpis/attributes.py</code> <pre><code>def as_dict(self, primitive_values: bool = True) -&gt; dict[str, Any]:\n    \"\"\"\n    Export attributes as dictionary for filtering.\n\n    Args:\n        primitive_values: If True, convert objects to strings for serialization\n\n    Returns:\n        Dictionary representation of attributes\n    \"\"\"\n    d = {\n        'flag': self.flag,\n        'model_flag': self.model_flag,\n        'object_name': self.object_name,\n        'aggregation': self.aggregation,\n        'dataset_name': self.dataset_name,\n        'dataset_type': self.dataset_type,\n        'value_comparison': self.value_comparison,\n        'arithmetic_operation': self.arithmetic_operation,\n        'reference_dataset_name': self.reference_dataset_name,\n        'variation_dataset_name': self.variation_dataset_name,\n        'name_prefix': self.name_prefix,\n        'name_suffix': self.name_suffix,\n        'custom_name': self.custom_name,\n        'unit': self.unit,\n        **self.dataset_attributes,\n        **self.extra_attributes\n    }\n    if primitive_values:\n        d = {k: _to_primitive(v) for k, v in d.items()}\n    return d\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/kpi/#mesqual.kpis.attributes.KPIAttributes.get","title":"get","text":"<pre><code>get(key: str, default: Any = None) -&gt; Any\n</code></pre> <p>Dict-like get interface for compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Attribute key to retrieve</p> required <code>default</code> <code>Any</code> <p>Default value if key not found</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Attribute value or default</p> Source code in <code>submodules/mesqual/mesqual/kpis/attributes.py</code> <pre><code>def get(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"\n    Dict-like get interface for compatibility.\n\n    Args:\n        key: Attribute key to retrieve\n        default: Default value if key not found\n\n    Returns:\n        Attribute value or default\n    \"\"\"\n    return self.as_dict().get(key, default)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/","title":"KPI Builders","text":"<p>KPI Builders - Fluent API for creating KPI definitions in bulk.</p>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/base/","title":"KPI Builder Base Class","text":""},{"location":"mesqual-package-documentation/api_reference/kpis/builders/base/#mesqual.kpis.builders.base.KPIBuilder","title":"KPIBuilder","text":"<p>               Bases: <code>ABC</code>, <code>Generic[KPIDefinitionType]</code></p> <p>Abstract base class for all KPI builders.</p> <p>Provides common functionality for fluent builder patterns used in KPI definition creation. All builders support optional name prefixes and suffixes for customizing KPI names.</p> <p>Subclasses must implement: - build(): Generate list of KPI definitions from builder configuration</p> <p>Common pattern:</p> <pre><code>&gt;&gt;&gt; builder = SomeKPIBuilder()\n&gt;&gt;&gt; definitions = (\n...     builder\n...     .some_config_method(...)\n...     .with_name_prefix(\"Custom: \")\n...     .with_name_suffix(\" (v2)\")\n...     .with_extra_attributes(my_category='my_cat_1', my_group='my_grp_1')\n...     .build()\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/builders/base.py</code> <pre><code>class KPIBuilder(ABC, Generic[KPIDefinitionType]):\n    \"\"\"\n    Abstract base class for all KPI builders.\n\n    Provides common functionality for fluent builder patterns used in KPI definition creation.\n    All builders support optional name prefixes and suffixes for customizing KPI names.\n\n    Subclasses must implement:\n    - build(): Generate list of KPI definitions from builder configuration\n\n    Common pattern:\n\n        &gt;&gt;&gt; builder = SomeKPIBuilder()\n        &gt;&gt;&gt; definitions = (\n        ...     builder\n        ...     .some_config_method(...)\n        ...     .with_name_prefix(\"Custom: \")\n        ...     .with_name_suffix(\" (v2)\")\n        ...     .with_extra_attributes(my_category='my_cat_1', my_group='my_grp_1')\n        ...     .build()\n        ... )\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize builder with common attributes.\"\"\"\n        self._name_prefix: str = ''\n        self._name_suffix: str = ''\n        self._extra_attributes: dict = dict()\n        self._custom_name: str | None = None\n\n    def with_name_prefix(self, prefix: str) -&gt; KPIBuilder:\n        \"\"\"\n        Set name prefix for generated KPIs.\n\n        Args:\n            prefix: Prefix string to prepend to KPI names\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        self._name_prefix = prefix\n        return self\n\n    def with_name_suffix(self, suffix: str) -&gt; KPIBuilder:\n        \"\"\"\n        Set name suffix for generated KPIs.\n\n        Args:\n            suffix: Suffix string to append to KPI names\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        self._name_suffix = suffix\n        return self\n\n    def with_custom_name(self, name: str) -&gt; KPIBuilder:\n        \"\"\"\n        Set custom name that completely overrides automatic name generation.\n\n        When set, the KPI will use this exact name instead of generating one\n        from flag, aggregation, object, etc.\n\n        Args:\n            name: Custom name for the KPI\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        self._custom_name = name\n        return self\n\n    def with_extra_attributes(self, **kwargs) -&gt; KPIBuilder:\n        \"\"\"\n        Set additional extra (custom) attributes.\n\n        When set, the KPI will contain these attributes under \"kpi.attributes.extra_attributes\".\n\n        Kwargs:\n            kwargs: Custom key-value attributes.\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        self._extra_attributes.update(**kwargs)\n        return self\n\n    @abstractmethod\n    def build(self) -&gt; list[KPIDefinitionType]:\n        \"\"\"\n        Generate all KPI definitions from builder configuration.\n\n        Returns:\n            List of KPI definition instances\n        \"\"\"\n        pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/base/#mesqual.kpis.builders.base.KPIBuilder.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize builder with common attributes.</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/base.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize builder with common attributes.\"\"\"\n    self._name_prefix: str = ''\n    self._name_suffix: str = ''\n    self._extra_attributes: dict = dict()\n    self._custom_name: str | None = None\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/base/#mesqual.kpis.builders.base.KPIBuilder.with_name_prefix","title":"with_name_prefix","text":"<pre><code>with_name_prefix(prefix: str) -&gt; KPIBuilder\n</code></pre> <p>Set name prefix for generated KPIs.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Prefix string to prepend to KPI names</p> required <p>Returns:</p> Type Description <code>KPIBuilder</code> <p>Self for method chaining</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/base.py</code> <pre><code>def with_name_prefix(self, prefix: str) -&gt; KPIBuilder:\n    \"\"\"\n    Set name prefix for generated KPIs.\n\n    Args:\n        prefix: Prefix string to prepend to KPI names\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    self._name_prefix = prefix\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/base/#mesqual.kpis.builders.base.KPIBuilder.with_name_suffix","title":"with_name_suffix","text":"<pre><code>with_name_suffix(suffix: str) -&gt; KPIBuilder\n</code></pre> <p>Set name suffix for generated KPIs.</p> <p>Parameters:</p> Name Type Description Default <code>suffix</code> <code>str</code> <p>Suffix string to append to KPI names</p> required <p>Returns:</p> Type Description <code>KPIBuilder</code> <p>Self for method chaining</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/base.py</code> <pre><code>def with_name_suffix(self, suffix: str) -&gt; KPIBuilder:\n    \"\"\"\n    Set name suffix for generated KPIs.\n\n    Args:\n        suffix: Suffix string to append to KPI names\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    self._name_suffix = suffix\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/base/#mesqual.kpis.builders.base.KPIBuilder.with_custom_name","title":"with_custom_name","text":"<pre><code>with_custom_name(name: str) -&gt; KPIBuilder\n</code></pre> <p>Set custom name that completely overrides automatic name generation.</p> <p>When set, the KPI will use this exact name instead of generating one from flag, aggregation, object, etc.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Custom name for the KPI</p> required <p>Returns:</p> Type Description <code>KPIBuilder</code> <p>Self for method chaining</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/base.py</code> <pre><code>def with_custom_name(self, name: str) -&gt; KPIBuilder:\n    \"\"\"\n    Set custom name that completely overrides automatic name generation.\n\n    When set, the KPI will use this exact name instead of generating one\n    from flag, aggregation, object, etc.\n\n    Args:\n        name: Custom name for the KPI\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    self._custom_name = name\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/base/#mesqual.kpis.builders.base.KPIBuilder.with_extra_attributes","title":"with_extra_attributes","text":"<pre><code>with_extra_attributes(**kwargs) -&gt; KPIBuilder\n</code></pre> <p>Set additional extra (custom) attributes.</p> <p>When set, the KPI will contain these attributes under \"kpi.attributes.extra_attributes\".</p> Kwargs <p>kwargs: Custom key-value attributes.</p> <p>Returns:</p> Type Description <code>KPIBuilder</code> <p>Self for method chaining</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/base.py</code> <pre><code>def with_extra_attributes(self, **kwargs) -&gt; KPIBuilder:\n    \"\"\"\n    Set additional extra (custom) attributes.\n\n    When set, the KPI will contain these attributes under \"kpi.attributes.extra_attributes\".\n\n    Kwargs:\n        kwargs: Custom key-value attributes.\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    self._extra_attributes.update(**kwargs)\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/base/#mesqual.kpis.builders.base.KPIBuilder.build","title":"build  <code>abstractmethod</code>","text":"<pre><code>build() -&gt; list[KPIDefinitionType]\n</code></pre> <p>Generate all KPI definitions from builder configuration.</p> <p>Returns:</p> Type Description <code>list[KPIDefinitionType]</code> <p>List of KPI definition instances</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/base.py</code> <pre><code>@abstractmethod\ndef build(self) -&gt; list[KPIDefinitionType]:\n    \"\"\"\n    Generate all KPI definitions from builder configuration.\n\n    Returns:\n        List of KPI definition instances\n    \"\"\"\n    pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/comparison_builder/","title":"Comparison KPI Builder","text":""},{"location":"mesqual-package-documentation/api_reference/kpis/builders/comparison_builder/#mesqual.kpis.builders.comparison_builder.ComparisonKPIBuilder","title":"ComparisonKPIBuilder","text":"<p>               Bases: <code>KPIBuilder[ComparisonKPIDefinition]</code></p> <p>Builder for creating comparison KPI definitions from scenario KPIs.</p> <p>Takes existing KPI definitions (typically for scenarios) and creates comparison definitions that compute differences, percentage changes, or other comparisons between datasets.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Create base scenario definitions\n&gt;&gt;&gt; base_defs = (\n...     FlagAggKPIBuilder()\n...     .for_flag('BZ.Results.price')\n...     .with_aggregation(Aggregations.Mean)\n...     .build()\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create comparison definitions\n&gt;&gt;&gt; comp_builder = ComparisonKPIBuilder(base_defs)\n&gt;&gt;&gt; comp_defs = (\n...     comp_builder\n...     .with_comparisons([\n...         ValueComparisons.Increase,\n...         ValueComparisons.PercentageIncrease\n...     ])\n...     .build()\n... )\n&gt;&gt;&gt; # Creates 2 comparison definitions (1 base \u00d7 2 comparisons)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/builders/comparison_builder.py</code> <pre><code>class ComparisonKPIBuilder(KPIBuilder[ComparisonKPIDefinition]):\n    \"\"\"\n    Builder for creating comparison KPI definitions from scenario KPIs.\n\n    Takes existing KPI definitions (typically for scenarios) and creates\n    comparison definitions that compute differences, percentage changes,\n    or other comparisons between datasets.\n\n    Example:\n\n        &gt;&gt;&gt; # Create base scenario definitions\n        &gt;&gt;&gt; base_defs = (\n        ...     FlagAggKPIBuilder()\n        ...     .for_flag('BZ.Results.price')\n        ...     .with_aggregation(Aggregations.Mean)\n        ...     .build()\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create comparison definitions\n        &gt;&gt;&gt; comp_builder = ComparisonKPIBuilder(base_defs)\n        &gt;&gt;&gt; comp_defs = (\n        ...     comp_builder\n        ...     .with_comparisons([\n        ...         ValueComparisons.Increase,\n        ...         ValueComparisons.PercentageIncrease\n        ...     ])\n        ...     .build()\n        ... )\n        &gt;&gt;&gt; # Creates 2 comparison definitions (1 base \u00d7 2 comparisons)\n    \"\"\"\n\n    def __init__(self, base_definitions: list[KPIDefinition]):\n        \"\"\"\n        Initialize builder with base KPI definitions.\n\n        Args:\n            base_definitions: List of KPI definitions to create comparisons from\n        \"\"\"\n        super().__init__()\n        self._base_definitions = base_definitions\n        self._comparisons: list[ValueComparison] = []\n\n    def with_comparison(self, comp: ValueComparison) -&gt; ComparisonKPIBuilder:\n        \"\"\"\n        Set a single comparison operation.\n\n        Args:\n            comp: Comparison operation (e.g., ValueComparisons.Increase)\n\n        Returns:\n            Self for chaining\n        \"\"\"\n        self._comparisons = [comp]\n        return self\n\n    def with_comparisons(self, comps: list[ValueComparison]) -&gt; ComparisonKPIBuilder:\n        \"\"\"\n        Set multiple comparison operations.\n\n        Args:\n            comps: List of comparison operations\n\n        Returns:\n            Self for chaining\n        \"\"\"\n        self._comparisons = comps\n        return self\n\n    def build(self) -&gt; list[ComparisonKPIDefinition]:\n        \"\"\"\n        Generate all comparison KPI definitions.\n\n        Creates the Cartesian product of base_definitions \u00d7 comparisons.\n\n        Returns:\n            List of ComparisonKPIDefinition instances\n\n        Example:\n\n            &gt;&gt;&gt; base_defs = [def1, def2, def3]  # 3 base definitions\n            &gt;&gt;&gt; comp_builder = ComparisonKPIBuilder(base_defs)\n            &gt;&gt;&gt; comp_defs = comp_builder.with_comparisons([ValueComparisons.Increase, ValueComparisons.PercentageIncrease]).build()\n            &gt;&gt;&gt; len(comp_defs)  # 3 base \u00d7 2 comparisons = 6\n                6\n        \"\"\"\n        definitions = []\n\n        for base_def in self._base_definitions:\n            for comp in self._comparisons:\n                comp_def = ComparisonKPIDefinition(\n                    base_definition=base_def,\n                    comparison=comp,\n                    name_prefix=self._name_prefix,\n                    name_suffix=self._name_suffix,\n                    custom_name=self._custom_name,\n                    extra_attributes=self._extra_attributes,\n                )\n                definitions.append(comp_def)\n\n        return definitions\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/comparison_builder/#mesqual.kpis.builders.comparison_builder.ComparisonKPIBuilder.__init__","title":"__init__","text":"<pre><code>__init__(base_definitions: list[KPIDefinition])\n</code></pre> <p>Initialize builder with base KPI definitions.</p> <p>Parameters:</p> Name Type Description Default <code>base_definitions</code> <code>list[KPIDefinition]</code> <p>List of KPI definitions to create comparisons from</p> required Source code in <code>submodules/mesqual/mesqual/kpis/builders/comparison_builder.py</code> <pre><code>def __init__(self, base_definitions: list[KPIDefinition]):\n    \"\"\"\n    Initialize builder with base KPI definitions.\n\n    Args:\n        base_definitions: List of KPI definitions to create comparisons from\n    \"\"\"\n    super().__init__()\n    self._base_definitions = base_definitions\n    self._comparisons: list[ValueComparison] = []\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/comparison_builder/#mesqual.kpis.builders.comparison_builder.ComparisonKPIBuilder.with_comparison","title":"with_comparison","text":"<pre><code>with_comparison(comp: ValueComparison) -&gt; ComparisonKPIBuilder\n</code></pre> <p>Set a single comparison operation.</p> <p>Parameters:</p> Name Type Description Default <code>comp</code> <code>ValueComparison</code> <p>Comparison operation (e.g., ValueComparisons.Increase)</p> required <p>Returns:</p> Type Description <code>ComparisonKPIBuilder</code> <p>Self for chaining</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/comparison_builder.py</code> <pre><code>def with_comparison(self, comp: ValueComparison) -&gt; ComparisonKPIBuilder:\n    \"\"\"\n    Set a single comparison operation.\n\n    Args:\n        comp: Comparison operation (e.g., ValueComparisons.Increase)\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._comparisons = [comp]\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/comparison_builder/#mesqual.kpis.builders.comparison_builder.ComparisonKPIBuilder.with_comparisons","title":"with_comparisons","text":"<pre><code>with_comparisons(comps: list[ValueComparison]) -&gt; ComparisonKPIBuilder\n</code></pre> <p>Set multiple comparison operations.</p> <p>Parameters:</p> Name Type Description Default <code>comps</code> <code>list[ValueComparison]</code> <p>List of comparison operations</p> required <p>Returns:</p> Type Description <code>ComparisonKPIBuilder</code> <p>Self for chaining</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/comparison_builder.py</code> <pre><code>def with_comparisons(self, comps: list[ValueComparison]) -&gt; ComparisonKPIBuilder:\n    \"\"\"\n    Set multiple comparison operations.\n\n    Args:\n        comps: List of comparison operations\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._comparisons = comps\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/comparison_builder/#mesqual.kpis.builders.comparison_builder.ComparisonKPIBuilder.build","title":"build","text":"<pre><code>build() -&gt; list[ComparisonKPIDefinition]\n</code></pre> <p>Generate all comparison KPI definitions.</p> <p>Creates the Cartesian product of base_definitions \u00d7 comparisons.</p> <p>Returns:</p> Type Description <code>list[ComparisonKPIDefinition]</code> <p>List of ComparisonKPIDefinition instances</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; base_defs = [def1, def2, def3]  # 3 base definitions\n&gt;&gt;&gt; comp_builder = ComparisonKPIBuilder(base_defs)\n&gt;&gt;&gt; comp_defs = comp_builder.with_comparisons([ValueComparisons.Increase, ValueComparisons.PercentageIncrease]).build()\n&gt;&gt;&gt; len(comp_defs)  # 3 base \u00d7 2 comparisons = 6\n    6\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/builders/comparison_builder.py</code> <pre><code>def build(self) -&gt; list[ComparisonKPIDefinition]:\n    \"\"\"\n    Generate all comparison KPI definitions.\n\n    Creates the Cartesian product of base_definitions \u00d7 comparisons.\n\n    Returns:\n        List of ComparisonKPIDefinition instances\n\n    Example:\n\n        &gt;&gt;&gt; base_defs = [def1, def2, def3]  # 3 base definitions\n        &gt;&gt;&gt; comp_builder = ComparisonKPIBuilder(base_defs)\n        &gt;&gt;&gt; comp_defs = comp_builder.with_comparisons([ValueComparisons.Increase, ValueComparisons.PercentageIncrease]).build()\n        &gt;&gt;&gt; len(comp_defs)  # 3 base \u00d7 2 comparisons = 6\n            6\n    \"\"\"\n    definitions = []\n\n    for base_def in self._base_definitions:\n        for comp in self._comparisons:\n            comp_def = ComparisonKPIDefinition(\n                base_definition=base_def,\n                comparison=comp,\n                name_prefix=self._name_prefix,\n                name_suffix=self._name_suffix,\n                custom_name=self._custom_name,\n                extra_attributes=self._extra_attributes,\n            )\n            definitions.append(comp_def)\n\n    return definitions\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/flag_agg_builder/","title":"Flag Aggregation KPI Builder","text":""},{"location":"mesqual-package-documentation/api_reference/kpis/builders/flag_agg_builder/#mesqual.kpis.builders.flag_agg_builder.FlagAggKPIBuilder","title":"FlagAggKPIBuilder","text":"<p>               Bases: <code>KPIBuilder[FlagAggKPIDefinition]</code></p> <p>Fluent builder for creating FlagAggregation KPI definitions in bulk.</p> <p>Provides a clean, declarative API for specifying multiple KPI definitions at once. Particularly useful for creating large numbers of KPIs with different combinations of flags and aggregations.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; builder = FlagAggKPIBuilder()\n&gt;&gt;&gt; definitions = (\n...     builder\n...     .for_flags(['BZ.market_price', 'BZ.net_position'])\n...     .with_aggregations([Aggregations.Mean, Aggregations.Max, Aggregations.Min])\n...     .for_all_objects()\n...     .build()\n... )\n&gt;&gt;&gt; len(definitions)  # 2 flags \u00d7 3 aggs = 6\n</code></pre> <p>All methods return self for chaining.</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/flag_agg_builder.py</code> <pre><code>class FlagAggKPIBuilder(KPIBuilder[FlagAggKPIDefinition]):\n    \"\"\"\n    Fluent builder for creating FlagAggregation KPI definitions in bulk.\n\n    Provides a clean, declarative API for specifying multiple KPI definitions\n    at once. Particularly useful for creating large numbers of KPIs with\n    different combinations of flags and aggregations.\n\n    Example:\n\n        &gt;&gt;&gt; builder = FlagAggKPIBuilder()\n        &gt;&gt;&gt; definitions = (\n        ...     builder\n        ...     .for_flags(['BZ.market_price', 'BZ.net_position'])\n        ...     .with_aggregations([Aggregations.Mean, Aggregations.Max, Aggregations.Min])\n        ...     .for_all_objects()\n        ...     .build()\n        ... )\n        &gt;&gt;&gt; len(definitions)  # 2 flags \u00d7 3 aggs = 6\n\n    All methods return self for chaining.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize builder with default values.\"\"\"\n        super().__init__()\n        self._flags: list[FlagTypeProtocol] = []\n        self._aggregations: list[Aggregation] = []\n        self._objects: list[Hashable] | Literal['auto'] = 'auto'\n        self._model_flags: dict[FlagTypeProtocol, FlagTypeProtocol] = {}\n\n    def for_flag(self, flag: FlagTypeProtocol) -&gt; FlagAggKPIBuilder:\n        \"\"\"\n        Set a single flag.\n\n        Args:\n            flag: Flag (e.g., 'BZ.Results.market_price')\n\n        Returns:\n            Self for chaining\n        \"\"\"\n        self._flags = [flag]\n        return self\n\n    def for_flags(self, flags: list[FlagTypeProtocol]) -&gt; FlagAggKPIBuilder:\n        \"\"\"\n        Set multiple flags.\n\n        Args:\n            flags: List of flags\n\n        Returns:\n            Self for chaining\n        \"\"\"\n        self._flags = flags\n        return self\n\n    def with_aggregation(self, agg: Aggregation) -&gt; FlagAggKPIBuilder:\n        \"\"\"\n        Set a single aggregation.\n\n        Args:\n            agg: Aggregation function\n\n        Returns:\n            Self for chaining\n        \"\"\"\n        self._aggregations = [agg]\n        return self\n\n    def with_aggregations(self, aggs: list[Aggregation]) -&gt; FlagAggKPIBuilder:\n        \"\"\"\n        Set multiple aggregations.\n\n        Args:\n            aggs: List of aggregation functions\n\n        Returns:\n            Self for chaining\n        \"\"\"\n        self._aggregations = aggs\n        return self\n\n    def for_all_objects(self) -&gt; FlagAggKPIBuilder:\n        \"\"\"\n        Auto-discover objects from data.\n\n        Objects will be detected from DataFrame columns when\n        generate_kpis() is called.\n\n        Returns:\n            Self for chaining\n        \"\"\"\n        self._objects = 'auto'\n        return self\n\n    def for_objects_with_model_properties(\n            self,\n            properties: dict[str, Any] | None = None,\n            query_expr: str | None = None,\n            filter_funcs: dict[str, Callable[[Any], bool]] | None = None\n    ) -&gt; 'FlagAggKPIBuilder':\n        \"\"\"\n        Filter objects based on model properties during KPI generation.\n\n        During KPI generation, the model DataFrame is fetched and filtered using the\n        specified conditions. Only objects that:\n        1. Pass all filter conditions, AND\n        2. Exist in both model and flag DataFrames\n        will be included in the generated KPIs.\n\n        Three modes of operation (can be combined with AND logic):\n            1. Property filters: Exact match or list membership\n            2. Query expression: Pandas query string evaluated on model DataFrame\n            3. Filter functions: Custom functions applied to property columns\n\n        All conditions across all modes are combined with AND logic.\n\n        Args:\n            properties: Dict of property names to values. Scalars for exact match,\n                lists/sets for membership checks.\n            query_expr: Pandas query expression (uses engine=\"python\")\n            filter_funcs: Dict of property names to filter functions applied column-wise\n\n        Returns:\n            Self for chaining\n\n        Examples:\n\n            # Property filter - exact match and list membership\n            builder.for_objects_with_model_properties(\n                properties={'country': 'DE', 'type': ['wind', 'solar']}\n            )\n\n            # Query expression\n            builder.for_objects_with_model_properties(\n                query_expr='country == \"DE\" and voltage_kV &gt; 200'\n            )\n\n            # Custom filter function\n            builder.for_objects_with_model_properties(\n                filter_funcs={'voltage_kV': lambda x: x &gt; 200}\n            )\n\n            # Combined - properties AND query\n            builder.for_objects_with_model_properties(\n                properties={'country': 'DE'},\n                query_expr='voltage_kV &gt; 200'\n            )\n        \"\"\"\n        self._objects = ModelPropertyFilter(\n            properties=properties,\n            query_expr=query_expr,\n            filter_funcs=filter_funcs\n        )\n        return self\n\n    def for_object(self, object: Hashable) -&gt; FlagAggKPIBuilder:\n        \"\"\"\n        Specify explicit object.\n\n        Args:\n            object: object name / ID\n\n        Returns:\n            Self for chaining\n        \"\"\"\n        self._objects = [object]\n        return self\n\n    def for_objects(self, objects: list[Hashable]) -&gt; FlagAggKPIBuilder:\n        \"\"\"\n        Specify explicit object list.\n\n        Args:\n            objects: List of object names\n\n        Returns:\n            Self for chaining\n        \"\"\"\n        self._objects = objects\n        return self\n\n    def with_model_flag(self, flag: FlagTypeProtocol, model_flag: FlagTypeProtocol) -&gt; FlagAggKPIBuilder:\n        \"\"\"\n        Set explicit model flag for a specific flag.\n\n        Args:\n            flag: Variable flag\n            model_flag: Corresponding model flag\n\n        Returns:\n            Self for chaining\n        \"\"\"\n        self._model_flags[flag] = model_flag\n        return self\n\n    def build(self) -&gt; list[FlagAggKPIDefinition]:\n        \"\"\"\n        Generate all KPI definitions from builder configuration.\n\n        Creates the Cartesian product of flags \u00d7 aggregations.\n\n        Returns:\n            List of FlagAggKPIDefinition instances\n\n        Example:\n\n            &gt;&gt;&gt; builder = FlagAggKPIBuilder()\n            &gt;&gt;&gt; definitions = (\n            ...     builder\n            ...     .for_flags(['BZ.market_price', 'BZ.net_position'])\n            ...     .with_aggregations([Aggregations.Mean, Aggregations.Max, Aggregations.Min])\n            ...     .build()\n            ... )\n            &gt;&gt;&gt; len(definitions)  # 2 flags \u00d7 3 aggs = 6\n        \"\"\"\n        definitions = []\n\n        for flag in self._flags:\n            for agg in self._aggregations:\n                definition = FlagAggKPIDefinition(\n                    flag=flag,\n                    aggregation=agg,\n                    model_flag=self._model_flags.get(flag),\n                    objects=self._objects,\n                    name_prefix=self._name_prefix,\n                    name_suffix=self._name_suffix,\n                    custom_name=self._custom_name,\n                    extra_attributes=self._extra_attributes,\n                )\n                definitions.append(definition)\n\n        return definitions\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/flag_agg_builder/#mesqual.kpis.builders.flag_agg_builder.FlagAggKPIBuilder.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize builder with default values.</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/flag_agg_builder.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize builder with default values.\"\"\"\n    super().__init__()\n    self._flags: list[FlagTypeProtocol] = []\n    self._aggregations: list[Aggregation] = []\n    self._objects: list[Hashable] | Literal['auto'] = 'auto'\n    self._model_flags: dict[FlagTypeProtocol, FlagTypeProtocol] = {}\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/flag_agg_builder/#mesqual.kpis.builders.flag_agg_builder.FlagAggKPIBuilder.for_flag","title":"for_flag","text":"<pre><code>for_flag(flag: FlagTypeProtocol) -&gt; FlagAggKPIBuilder\n</code></pre> <p>Set a single flag.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>FlagTypeProtocol</code> <p>Flag (e.g., 'BZ.Results.market_price')</p> required <p>Returns:</p> Type Description <code>FlagAggKPIBuilder</code> <p>Self for chaining</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/flag_agg_builder.py</code> <pre><code>def for_flag(self, flag: FlagTypeProtocol) -&gt; FlagAggKPIBuilder:\n    \"\"\"\n    Set a single flag.\n\n    Args:\n        flag: Flag (e.g., 'BZ.Results.market_price')\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._flags = [flag]\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/flag_agg_builder/#mesqual.kpis.builders.flag_agg_builder.FlagAggKPIBuilder.for_flags","title":"for_flags","text":"<pre><code>for_flags(flags: list[FlagTypeProtocol]) -&gt; FlagAggKPIBuilder\n</code></pre> <p>Set multiple flags.</p> <p>Parameters:</p> Name Type Description Default <code>flags</code> <code>list[FlagTypeProtocol]</code> <p>List of flags</p> required <p>Returns:</p> Type Description <code>FlagAggKPIBuilder</code> <p>Self for chaining</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/flag_agg_builder.py</code> <pre><code>def for_flags(self, flags: list[FlagTypeProtocol]) -&gt; FlagAggKPIBuilder:\n    \"\"\"\n    Set multiple flags.\n\n    Args:\n        flags: List of flags\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._flags = flags\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/flag_agg_builder/#mesqual.kpis.builders.flag_agg_builder.FlagAggKPIBuilder.with_aggregation","title":"with_aggregation","text":"<pre><code>with_aggregation(agg: Aggregation) -&gt; FlagAggKPIBuilder\n</code></pre> <p>Set a single aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>agg</code> <code>Aggregation</code> <p>Aggregation function</p> required <p>Returns:</p> Type Description <code>FlagAggKPIBuilder</code> <p>Self for chaining</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/flag_agg_builder.py</code> <pre><code>def with_aggregation(self, agg: Aggregation) -&gt; FlagAggKPIBuilder:\n    \"\"\"\n    Set a single aggregation.\n\n    Args:\n        agg: Aggregation function\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._aggregations = [agg]\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/flag_agg_builder/#mesqual.kpis.builders.flag_agg_builder.FlagAggKPIBuilder.with_aggregations","title":"with_aggregations","text":"<pre><code>with_aggregations(aggs: list[Aggregation]) -&gt; FlagAggKPIBuilder\n</code></pre> <p>Set multiple aggregations.</p> <p>Parameters:</p> Name Type Description Default <code>aggs</code> <code>list[Aggregation]</code> <p>List of aggregation functions</p> required <p>Returns:</p> Type Description <code>FlagAggKPIBuilder</code> <p>Self for chaining</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/flag_agg_builder.py</code> <pre><code>def with_aggregations(self, aggs: list[Aggregation]) -&gt; FlagAggKPIBuilder:\n    \"\"\"\n    Set multiple aggregations.\n\n    Args:\n        aggs: List of aggregation functions\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._aggregations = aggs\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/flag_agg_builder/#mesqual.kpis.builders.flag_agg_builder.FlagAggKPIBuilder.for_all_objects","title":"for_all_objects","text":"<pre><code>for_all_objects() -&gt; FlagAggKPIBuilder\n</code></pre> <p>Auto-discover objects from data.</p> <p>Objects will be detected from DataFrame columns when generate_kpis() is called.</p> <p>Returns:</p> Type Description <code>FlagAggKPIBuilder</code> <p>Self for chaining</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/flag_agg_builder.py</code> <pre><code>def for_all_objects(self) -&gt; FlagAggKPIBuilder:\n    \"\"\"\n    Auto-discover objects from data.\n\n    Objects will be detected from DataFrame columns when\n    generate_kpis() is called.\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._objects = 'auto'\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/flag_agg_builder/#mesqual.kpis.builders.flag_agg_builder.FlagAggKPIBuilder.for_objects_with_model_properties","title":"for_objects_with_model_properties","text":"<pre><code>for_objects_with_model_properties(properties: dict[str, Any] | None = None, query_expr: str | None = None, filter_funcs: dict[str, Callable[[Any], bool]] | None = None) -&gt; 'FlagAggKPIBuilder'\n</code></pre> <p>Filter objects based on model properties during KPI generation.</p> <p>During KPI generation, the model DataFrame is fetched and filtered using the specified conditions. Only objects that: 1. Pass all filter conditions, AND 2. Exist in both model and flag DataFrames will be included in the generated KPIs.</p> <p>Three modes of operation (can be combined with AND logic):     1. Property filters: Exact match or list membership     2. Query expression: Pandas query string evaluated on model DataFrame     3. Filter functions: Custom functions applied to property columns</p> <p>All conditions across all modes are combined with AND logic.</p> <p>Parameters:</p> Name Type Description Default <code>properties</code> <code>dict[str, Any] | None</code> <p>Dict of property names to values. Scalars for exact match, lists/sets for membership checks.</p> <code>None</code> <code>query_expr</code> <code>str | None</code> <p>Pandas query expression (uses engine=\"python\")</p> <code>None</code> <code>filter_funcs</code> <code>dict[str, Callable[[Any], bool]] | None</code> <p>Dict of property names to filter functions applied column-wise</p> <code>None</code> <p>Returns:</p> Type Description <code>'FlagAggKPIBuilder'</code> <p>Self for chaining</p> <p>Examples:</p> <pre><code># Property filter - exact match and list membership\nbuilder.for_objects_with_model_properties(\n    properties={'country': 'DE', 'type': ['wind', 'solar']}\n)\n\n# Query expression\nbuilder.for_objects_with_model_properties(\n    query_expr='country == \"DE\" and voltage_kV &gt; 200'\n)\n\n# Custom filter function\nbuilder.for_objects_with_model_properties(\n    filter_funcs={'voltage_kV': lambda x: x &gt; 200}\n)\n\n# Combined - properties AND query\nbuilder.for_objects_with_model_properties(\n    properties={'country': 'DE'},\n    query_expr='voltage_kV &gt; 200'\n)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/builders/flag_agg_builder.py</code> <pre><code>def for_objects_with_model_properties(\n        self,\n        properties: dict[str, Any] | None = None,\n        query_expr: str | None = None,\n        filter_funcs: dict[str, Callable[[Any], bool]] | None = None\n) -&gt; 'FlagAggKPIBuilder':\n    \"\"\"\n    Filter objects based on model properties during KPI generation.\n\n    During KPI generation, the model DataFrame is fetched and filtered using the\n    specified conditions. Only objects that:\n    1. Pass all filter conditions, AND\n    2. Exist in both model and flag DataFrames\n    will be included in the generated KPIs.\n\n    Three modes of operation (can be combined with AND logic):\n        1. Property filters: Exact match or list membership\n        2. Query expression: Pandas query string evaluated on model DataFrame\n        3. Filter functions: Custom functions applied to property columns\n\n    All conditions across all modes are combined with AND logic.\n\n    Args:\n        properties: Dict of property names to values. Scalars for exact match,\n            lists/sets for membership checks.\n        query_expr: Pandas query expression (uses engine=\"python\")\n        filter_funcs: Dict of property names to filter functions applied column-wise\n\n    Returns:\n        Self for chaining\n\n    Examples:\n\n        # Property filter - exact match and list membership\n        builder.for_objects_with_model_properties(\n            properties={'country': 'DE', 'type': ['wind', 'solar']}\n        )\n\n        # Query expression\n        builder.for_objects_with_model_properties(\n            query_expr='country == \"DE\" and voltage_kV &gt; 200'\n        )\n\n        # Custom filter function\n        builder.for_objects_with_model_properties(\n            filter_funcs={'voltage_kV': lambda x: x &gt; 200}\n        )\n\n        # Combined - properties AND query\n        builder.for_objects_with_model_properties(\n            properties={'country': 'DE'},\n            query_expr='voltage_kV &gt; 200'\n        )\n    \"\"\"\n    self._objects = ModelPropertyFilter(\n        properties=properties,\n        query_expr=query_expr,\n        filter_funcs=filter_funcs\n    )\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/flag_agg_builder/#mesqual.kpis.builders.flag_agg_builder.FlagAggKPIBuilder.for_object","title":"for_object","text":"<pre><code>for_object(object: Hashable) -&gt; FlagAggKPIBuilder\n</code></pre> <p>Specify explicit object.</p> <p>Parameters:</p> Name Type Description Default <code>object</code> <code>Hashable</code> <p>object name / ID</p> required <p>Returns:</p> Type Description <code>FlagAggKPIBuilder</code> <p>Self for chaining</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/flag_agg_builder.py</code> <pre><code>def for_object(self, object: Hashable) -&gt; FlagAggKPIBuilder:\n    \"\"\"\n    Specify explicit object.\n\n    Args:\n        object: object name / ID\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._objects = [object]\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/flag_agg_builder/#mesqual.kpis.builders.flag_agg_builder.FlagAggKPIBuilder.for_objects","title":"for_objects","text":"<pre><code>for_objects(objects: list[Hashable]) -&gt; FlagAggKPIBuilder\n</code></pre> <p>Specify explicit object list.</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <code>list[Hashable]</code> <p>List of object names</p> required <p>Returns:</p> Type Description <code>FlagAggKPIBuilder</code> <p>Self for chaining</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/flag_agg_builder.py</code> <pre><code>def for_objects(self, objects: list[Hashable]) -&gt; FlagAggKPIBuilder:\n    \"\"\"\n    Specify explicit object list.\n\n    Args:\n        objects: List of object names\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._objects = objects\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/flag_agg_builder/#mesqual.kpis.builders.flag_agg_builder.FlagAggKPIBuilder.with_model_flag","title":"with_model_flag","text":"<pre><code>with_model_flag(flag: FlagTypeProtocol, model_flag: FlagTypeProtocol) -&gt; FlagAggKPIBuilder\n</code></pre> <p>Set explicit model flag for a specific flag.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>FlagTypeProtocol</code> <p>Variable flag</p> required <code>model_flag</code> <code>FlagTypeProtocol</code> <p>Corresponding model flag</p> required <p>Returns:</p> Type Description <code>FlagAggKPIBuilder</code> <p>Self for chaining</p> Source code in <code>submodules/mesqual/mesqual/kpis/builders/flag_agg_builder.py</code> <pre><code>def with_model_flag(self, flag: FlagTypeProtocol, model_flag: FlagTypeProtocol) -&gt; FlagAggKPIBuilder:\n    \"\"\"\n    Set explicit model flag for a specific flag.\n\n    Args:\n        flag: Variable flag\n        model_flag: Corresponding model flag\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._model_flags[flag] = model_flag\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/builders/flag_agg_builder/#mesqual.kpis.builders.flag_agg_builder.FlagAggKPIBuilder.build","title":"build","text":"<pre><code>build() -&gt; list[FlagAggKPIDefinition]\n</code></pre> <p>Generate all KPI definitions from builder configuration.</p> <p>Creates the Cartesian product of flags \u00d7 aggregations.</p> <p>Returns:</p> Type Description <code>list[FlagAggKPIDefinition]</code> <p>List of FlagAggKPIDefinition instances</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; builder = FlagAggKPIBuilder()\n&gt;&gt;&gt; definitions = (\n...     builder\n...     .for_flags(['BZ.market_price', 'BZ.net_position'])\n...     .with_aggregations([Aggregations.Mean, Aggregations.Max, Aggregations.Min])\n...     .build()\n... )\n&gt;&gt;&gt; len(definitions)  # 2 flags \u00d7 3 aggs = 6\n</code></pre> Source code in <code>submodules/mesqual/mesqual/kpis/builders/flag_agg_builder.py</code> <pre><code>def build(self) -&gt; list[FlagAggKPIDefinition]:\n    \"\"\"\n    Generate all KPI definitions from builder configuration.\n\n    Creates the Cartesian product of flags \u00d7 aggregations.\n\n    Returns:\n        List of FlagAggKPIDefinition instances\n\n    Example:\n\n        &gt;&gt;&gt; builder = FlagAggKPIBuilder()\n        &gt;&gt;&gt; definitions = (\n        ...     builder\n        ...     .for_flags(['BZ.market_price', 'BZ.net_position'])\n        ...     .with_aggregations([Aggregations.Mean, Aggregations.Max, Aggregations.Min])\n        ...     .build()\n        ... )\n        &gt;&gt;&gt; len(definitions)  # 2 flags \u00d7 3 aggs = 6\n    \"\"\"\n    definitions = []\n\n    for flag in self._flags:\n        for agg in self._aggregations:\n            definition = FlagAggKPIDefinition(\n                flag=flag,\n                aggregation=agg,\n                model_flag=self._model_flags.get(flag),\n                objects=self._objects,\n                name_prefix=self._name_prefix,\n                name_suffix=self._name_suffix,\n                custom_name=self._custom_name,\n                extra_attributes=self._extra_attributes,\n            )\n            definitions.append(definition)\n\n    return definitions\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/","title":"KPI Definitions","text":"<p>KPI Definitions - Specifications for KPI computation.</p>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/base/","title":"Base KPI Definition","text":""},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/base/#mesqual.kpis.definitions.base.KPIDefinition","title":"KPIDefinition","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for KPI specifications.</p> <p>Defines WHAT to compute, not HOW to compute efficiently. Subclasses implement generate_kpis() to create KPI instances.</p> <p>A KPIDefinition is a lightweight specification that can be created once and used to generate KPIs for multiple datasets.</p> Source code in <code>submodules/mesqual/mesqual/kpis/definitions/base.py</code> <pre><code>class KPIDefinition(ABC):\n    \"\"\"\n    Abstract base for KPI specifications.\n\n    Defines WHAT to compute, not HOW to compute efficiently.\n    Subclasses implement generate_kpis() to create KPI instances.\n\n    A KPIDefinition is a lightweight specification that can be created\n    once and used to generate KPIs for multiple datasets.\n    \"\"\"\n\n    @abstractmethod\n    def generate_kpis(self, dataset: Dataset) -&gt; list[KPI]:\n        \"\"\"\n        Generate KPI instances from this definition.\n\n        This method is responsible for:\n        1. Fetching required data from dataset\n        2. Computing values\n        3. Creating KPI instances with proper attributes\n\n        Args:\n            dataset: Dataset to compute KPIs for\n\n        Returns:\n            List of computed KPI instances\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def required_flags(self) -&gt; set[FlagTypeProtocol]:\n        \"\"\"\n        Return set of flags needed for computation.\n\n        Returns:\n            Set of flags required by this definition\n        \"\"\"\n        pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/base/#mesqual.kpis.definitions.base.KPIDefinition.generate_kpis","title":"generate_kpis  <code>abstractmethod</code>","text":"<pre><code>generate_kpis(dataset: Dataset) -&gt; list[KPI]\n</code></pre> <p>Generate KPI instances from this definition.</p> <p>This method is responsible for: 1. Fetching required data from dataset 2. Computing values 3. Creating KPI instances with proper attributes</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset to compute KPIs for</p> required <p>Returns:</p> Type Description <code>list[KPI]</code> <p>List of computed KPI instances</p> Source code in <code>submodules/mesqual/mesqual/kpis/definitions/base.py</code> <pre><code>@abstractmethod\ndef generate_kpis(self, dataset: Dataset) -&gt; list[KPI]:\n    \"\"\"\n    Generate KPI instances from this definition.\n\n    This method is responsible for:\n    1. Fetching required data from dataset\n    2. Computing values\n    3. Creating KPI instances with proper attributes\n\n    Args:\n        dataset: Dataset to compute KPIs for\n\n    Returns:\n        List of computed KPI instances\n    \"\"\"\n    pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/base/#mesqual.kpis.definitions.base.KPIDefinition.required_flags","title":"required_flags  <code>abstractmethod</code>","text":"<pre><code>required_flags() -&gt; set[FlagTypeProtocol]\n</code></pre> <p>Return set of flags needed for computation.</p> <p>Returns:</p> Type Description <code>set[FlagTypeProtocol]</code> <p>Set of flags required by this definition</p> Source code in <code>submodules/mesqual/mesqual/kpis/definitions/base.py</code> <pre><code>@abstractmethod\ndef required_flags(self) -&gt; set[FlagTypeProtocol]:\n    \"\"\"\n    Return set of flags needed for computation.\n\n    Returns:\n        Set of flags required by this definition\n    \"\"\"\n    pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/comparison/","title":"Comparison KPI Definition","text":""},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/comparison/#mesqual.kpis.definitions.comparison.ComparisonKPIDefinition","title":"ComparisonKPIDefinition  <code>dataclass</code>","text":"<p>               Bases: <code>KPIDefinition</code></p> <p>KPI definition for comparing values between two datasets.</p> Creates KPIs by <ol> <li>Generating base KPIs from base_definition for reference and variation datasets</li> <li>Applying comparison operation to corresponding KPI values</li> <li>Creating comparison KPI instances with proper metadata</li> </ol> <p>This is typically used to compare scenarios (e.g., \"Price increase in high_res vs base scenario\").</p> <p>Attributes:</p> Name Type Description <code>base_definition</code> <code>KPIDefinition</code> <p>Definition to generate base KPIs from</p> <code>comparison</code> <code>ValueComparison</code> <p>Comparison operation to apply (e.g., ValueComparisons.Increase)</p> <code>name_prefix</code> <code>str</code> <p>Optional prefix for comparison KPI names</p> <code>name_suffix</code> <code>str</code> <p>Optional suffix for comparison KPI names</p> Source code in <code>submodules/mesqual/mesqual/kpis/definitions/comparison.py</code> <pre><code>@dataclass\nclass ComparisonKPIDefinition(KPIDefinition):\n    \"\"\"\n    KPI definition for comparing values between two datasets.\n\n    Creates KPIs by:\n        1. Generating base KPIs from base_definition for reference and variation datasets\n        2. Applying comparison operation to corresponding KPI values\n        3. Creating comparison KPI instances with proper metadata\n\n    This is typically used to compare scenarios (e.g., \"Price increase in\n    high_res vs base scenario\").\n\n    Attributes:\n        base_definition: Definition to generate base KPIs from\n        comparison: Comparison operation to apply (e.g., ValueComparisons.Increase)\n        name_prefix: Optional prefix for comparison KPI names\n        name_suffix: Optional suffix for comparison KPI names\n    \"\"\"\n\n    base_definition: KPIDefinition\n    comparison: ValueComparison\n    name_prefix: str = ''\n    name_suffix: str = ''\n    extra_attributes: dict = None\n    custom_name: str | None = None\n\n    def generate_kpis(self, dataset: DatasetComparison) -&gt; list[KPI]:\n        \"\"\"\n        Generate comparison KPIs from a DatasetComparison.\n\n        Process:\n            1. Get reference and variation datasets from comparison dataset\n            2. Generate base KPIs for both datasets\n            3. Match KPIs by object_name\n            4. Apply comparison operation\n            5. Create comparison KPI instances\n\n        Args:\n            dataset: DatasetComparison containing reference and variation datasets\n\n        Returns:\n            List of comparison KPI instances\n\n        Raises:\n            AttributeError: If dataset is not a DatasetComparison\n        \"\"\"\n        if not isinstance(dataset, DatasetComparison):\n            raise TypeError(\n                f\"ComparisonKPIDefinition requires a {DatasetComparison.__name__}, got {type(dataset).__name__}\"\n            )\n\n        reference_dataset = dataset.reference_dataset\n        variation_dataset = dataset.variation_dataset\n\n        reference_kpis = self.base_definition.generate_kpis(reference_dataset)\n        variation_kpis = self.base_definition.generate_kpis(variation_dataset)\n\n        variation_kpis_by_object = {\n            kpi.attributes.object_name: kpi\n            for kpi in variation_kpis\n        }\n\n        comparison_kpis = []\n\n        for ref_kpi in reference_kpis:\n            obj_name = ref_kpi.attributes.object_name\n            if obj_name not in variation_kpis_by_object:\n                continue\n\n            var_kpi = variation_kpis_by_object[obj_name]\n            comparison_value = self.comparison(var_kpi.value, ref_kpi.value)\n\n            # If custom_name is set and there are multiple objects, append object name\n            kpi_custom_name = self.custom_name\n            if self.custom_name and len(reference_kpis) &gt; 1:\n                kpi_custom_name = f\"{self.custom_name} {obj_name}\"\n\n            unit = self.comparison.unit or ref_kpi.attributes.unit\n            attributes = KPIAttributes(\n                flag=ref_kpi.attributes.flag,\n                model_flag=ref_kpi.attributes.model_flag,\n                object_name=obj_name,\n                aggregation=ref_kpi.attributes.aggregation,\n                dataset_name=dataset.name,\n                dataset_type=type(dataset),\n                value_comparison=self.comparison,\n                reference_dataset_name=reference_dataset.name,\n                variation_dataset_name=variation_dataset.name,\n                name_prefix=self.name_prefix,\n                name_suffix=self.name_suffix,\n                custom_name=kpi_custom_name,\n                unit=unit,\n                dataset_attributes=dataset.attributes,\n                extra_attributes=self.extra_attributes or dict()\n            )\n\n            comparison_kpi = KPI(\n                value=comparison_value,\n                attributes=attributes,\n                dataset=dataset\n            )\n            for k in [var_kpi, ref_kpi]:\n                obj_info = k.get_object_info_from_model()\n                if isinstance(obj_info, pd.Series) and not obj_info.empty:\n                    comparison_kpi._object_info = obj_info\n                    break\n\n            comparison_kpis.append(comparison_kpi)\n\n        return comparison_kpis\n\n    def required_flags(self) -&gt; set[FlagTypeProtocol]:\n        \"\"\"\n        Return required flags from base definition.\n\n        Returns:\n            Set of flags required by base definition\n        \"\"\"\n        return self.base_definition.required_flags()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/comparison/#mesqual.kpis.definitions.comparison.ComparisonKPIDefinition.generate_kpis","title":"generate_kpis","text":"<pre><code>generate_kpis(dataset: DatasetComparison) -&gt; list[KPI]\n</code></pre> <p>Generate comparison KPIs from a DatasetComparison.</p> Process <ol> <li>Get reference and variation datasets from comparison dataset</li> <li>Generate base KPIs for both datasets</li> <li>Match KPIs by object_name</li> <li>Apply comparison operation</li> <li>Create comparison KPI instances</li> </ol> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DatasetComparison</code> <p>DatasetComparison containing reference and variation datasets</p> required <p>Returns:</p> Type Description <code>list[KPI]</code> <p>List of comparison KPI instances</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If dataset is not a DatasetComparison</p> Source code in <code>submodules/mesqual/mesqual/kpis/definitions/comparison.py</code> <pre><code>def generate_kpis(self, dataset: DatasetComparison) -&gt; list[KPI]:\n    \"\"\"\n    Generate comparison KPIs from a DatasetComparison.\n\n    Process:\n        1. Get reference and variation datasets from comparison dataset\n        2. Generate base KPIs for both datasets\n        3. Match KPIs by object_name\n        4. Apply comparison operation\n        5. Create comparison KPI instances\n\n    Args:\n        dataset: DatasetComparison containing reference and variation datasets\n\n    Returns:\n        List of comparison KPI instances\n\n    Raises:\n        AttributeError: If dataset is not a DatasetComparison\n    \"\"\"\n    if not isinstance(dataset, DatasetComparison):\n        raise TypeError(\n            f\"ComparisonKPIDefinition requires a {DatasetComparison.__name__}, got {type(dataset).__name__}\"\n        )\n\n    reference_dataset = dataset.reference_dataset\n    variation_dataset = dataset.variation_dataset\n\n    reference_kpis = self.base_definition.generate_kpis(reference_dataset)\n    variation_kpis = self.base_definition.generate_kpis(variation_dataset)\n\n    variation_kpis_by_object = {\n        kpi.attributes.object_name: kpi\n        for kpi in variation_kpis\n    }\n\n    comparison_kpis = []\n\n    for ref_kpi in reference_kpis:\n        obj_name = ref_kpi.attributes.object_name\n        if obj_name not in variation_kpis_by_object:\n            continue\n\n        var_kpi = variation_kpis_by_object[obj_name]\n        comparison_value = self.comparison(var_kpi.value, ref_kpi.value)\n\n        # If custom_name is set and there are multiple objects, append object name\n        kpi_custom_name = self.custom_name\n        if self.custom_name and len(reference_kpis) &gt; 1:\n            kpi_custom_name = f\"{self.custom_name} {obj_name}\"\n\n        unit = self.comparison.unit or ref_kpi.attributes.unit\n        attributes = KPIAttributes(\n            flag=ref_kpi.attributes.flag,\n            model_flag=ref_kpi.attributes.model_flag,\n            object_name=obj_name,\n            aggregation=ref_kpi.attributes.aggregation,\n            dataset_name=dataset.name,\n            dataset_type=type(dataset),\n            value_comparison=self.comparison,\n            reference_dataset_name=reference_dataset.name,\n            variation_dataset_name=variation_dataset.name,\n            name_prefix=self.name_prefix,\n            name_suffix=self.name_suffix,\n            custom_name=kpi_custom_name,\n            unit=unit,\n            dataset_attributes=dataset.attributes,\n            extra_attributes=self.extra_attributes or dict()\n        )\n\n        comparison_kpi = KPI(\n            value=comparison_value,\n            attributes=attributes,\n            dataset=dataset\n        )\n        for k in [var_kpi, ref_kpi]:\n            obj_info = k.get_object_info_from_model()\n            if isinstance(obj_info, pd.Series) and not obj_info.empty:\n                comparison_kpi._object_info = obj_info\n                break\n\n        comparison_kpis.append(comparison_kpi)\n\n    return comparison_kpis\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/comparison/#mesqual.kpis.definitions.comparison.ComparisonKPIDefinition.required_flags","title":"required_flags","text":"<pre><code>required_flags() -&gt; set[FlagTypeProtocol]\n</code></pre> <p>Return required flags from base definition.</p> <p>Returns:</p> Type Description <code>set[FlagTypeProtocol]</code> <p>Set of flags required by base definition</p> Source code in <code>submodules/mesqual/mesqual/kpis/definitions/comparison.py</code> <pre><code>def required_flags(self) -&gt; set[FlagTypeProtocol]:\n    \"\"\"\n    Return required flags from base definition.\n\n    Returns:\n        Set of flags required by base definition\n    \"\"\"\n    return self.base_definition.required_flags()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/custom/","title":"Custom KPI Definition","text":""},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/custom/#mesqual.kpis.definitions.custom.CustomKPIDefinition","title":"CustomKPIDefinition","text":"<p>               Bases: <code>KPIDefinition</code></p> <p>Base class for custom KPI computation logic.</p> Supports two computation patterns <ol> <li>Batch computation: Override compute_batch()</li> <li>Per-object computation: Override compute_for_object()</li> </ol> Choose the pattern that best fits your use case <ul> <li>Use batch for efficient vectorized operations across all objects</li> <li>Use per_object for complex logic that varies significantly per object</li> </ul> Source code in <code>submodules/mesqual/mesqual/kpis/definitions/custom.py</code> <pre><code>class CustomKPIDefinition(KPIDefinition):\n    \"\"\"\n    Base class for custom KPI computation logic.\n\n    Supports two computation patterns:\n        1. Batch computation: Override compute_batch()\n        2. Per-object computation: Override compute_for_object()\n\n    Choose the pattern that best fits your use case:\n        - Use batch for efficient vectorized operations across all objects\n        - Use per_object for complex logic that varies significantly per object\n    \"\"\"\n\n    def __init__(\n        self,\n        flag: FlagTypeProtocol,\n        model_flag: FlagTypeProtocol | None = None,\n        objects: list[Hashable] | Literal['auto'] = 'auto',\n        name_prefix: str = '',\n        name_suffix: str = '',\n        extra_attributes: dict = None,\n        aggregation: Aggregation | None = None\n    ):\n        \"\"\"\n        Initialize custom KPI definition.\n\n        Args:\n            flag: Variable flag for the KPI\n            model_flag: Optional model flag (auto-inferred if None)\n            objects: List of objects or 'auto' to discover\n            name_prefix: Prefix for KPI names\n            name_suffix: Suffix for KPI names\n            aggregation: Optional aggregation (for metadata only, not used in computation)\n        \"\"\"\n        self.flag = flag\n        self.model_flag = model_flag\n        self.objects = objects\n        self.name_prefix = name_prefix\n        self.name_suffix = name_suffix\n        self.extra_attributes = extra_attributes\n        self.aggregation = aggregation\n\n    def generate_kpis(self, dataset: Dataset) -&gt; list[KPI]:\n        \"\"\"\n        Generate KPIs using either per-object or batch computation.\n\n        Args:\n            dataset: Dataset to compute KPIs for\n\n        Returns:\n            List of computed KPI instances\n        \"\"\"\n        model_flag = self.model_flag or dataset.flag_index.get_linked_model_flag(self.flag)\n\n        if self.objects == 'auto':\n            objects = dataset.fetch(self.flag).columns.tolist()\n        else:\n            objects = self.objects\n\n        try:\n            return self._generate_kpis_batch(dataset, model_flag, objects)\n        except NotImplementedError:\n            pass\n        try:\n            return self._generate_kpis_per_object(dataset, model_flag, objects)\n        except NotImplementedError:\n            raise NotImplementedError(\"Must override compute_for_object() or compute_batch().\")\n\n    def _generate_kpis_per_object(\n        self,\n        dataset: Dataset,\n        model_flag: FlagTypeProtocol,\n        objects: list[Hashable]\n    ) -&gt; list[KPI]:\n        \"\"\"\n        Generate KPIs by calling compute_for_object() for each object.\n\n        Args:\n            dataset: Dataset to compute for\n            model_flag: Model flag for objects\n            objects: List of object names\n\n        Returns:\n            List of KPI instances\n        \"\"\"\n        kpis = []\n        for obj in objects:\n            value = self.compute_for_object(dataset, obj)\n\n            attributes = self._build_attributes(obj, dataset, model_flag)\n\n            kpi = KPI(\n                value=value,\n                attributes=attributes,\n                dataset=dataset\n            )\n            kpis.append(kpi)\n\n        return kpis\n\n    def _generate_kpis_batch(\n        self,\n        dataset: Dataset,\n        model_flag: FlagTypeProtocol,\n        objects: list[Hashable]\n    ) -&gt; list[KPI]:\n        \"\"\"\n        Generate KPIs by calling compute_batch() once.\n\n        Args:\n            dataset: Dataset to compute for\n            model_flag: Model flag for objects\n            objects: List of object names\n\n        Returns:\n            List of KPI instances\n        \"\"\"\n        # Compute all values at once\n        values_dict = self.compute_batch(dataset, objects)\n\n        kpis = []\n        for obj, value in values_dict.items():\n            attributes = self._build_attributes(obj, dataset, model_flag)\n\n            kpi = KPI(\n                value=value,\n                attributes=attributes,\n                dataset=dataset\n            )\n            kpis.append(kpi)\n\n        return kpis\n\n    def compute_for_object(self, dataset: Dataset, object_name: Hashable) -&gt; Any:\n        \"\"\"\n        Compute KPI value for a single object.\n\n        Override this for per-object computation pattern.\n\n        Args:\n            dataset: Dataset to fetch data from\n            object_name: Name of the object to compute for\n\n        Returns:\n            Computed KPI value\n\n        Raises:\n            NotImplementedError: If not overridden\n        \"\"\"\n        raise NotImplementedError(\"Must override compute_for_object() or compute_batch()\")\n\n    def compute_batch(self, dataset: Dataset, objects: list[Hashable]) -&gt; dict[Hashable, Any]:\n        \"\"\"\n        Compute KPI values for all objects at once.\n\n        Override this for batch computation pattern.\n\n        Args:\n            dataset: Dataset to fetch data from\n            objects: List of object names to compute for\n\n        Returns:\n            Dict mapping object_name \u2192 value\n\n        Raises:\n            NotImplementedError: If not overridden\n        \"\"\"\n        raise NotImplementedError(\"Must override compute_for_object() or compute_batch()\")\n\n    @abstractmethod\n    def get_unit(self) -&gt; Units.Unit:\n        \"\"\"\n        Return the unit for this KPI type.\n\n        Returns:\n            Physical unit for the KPI values\n        \"\"\"\n        pass\n\n    def _build_attributes(\n        self,\n        object_name: Hashable,\n        dataset: Dataset,\n        model_flag: FlagTypeProtocol\n    ) -&gt; KPIAttributes:\n        \"\"\"\n        Build KPIAttributes for a KPI instance.\n\n        Args:\n            object_name: Object identifier\n            dataset: Source dataset\n            model_flag: Model flag for the object\n\n        Returns:\n            KPIAttributes instance\n        \"\"\"\n\n        return KPIAttributes(\n            flag=self.flag,\n            model_flag=model_flag,\n            object_name=object_name,\n            aggregation=self.aggregation,\n            dataset_name=dataset.name,\n            dataset_type=type(dataset),\n            name_prefix=self.name_prefix,\n            name_suffix=self.name_suffix,\n            unit=self.get_unit(),\n            dataset_attributes=dataset.attributes,\n            extra_attributes=self.extra_attributes or dict()\n        )\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/custom/#mesqual.kpis.definitions.custom.CustomKPIDefinition.__init__","title":"__init__","text":"<pre><code>__init__(flag: FlagTypeProtocol, model_flag: FlagTypeProtocol | None = None, objects: list[Hashable] | Literal['auto'] = 'auto', name_prefix: str = '', name_suffix: str = '', extra_attributes: dict = None, aggregation: Aggregation | None = None)\n</code></pre> <p>Initialize custom KPI definition.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>FlagTypeProtocol</code> <p>Variable flag for the KPI</p> required <code>model_flag</code> <code>FlagTypeProtocol | None</code> <p>Optional model flag (auto-inferred if None)</p> <code>None</code> <code>objects</code> <code>list[Hashable] | Literal['auto']</code> <p>List of objects or 'auto' to discover</p> <code>'auto'</code> <code>name_prefix</code> <code>str</code> <p>Prefix for KPI names</p> <code>''</code> <code>name_suffix</code> <code>str</code> <p>Suffix for KPI names</p> <code>''</code> <code>aggregation</code> <code>Aggregation | None</code> <p>Optional aggregation (for metadata only, not used in computation)</p> <code>None</code> Source code in <code>submodules/mesqual/mesqual/kpis/definitions/custom.py</code> <pre><code>def __init__(\n    self,\n    flag: FlagTypeProtocol,\n    model_flag: FlagTypeProtocol | None = None,\n    objects: list[Hashable] | Literal['auto'] = 'auto',\n    name_prefix: str = '',\n    name_suffix: str = '',\n    extra_attributes: dict = None,\n    aggregation: Aggregation | None = None\n):\n    \"\"\"\n    Initialize custom KPI definition.\n\n    Args:\n        flag: Variable flag for the KPI\n        model_flag: Optional model flag (auto-inferred if None)\n        objects: List of objects or 'auto' to discover\n        name_prefix: Prefix for KPI names\n        name_suffix: Suffix for KPI names\n        aggregation: Optional aggregation (for metadata only, not used in computation)\n    \"\"\"\n    self.flag = flag\n    self.model_flag = model_flag\n    self.objects = objects\n    self.name_prefix = name_prefix\n    self.name_suffix = name_suffix\n    self.extra_attributes = extra_attributes\n    self.aggregation = aggregation\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/custom/#mesqual.kpis.definitions.custom.CustomKPIDefinition.generate_kpis","title":"generate_kpis","text":"<pre><code>generate_kpis(dataset: Dataset) -&gt; list[KPI]\n</code></pre> <p>Generate KPIs using either per-object or batch computation.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset to compute KPIs for</p> required <p>Returns:</p> Type Description <code>list[KPI]</code> <p>List of computed KPI instances</p> Source code in <code>submodules/mesqual/mesqual/kpis/definitions/custom.py</code> <pre><code>def generate_kpis(self, dataset: Dataset) -&gt; list[KPI]:\n    \"\"\"\n    Generate KPIs using either per-object or batch computation.\n\n    Args:\n        dataset: Dataset to compute KPIs for\n\n    Returns:\n        List of computed KPI instances\n    \"\"\"\n    model_flag = self.model_flag or dataset.flag_index.get_linked_model_flag(self.flag)\n\n    if self.objects == 'auto':\n        objects = dataset.fetch(self.flag).columns.tolist()\n    else:\n        objects = self.objects\n\n    try:\n        return self._generate_kpis_batch(dataset, model_flag, objects)\n    except NotImplementedError:\n        pass\n    try:\n        return self._generate_kpis_per_object(dataset, model_flag, objects)\n    except NotImplementedError:\n        raise NotImplementedError(\"Must override compute_for_object() or compute_batch().\")\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/custom/#mesqual.kpis.definitions.custom.CustomKPIDefinition.compute_for_object","title":"compute_for_object","text":"<pre><code>compute_for_object(dataset: Dataset, object_name: Hashable) -&gt; Any\n</code></pre> <p>Compute KPI value for a single object.</p> <p>Override this for per-object computation pattern.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset to fetch data from</p> required <code>object_name</code> <code>Hashable</code> <p>Name of the object to compute for</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Computed KPI value</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not overridden</p> Source code in <code>submodules/mesqual/mesqual/kpis/definitions/custom.py</code> <pre><code>def compute_for_object(self, dataset: Dataset, object_name: Hashable) -&gt; Any:\n    \"\"\"\n    Compute KPI value for a single object.\n\n    Override this for per-object computation pattern.\n\n    Args:\n        dataset: Dataset to fetch data from\n        object_name: Name of the object to compute for\n\n    Returns:\n        Computed KPI value\n\n    Raises:\n        NotImplementedError: If not overridden\n    \"\"\"\n    raise NotImplementedError(\"Must override compute_for_object() or compute_batch()\")\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/custom/#mesqual.kpis.definitions.custom.CustomKPIDefinition.compute_batch","title":"compute_batch","text":"<pre><code>compute_batch(dataset: Dataset, objects: list[Hashable]) -&gt; dict[Hashable, Any]\n</code></pre> <p>Compute KPI values for all objects at once.</p> <p>Override this for batch computation pattern.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset to fetch data from</p> required <code>objects</code> <code>list[Hashable]</code> <p>List of object names to compute for</p> required <p>Returns:</p> Type Description <code>dict[Hashable, Any]</code> <p>Dict mapping object_name \u2192 value</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not overridden</p> Source code in <code>submodules/mesqual/mesqual/kpis/definitions/custom.py</code> <pre><code>def compute_batch(self, dataset: Dataset, objects: list[Hashable]) -&gt; dict[Hashable, Any]:\n    \"\"\"\n    Compute KPI values for all objects at once.\n\n    Override this for batch computation pattern.\n\n    Args:\n        dataset: Dataset to fetch data from\n        objects: List of object names to compute for\n\n    Returns:\n        Dict mapping object_name \u2192 value\n\n    Raises:\n        NotImplementedError: If not overridden\n    \"\"\"\n    raise NotImplementedError(\"Must override compute_for_object() or compute_batch()\")\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/custom/#mesqual.kpis.definitions.custom.CustomKPIDefinition.get_unit","title":"get_unit  <code>abstractmethod</code>","text":"<pre><code>get_unit() -&gt; Unit\n</code></pre> <p>Return the unit for this KPI type.</p> <p>Returns:</p> Type Description <code>Unit</code> <p>Physical unit for the KPI values</p> Source code in <code>submodules/mesqual/mesqual/kpis/definitions/custom.py</code> <pre><code>@abstractmethod\ndef get_unit(self) -&gt; Units.Unit:\n    \"\"\"\n    Return the unit for this KPI type.\n\n    Returns:\n        Physical unit for the KPI values\n    \"\"\"\n    pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/flag_aggregation/","title":"Flag Aggregation Definition","text":""},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/flag_aggregation/#mesqual.kpis.definitions.flag_aggregation.FlagAggKPIDefinition","title":"FlagAggKPIDefinition  <code>dataclass</code>","text":"<p>               Bases: <code>KPIDefinition</code></p> <p>Simple flag + aggregation KPI definition.</p> <p>Computes KPIs by: 1. Fetching flag data (DataFrame with objects as columns) 2. Applying aggregation (vectorized across all columns) 3. Creating one KPI per object (column)</p> <p>This is the most common KPI type for energy system analysis.</p> Source code in <code>submodules/mesqual/mesqual/kpis/definitions/flag_aggregation.py</code> <pre><code>@dataclass\nclass FlagAggKPIDefinition(KPIDefinition):\n    \"\"\"\n    Simple flag + aggregation KPI definition.\n\n    Computes KPIs by:\n    1. Fetching flag data (DataFrame with objects as columns)\n    2. Applying aggregation (vectorized across all columns)\n    3. Creating one KPI per object (column)\n\n    This is the most common KPI type for energy system analysis.\n    \"\"\"\n\n    flag: FlagTypeProtocol\n    aggregation: Aggregation\n    model_flag: FlagTypeProtocol | None = None  # Auto-inferred if None\n    objects: list[Hashable] | Literal['auto'] | ModelPropertyFilter = 'auto'  # 'auto' = discover from data\n    extra_attributes: dict = None\n    name_prefix: str = ''\n    name_suffix: str = ''\n    custom_name: str | None = None  # Complete name override\n\n    def generate_kpis(self, dataset: Dataset) -&gt; list[KPI]:\n        \"\"\"\n        Generate KPIs by batch computation.\n\n        Process:\n        1. Infer model_flag if not provided\n        2. Fetch data (DataFrame)\n        3. Discover objects from DataFrame columns if objects='auto'\n        4. Apply aggregation to entire DataFrame (vectorized)\n        5. Create KPI instance per object with metadata\n\n        Args:\n            dataset: Dataset to compute KPIs for\n\n        Returns:\n            List of computed KPI instances\n        \"\"\"\n        from mesqual.kpis.builders.flag_agg_builder import ModelPropertyFilter\n\n        model_flag = self.model_flag or dataset.flag_index.get_linked_model_flag(self.flag)\n\n        df = dataset.fetch(self.flag)\n\n        model_df = dataset.fetch(model_flag) if dataset.flag_is_accepted(model_flag) else None\n        if self.objects == 'auto':\n            objects = df.columns.tolist()\n        elif isinstance(self.objects, ModelPropertyFilter):\n            if model_df is None:\n                raise Exception(f'No Model DF found for model_flag \"{model_flag}\" in variable_flag \"{self.flag}\"')\n            filtered_model_df_objects = self.objects.apply_filter(model_df)\n            objects = [o for o in df.columns if o in filtered_model_df_objects]\n        else:\n            objects = self.objects\n\n        aggregated = self.aggregation(df)  # Returns Series with one value per column\n\n        dataset_type = type(dataset)\n\n        unit = dataset.flag_index.get_unit(self.flag)\n        if self.aggregation.unit is not None:\n            unit = self.aggregation.unit\n\n        kpis = []\n        for obj in objects:\n            if obj not in aggregated.index:\n                # TODO: optional warning if object listed but not present in flag\n                continue  # Skip objects not in aggregated results\n\n            # If custom_name is set and there are multiple objects, append object name\n            kpi_custom_name = self.custom_name\n            if self.custom_name and len(objects) &gt; 1:\n                kpi_custom_name = f\"{self.custom_name} {obj}\"\n\n            attributes = KPIAttributes(\n                flag=self.flag,\n                model_flag=model_flag,\n                object_name=obj,\n                aggregation=self.aggregation,\n                dataset_name=dataset.name,\n                dataset_type=dataset_type,\n                name_prefix=self.name_prefix,\n                name_suffix=self.name_suffix,\n                custom_name=kpi_custom_name,\n                unit=unit,\n                dataset_attributes=dataset.attributes,\n                extra_attributes=self.extra_attributes or dict()\n            )\n\n            kpi = KPI(\n                value=aggregated[obj],\n                attributes=attributes,\n                dataset=dataset\n            )\n            if (model_df is not None) and (obj in model_df.index):\n                kpi._object_info = model_df.loc[obj]\n            kpis.append(kpi)\n\n        return kpis\n\n    def required_flags(self) -&gt; set[FlagTypeProtocol]:\n        \"\"\"\n        Return required flags.\n\n        Returns:\n            Set of required flags\n        \"\"\"\n        flags = {self.flag}\n        if self.model_flag:\n            flags.add(self.model_flag)\n        return flags\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/flag_aggregation/#mesqual.kpis.definitions.flag_aggregation.FlagAggKPIDefinition.generate_kpis","title":"generate_kpis","text":"<pre><code>generate_kpis(dataset: Dataset) -&gt; list[KPI]\n</code></pre> <p>Generate KPIs by batch computation.</p> <p>Process: 1. Infer model_flag if not provided 2. Fetch data (DataFrame) 3. Discover objects from DataFrame columns if objects='auto' 4. Apply aggregation to entire DataFrame (vectorized) 5. Create KPI instance per object with metadata</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset to compute KPIs for</p> required <p>Returns:</p> Type Description <code>list[KPI]</code> <p>List of computed KPI instances</p> Source code in <code>submodules/mesqual/mesqual/kpis/definitions/flag_aggregation.py</code> <pre><code>def generate_kpis(self, dataset: Dataset) -&gt; list[KPI]:\n    \"\"\"\n    Generate KPIs by batch computation.\n\n    Process:\n    1. Infer model_flag if not provided\n    2. Fetch data (DataFrame)\n    3. Discover objects from DataFrame columns if objects='auto'\n    4. Apply aggregation to entire DataFrame (vectorized)\n    5. Create KPI instance per object with metadata\n\n    Args:\n        dataset: Dataset to compute KPIs for\n\n    Returns:\n        List of computed KPI instances\n    \"\"\"\n    from mesqual.kpis.builders.flag_agg_builder import ModelPropertyFilter\n\n    model_flag = self.model_flag or dataset.flag_index.get_linked_model_flag(self.flag)\n\n    df = dataset.fetch(self.flag)\n\n    model_df = dataset.fetch(model_flag) if dataset.flag_is_accepted(model_flag) else None\n    if self.objects == 'auto':\n        objects = df.columns.tolist()\n    elif isinstance(self.objects, ModelPropertyFilter):\n        if model_df is None:\n            raise Exception(f'No Model DF found for model_flag \"{model_flag}\" in variable_flag \"{self.flag}\"')\n        filtered_model_df_objects = self.objects.apply_filter(model_df)\n        objects = [o for o in df.columns if o in filtered_model_df_objects]\n    else:\n        objects = self.objects\n\n    aggregated = self.aggregation(df)  # Returns Series with one value per column\n\n    dataset_type = type(dataset)\n\n    unit = dataset.flag_index.get_unit(self.flag)\n    if self.aggregation.unit is not None:\n        unit = self.aggregation.unit\n\n    kpis = []\n    for obj in objects:\n        if obj not in aggregated.index:\n            # TODO: optional warning if object listed but not present in flag\n            continue  # Skip objects not in aggregated results\n\n        # If custom_name is set and there are multiple objects, append object name\n        kpi_custom_name = self.custom_name\n        if self.custom_name and len(objects) &gt; 1:\n            kpi_custom_name = f\"{self.custom_name} {obj}\"\n\n        attributes = KPIAttributes(\n            flag=self.flag,\n            model_flag=model_flag,\n            object_name=obj,\n            aggregation=self.aggregation,\n            dataset_name=dataset.name,\n            dataset_type=dataset_type,\n            name_prefix=self.name_prefix,\n            name_suffix=self.name_suffix,\n            custom_name=kpi_custom_name,\n            unit=unit,\n            dataset_attributes=dataset.attributes,\n            extra_attributes=self.extra_attributes or dict()\n        )\n\n        kpi = KPI(\n            value=aggregated[obj],\n            attributes=attributes,\n            dataset=dataset\n        )\n        if (model_df is not None) and (obj in model_df.index):\n            kpi._object_info = model_df.loc[obj]\n        kpis.append(kpi)\n\n    return kpis\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/kpis/definitions/flag_aggregation/#mesqual.kpis.definitions.flag_aggregation.FlagAggKPIDefinition.required_flags","title":"required_flags","text":"<pre><code>required_flags() -&gt; set[FlagTypeProtocol]\n</code></pre> <p>Return required flags.</p> <p>Returns:</p> Type Description <code>set[FlagTypeProtocol]</code> <p>Set of required flags</p> Source code in <code>submodules/mesqual/mesqual/kpis/definitions/flag_aggregation.py</code> <pre><code>def required_flags(self) -&gt; set[FlagTypeProtocol]:\n    \"\"\"\n    Return required flags.\n\n    Returns:\n        Set of required flags\n    \"\"\"\n    flags = {self.flag}\n    if self.model_flag:\n        flags.add(self.model_flag)\n    return flags\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/","title":"MESQUAL Folium Utils","text":""},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/#mesqual.utils.folium_utils","title":"folium_utils","text":""},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/screenshotter/","title":"MESQUAL Folium Util <code>Screenshotter</code>","text":""},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/screenshotter/#mesqual.utils.folium_utils.screenshotter","title":"screenshotter","text":""},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/screenshotter/#mesqual.utils.folium_utils.screenshotter.ScreenConfig","title":"ScreenConfig  <code>dataclass</code>","text":"<p>Configuration for the virtual browser viewport.</p> <p>Attributes:</p> Name Type Description <code>width</code> <code>int</code> <p>Viewport width in CSS pixels.</p> <code>height</code> <code>int</code> <p>Viewport height in CSS pixels.</p> <code>device_pixel_ratio</code> <code>float</code> <p>Scale factor for high-DPI rendering. A value of 2.0 produces 2x resolution output (e.g., 1920x1080 viewport -&gt; 3840x2160 pixels).</p> Source code in <code>submodules/mesqual/mesqual/utils/folium_utils/screenshotter.py</code> <pre><code>@dataclass\nclass ScreenConfig:\n    \"\"\"Configuration for the virtual browser viewport.\n\n    Attributes:\n        width: Viewport width in CSS pixels.\n        height: Viewport height in CSS pixels.\n        device_pixel_ratio: Scale factor for high-DPI rendering.\n            A value of 2.0 produces 2x resolution output (e.g., 1920x1080 viewport -&gt; 3840x2160 pixels).\n    \"\"\"\n    width: int = 1920\n    height: int = 1080\n    device_pixel_ratio: float = 1.0\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/screenshotter/#mesqual.utils.folium_utils.screenshotter.FrameConfig","title":"FrameConfig  <code>dataclass</code>","text":"<p>Configuration for the screenshot crop frame.</p> <p>The frame is centered on the screen by default. Use offsets to shift the crop area.</p> <p>Attributes:</p> Name Type Description <code>width</code> <code>int</code> <p>Frame width in CSS pixels.</p> <code>height</code> <code>int</code> <p>Frame height in CSS pixels.</p> <code>offset_center_x</code> <code>int</code> <p>Horizontal offset from screen center. Positive values shift right.</p> <code>offset_center_y</code> <code>int</code> <p>Vertical offset from screen center. Positive values shift down.</p> Source code in <code>submodules/mesqual/mesqual/utils/folium_utils/screenshotter.py</code> <pre><code>@dataclass\nclass FrameConfig:\n    \"\"\"Configuration for the screenshot crop frame.\n\n    The frame is centered on the screen by default. Use offsets to shift the crop area.\n\n    Attributes:\n        width: Frame width in CSS pixels.\n        height: Frame height in CSS pixels.\n        offset_center_x: Horizontal offset from screen center. Positive values shift right.\n        offset_center_y: Vertical offset from screen center. Positive values shift down.\n    \"\"\"\n    width: int\n    height: int\n    offset_center_x: int = 0\n    offset_center_y: int = 0\n\n    def to_crop_box(self, image_width: int, image_height: int, dpr: float) -&gt; tuple[int, int, int, int]:\n        \"\"\"Convert frame config to PIL crop box coordinates.\n\n        Args:\n            image_width: Actual screenshot width in pixels.\n            image_height: Actual screenshot height in pixels.\n            dpr: Device pixel ratio for scaling.\n\n        Returns:\n            Tuple of (left, top, right, bottom) pixel coordinates.\n        \"\"\"\n        center_x = image_width // 2 + int(self.offset_center_x * dpr)\n        center_y = image_height // 2 + int(self.offset_center_y * dpr)\n        scaled_width = int(self.width * dpr)\n        scaled_height = int(self.height * dpr)\n        left = center_x - scaled_width // 2\n        top = center_y - scaled_height // 2\n        right = left + scaled_width\n        bottom = top + scaled_height\n        return (left, top, right, bottom)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/screenshotter/#mesqual.utils.folium_utils.screenshotter.FrameConfig.to_crop_box","title":"to_crop_box","text":"<pre><code>to_crop_box(image_width: int, image_height: int, dpr: float) -&gt; tuple[int, int, int, int]\n</code></pre> <p>Convert frame config to PIL crop box coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>image_width</code> <code>int</code> <p>Actual screenshot width in pixels.</p> required <code>image_height</code> <code>int</code> <p>Actual screenshot height in pixels.</p> required <code>dpr</code> <code>float</code> <p>Device pixel ratio for scaling.</p> required <p>Returns:</p> Type Description <code>tuple[int, int, int, int]</code> <p>Tuple of (left, top, right, bottom) pixel coordinates.</p> Source code in <code>submodules/mesqual/mesqual/utils/folium_utils/screenshotter.py</code> <pre><code>def to_crop_box(self, image_width: int, image_height: int, dpr: float) -&gt; tuple[int, int, int, int]:\n    \"\"\"Convert frame config to PIL crop box coordinates.\n\n    Args:\n        image_width: Actual screenshot width in pixels.\n        image_height: Actual screenshot height in pixels.\n        dpr: Device pixel ratio for scaling.\n\n    Returns:\n        Tuple of (left, top, right, bottom) pixel coordinates.\n    \"\"\"\n    center_x = image_width // 2 + int(self.offset_center_x * dpr)\n    center_y = image_height // 2 + int(self.offset_center_y * dpr)\n    scaled_width = int(self.width * dpr)\n    scaled_height = int(self.height * dpr)\n    left = center_x - scaled_width // 2\n    top = center_y - scaled_height // 2\n    right = left + scaled_width\n    bottom = top + scaled_height\n    return (left, top, right, bottom)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/screenshotter/#mesqual.utils.folium_utils.screenshotter.MapViewConfig","title":"MapViewConfig  <code>dataclass</code>","text":"<p>Configuration for the Leaflet map view.</p> <p>Attributes:</p> Name Type Description <code>center_lat</code> <code>float | None</code> <p>Latitude of map center.</p> <code>center_lng</code> <code>float | None</code> <p>Longitude of map center.</p> <code>zoom</code> <code>float | None</code> <p>Map zoom level. Supports fractional values (e.g., 4.5).</p> Source code in <code>submodules/mesqual/mesqual/utils/folium_utils/screenshotter.py</code> <pre><code>@dataclass\nclass MapViewConfig:\n    \"\"\"Configuration for the Leaflet map view.\n\n    Attributes:\n        center_lat: Latitude of map center.\n        center_lng: Longitude of map center.\n        zoom: Map zoom level. Supports fractional values (e.g., 4.5).\n    \"\"\"\n    center_lat: float | None = None\n    center_lng: float | None = None\n    zoom: float | None = None\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/screenshotter/#mesqual.utils.folium_utils.screenshotter.LegendInfo","title":"LegendInfo  <code>dataclass</code>","text":"<p>Information about a detected legend element.</p> <p>Attributes:</p> Name Type Description <code>element_id</code> <code>str</code> <p>DOM element ID of the legend.</p> <code>title</code> <code>str | None</code> <p>Legend title text, if present.</p> <code>width</code> <code>int</code> <p>Legend width in CSS pixels.</p> <code>height</code> <code>int</code> <p>Legend height in CSS pixels.</p> Source code in <code>submodules/mesqual/mesqual/utils/folium_utils/screenshotter.py</code> <pre><code>@dataclass\nclass LegendInfo:\n    \"\"\"Information about a detected legend element.\n\n    Attributes:\n        element_id: DOM element ID of the legend.\n        title: Legend title text, if present.\n        width: Legend width in CSS pixels.\n        height: Legend height in CSS pixels.\n    \"\"\"\n    element_id: str\n    title: str | None\n    width: int\n    height: int\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/screenshotter/#mesqual.utils.folium_utils.screenshotter.FoliumScreenshotter","title":"FoliumScreenshotter","text":"<p>Automated screenshot capture for Folium HTML maps.</p> <p>Uses headless Chrome via Selenium to render Folium maps and capture screenshots of map layers and legends. Supports high-DPI rendering, custom crop frames, and automatic detection of base layers and legend elements.</p> <p>Parameters:</p> Name Type Description Default <code>html_path</code> <code>str | Path</code> <p>Path to the Folium-generated HTML file.</p> required <code>screen_config</code> <code>ScreenConfig | None</code> <p>Virtual browser viewport configuration.</p> <code>None</code> <code>frame_config</code> <code>FrameConfig | None</code> <p>Screenshot crop frame configuration. If None, captures full viewport.</p> <code>None</code> <code>map_view_config</code> <code>MapViewConfig | None</code> <p>Leaflet map view settings (center, zoom).</p> <code>None</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; screenshotter = FoliumScreenshotter(\n...     \"map.html\",\n...     screen_config=ScreenConfig(1920, 1080, 2.0),\n...     frame_config=FrameConfig(800, 600),\n...     map_view_config=MapViewConfig(52.52, 13.405, 10),\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Capture all base layers\n&gt;&gt;&gt; screenshotter.capture_all_base_layers(\"output/layers\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Capture legends separately\n&gt;&gt;&gt; screenshotter.capture_legends(\"output/legends\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/folium_utils/screenshotter.py</code> <pre><code>class FoliumScreenshotter:\n    \"\"\"Automated screenshot capture for Folium HTML maps.\n\n    Uses headless Chrome via Selenium to render Folium maps and capture screenshots\n    of map layers and legends. Supports high-DPI rendering, custom crop frames,\n    and automatic detection of base layers and legend elements.\n\n    Args:\n        html_path: Path to the Folium-generated HTML file.\n        screen_config: Virtual browser viewport configuration.\n        frame_config: Screenshot crop frame configuration. If None, captures full viewport.\n        map_view_config: Leaflet map view settings (center, zoom).\n\n    Example:\n\n        &gt;&gt;&gt; screenshotter = FoliumScreenshotter(\n        ...     \"map.html\",\n        ...     screen_config=ScreenConfig(1920, 1080, 2.0),\n        ...     frame_config=FrameConfig(800, 600),\n        ...     map_view_config=MapViewConfig(52.52, 13.405, 10),\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Capture all base layers\n        &gt;&gt;&gt; screenshotter.capture_all_base_layers(\"output/layers\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Capture legends separately\n        &gt;&gt;&gt; screenshotter.capture_legends(\"output/legends\")\n\n    \"\"\"\n\n    def __init__(\n            self,\n            html_path: str | Path,\n            screen_config: ScreenConfig | None = None,\n            frame_config: FrameConfig | None = None,\n            map_view_config: MapViewConfig | None = None\n    ):\n        self._html_path = Path(html_path).resolve()\n        self._screen_config = screen_config or ScreenConfig()\n        self._frame_config = frame_config\n        self._map_view_config = map_view_config or MapViewConfig()\n        self._driver: webdriver.Chrome | None = None\n\n    def _start_browser(self) -&gt; None:\n        options = Options()\n        options.add_argument(\"--headless=new\")\n        options.add_argument(\"--disable-gpu\")\n        options.add_argument(\"--no-sandbox\")\n        options.add_argument(f\"--window-size={self._screen_config.width},{self._screen_config.height}\")\n        options.add_argument(f\"--force-device-scale-factor={self._screen_config.device_pixel_ratio}\")\n        options.add_argument(\"--high-dpi-support=1\")\n        self._driver = webdriver.Chrome(options=options)\n\n    def _stop_browser(self) -&gt; None:\n        if self._driver:\n            self._driver.quit()\n            self._driver = None\n\n    def _load_map(self) -&gt; None:\n        self._driver.get(f\"file://{self._html_path}\")\n        WebDriverWait(self._driver, 10).until(\n            EC.presence_of_element_located((By.CLASS_NAME, \"leaflet-container\"))\n        )\n        self._driver.execute_script(\"\"\"\n            let container = document.querySelector('.leaflet-container');\n            container.style.position = 'absolute';\n            container.style.top = '0';\n            container.style.left = '0';\n            container.style.width = '100vw';\n            container.style.height = '100vh';\n            container.style.margin = '0';\n            container.style.padding = '0';\n            document.body.style.margin = '0';\n            document.body.style.padding = '0';\n            document.body.style.overflow = 'hidden';\n        \"\"\")\n        time.sleep(1)\n\n    def _get_map_object_name(self) -&gt; str:\n        return self._driver.execute_script(\"\"\"\n            for (let key in window) {\n                if (window[key] instanceof L.Map) return key;\n            }\n            return null;\n        \"\"\")\n\n    def _set_view(self, map_name: str) -&gt; None:\n        cfg = self._map_view_config\n        if cfg.center_lat is not None and cfg.center_lng is not None:\n            zoom_expr = str(cfg.zoom) if cfg.zoom else f\"{map_name}.getZoom()\"\n            self._driver.execute_script(\n                f\"{map_name}.setView([{cfg.center_lat}, {cfg.center_lng}], {zoom_expr});\"\n            )\n        elif cfg.zoom is not None:\n            self._driver.execute_script(f\"{map_name}.setZoom({cfg.zoom});\")\n        self._driver.execute_script(f\"{map_name}.invalidateSize();\")\n        time.sleep(1)\n\n    def _get_base_layers(self) -&gt; list[dict]:\n        return self._driver.execute_script(\"\"\"\n            let result = [];\n            document.querySelectorAll('.leaflet-control-layers-base label').forEach((label, idx) =&gt; {\n                let input = label.querySelector('input');\n                let name = label.textContent.trim();\n                result.push({\n                    name: name,\n                    index: idx,\n                    checked: input.checked\n                });\n            });\n            return result;\n        \"\"\")\n\n    def _select_base_layer(self, index: int) -&gt; None:\n        self._driver.execute_script(f\"\"\"\n            let inputs = document.querySelectorAll('.leaflet-control-layers-base input');\n            if (inputs[{index}]) inputs[{index}].click();\n        \"\"\")\n        time.sleep(1)\n\n    def _detect_legends(self) -&gt; list[LegendInfo]:\n        legends_data = self._driver.execute_script(\"\"\"\n            let legends = [];\n\n            // Method 1: Find by ID pattern (*Legend_*)\n            document.querySelectorAll('[id*=\"Legend_\"]').forEach(el =&gt; {\n                let titleEl = el.querySelector('.legend-title');\n                let rect = el.getBoundingClientRect();\n                legends.push({\n                    id: el.id,\n                    title: titleEl ? titleEl.textContent.trim() : null,\n                    width: rect.width,\n                    height: rect.height\n                });\n            });\n\n            // Method 2: Find by structure (position:fixed + .legend-content)\n            if (legends.length === 0) {\n                document.querySelectorAll('div').forEach(el =&gt; {\n                    let style = window.getComputedStyle(el);\n                    if (style.position === 'fixed' &amp;&amp; el.querySelector('.legend-content')) {\n                        let titleEl = el.querySelector('.legend-title');\n                        let rect = el.getBoundingClientRect();\n                        legends.push({\n                            id: el.id || `legend_${legends.length}`,\n                            title: titleEl ? titleEl.textContent.trim() : null,\n                            width: rect.width,\n                            height: rect.height\n                        });\n                    }\n                });\n            }\n\n            return legends;\n        \"\"\")\n\n        return [\n            LegendInfo(\n                element_id=l[\"id\"],\n                title=l[\"title\"],\n                width=int(l[\"width\"]),\n                height=int(l[\"height\"])\n            )\n            for l in legends_data\n        ]\n\n    def _hide_layer_control(self) -&gt; None:\n        self._driver.execute_script(\"\"\"\n            let control = document.querySelector('.leaflet-control-layers');\n            if (control) control.style.display = 'none';\n        \"\"\")\n\n    def _show_layer_control(self) -&gt; None:\n        self._driver.execute_script(\"\"\"\n            let control = document.querySelector('.leaflet-control-layers');\n            if (control) control.style.display = '';\n        \"\"\")\n\n    def _hide_legends(self) -&gt; None:\n        self._driver.execute_script(\"\"\"\n            // By ID pattern\n            document.querySelectorAll('[id*=\"Legend_\"]').forEach(el =&gt; {\n                el.style.display = 'none';\n            });\n\n            // By structure (position:fixed + .legend-content)\n            document.querySelectorAll('div').forEach(el =&gt; {\n                let style = window.getComputedStyle(el);\n                if (style.position === 'fixed' &amp;&amp; el.querySelector('.legend-content')) {\n                    el.style.display = 'none';\n                }\n            });\n        \"\"\")\n\n    def _show_legends(self) -&gt; None:\n        self._driver.execute_script(\"\"\"\n            document.querySelectorAll('[id*=\"Legend_\"]').forEach(el =&gt; {\n                el.style.display = '';\n            });\n\n            document.querySelectorAll('div').forEach(el =&gt; {\n                let style = window.getComputedStyle(el);\n                if (style.position === 'fixed' &amp;&amp; el.querySelector('.legend-content')) {\n                    el.style.display = '';\n                }\n            });\n        \"\"\")\n\n    def _hide_ui_elements(self) -&gt; None:\n        self._hide_layer_control()\n        self._hide_legends()\n\n    def _show_ui_elements(self) -&gt; None:\n        self._show_layer_control()\n        self._show_legends()\n\n    def _take_screenshot(self, output_path: Path) -&gt; None:\n        self._hide_ui_elements()\n\n        png_bytes = self._driver.get_screenshot_as_png()\n        image = Image.open(io.BytesIO(png_bytes))\n\n        if self._frame_config:\n            crop_box = self._frame_config.to_crop_box(\n                image.width,\n                image.height,\n                self._screen_config.device_pixel_ratio\n            )\n            crop_box = (\n                max(0, crop_box[0]),\n                max(0, crop_box[1]),\n                min(image.width, crop_box[2]),\n                min(image.height, crop_box[3])\n            )\n            image = image.crop(crop_box)\n\n        image.save(output_path)\n\n        self._show_ui_elements()\n\n    def _take_element_screenshot(self, element_id: str, output_path: Path) -&gt; None:\n        self._hide_layer_control()\n\n        self._driver.execute_script(\"\"\"\n            let targetId = arguments[0];\n            document.querySelectorAll('[id*=\"Legend_\"]').forEach(el =&gt; {\n                if (el.id !== targetId) el.style.display = 'none';\n            });\n        \"\"\", element_id)\n\n        dpr = self._screen_config.device_pixel_ratio\n        rect = self._driver.execute_script(\"\"\"\n            let el = document.getElementById(arguments[0]);\n            let rect = el.getBoundingClientRect();\n            return {left: rect.left, top: rect.top, width: rect.width, height: rect.height};\n        \"\"\", element_id)\n\n        png_bytes = self._driver.get_screenshot_as_png()\n        image = Image.open(io.BytesIO(png_bytes))\n\n        crop_box = (\n            int(rect[\"left\"] * dpr),\n            int(rect[\"top\"] * dpr),\n            int((rect[\"left\"] + rect[\"width\"]) * dpr),\n            int((rect[\"top\"] + rect[\"height\"]) * dpr)\n        )\n\n        crop_box = (\n            max(0, crop_box[0]),\n            max(0, crop_box[1]),\n            min(image.width, crop_box[2]),\n            min(image.height, crop_box[3])\n        )\n\n        image = image.crop(crop_box)\n        image.save(output_path)\n\n        self._show_ui_elements()\n\n    def _sanitize_filename(self, name: str) -&gt; str:\n        return \"\".join(c if c.isalnum() or c in \"._- \" else \"_\" for c in name).strip()\n\n    def set_frame(\n            self,\n            width: int,\n            height: int,\n            offset_center_x: int = 0,\n            offset_center_y: int = 0\n    ) -&gt; \"FoliumScreenshotter\":\n        \"\"\"Set the screenshot crop frame.\n\n        Args:\n            width: Frame width in CSS pixels.\n            height: Frame height in CSS pixels.\n            offset_center_x: Horizontal offset from screen center.\n            offset_center_y: Vertical offset from screen center.\n\n        Returns:\n            Self for method chaining.\n        \"\"\"\n        self._frame_config = FrameConfig(width, height, offset_center_x, offset_center_y)\n        return self\n\n    def set_map_view(self, lat: float, lng: float, zoom: float | None = None) -&gt; \"FoliumScreenshotter\":\n        \"\"\"Set the map view center and zoom level.\n\n        Args:\n            lat: Center latitude.\n            lng: Center longitude.\n            zoom: Zoom level. Supports fractional values.\n\n        Returns:\n            Self for method chaining.\n        \"\"\"\n        self._map_view_config = MapViewConfig(lat, lng, zoom)\n        return self\n\n    def capture_all_base_layers(self, output_dir: str | Path) -&gt; list[Path]:\n        \"\"\"Capture screenshots of all base layers (overlay=False feature groups).\n\n        Iterates through each base layer in the Leaflet layer control,\n        selects it, and takes a screenshot. UI elements (layer control, legends)\n        are hidden during capture.\n\n        Args:\n            output_dir: Directory to save screenshots. Created if it doesn't exist.\n\n        Returns:\n            List of paths to saved screenshot files.\n\n        Raises:\n            RuntimeError: If no Leaflet map object is found in the HTML.\n        \"\"\"\n        output_dir = Path(output_dir)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        saved_files = []\n\n        try:\n            self._start_browser()\n            self._load_map()\n\n            map_name = self._get_map_object_name()\n            if not map_name:\n                raise RuntimeError(\"Could not find Leaflet map object\")\n\n            self._set_view(map_name)\n            base_layers = self._get_base_layers()\n\n            if not base_layers:\n                print(\"No base layers (overlay=False) found\")\n                return saved_files\n\n            for layer in base_layers:\n                self._select_base_layer(layer[\"index\"])\n                filename = f\"{self._sanitize_filename(layer['name'])}.png\"\n                output_path = output_dir / filename\n                self._take_screenshot(output_path)\n                saved_files.append(output_path)\n                print(f\"Saved: {output_path}\")\n\n            return saved_files\n        finally:\n            self._stop_browser()\n\n    def capture_single_view(self, output_path: str | Path) -&gt; Path:\n        \"\"\"Capture a single screenshot of the current map view.\n\n        Takes a screenshot with the currently active base layer.\n        UI elements (layer control, legends) are hidden during capture.\n\n        Args:\n            output_path: Path for the output PNG file.\n\n        Returns:\n            Path to the saved screenshot file.\n        \"\"\"\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        try:\n            self._start_browser()\n            self._load_map()\n\n            map_name = self._get_map_object_name()\n            if map_name:\n                self._set_view(map_name)\n\n            self._take_screenshot(output_path)\n            return output_path\n        finally:\n            self._stop_browser()\n\n    def capture_legends(self, output_dir: str | Path) -&gt; dict[str, Path]:\n        \"\"\"Capture screenshots of all detected legend elements.\n\n        Detects legends by ID pattern (`*Legend_*`) or by structure\n        (position:fixed elements containing `.legend-content`).\n        Each legend is captured individually with other legends hidden.\n\n        Args:\n            output_dir: Directory to save legend screenshots.\n\n        Returns:\n            Dictionary mapping legend names (title or element ID) to file paths.\n        \"\"\"\n        output_dir = Path(output_dir)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        saved_files = {}\n\n        try:\n            self._start_browser()\n            self._load_map()\n\n            legends = self._detect_legends()\n\n            if not legends:\n                print(\"No legends detected\")\n                return saved_files\n\n            for legend in legends:\n                name = legend.title or legend.element_id\n                filename = f\"{self._sanitize_filename(name)}.png\"\n                output_path = output_dir / filename\n                self._take_element_screenshot(legend.element_id, output_path)\n                saved_files[name] = output_path\n                print(f\"Saved legend: {output_path} ({legend.width}x{legend.height}px)\")\n\n            return saved_files\n        finally:\n            self._stop_browser()\n\n    def capture_all(self, output_dir: str | Path) -&gt; dict[str, list[Path] | dict[str, Path]]:\n        \"\"\"Capture screenshots of all base layers and legends.\n\n        Convenience method that captures everything in a single browser session.\n        Outputs are organized into subdirectories: `layers/` for base layer\n        screenshots and `legends/` for legend screenshots.\n\n        Args:\n            output_dir: Root directory for output. Subdirectories are created automatically.\n\n        Returns:\n            Dictionary with keys:\n\n            - `base_layers`: List of paths to layer screenshots.\n            - `legends`: Dictionary mapping legend names to file paths.\n\n        Raises:\n            RuntimeError: If no Leaflet map object is found in the HTML.\n        \"\"\"\n        output_dir = Path(output_dir)\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        results = {\n            \"base_layers\": [],\n            \"legends\": {}\n        }\n\n        try:\n            self._start_browser()\n            self._load_map()\n\n            map_name = self._get_map_object_name()\n            if not map_name:\n                raise RuntimeError(\"Could not find Leaflet map object\")\n\n            self._set_view(map_name)\n\n            legends = self._detect_legends()\n            legends_dir = output_dir / \"legends\"\n            legends_dir.mkdir(exist_ok=True)\n\n            for legend in legends:\n                name = legend.title or legend.element_id\n                filename = f\"{self._sanitize_filename(name)}.png\"\n                output_path = legends_dir / filename\n                self._take_element_screenshot(legend.element_id, output_path)\n                results[\"legends\"][name] = output_path\n                print(f\"Saved legend: {output_path}\")\n\n            base_layers = self._get_base_layers()\n            layers_dir = output_dir / \"layers\"\n            layers_dir.mkdir(exist_ok=True)\n\n            for layer in base_layers:\n                self._select_base_layer(layer[\"index\"])\n                filename = f\"{self._sanitize_filename(layer['name'])}.png\"\n                output_path = layers_dir / filename\n                self._take_screenshot(output_path)\n                results[\"base_layers\"].append(output_path)\n                print(f\"Saved layer: {output_path}\")\n\n            return results\n        finally:\n            self._stop_browser()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/screenshotter/#mesqual.utils.folium_utils.screenshotter.FoliumScreenshotter.set_frame","title":"set_frame","text":"<pre><code>set_frame(width: int, height: int, offset_center_x: int = 0, offset_center_y: int = 0) -&gt; FoliumScreenshotter\n</code></pre> <p>Set the screenshot crop frame.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>int</code> <p>Frame width in CSS pixels.</p> required <code>height</code> <code>int</code> <p>Frame height in CSS pixels.</p> required <code>offset_center_x</code> <code>int</code> <p>Horizontal offset from screen center.</p> <code>0</code> <code>offset_center_y</code> <code>int</code> <p>Vertical offset from screen center.</p> <code>0</code> <p>Returns:</p> Type Description <code>FoliumScreenshotter</code> <p>Self for method chaining.</p> Source code in <code>submodules/mesqual/mesqual/utils/folium_utils/screenshotter.py</code> <pre><code>def set_frame(\n        self,\n        width: int,\n        height: int,\n        offset_center_x: int = 0,\n        offset_center_y: int = 0\n) -&gt; \"FoliumScreenshotter\":\n    \"\"\"Set the screenshot crop frame.\n\n    Args:\n        width: Frame width in CSS pixels.\n        height: Frame height in CSS pixels.\n        offset_center_x: Horizontal offset from screen center.\n        offset_center_y: Vertical offset from screen center.\n\n    Returns:\n        Self for method chaining.\n    \"\"\"\n    self._frame_config = FrameConfig(width, height, offset_center_x, offset_center_y)\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/screenshotter/#mesqual.utils.folium_utils.screenshotter.FoliumScreenshotter.set_map_view","title":"set_map_view","text":"<pre><code>set_map_view(lat: float, lng: float, zoom: float | None = None) -&gt; FoliumScreenshotter\n</code></pre> <p>Set the map view center and zoom level.</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Center latitude.</p> required <code>lng</code> <code>float</code> <p>Center longitude.</p> required <code>zoom</code> <code>float | None</code> <p>Zoom level. Supports fractional values.</p> <code>None</code> <p>Returns:</p> Type Description <code>FoliumScreenshotter</code> <p>Self for method chaining.</p> Source code in <code>submodules/mesqual/mesqual/utils/folium_utils/screenshotter.py</code> <pre><code>def set_map_view(self, lat: float, lng: float, zoom: float | None = None) -&gt; \"FoliumScreenshotter\":\n    \"\"\"Set the map view center and zoom level.\n\n    Args:\n        lat: Center latitude.\n        lng: Center longitude.\n        zoom: Zoom level. Supports fractional values.\n\n    Returns:\n        Self for method chaining.\n    \"\"\"\n    self._map_view_config = MapViewConfig(lat, lng, zoom)\n    return self\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/screenshotter/#mesqual.utils.folium_utils.screenshotter.FoliumScreenshotter.capture_all_base_layers","title":"capture_all_base_layers","text":"<pre><code>capture_all_base_layers(output_dir: str | Path) -&gt; list[Path]\n</code></pre> <p>Capture screenshots of all base layers (overlay=False feature groups).</p> <p>Iterates through each base layer in the Leaflet layer control, selects it, and takes a screenshot. UI elements (layer control, legends) are hidden during capture.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str | Path</code> <p>Directory to save screenshots. Created if it doesn't exist.</p> required <p>Returns:</p> Type Description <code>list[Path]</code> <p>List of paths to saved screenshot files.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no Leaflet map object is found in the HTML.</p> Source code in <code>submodules/mesqual/mesqual/utils/folium_utils/screenshotter.py</code> <pre><code>def capture_all_base_layers(self, output_dir: str | Path) -&gt; list[Path]:\n    \"\"\"Capture screenshots of all base layers (overlay=False feature groups).\n\n    Iterates through each base layer in the Leaflet layer control,\n    selects it, and takes a screenshot. UI elements (layer control, legends)\n    are hidden during capture.\n\n    Args:\n        output_dir: Directory to save screenshots. Created if it doesn't exist.\n\n    Returns:\n        List of paths to saved screenshot files.\n\n    Raises:\n        RuntimeError: If no Leaflet map object is found in the HTML.\n    \"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    saved_files = []\n\n    try:\n        self._start_browser()\n        self._load_map()\n\n        map_name = self._get_map_object_name()\n        if not map_name:\n            raise RuntimeError(\"Could not find Leaflet map object\")\n\n        self._set_view(map_name)\n        base_layers = self._get_base_layers()\n\n        if not base_layers:\n            print(\"No base layers (overlay=False) found\")\n            return saved_files\n\n        for layer in base_layers:\n            self._select_base_layer(layer[\"index\"])\n            filename = f\"{self._sanitize_filename(layer['name'])}.png\"\n            output_path = output_dir / filename\n            self._take_screenshot(output_path)\n            saved_files.append(output_path)\n            print(f\"Saved: {output_path}\")\n\n        return saved_files\n    finally:\n        self._stop_browser()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/screenshotter/#mesqual.utils.folium_utils.screenshotter.FoliumScreenshotter.capture_single_view","title":"capture_single_view","text":"<pre><code>capture_single_view(output_path: str | Path) -&gt; Path\n</code></pre> <p>Capture a single screenshot of the current map view.</p> <p>Takes a screenshot with the currently active base layer. UI elements (layer control, legends) are hidden during capture.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str | Path</code> <p>Path for the output PNG file.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved screenshot file.</p> Source code in <code>submodules/mesqual/mesqual/utils/folium_utils/screenshotter.py</code> <pre><code>def capture_single_view(self, output_path: str | Path) -&gt; Path:\n    \"\"\"Capture a single screenshot of the current map view.\n\n    Takes a screenshot with the currently active base layer.\n    UI elements (layer control, legends) are hidden during capture.\n\n    Args:\n        output_path: Path for the output PNG file.\n\n    Returns:\n        Path to the saved screenshot file.\n    \"\"\"\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    try:\n        self._start_browser()\n        self._load_map()\n\n        map_name = self._get_map_object_name()\n        if map_name:\n            self._set_view(map_name)\n\n        self._take_screenshot(output_path)\n        return output_path\n    finally:\n        self._stop_browser()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/screenshotter/#mesqual.utils.folium_utils.screenshotter.FoliumScreenshotter.capture_legends","title":"capture_legends","text":"<pre><code>capture_legends(output_dir: str | Path) -&gt; dict[str, Path]\n</code></pre> <p>Capture screenshots of all detected legend elements.</p> <p>Detects legends by ID pattern (<code>*Legend_*</code>) or by structure (position:fixed elements containing <code>.legend-content</code>). Each legend is captured individually with other legends hidden.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str | Path</code> <p>Directory to save legend screenshots.</p> required <p>Returns:</p> Type Description <code>dict[str, Path]</code> <p>Dictionary mapping legend names (title or element ID) to file paths.</p> Source code in <code>submodules/mesqual/mesqual/utils/folium_utils/screenshotter.py</code> <pre><code>def capture_legends(self, output_dir: str | Path) -&gt; dict[str, Path]:\n    \"\"\"Capture screenshots of all detected legend elements.\n\n    Detects legends by ID pattern (`*Legend_*`) or by structure\n    (position:fixed elements containing `.legend-content`).\n    Each legend is captured individually with other legends hidden.\n\n    Args:\n        output_dir: Directory to save legend screenshots.\n\n    Returns:\n        Dictionary mapping legend names (title or element ID) to file paths.\n    \"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    saved_files = {}\n\n    try:\n        self._start_browser()\n        self._load_map()\n\n        legends = self._detect_legends()\n\n        if not legends:\n            print(\"No legends detected\")\n            return saved_files\n\n        for legend in legends:\n            name = legend.title or legend.element_id\n            filename = f\"{self._sanitize_filename(name)}.png\"\n            output_path = output_dir / filename\n            self._take_element_screenshot(legend.element_id, output_path)\n            saved_files[name] = output_path\n            print(f\"Saved legend: {output_path} ({legend.width}x{legend.height}px)\")\n\n        return saved_files\n    finally:\n        self._stop_browser()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/folium_utils/screenshotter/#mesqual.utils.folium_utils.screenshotter.FoliumScreenshotter.capture_all","title":"capture_all","text":"<pre><code>capture_all(output_dir: str | Path) -&gt; dict[str, list[Path] | dict[str, Path]]\n</code></pre> <p>Capture screenshots of all base layers and legends.</p> <p>Convenience method that captures everything in a single browser session. Outputs are organized into subdirectories: <code>layers/</code> for base layer screenshots and <code>legends/</code> for legend screenshots.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str | Path</code> <p>Root directory for output. Subdirectories are created automatically.</p> required <p>Returns:</p> Type Description <code>dict[str, list[Path] | dict[str, Path]]</code> <p>Dictionary with keys:</p> <code>dict[str, list[Path] | dict[str, Path]]</code> <ul> <li><code>base_layers</code>: List of paths to layer screenshots.</li> </ul> <code>dict[str, list[Path] | dict[str, Path]]</code> <ul> <li><code>legends</code>: Dictionary mapping legend names to file paths.</li> </ul> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no Leaflet map object is found in the HTML.</p> Source code in <code>submodules/mesqual/mesqual/utils/folium_utils/screenshotter.py</code> <pre><code>def capture_all(self, output_dir: str | Path) -&gt; dict[str, list[Path] | dict[str, Path]]:\n    \"\"\"Capture screenshots of all base layers and legends.\n\n    Convenience method that captures everything in a single browser session.\n    Outputs are organized into subdirectories: `layers/` for base layer\n    screenshots and `legends/` for legend screenshots.\n\n    Args:\n        output_dir: Root directory for output. Subdirectories are created automatically.\n\n    Returns:\n        Dictionary with keys:\n\n        - `base_layers`: List of paths to layer screenshots.\n        - `legends`: Dictionary mapping legend names to file paths.\n\n    Raises:\n        RuntimeError: If no Leaflet map object is found in the HTML.\n    \"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    results = {\n        \"base_layers\": [],\n        \"legends\": {}\n    }\n\n    try:\n        self._start_browser()\n        self._load_map()\n\n        map_name = self._get_map_object_name()\n        if not map_name:\n            raise RuntimeError(\"Could not find Leaflet map object\")\n\n        self._set_view(map_name)\n\n        legends = self._detect_legends()\n        legends_dir = output_dir / \"legends\"\n        legends_dir.mkdir(exist_ok=True)\n\n        for legend in legends:\n            name = legend.title or legend.element_id\n            filename = f\"{self._sanitize_filename(name)}.png\"\n            output_path = legends_dir / filename\n            self._take_element_screenshot(legend.element_id, output_path)\n            results[\"legends\"][name] = output_path\n            print(f\"Saved legend: {output_path}\")\n\n        base_layers = self._get_base_layers()\n        layers_dir = output_dir / \"layers\"\n        layers_dir.mkdir(exist_ok=True)\n\n        for layer in base_layers:\n            self._select_base_layer(layer[\"index\"])\n            filename = f\"{self._sanitize_filename(layer['name'])}.png\"\n            output_path = layers_dir / filename\n            self._take_screenshot(output_path)\n            results[\"base_layers\"].append(output_path)\n            print(f\"Saved layer: {output_path}\")\n\n        return results\n    finally:\n        self._stop_browser()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/","title":"MESQUAL Pandas Utils","text":""},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/#mesqual.utils.pandas_utils","title":"pandas_utils","text":"<p>MESQUAL pandas utilities for energy systems data analysis.</p> <p>This package provides specialized pandas utilities designed for energy systems analysis workflows in the MESQUAL framework. These utilities are optimized for working with MultiIndex data structures that are common in multi-scenario energy modeling studies.</p> <p>The utilities support MESQUAL's core data flow patterns where energy data is typically structured with multiple dimensions (scenarios, time periods, network components, technologies) and requires specialized operations for filtering, transformation, and aggregation.</p> <p>Modules:</p> Name Description <code>prepend_model_prop_levels</code> <p>Add model properties as MultiIndex levels to time-series data</p> <code>filter_by_model_query</code> <p>Filter time-series using model metadata queries</p> <code>flatten_df</code> <p>Convert MultiIndex DataFrames to flat format for visualization</p> <code>sort_multiindex</code> <p>Sort MultiIndex levels with custom ordering</p> <code>xs_df</code> <p>Enhanced cross-section interface for MultiIndex DataFrames</p> <code>merge_multi_index_levels</code> <p>Combine multiple MultiIndex levels into single level</p> <code>add_index_as_column</code> <p>Convert index levels to DataFrame columns</p> <p>Examples:</p> <pre><code>Basic usage for energy systems analysis:\n&gt;&gt;&gt; from mesqual.utils.pandas_utils import prepend_model_prop_levels, filter_by_model_query\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load generator model and time-series data\n&gt;&gt;&gt; generators = study.scen.fetch('generators')  # Model metadata\n&gt;&gt;&gt; generation = study.scen.fetch('generators_t.p')  # Time-series data\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Add technology and zone properties to time-series\n&gt;&gt;&gt; gen_with_props = prepend_model_prop_levels(\n...     generation, generators, 'technology', 'zone'\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Filter for renewable generators only\n&gt;&gt;&gt; renewable_gen = filter_by_model_query(\n...     gen_with_props, generators, 'technology.isin([\"solar\", \"wind\"])'\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Aggregate by technology and zone\n&gt;&gt;&gt; tech_zone_totals = renewable_gen.T.groupby(level=['technology', 'zone']).sum().T\n</code></pre> Architecture Integration <p>These utilities integrate seamlessly with MESQUAL's three-tier architecture:</p> <ul> <li>General utilities (this package): Platform-agnostic data transformations</li> <li>Platform-specific: Used by platform interfaces (mesqual-pypsa, etc.)</li> <li>Study-specific: Extended in individual studies for custom analysis</li> </ul> <p>They preserve MESQUAL's MultiIndex data flow patterns while enabling flexible data manipulation and transformation for energy systems analysis workflows.</p> Performance Notes <ul> <li>Operations maintain MultiIndex structures for memory efficiency</li> <li>Bulk operations are preferred over iterative transformations</li> <li>Query-based filtering minimizes data copying</li> <li>Lazy evaluation patterns supported where possible</li> </ul>"},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/combine_df/","title":"MESQUAL Pandas Util <code>combine_df</code>","text":""},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/combine_df/#mesqual.utils.pandas_utils.combine_df.combine_dfs","title":"combine_dfs","text":"<pre><code>combine_dfs(dfs: Iterable[Series | DataFrame], keep_first: bool = True)\n</code></pre> <p>Combine multiple DataFrames or Series using intelligent merging strategies.</p> <p>This function automatically determines how to combine DataFrames based on their index and column structure: - If DataFrames share indices but not columns: concatenate along columns (axis=1) - If DataFrames share columns but not indices: concatenate along rows (axis=0) - Otherwise: use combine_first() to fill missing values</p> <p>Parameters:</p> Name Type Description Default <code>dfs</code> <code>Iterable[Series | DataFrame]</code> <p>An iterable of pandas DataFrames or Series to combine.</p> required <code>keep_first</code> <code>bool</code> <p>If True, prioritize values from earlier DataFrames when using combine_first(). If False, prioritize values from later DataFrames.</p> <code>True</code> <p>Returns:</p> Type Description <p>A single DataFrame or Series containing the combined data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no DataFrames are provided in the iterable.</p> Energy Domain Context <p>In Energy Systems Analysis, you often deal with fragmented data, stored or imported from different locations. For example:</p> <pre><code>- You have multiple simulation results that you want to concatenate,\n    e.g. one each model covers only one month and you\n    need to merge those into a single df\n- You have a yearly simulation result, but one week must be\n    replaced with another result, because you had to re-run\n    that with a different setting. So only that week should\n    be overwritten.\n- You have a local csv file with static properties that you\n    want to merge with the model_df that is coming from the\n    simulation platform.\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df1 = pd.DataFrame({'A': [1, 2]}, index=['x', 'y'])\n&gt;&gt;&gt; df2 = pd.DataFrame({'B': [3, 4]}, index=['x', 'y'])\n&gt;&gt;&gt; result = combine_dfs([df1, df2])  # Column concat\n&gt;&gt;&gt; print(result)\n       A  B\n    x  1  3\n    y  2  4\n\n&gt;&gt;&gt; df3 = pd.DataFrame({'A': [5, 6]}, index=['z', 'w'])\n&gt;&gt;&gt; result = combine_dfs([df1, df3])  # Row concat\n&gt;&gt;&gt; print(result)\n       A\n    x  1\n    y  2\n    z  5\n    w  6\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/pandas_utils/combine_df.py</code> <pre><code>def combine_dfs(dfs: Iterable[pd.Series | pd.DataFrame], keep_first: bool = True):\n    \"\"\"Combine multiple DataFrames or Series using intelligent merging strategies.\n\n    This function automatically determines how to combine DataFrames based on their\n    index and column structure:\n    - If DataFrames share indices but not columns: concatenate along columns (axis=1)\n    - If DataFrames share columns but not indices: concatenate along rows (axis=0)\n    - Otherwise: use combine_first() to fill missing values\n\n    Args:\n        dfs: An iterable of pandas DataFrames or Series to combine.\n        keep_first: If True, prioritize values from earlier DataFrames when using\n            combine_first(). If False, prioritize values from later DataFrames.\n\n    Returns:\n        A single DataFrame or Series containing the combined data.\n\n    Raises:\n        ValueError: If no DataFrames are provided in the iterable.\n\n    Energy Domain Context:\n        In Energy Systems Analysis, you often deal with fragmented data,\n        stored or imported from different locations. For example:\n\n            - You have multiple simulation results that you want to concatenate,\n                e.g. one each model covers only one month and you\n                need to merge those into a single df\n            - You have a yearly simulation result, but one week must be\n                replaced with another result, because you had to re-run\n                that with a different setting. So only that week should\n                be overwritten.\n            - You have a local csv file with static properties that you\n                want to merge with the model_df that is coming from the\n                simulation platform.\n\n    Examples:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df1 = pd.DataFrame({'A': [1, 2]}, index=['x', 'y'])\n        &gt;&gt;&gt; df2 = pd.DataFrame({'B': [3, 4]}, index=['x', 'y'])\n        &gt;&gt;&gt; result = combine_dfs([df1, df2])  # Column concat\n        &gt;&gt;&gt; print(result)\n               A  B\n            x  1  3\n            y  2  4\n\n        &gt;&gt;&gt; df3 = pd.DataFrame({'A': [5, 6]}, index=['z', 'w'])\n        &gt;&gt;&gt; result = combine_dfs([df1, df3])  # Row concat\n        &gt;&gt;&gt; print(result)\n               A\n            x  1\n            y  2\n            z  5\n            w  6\n    \"\"\"\n    size = sum(1 for _ in dfs)\n    if size == 0:\n        raise ValueError(\"You need to pass at least one DataFrame / Series.\")\n    if size == 1:\n        return [df for df in dfs][0]\n\n    def merge_func(df1, df2):\n        if isinstance(df1, pd.DataFrame) and isinstance(df2, pd.DataFrame):\n            column_intersection = set(df1.columns).intersection(df2.columns)\n            index_intersection = set(df1.index).intersection(df2.index)\n            if (len(column_intersection) == 0) and (len(index_intersection) &gt; 0):\n                return pd.concat([df1, df2], axis=1)\n            elif (len(column_intersection) &gt; 0) and (len(index_intersection) == 0):\n                return pd.concat([df1, df2], axis=0)\n\n        if keep_first:\n            return df1.combine_first(df2)\n        else:\n            return df2.combine_first(df1)\n\n    return reduce(merge_func, dfs)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/filter/","title":"MESQUAL Pandas Util <code>filter_by_model_query</code>","text":""},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/filter/#mesqual.utils.pandas_utils.filter.filter_by_model_query","title":"filter_by_model_query","text":"<pre><code>filter_by_model_query(df: Series | DataFrame, model_df: DataFrame, query: str = None, match_on_level: int | str = None) -&gt; DataFrame | Series\n</code></pre> <p>Filter DataFrame or Series based on a query applied to a model DataFrame.</p> <p>This function filters data by applying a pandas query to a model DataFrame and using the resulting index to filter the target DataFrame or Series. It handles both simple and MultiIndex cases automatically.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Series | DataFrame</code> <p>The DataFrame or Series to filter.</p> required <code>model_df</code> <code>DataFrame</code> <p>The model DataFrame containing metadata used for filtering. Must have an index that can be matched against df's axis.</p> required <code>query</code> <code>str</code> <p>A pandas query string to apply to model_df. If None or empty, returns df unchanged. Uses pandas query syntax.</p> <code>None</code> <code>match_on_level</code> <code>int | str</code> <p>For MultiIndex cases, specifies which level to match on. Can be an integer (level position) or string (level name).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame | Series</code> <p>Filtered DataFrame or Series with the same type as input df.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # You have a generation time-series df\n&gt;&gt;&gt; print(gen_df)  # Original DataFrame\n    generator            GenA  GenB  GenC  SolarA  WindA\n    2024-01-01 00:00:00   100   200   150      50     80\n    2024-01-01 01:00:00   120   180   170      60     90\n    2024-01-01 02:00:00   110   190   160      55     85\n\n&gt;&gt;&gt; # You have a generator model df\n&gt;&gt;&gt; print(model_df)\n              zone technology  is_res\n    generator\n    GenA        DE    nuclear   False\n    GenB        DE       coal   False\n    GenC        FR        gas   False\n    SolarA      DE      solar    True\n    WindA       NL       wind    True\n\n&gt;&gt;&gt; only_de_conv = filter_by_model_query(gen_df, model_df, '(not is_res) and (zone == \"DE\")')\n&gt;&gt;&gt; print(only_de_conv)  # DataFrame with only non-res generators in DE\n    generator            GenA GenB\n    2024-01-01 00:00:00   100  200\n    2024-01-01 01:00:00   120  180\n    2024-01-01 02:00:00   110  190\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/pandas_utils/filter.py</code> <pre><code>def filter_by_model_query(\n        df: pd.Series | pd.DataFrame,\n        model_df: pd.DataFrame,\n        query: str = None,\n        match_on_level: int | str = None,\n) -&gt; pd.DataFrame | pd.Series:\n    \"\"\"Filter DataFrame or Series based on a query applied to a model DataFrame.\n\n    This function filters data by applying a pandas query to a model DataFrame and\n    using the resulting index to filter the target DataFrame or Series. It handles\n    both simple and MultiIndex cases automatically.\n\n    Args:\n        df: The DataFrame or Series to filter.\n        model_df: The model DataFrame containing metadata used for filtering.\n            Must have an index that can be matched against df's axis.\n        query: A pandas query string to apply to model_df. If None or empty,\n            returns df unchanged. Uses pandas query syntax.\n        match_on_level: For MultiIndex cases, specifies which level to match on.\n            Can be an integer (level position) or string (level name).\n\n    Returns:\n        Filtered DataFrame or Series with the same type as input df.\n\n    Example:\n\n        &gt;&gt;&gt; # You have a generation time-series df\n        &gt;&gt;&gt; print(gen_df)  # Original DataFrame\n            generator            GenA  GenB  GenC  SolarA  WindA\n            2024-01-01 00:00:00   100   200   150      50     80\n            2024-01-01 01:00:00   120   180   170      60     90\n            2024-01-01 02:00:00   110   190   160      55     85\n\n        &gt;&gt;&gt; # You have a generator model df\n        &gt;&gt;&gt; print(model_df)\n                      zone technology  is_res\n            generator\n            GenA        DE    nuclear   False\n            GenB        DE       coal   False\n            GenC        FR        gas   False\n            SolarA      DE      solar    True\n            WindA       NL       wind    True\n\n        &gt;&gt;&gt; only_de_conv = filter_by_model_query(gen_df, model_df, '(not is_res) and (zone == \"DE\")')\n        &gt;&gt;&gt; print(only_de_conv)  # DataFrame with only non-res generators in DE\n            generator            GenA GenB\n            2024-01-01 00:00:00   100  200\n            2024-01-01 01:00:00   120  180\n            2024-01-01 02:00:00   110  190\n    \"\"\"\n    if query is None or query == '':\n        return df\n\n    axis, idx_selection_level = get_matching_axis_and_level(df, model_df.index, match_on_level)\n    idx = df.axes[axis]\n\n    selection = model_df.query(query, engine='python').copy(deep=True).index\n    selection = list(set(selection).intersection(idx.get_level_values(idx_selection_level)))\n\n    if isinstance(idx, pd.MultiIndex):\n        selection = [i for i in idx if i[idx_selection_level] in selection]\n\n    if isinstance(df, pd.Series):\n        return df[selection] if axis == 0 else df.loc[selection]\n    else:\n        return df.loc[selection, :] if axis == 0 else df.loc[:, selection]\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/flatten_df/","title":"MESQUAL Pandas Util <code>flatten_df</code>","text":""},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/flatten_df/#mesqual.utils.pandas_utils.flatten_df.flatten_df","title":"flatten_df","text":"<pre><code>flatten_df(df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Transform a time-series DataFrame into a flat format with one value per row.</p> <p>Converts a DataFrame with multi-level columns (objects/variables/properties) and time-based indices into a long-format DataFrame where each row contains a single value with its corresponding metadata.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame with potentially multi-level columns and indices. Typically represents time-series data with multiple variables, objects, or properties.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Flattened DataFrame in long format where:</p> <ul> <li>Each row represents one data point</li> <li>Original index levels become columns</li> <li>Original column levels become the 'variable' column</li> <li>Data values are in the 'value' column</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample multi-level DataFrame\n&gt;&gt;&gt; dt_idx = pd.date_range('2024-01-01', periods=3, freq='h', name='datetime')\n&gt;&gt;&gt; cols = pd.MultiIndex.from_product([['DE', 'FR'], ['price']], names=['zone', 'type'])\n&gt;&gt;&gt; df = pd.DataFrame(np.random.rand(3, 2), index=dt_idx, columns=cols)\n&gt;&gt;&gt; print(df.head())\n    zone                    DE            FR\n    type                 price volume  price volume\n    datetime\n    2024-01-01 00:00:00  37.45  95.07  73.20  59.87\n    2024-01-01 06:00:00  15.60  15.60   5.81  86.62\n    2024-01-01 12:00:00  60.11  70.81   2.06  96.99\n    2024-01-01 18:00:00  83.24  21.23  18.18  18.34\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Flatten the DataFrame\n&gt;&gt;&gt; flat_df = flatten_df(df)\n&gt;&gt;&gt; print(flat_df.head())\n                  datetime zone    type  value\n    0  2024-01-01 00:00:00   DE   price  37.45\n    1  2024-01-01 06:00:00   DE   price  15.60\n    2  2024-01-01 12:00:00   DE   price  60.11\n    3  2024-01-01 18:00:00   DE   price  83.24\n    4  2024-01-01 00:00:00   DE  volume  95.07\n    5  2024-01-01 06:00:00   DE  volume  15.60\n    6  2024-01-01 12:00:00   DE  volume  70.81\n    7  2024-01-01 18:00:00   DE  volume  21.23\n    8  2024-01-01 00:00:00   FR   price  73.20\n    9  2024-01-01 06:00:00   FR   price   5.81\n    10 2024-01-01 12:00:00   FR   price   2.06\n    11 2024-01-01 18:00:00   FR   price  18.18\n    12 2024-01-01 00:00:00   FR  volume  59.87\n    13 2024-01-01 06:00:00   FR  volume  86.62\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/pandas_utils/flatten_df.py</code> <pre><code>def flatten_df(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Transform a time-series DataFrame into a flat format with one value per row.\n\n    Converts a DataFrame with multi-level columns (objects/variables/properties)\n    and time-based indices into a long-format DataFrame where each row contains\n    a single value with its corresponding metadata.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame with potentially multi-level columns\n            and indices. Typically represents time-series data with multiple\n            variables, objects, or properties.\n\n    Returns:\n        pd.DataFrame: Flattened DataFrame in long format where:\n\n            - Each row represents one data point\n            - Original index levels become columns\n            - Original column levels become the 'variable' column\n            - Data values are in the 'value' column\n\n    Examples:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample multi-level DataFrame\n        &gt;&gt;&gt; dt_idx = pd.date_range('2024-01-01', periods=3, freq='h', name='datetime')\n        &gt;&gt;&gt; cols = pd.MultiIndex.from_product([['DE', 'FR'], ['price']], names=['zone', 'type'])\n        &gt;&gt;&gt; df = pd.DataFrame(np.random.rand(3, 2), index=dt_idx, columns=cols)\n        &gt;&gt;&gt; print(df.head())\n            zone                    DE            FR\n            type                 price volume  price volume\n            datetime\n            2024-01-01 00:00:00  37.45  95.07  73.20  59.87\n            2024-01-01 06:00:00  15.60  15.60   5.81  86.62\n            2024-01-01 12:00:00  60.11  70.81   2.06  96.99\n            2024-01-01 18:00:00  83.24  21.23  18.18  18.34\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Flatten the DataFrame\n        &gt;&gt;&gt; flat_df = flatten_df(df)\n        &gt;&gt;&gt; print(flat_df.head())\n                          datetime zone    type  value\n            0  2024-01-01 00:00:00   DE   price  37.45\n            1  2024-01-01 06:00:00   DE   price  15.60\n            2  2024-01-01 12:00:00   DE   price  60.11\n            3  2024-01-01 18:00:00   DE   price  83.24\n            4  2024-01-01 00:00:00   DE  volume  95.07\n            5  2024-01-01 06:00:00   DE  volume  15.60\n            6  2024-01-01 12:00:00   DE  volume  70.81\n            7  2024-01-01 18:00:00   DE  volume  21.23\n            8  2024-01-01 00:00:00   FR   price  73.20\n            9  2024-01-01 06:00:00   FR   price   5.81\n            10 2024-01-01 12:00:00   FR   price   2.06\n            11 2024-01-01 18:00:00   FR   price  18.18\n            12 2024-01-01 00:00:00   FR  volume  59.87\n            13 2024-01-01 06:00:00   FR  volume  86.62\n    \"\"\"\n\n    data = df.copy()\n    if any(i is None for i in data.columns.names):\n        if data.columns.nlevels == 1:\n            data.columns.name = 'columns'\n        else:\n            data.columns.names = [f'column_level_{i}' if name is None else name for i, name in enumerate(df.columns.names)]\n    if any(i is None for i in data.index.names):\n        if data.index.nlevels == 1:\n            data.index.name = 'index'\n        else:\n            data.index.names = [f'index_level_{i}' if name is None else name for i, name in enumerate(df.index.names)]\n\n    depth_cols = data.columns.nlevels\n    idx_names = list(data.index.names)\n    if depth_cols &gt; 1:\n        idx_cols = [(i, ) + tuple('' for _ in range(depth_cols - 1)) for i in idx_names]\n    else:\n        idx_cols = idx_names\n    data = data.reset_index().melt(id_vars=idx_cols)\n    data = data.rename(columns={tup: name for tup, name in zip(idx_cols, idx_names)})\n\n    return data\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/merge_multi_index_levels/","title":"MESQUAL Pandas Util <code>merge_multi_index_levels</code>","text":""},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/merge_multi_index_levels/#mesqual.utils.pandas_utils.merge_multi_index_levels.merge_multi_index_levels","title":"merge_multi_index_levels","text":"<pre><code>merge_multi_index_levels(multi_index: MultiIndex, levels: list[str], name_of_new_level: str, join_levels_by: str = ' - ', append_new_level_as_last: bool = True) -&gt; MultiIndex\n</code></pre> <p>Merge multiple levels of a MultiIndex into a single new level.</p> <p>Combines specified levels from a pandas MultiIndex by joining their values with a separator string, creating a new level while preserving other levels.</p> <p>Parameters:</p> Name Type Description Default <code>multi_index</code> <code>MultiIndex</code> <p>The MultiIndex to modify.</p> required <code>levels</code> <code>list[str]</code> <p>List of level names to merge together.</p> required <code>name_of_new_level</code> <code>str</code> <p>Name for the newly created merged level.</p> required <code>join_levels_by</code> <code>str</code> <p>String used to join the level values. Defaults to ' - '.</p> <code>' - '</code> <code>append_new_level_as_last</code> <code>bool</code> <p>If True, append new level at the end. If False, prepend at the beginning. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>MultiIndex</code> <p>A new MultiIndex with the specified levels merged into a single level.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; index = pd.MultiIndex.from_tuples([\n...     ('DE', 'solar', '2024'),\n...     ('DE', 'wind', '2024'),\n...     ('FR', 'nuclear', '2024')\n... ], names=['country', 'technology', 'year'])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Merge country and technology levels\n&gt;&gt;&gt; new_index = merge_multi_index_levels(\n...     index,\n...     ['country', 'technology'],\n...     'location_tech',\n...     join_levels_by='_'\n... )\n&gt;&gt;&gt; print(new_index.names)\n    ['year', 'location_tech']\n&gt;&gt;&gt; print(new_index.tolist())\n    [('2024', 'DE_solar'), ('2024', 'DE_wind'), ('2024', 'FR_nuclear')]\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/pandas_utils/merge_multi_index_levels.py</code> <pre><code>def merge_multi_index_levels(\n        multi_index: pd.MultiIndex,\n        levels: list[str],\n        name_of_new_level: str,\n        join_levels_by: str = ' - ',\n        append_new_level_as_last: bool = True\n) -&gt; pd.MultiIndex:\n    \"\"\"Merge multiple levels of a MultiIndex into a single new level.\n\n    Combines specified levels from a pandas MultiIndex by joining their values\n    with a separator string, creating a new level while preserving other levels.\n\n    Args:\n        multi_index: The MultiIndex to modify.\n        levels: List of level names to merge together.\n        name_of_new_level: Name for the newly created merged level.\n        join_levels_by: String used to join the level values. Defaults to ' - '.\n        append_new_level_as_last: If True, append new level at the end. If False,\n            prepend at the beginning. Defaults to True.\n\n    Returns:\n        A new MultiIndex with the specified levels merged into a single level.\n\n    Examples:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; index = pd.MultiIndex.from_tuples([\n        ...     ('DE', 'solar', '2024'),\n        ...     ('DE', 'wind', '2024'),\n        ...     ('FR', 'nuclear', '2024')\n        ... ], names=['country', 'technology', 'year'])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Merge country and technology levels\n        &gt;&gt;&gt; new_index = merge_multi_index_levels(\n        ...     index,\n        ...     ['country', 'technology'],\n        ...     'location_tech',\n        ...     join_levels_by='_'\n        ... )\n        &gt;&gt;&gt; print(new_index.names)\n            ['year', 'location_tech']\n        &gt;&gt;&gt; print(new_index.tolist())\n            [('2024', 'DE_solar'), ('2024', 'DE_wind'), ('2024', 'FR_nuclear')]\n    \"\"\"\n    df_index = multi_index.to_frame()\n    merged_level = df_index[levels].astype(str).agg(join_levels_by.join, axis=1)\n    remaining_levels = [level for level in multi_index.names if level not in levels]\n\n    if append_new_level_as_last:\n        new_level_order = remaining_levels + [name_of_new_level]\n    else:\n        new_level_order = [name_of_new_level] + remaining_levels\n\n    df_index = df_index[remaining_levels]\n    df_index[name_of_new_level] = merged_level\n\n    return pd.MultiIndex.from_frame(df_index[new_level_order])\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/pend_props/","title":"MESQUAL Pandas Util <code>prepend_model_prop_levels</code>","text":""},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/pend_props/#mesqual.utils.pandas_utils.pend_props.prepend_model_prop_levels","title":"prepend_model_prop_levels","text":"<pre><code>prepend_model_prop_levels(data: Series | DataFrame, model: DataFrame, *properties, prepend_to_top: bool = True, match_on_level: str = None) -&gt; Series | DataFrame\n</code></pre> <p>Prepend model properties as new index levels to data.</p> <p>Searches for an index level in data that matches the model's index, then prepends specified properties from the model as new index levels.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series | DataFrame</code> <p>The pandas object to add properties to.</p> required <code>model</code> <code>DataFrame</code> <p>DataFrame containing properties to prepend, with matching index.</p> required <code>*properties</code> <p>Column names from model to use as new index levels.</p> <code>()</code> <code>prepend_to_top</code> <code>bool</code> <p>If True, add properties at the beginning of index levels. If False, add at the end.</p> <code>True</code> <code>match_on_level</code> <code>str</code> <p>Optional level name to constrain matching to specific level. Useful in case the there are multiple index levels in data that match the model's index</p> <code>None</code> <p>Returns:</p> Type Description <code>Series | DataFrame</code> <p>Copy of data with properties prepended as new index levels.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any property is not found in model columns.</p> Energy Domain Context <p>In Energy Systems Analysis, you often have to groupby and aggregate by certain properties. This module makes it easy to include the properties as a new index level before performing the groupby - agg pipeline.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # You have a generation time-series df\n&gt;&gt;&gt; print(gen_df)  # Original DataFrame\n    generator            GenA  GenB  GenC  SolarA  WindA\n    2024-01-01 00:00:00   100   200   150      50     80\n    2024-01-01 01:00:00   120   180   170      60     90\n    2024-01-01 02:00:00   110   190   160      55     85\n\n&gt;&gt;&gt; # You have a generator model df\n&gt;&gt;&gt; print(model_df)\n              zone technology  is_res\n    generator\n    GenA        DE    nuclear   False\n    GenB        DE       coal   False\n    GenC        FR        gas   False\n    SolarA      DE      solar    True\n    WindA       NL       wind    True\n\n&gt;&gt;&gt; gen_with_props = prepend_model_prop_levels(gen_df, model_df, 'zone', 'is_res')\n&gt;&gt;&gt; print(gen_with_props)  # DataFrame with prepended properties\n    is_res              False            True\n    zone                   DE        FR     DE    NL\n    generator            GenA GenB GenC SolarA WindA\n    2024-01-01 00:00:00   100  200  150     50    80\n    2024-01-01 01:00:00   120  180  170     60    90\n    2024-01-01 02:00:00   110  190  160     55    85\n\n&gt;&gt;&gt; gen_by_zone_and_type = gen_with_props.T.groupby(level=['zone', 'is_res']).sum().T\n&gt;&gt;&gt; print(gen_by_zone_and_type)  # grouped and aggregated\n    zone                   DE          FR    NL\n    is_res              False True  False True\n    2024-01-01 00:00:00   300    50   150    80\n    2024-01-01 01:00:00   300    60   170    90\n    2024-01-01 02:00:00   300    55   160    85\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/pandas_utils/pend_props.py</code> <pre><code>def prepend_model_prop_levels(\n        data: pd.Series | pd.DataFrame,\n        model: pd.DataFrame,\n        *properties,\n        prepend_to_top: bool = True,\n        match_on_level: str = None,\n) -&gt; pd.Series | pd.DataFrame:\n    \"\"\"Prepend model properties as new index levels to data.\n\n    Searches for an index level in data that matches the model's index, then\n    prepends specified properties from the model as new index levels.\n\n    Args:\n        data: The pandas object to add properties to.\n        model: DataFrame containing properties to prepend, with matching index.\n        *properties: Column names from model to use as new index levels.\n        prepend_to_top: If True, add properties at the beginning of index levels.\n            If False, add at the end.\n        match_on_level: Optional level name to constrain matching to specific level.\n            Useful in case the there are multiple index levels in data that match\n            the model's index\n\n    Returns:\n        Copy of data with properties prepended as new index levels.\n\n    Raises:\n        ValueError: If any property is not found in model columns.\n\n    Energy Domain Context:\n        In Energy Systems Analysis, you often have to groupby and aggregate\n        by certain properties. This module makes it easy to include the properties\n        as a new index level before performing the groupby - agg pipeline.\n\n    Example:\n\n        &gt;&gt;&gt; # You have a generation time-series df\n        &gt;&gt;&gt; print(gen_df)  # Original DataFrame\n            generator            GenA  GenB  GenC  SolarA  WindA\n            2024-01-01 00:00:00   100   200   150      50     80\n            2024-01-01 01:00:00   120   180   170      60     90\n            2024-01-01 02:00:00   110   190   160      55     85\n\n        &gt;&gt;&gt; # You have a generator model df\n        &gt;&gt;&gt; print(model_df)\n                      zone technology  is_res\n            generator\n            GenA        DE    nuclear   False\n            GenB        DE       coal   False\n            GenC        FR        gas   False\n            SolarA      DE      solar    True\n            WindA       NL       wind    True\n\n        &gt;&gt;&gt; gen_with_props = prepend_model_prop_levels(gen_df, model_df, 'zone', 'is_res')\n        &gt;&gt;&gt; print(gen_with_props)  # DataFrame with prepended properties\n            is_res              False            True\n            zone                   DE        FR     DE    NL\n            generator            GenA GenB GenC SolarA WindA\n            2024-01-01 00:00:00   100  200  150     50    80\n            2024-01-01 01:00:00   120  180  170     60    90\n            2024-01-01 02:00:00   110  190  160     55    85\n\n        &gt;&gt;&gt; gen_by_zone_and_type = gen_with_props.T.groupby(level=['zone', 'is_res']).sum().T\n        &gt;&gt;&gt; print(gen_by_zone_and_type)  # grouped and aggregated\n            zone                   DE          FR    NL\n            is_res              False True  False True\n            2024-01-01 00:00:00   300    50   150    80\n            2024-01-01 01:00:00   300    60   170    90\n            2024-01-01 02:00:00   300    55   160    85\n    \"\"\"\n    tmp = data.copy()\n    properties = [p for p in properties if not ((p is None) or (p == ''))]\n\n    if not properties:\n        return tmp\n\n    for prop in properties:\n        if prop not in model.columns.tolist():\n            raise ValueError(f'Property unavailable: {prop} was not found in your model_df.')\n    axis, level = get_matching_axis_and_level(data, model.index, match_on_level)\n\n    match_keys = tmp.axes[axis].get_level_values(level)\n    new_index = tmp.axes[axis].to_frame(index=False)\n    for prop in properties:\n        if prop not in new_index:\n            loc = 0 if prepend_to_top else len(new_index.columns)\n            new_index.insert(loc, prop, model.loc[match_keys, prop].values)\n    new_index = pd.MultiIndex.from_frame(new_index)\n    if axis == 0:\n        tmp.index = new_index\n    else:\n        tmp.columns = new_index\n\n    if is_series(data):\n        tmp: pd.Series = tmp\n        return tmp\n    elif is_dataframe(data):\n        tmp: pd.DataFrame = tmp\n        return tmp\n    else:\n        return tmp\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/set_new_column/","title":"MESQUAL Pandas Util <code>set_new_column</code>","text":""},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/set_new_column/#mesqual.utils.pandas_utils.set_new_column.set_column","title":"set_column","text":"<pre><code>set_column(df: DataFrame, new_column_name: Hashable, new_column_values: Series | DataFrame) -&gt; DataFrame\n</code></pre> <p>Set or replace a column in a DataFrame with new values.</p> <p>Adds a new column or replaces an existing column in a DataFrame. Handles both Series and DataFrame inputs, with special logic for MultiIndex columns when using DataFrame inputs.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to modify.</p> required <code>new_column_name</code> <code>Hashable</code> <p>Name/key for the new column.</p> required <code>new_column_values</code> <code>Series | DataFrame</code> <p>Values for the new column. Can be a Series for simple columns or a DataFrame for MultiIndex column structures.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A copy of the DataFrame with the new column added or existing column replaced.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If length of df and new_column_values don't match, or if new_column_values DataFrame has incorrect number of column levels.</p> <code>TypeError</code> <p>If new_column_values is neither Series nor DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Add Series as new column\n&gt;&gt;&gt; new_series = pd.Series([7, 8, 9])\n&gt;&gt;&gt; result = set_column(df, 'C', new_series)\n&gt;&gt;&gt; print(result.columns.tolist())\n    ['A', 'B', 'C']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Replace existing column\n&gt;&gt;&gt; replacement = pd.Series([10, 11, 12])\n&gt;&gt;&gt; result = set_column(df, 'A', replacement)\n&gt;&gt;&gt; print(result['A'].tolist())\n    [10, 11, 12]\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/pandas_utils/set_new_column.py</code> <pre><code>def set_column(\n        df: pd.DataFrame,\n        new_column_name: Hashable,\n        new_column_values: pd.Series | pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Set or replace a column in a DataFrame with new values.\n\n    Adds a new column or replaces an existing column in a DataFrame. Handles both\n    Series and DataFrame inputs, with special logic for MultiIndex columns when\n    using DataFrame inputs.\n\n    Args:\n        df: The DataFrame to modify.\n        new_column_name: Name/key for the new column.\n        new_column_values: Values for the new column. Can be a Series for simple\n            columns or a DataFrame for MultiIndex column structures.\n\n    Returns:\n        A copy of the DataFrame with the new column added or existing column replaced.\n\n    Raises:\n        ValueError: If length of df and new_column_values don't match, or if\n            new_column_values DataFrame has incorrect number of column levels.\n        TypeError: If new_column_values is neither Series nor DataFrame.\n\n    Examples:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Add Series as new column\n        &gt;&gt;&gt; new_series = pd.Series([7, 8, 9])\n        &gt;&gt;&gt; result = set_column(df, 'C', new_series)\n        &gt;&gt;&gt; print(result.columns.tolist())\n            ['A', 'B', 'C']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Replace existing column\n        &gt;&gt;&gt; replacement = pd.Series([10, 11, 12])\n        &gt;&gt;&gt; result = set_column(df, 'A', replacement)\n        &gt;&gt;&gt; print(result['A'].tolist())\n            [10, 11, 12]\n    \"\"\"\n\n    dff = df.copy()\n\n    if not len(dff) == len(new_column_values):\n        raise ValueError('Length of dff and new_column_values must be equal.')\n\n    # TODO optional: check index\n\n    if isinstance(new_column_values, pd.Series):\n        dff[new_column_name] = new_column_values\n        return dff\n\n    if isinstance(new_column_values, pd.DataFrame):\n        if not new_column_values.columns.nlevels == (dff.columns.nlevels - 1):\n            raise ValueError(\n                'Your new_column_values must have n-1 column levels, where n is the number of levels in dff.'\n            )\n\n        if new_column_name in dff.columns:\n            dff = dff.drop(columns=[new_column_name])\n\n        new_column_values = pd.concat({new_column_name: new_column_values}, axis=1, names=[dff.columns.names[0]])\n        dff = pd.concat([dff, new_column_values], axis=1)\n        return dff\n\n    else:\n        raise TypeError('Used new_column_values type not accepted.')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/sort_multiindex/","title":"MESQUAL Pandas Util <code>sort_multiindex</code>","text":""},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/sort_multiindex/#mesqual.utils.pandas_utils.sort_multiindex.sort_multiindex","title":"sort_multiindex","text":"<pre><code>sort_multiindex(df: DataFrame, custom_order: list[str | int], level: str | int, axis: int = 0) -&gt; DataFrame\n</code></pre> <p>Sort a DataFrame's MultiIndex at a specific level using a custom order.</p> <p>Reorders the specified level according to custom_order while preserving the existing order of all other levels. This allows for sequential sorting operations where each sort maintains previous orderings.</p> <p>Values in the target level that are not included in custom_order will be appended at the end, maintaining their original relative order.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame with MultiIndex to sort.</p> required <code>custom_order</code> <code>list[str | int]</code> <p>List of values defining the desired order for the specified level.</p> required <code>level</code> <code>str | int</code> <p>Level to sort. Can be level name (str) or level number (int).</p> required <code>axis</code> <code>int</code> <p>Axis to sort along. 0 for index (rows), 1 for columns.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with reordered MultiIndex according to the custom order.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If axis is not 0 or 1, or if level is not a valid string or integer.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; idx = pd.MultiIndex.from_arrays([['A', 'A', 'B', 'B'], [1, 2, 1, 2]])\n&gt;&gt;&gt; df = pd.DataFrame({'val': [10, 20, 30, 40]}, index=idx)\n&gt;&gt;&gt; sort_multiindex(df, [2, 1], level=1)  # Sort second level\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/pandas_utils/sort_multiindex.py</code> <pre><code>def sort_multiindex(df: pd.DataFrame, custom_order: list[str | int], level: str | int, axis: int = 0) -&gt; pd.DataFrame:\n    \"\"\"Sort a DataFrame's MultiIndex at a specific level using a custom order.\n\n    Reorders the specified level according to custom_order while preserving\n    the existing order of all other levels. This allows for sequential sorting\n    operations where each sort maintains previous orderings.\n\n    Values in the target level that are not included in custom_order will be\n    appended at the end, maintaining their original relative order.\n\n    Args:\n        df: The DataFrame with MultiIndex to sort.\n        custom_order: List of values defining the desired order for the specified level.\n        level: Level to sort. Can be level name (str) or level number (int).\n        axis: Axis to sort along. 0 for index (rows), 1 for columns.\n\n    Returns:\n        DataFrame with reordered MultiIndex according to the custom order.\n\n    Raises:\n        ValueError: If axis is not 0 or 1, or if level is not a valid string or integer.\n\n    Example:\n\n        &gt;&gt;&gt; idx = pd.MultiIndex.from_arrays([['A', 'A', 'B', 'B'], [1, 2, 1, 2]])\n        &gt;&gt;&gt; df = pd.DataFrame({'val': [10, 20, 30, 40]}, index=idx)\n        &gt;&gt;&gt; sort_multiindex(df, [2, 1], level=1)  # Sort second level\n    \"\"\"\n    if axis not in [0, 1]:\n        raise ValueError(\"axis must be 0 (rows) or 1 (columns)\")\n\n    idx = df.axes[axis]\n    if isinstance(idx, pd.MultiIndex):\n        if isinstance(level, str):\n            level_num = idx.names.index(level)\n        elif isinstance(level, int):\n            level_num = level\n        else:\n            raise ValueError(\"level must be a string (level name) or an integer (level number)\")\n\n        idx_tuples = idx.to_list()\n        remaining_values_in_level_to_sort = [i for i in idx.get_level_values(level_num).unique() if\n                                             i not in custom_order]\n        ordered_tuples = []\n        for value in custom_order + remaining_values_in_level_to_sort:\n            for tuple_item in idx_tuples:\n                if tuple_item[level_num] == value:\n                    ordered_tuples.append(tuple_item)\n        new_index = pd.MultiIndex.from_tuples(ordered_tuples, names=idx.names)\n    else:\n        idx_values = idx.to_list()\n        remaining_values = [i for i in idx.unique() if i not in custom_order]\n        ordered_values = []\n        for value in custom_order + remaining_values:\n            for idx_value in idx_values:\n                if idx_value == value:\n                    ordered_values.append(idx_value)\n        new_index = pd.Index(ordered_values, name=idx.name)\n\n    if axis == 0:\n        return df.reindex(new_index)\n    else:\n        return df.reindex(columns=new_index)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/xs_df/","title":"MESQUAL Pandas Util <code>xs_df</code>","text":""},{"location":"mesqual-package-documentation/api_reference/utils/pandas_utils/xs_df/#mesqual.utils.pandas_utils.xs_df.xs_df","title":"xs_df","text":"<pre><code>xs_df(df: DataFrame, keys: Hashable | list[Hashable], axis: Axis = 0, level: Hashable = None) -&gt; DataFrame\n</code></pre> <p>Extract cross-section from MultiIndex DataFrame with support for multiple keys.</p> <p>This function provides a flexible interface to pandas .xs() method with enhanced functionality for MESQUAL's MultiIndex data structures. It supports both single and multiple key selection, making it particularly useful for energy systems analysis where data often has complex hierarchical structures.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame with MultiIndex (either on index or columns).</p> required <code>keys</code> <code>Hashable | list[Hashable]</code> <p>Single key or list of keys to select from the specified level. For single keys, uses pandas .xs() method with drop_level=True. For multiple keys, uses .isin() for efficient selection.</p> required <code>axis</code> <code>Axis</code> <p>Axis to operate on. Can be 0/'index'/'rows' for index operations or 1/'columns' for column operations. Defaults to 0.</p> <code>0</code> <code>level</code> <code>Hashable</code> <p>Name or position of the MultiIndex level to select from. Must be specified for MultiIndex operations.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with cross-section data. For single keys, the specified level</p> <code>DataFrame</code> <p>is dropped. For multiple keys, the level is preserved.</p> <p>Examples:</p> <p>Single dataset selection from MESQUAL multi-scenario data:</p> <pre><code>&gt;&gt;&gt; multi_scenario_prices = study.scen.fetch('buses_t.marginal_price')\n&gt;&gt;&gt; base_prices = xs_df(multi_scenario_prices, 'base', level='dataset')\n</code></pre> <p>Multiple scenario selection:</p> <pre><code>&gt;&gt;&gt; scenarios = ['base', 'high_renewable', 'low_cost']\n&gt;&gt;&gt; selected_data = xs_df(multi_scenario_prices, scenarios, level='dataset')\n</code></pre> <p>Column-wise selection for specific buses:</p> <pre><code>&gt;&gt;&gt; bus_names = ['Bus_1', 'Bus_2', 'Bus_3']\n&gt;&gt;&gt; selected_buses = xs_df(price_data, bus_names, axis='columns', level='Bus')\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/pandas_utils/xs_df.py</code> <pre><code>def xs_df(\n        df: pd.DataFrame,\n        keys: Hashable | list[Hashable],\n        axis: Axis = 0,\n        level: Hashable = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Extract cross-section from MultiIndex DataFrame with support for multiple keys.\n\n    This function provides a flexible interface to pandas .xs() method with enhanced\n    functionality for MESQUAL's MultiIndex data structures. It supports both single\n    and multiple key selection, making it particularly useful for energy systems\n    analysis where data often has complex hierarchical structures.\n\n    Args:\n        df: Input DataFrame with MultiIndex (either on index or columns).\n        keys: Single key or list of keys to select from the specified level.\n            For single keys, uses pandas .xs() method with drop_level=True.\n            For multiple keys, uses .isin() for efficient selection.\n        axis: Axis to operate on. Can be 0/'index'/'rows' for index operations\n            or 1/'columns' for column operations. Defaults to 0.\n        level: Name or position of the MultiIndex level to select from.\n            Must be specified for MultiIndex operations.\n\n    Returns:\n        DataFrame with cross-section data. For single keys, the specified level\n        is dropped. For multiple keys, the level is preserved.\n\n    Examples:\n        Single dataset selection from MESQUAL multi-scenario data:\n        &gt;&gt;&gt; multi_scenario_prices = study.scen.fetch('buses_t.marginal_price')\n        &gt;&gt;&gt; base_prices = xs_df(multi_scenario_prices, 'base', level='dataset')\n\n        Multiple scenario selection:\n        &gt;&gt;&gt; scenarios = ['base', 'high_renewable', 'low_cost']\n        &gt;&gt;&gt; selected_data = xs_df(multi_scenario_prices, scenarios, level='dataset')\n\n        Column-wise selection for specific buses:\n        &gt;&gt;&gt; bus_names = ['Bus_1', 'Bus_2', 'Bus_3']\n        &gt;&gt;&gt; selected_buses = xs_df(price_data, bus_names, axis='columns', level='Bus')\n    \"\"\"\n    if isinstance(keys, list):\n        if axis in [0, 'index', 'rows']:\n            return df.iloc[df.index.get_level_values(level).isin(keys)]\n        return df.iloc[:, df.columns.get_level_values(level).isin(keys)]\n    return df.xs(keys, level=level, axis=axis, drop_level=True)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/","title":"MESQUAL Plotly Utils","text":""},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/#mesqual.utils.plotly_utils","title":"plotly_utils","text":"<p>Plotly utilities for enhanced figure styling and subplot manipulation.</p> <p>This package provides utilities for working with Plotly figures in the MESQUAL framework, including:</p> <ul> <li>plotly_theme: Custom themes and color palettes (template) for consistent visualization styling</li> <li>figure_utils: Common figure modifications like titles, annotations, and axis controls</li> <li>px_category_utils: Tools for working with categorical data in faceted Plotly Express plots</li> </ul> <p>The utilities enable precise control over figure appearance and facilitate adding elements to specific subplots in complex multi-panel visualizations.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from mesqual.utils.plotly_utils import PlotlyTheme, figure_utils\n&gt;&gt;&gt; from mesqual.utils.plotly_utils.plotly_theme import colors\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply consistent theming\n&gt;&gt;&gt; theme = PlotlyTheme(default_colorway=colors.qualitative.default)\n&gt;&gt;&gt; theme.apply()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Style figures\n&gt;&gt;&gt; figure_utils.set_title(fig, \"Energy Analysis Dashboard\")\n&gt;&gt;&gt; figure_utils.make_annotations_bold(fig)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/figure_utils/","title":"MESQUAL Plotly Util <code>figure_utils</code>","text":""},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mesqual.utils.plotly_utils.figure_utils","title":"figure_utils","text":"<p>Utility functions for styling and modifying Plotly figures.</p> <p>This module provides convenience functions for common figure modifications such as title formatting, annotation styling, axis configuration, and adding interactive controls to Plotly figures.</p>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mesqual.utils.plotly_utils.figure_utils.set_title","title":"set_title","text":"<pre><code>set_title(fig: Figure, title: str)\n</code></pre> <p>Set a centered, bold title for the figure.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to modify.</p> required <code>title</code> <code>str</code> <p>Title text to display.</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; set_title(fig, \"Sales Performance Dashboard\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/figure_utils.py</code> <pre><code>def set_title(fig: go.Figure, title: str):\n    \"\"\"Set a centered, bold title for the figure.\n\n    Args:\n        fig: Plotly figure object to modify.\n        title: Title text to display.\n\n    Example:\n\n        &gt;&gt;&gt; set_title(fig, \"Sales Performance Dashboard\")\n    \"\"\"\n    title = f'&lt;b&gt;{title}&lt;/b&gt;'\n    fig.update_layout(title_text=title, title_x=0.5)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mesqual.utils.plotly_utils.figure_utils.remove_category_in_annotations","title":"remove_category_in_annotations","text":"<pre><code>remove_category_in_annotations(fig: Figure)\n</code></pre> <p>Remove category names from subplot annotations, keeping only values.</p> <p>Modifies annotation text to show only the part after '=' for cleaner subplot labels (e.g., 'sex=Male' becomes 'Male').</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object with annotations to modify.</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; remove_category_in_annotations(fig)  # 'smoker=Yes' \u2192 'Yes'\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/figure_utils.py</code> <pre><code>def remove_category_in_annotations(fig: go.Figure):\n    \"\"\"Remove category names from subplot annotations, keeping only values.\n\n    Modifies annotation text to show only the part after '=' for cleaner\n    subplot labels (e.g., 'sex=Male' becomes 'Male').\n\n    Args:\n        fig: Plotly figure object with annotations to modify.\n\n    Example:\n\n        &gt;&gt;&gt; remove_category_in_annotations(fig)  # 'smoker=Yes' \u2192 'Yes'\n    \"\"\"\n    fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mesqual.utils.plotly_utils.figure_utils.make_annotations_bold","title":"make_annotations_bold","text":"<pre><code>make_annotations_bold(fig: Figure)\n</code></pre> <p>Apply bold formatting to all figure annotations.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object with annotations to modify.</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; make_annotations_bold(fig)  # Makes all subplot labels bold\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/figure_utils.py</code> <pre><code>def make_annotations_bold(fig: go.Figure):\n    \"\"\"Apply bold formatting to all figure annotations.\n\n    Args:\n        fig: Plotly figure object with annotations to modify.\n\n    Example:\n\n        &gt;&gt;&gt; make_annotations_bold(fig)  # Makes all subplot labels bold\n    \"\"\"\n    fig.for_each_annotation(lambda a: a.update(text='&lt;b&gt;' + a.text + '&lt;/b&gt;'))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mesqual.utils.plotly_utils.figure_utils.unmatch_xaxes","title":"unmatch_xaxes","text":"<pre><code>unmatch_xaxes(fig: Figure)\n</code></pre> <p>Remove x-axis matching across subplots.</p> <p>Allows each subplot to have independent x-axis ranges and scaling.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to modify.</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; unmatch_xaxes(fig)  # Each subplot can have different x-ranges\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/figure_utils.py</code> <pre><code>def unmatch_xaxes(fig: go.Figure):\n    \"\"\"Remove x-axis matching across subplots.\n\n    Allows each subplot to have independent x-axis ranges and scaling.\n\n    Args:\n        fig: Plotly figure object to modify.\n\n    Example:\n\n        &gt;&gt;&gt; unmatch_xaxes(fig)  # Each subplot can have different x-ranges\n    \"\"\"\n    fig.update_xaxes(matches=None)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mesqual.utils.plotly_utils.figure_utils.unmatch_yaxes","title":"unmatch_yaxes","text":"<pre><code>unmatch_yaxes(fig: Figure)\n</code></pre> <p>Remove y-axis matching across subplots.</p> <p>Allows each subplot to have independent y-axis ranges and scaling.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to modify.</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; unmatch_yaxes(fig)  # Each subplot can have different y-ranges\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/figure_utils.py</code> <pre><code>def unmatch_yaxes(fig: go.Figure):\n    \"\"\"Remove y-axis matching across subplots.\n\n    Allows each subplot to have independent y-axis ranges and scaling.\n\n    Args:\n        fig: Plotly figure object to modify.\n\n    Example:\n\n        &gt;&gt;&gt; unmatch_yaxes(fig)  # Each subplot can have different y-ranges\n    \"\"\"\n    fig.update_yaxes(matches=None)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mesqual.utils.plotly_utils.figure_utils.reverse_legend_traceorder","title":"reverse_legend_traceorder","text":"<pre><code>reverse_legend_traceorder(fig: Figure)\n</code></pre> <p>Reverse the order of legend entries.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to modify.</p> required <p>Example:</p> <pre><code>&gt;&gt;&gt; reverse_legend_traceorder(fig)  # Last trace appears first in legend\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/figure_utils.py</code> <pre><code>def reverse_legend_traceorder(fig: go.Figure):\n    \"\"\"Reverse the order of legend entries.\n\n    Args:\n        fig: Plotly figure object to modify.\n\n    Example:\n\n        &gt;&gt;&gt; reverse_legend_traceorder(fig)  # Last trace appears first in legend\n    \"\"\"\n    fig.update_layout(legend_traceorder=\"reversed\")\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/figure_utils/#mesqual.utils.plotly_utils.figure_utils.add_datetime_rangeslider","title":"add_datetime_rangeslider","text":"<pre><code>add_datetime_rangeslider(fig: Figure)\n</code></pre> <p>Add an interactive datetime range slider and selector to the figure.</p> <p>Adds a range slider below the plot and time period selector buttons for easy navigation of time series data.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object to modify.</p> required <p>Returns:</p> Type Description <p>Modified figure object with range controls.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; fig = add_datetime_rangeslider(fig)\n&gt;&gt;&gt; fig.show()  # Now includes 1d, 1w, 1m, 6m, YTD, 1y buttons\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/figure_utils.py</code> <pre><code>def add_datetime_rangeslider(fig: go.Figure):\n    \"\"\"Add an interactive datetime range slider and selector to the figure.\n\n    Adds a range slider below the plot and time period selector buttons\n    for easy navigation of time series data.\n\n    Args:\n        fig: Plotly figure object to modify.\n\n    Returns:\n        Modified figure object with range controls.\n\n    Example:\n\n        &gt;&gt;&gt; fig = add_datetime_rangeslider(fig)\n        &gt;&gt;&gt; fig.show()  # Now includes 1d, 1w, 1m, 6m, YTD, 1y buttons\n    \"\"\"\n    fig.update_xaxes(\n        rangeslider_visible=True,\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1, label=\"1d\", step=\"day\", stepmode=\"backward\"),\n                dict(count=7, label=\"1w\", step=\"day\", stepmode=\"backward\"),\n                dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n                dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n                dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n                dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n                dict(step=\"all\")\n            ])\n        )\n    )\n    fig.update_layout(yaxis_fixedrange=False)\n    return fig\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/plotly_theme/","title":"MESQUAL Plotly Util <code>PlotlyTheme</code>","text":""},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mesqual.utils.plotly_utils.plotly_theme","title":"plotly_theme","text":"<p>Custom Plotly theme and color palette definitions for consistent visualization styling.</p> <p>This module provides a comprehensive theming system for Plotly figures, including predefined color palettes, theme configuration, and template application. It enables consistent styling across all visualizations in the MESQUAL framework.</p>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mesqual.utils.plotly_utils.plotly_theme.colors","title":"colors  <code>module-attribute</code>","text":"<pre><code>colors = ColorPalette\n</code></pre> <p>Global color palette instance for convenient access to all color schemes.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; colors.primary.blue\n'#0984e3'\n&gt;&gt;&gt; colors.sequential.default\n['#00b894', '#0984e3', '#d63031']\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mesqual.utils.plotly_utils.plotly_theme.ConstantsIterable","title":"ConstantsIterable","text":"<p>Base class for creating iterable constant collections.</p> <p>Provides dictionary-like interface methods (items, values, keys) for accessing class attributes as constants.</p> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/plotly_theme.py</code> <pre><code>class ConstantsIterable:\n    \"\"\"Base class for creating iterable constant collections.\n\n    Provides dictionary-like interface methods (items, values, keys) for\n    accessing class attributes as constants.\n    \"\"\"\n    @classmethod\n    def items(cls):\n        \"\"\"Yield (name, value) pairs for all non-private, non-callable attributes.\n\n        Yields:\n            Tuple of (attribute_name, attribute_value) for class constants.\n        \"\"\"\n        for attr_name in dir(cls):\n            if not attr_name.startswith('__') and not callable(getattr(cls, attr_name)):\n                yield attr_name, getattr(cls, attr_name)\n\n    @classmethod\n    def values(cls):\n        \"\"\"Yield values for all non-private, non-callable attributes.\n\n        Yields:\n            Attribute values for class constants.\n        \"\"\"\n        for attr_name in dir(cls):\n            if not attr_name.startswith('__') and not callable(getattr(cls, attr_name)):\n                yield getattr(cls, attr_name)\n\n    @classmethod\n    def keys(cls):\n        \"\"\"Yield names for all non-private, non-callable attributes.\n\n        Yields:\n            Attribute names for class constants.\n        \"\"\"\n        for attr_name in dir(cls):\n            if not attr_name.startswith('__') and not callable(getattr(cls, attr_name)):\n                yield attr_name\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mesqual.utils.plotly_utils.plotly_theme.ConstantsIterable.items","title":"items  <code>classmethod</code>","text":"<pre><code>items()\n</code></pre> <p>Yield (name, value) pairs for all non-private, non-callable attributes.</p> <p>Yields:</p> Type Description <p>Tuple of (attribute_name, attribute_value) for class constants.</p> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/plotly_theme.py</code> <pre><code>@classmethod\ndef items(cls):\n    \"\"\"Yield (name, value) pairs for all non-private, non-callable attributes.\n\n    Yields:\n        Tuple of (attribute_name, attribute_value) for class constants.\n    \"\"\"\n    for attr_name in dir(cls):\n        if not attr_name.startswith('__') and not callable(getattr(cls, attr_name)):\n            yield attr_name, getattr(cls, attr_name)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mesqual.utils.plotly_utils.plotly_theme.ConstantsIterable.values","title":"values  <code>classmethod</code>","text":"<pre><code>values()\n</code></pre> <p>Yield values for all non-private, non-callable attributes.</p> <p>Yields:</p> Type Description <p>Attribute values for class constants.</p> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/plotly_theme.py</code> <pre><code>@classmethod\ndef values(cls):\n    \"\"\"Yield values for all non-private, non-callable attributes.\n\n    Yields:\n        Attribute values for class constants.\n    \"\"\"\n    for attr_name in dir(cls):\n        if not attr_name.startswith('__') and not callable(getattr(cls, attr_name)):\n            yield getattr(cls, attr_name)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mesqual.utils.plotly_utils.plotly_theme.ConstantsIterable.keys","title":"keys  <code>classmethod</code>","text":"<pre><code>keys()\n</code></pre> <p>Yield names for all non-private, non-callable attributes.</p> <p>Yields:</p> Type Description <p>Attribute names for class constants.</p> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/plotly_theme.py</code> <pre><code>@classmethod\ndef keys(cls):\n    \"\"\"Yield names for all non-private, non-callable attributes.\n\n    Yields:\n        Attribute names for class constants.\n    \"\"\"\n    for attr_name in dir(cls):\n        if not attr_name.startswith('__') and not callable(getattr(cls, attr_name)):\n            yield attr_name\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mesqual.utils.plotly_utils.plotly_theme.PrimaryColors","title":"PrimaryColors","text":"<p>               Bases: <code>ConstantsIterable</code></p> <p>Primary color palette with vibrant, distinct colors.</p> <p>Provides a curated set of primary colors suitable for categorical data visualization and general plotting needs.</p> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/plotly_theme.py</code> <pre><code>class PrimaryColors(ConstantsIterable):\n    \"\"\"Primary color palette with vibrant, distinct colors.\n\n    Provides a curated set of primary colors suitable for categorical data\n    visualization and general plotting needs.\n    \"\"\"\n    mint = '#00b894'\n    cyan = '#00cec9'\n    blue = '#0984e3'\n    red = '#d63031'\n    pink = '#e84393'\n    green_light = '#badc58'\n    green_bold = '#6ab04c'\n    orange_light = '#fdcb6e'\n    orange_bold = '#e17055'\n    purple_light = '#a29bfe'\n    purple_bold = '#6c5ce7'\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mesqual.utils.plotly_utils.plotly_theme.SequentialColors","title":"SequentialColors","text":"<p>               Bases: <code>ConstantsIterable</code></p> <p>Sequential color palettes for ordered data visualization.</p> <p>Contains multi-hue and single-hue sequential palettes appropriate for displaying ordered data such as numerical ranges or intensity maps.</p> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/plotly_theme.py</code> <pre><code>class SequentialColors(ConstantsIterable):\n    \"\"\"Sequential color palettes for ordered data visualization.\n\n    Contains multi-hue and single-hue sequential palettes appropriate for\n    displaying ordered data such as numerical ranges or intensity maps.\n    \"\"\"\n    mint_blue_red = ['#00b894', '#0984e3', '#d63031']\n    blue_cyan_pink = ['#0984e3', '#00cec9', '#e84393']\n    shades_of_mint = ['#e6fff7', '#55efc4', '#00b894', '#009677', '#006b54']\n    shades_of_cyan = ['#e6ffff', '#8ee8e7', '#00cec9', '#00a29a', '#00756e']\n    shades_of_blue = ['#e6f4ff', '#74b9ff', '#0984e3', '#0063b1', '#004680']\n    shades_of_red = ['#ffe6e6', '#ff7675', '#d63031', '#b02525', '#801b1b']\n    shades_of_pink = ['#ffe6f3', '#fd79a8', '#e84393', '#c13584', '#962264']\n    default = mint_blue_red\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mesqual.utils.plotly_utils.plotly_theme.DivergingColors","title":"DivergingColors","text":"<p>               Bases: <code>ConstantsIterable</code></p> <p>Diverging color palettes for data with meaningful midpoint.</p> <p>Provides color schemes that emphasize deviations from a central value, suitable for correlation matrices, anomaly detection, and comparative analysis.</p> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/plotly_theme.py</code> <pre><code>class DivergingColors(ConstantsIterable):\n    \"\"\"Diverging color palettes for data with meaningful midpoint.\n\n    Provides color schemes that emphasize deviations from a central value,\n    suitable for correlation matrices, anomaly detection, and comparative analysis.\n    \"\"\"\n    blue_mint = SequentialColors.shades_of_blue[::-1] + SequentialColors.shades_of_mint\n    red_mint = SequentialColors.shades_of_red[::-1] + SequentialColors.shades_of_mint\n    pink_blue = SequentialColors.shades_of_pink[::-1] + SequentialColors.shades_of_blue\n    pink_cyan = SequentialColors.shades_of_pink[::-1] + SequentialColors.shades_of_cyan\n    default = blue_mint\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mesqual.utils.plotly_utils.plotly_theme.CyclicalColors","title":"CyclicalColors","text":"<p>               Bases: <code>ConstantsIterable</code></p> <p>Cyclical color palettes for periodic data.</p> <p>Reserved for future implementation of color schemes appropriate for cyclical data such as seasonal patterns or angular measurements.</p> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/plotly_theme.py</code> <pre><code>class CyclicalColors(ConstantsIterable):\n    \"\"\"Cyclical color palettes for periodic data.\n\n    Reserved for future implementation of color schemes appropriate for\n    cyclical data such as seasonal patterns or angular measurements.\n    \"\"\"\n    default = None\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mesqual.utils.plotly_utils.plotly_theme.QualitativeColors","title":"QualitativeColors","text":"<p>               Bases: <code>ConstantsIterable</code></p> <p>Qualitative color palettes for categorical data.</p> <p>Provides distinct, visually separable colors for categorical variables without inherent ordering.</p> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/plotly_theme.py</code> <pre><code>class QualitativeColors(ConstantsIterable):\n    \"\"\"Qualitative color palettes for categorical data.\n\n    Provides distinct, visually separable colors for categorical variables\n    without inherent ordering.\n    \"\"\"\n    default = [\n        PrimaryColors.blue,\n        PrimaryColors.mint,\n        PrimaryColors.cyan,\n        PrimaryColors.red,\n        PrimaryColors.pink,\n        PrimaryColors.green_light,\n        PrimaryColors.green_bold,\n        PrimaryColors.orange_light,\n        PrimaryColors.orange_bold,\n        PrimaryColors.purple_light,\n        PrimaryColors.purple_bold,\n    ]\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mesqual.utils.plotly_utils.plotly_theme.ColorPalette","title":"ColorPalette","text":"<p>Central access point for all color palette categories.</p> <p>Organizes color palettes by type (primary, sequential, diverging, etc.) for easy access and consistent usage across the framework.</p> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/plotly_theme.py</code> <pre><code>class ColorPalette:\n    \"\"\"Central access point for all color palette categories.\n\n    Organizes color palettes by type (primary, sequential, diverging, etc.)\n    for easy access and consistent usage across the framework.\n    \"\"\"\n    primary = PrimaryColors\n    sequential = SequentialColors\n    diverging = DivergingColors\n    cyclical = CyclicalColors\n    qualitative = QualitativeColors\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mesqual.utils.plotly_utils.plotly_theme.PlotlyTheme","title":"PlotlyTheme  <code>dataclass</code>","text":"<p>Configurable Plotly theme for consistent figure styling.</p> <p>This dataclass encapsulates all theme settings and provides methods to apply them to Plotly's global template system. It supports customization of colors, fonts, backgrounds, axis styling, and optional watermarking.</p> <p>Attributes:</p> Name Type Description <code>default_colorway</code> <code>list[str]</code> <p>Default color sequence for traces.</p> <code>font</code> <code>dict</code> <p>Font configuration dictionary.</p> <code>paper_color</code> <code>str</code> <p>Background color outside the plot area.</p> <code>background_color</code> <code>str</code> <p>Background color of the plot area.</p> <code>xaxis</code> <code>dict</code> <p>X-axis styling configuration.</p> <code>yaxis</code> <code>dict</code> <p>Y-axis styling configuration.</p> <code>legend</code> <code>dict</code> <p>Legend styling configuration.</p> <code>watermark_text</code> <code>str</code> <p>Optional watermark text to display.</p> <code>watermark_position</code> <code>tuple[float, float]</code> <p>(x, y) position for watermark (paper coordinates).</p> <code>watermark_opacity</code> <code>float</code> <p>Opacity level for watermark (0.0 to 1.0).</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; theme = PlotlyTheme(\n...     default_colorway=colors.qualitative.default,\n...     watermark_text=\"MESQUAL\"\n... )\n&gt;&gt;&gt; theme.apply()\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/plotly_theme.py</code> <pre><code>@dataclass\nclass PlotlyTheme:\n    \"\"\"Configurable Plotly theme for consistent figure styling.\n\n    This dataclass encapsulates all theme settings and provides methods to\n    apply them to Plotly's global template system. It supports customization\n    of colors, fonts, backgrounds, axis styling, and optional watermarking.\n\n    Attributes:\n        default_colorway: Default color sequence for traces.\n        font: Font configuration dictionary.\n        paper_color: Background color outside the plot area.\n        background_color: Background color of the plot area.\n        xaxis: X-axis styling configuration.\n        yaxis: Y-axis styling configuration.\n        legend: Legend styling configuration.\n        watermark_text: Optional watermark text to display.\n        watermark_position: (x, y) position for watermark (paper coordinates).\n        watermark_opacity: Opacity level for watermark (0.0 to 1.0).\n\n    Example:\n\n        &gt;&gt;&gt; theme = PlotlyTheme(\n        ...     default_colorway=colors.qualitative.default,\n        ...     watermark_text=\"MESQUAL\"\n        ... )\n        &gt;&gt;&gt; theme.apply()\n    \"\"\"\n    default_colorway: list[str] = field(default_factory=list)\n    font: dict = field(default_factory=dict)\n    paper_color: str = '#ffffff'\n    background_color: str = '#F2F2F2'\n    xaxis: dict = field(default_factory=dict)\n    yaxis: dict = field(default_factory=dict)\n    legend: dict = field(default_factory=dict)\n    watermark_text: str = None\n    watermark_position: tuple[float, float] = (0.99, 0.01)\n    watermark_opacity: float = 0.1\n\n    def apply(self) -&gt; None:\n        \"\"\"Apply the theme settings to Plotly's global template system.\n\n        Creates a custom template with all specified settings and sets it as\n        the default template for all subsequent figure creation. The template\n        includes styling for layout, axes, legends, and optionally watermarks.\n\n        Note:\n            This method modifies Plotly's global state and affects all figures\n            created after calling this method.\n\n        Example:\n\n            &gt;&gt;&gt; theme = PlotlyTheme(watermark_text=\"My Project\")\n            &gt;&gt;&gt; theme.apply()\n            &gt;&gt;&gt; fig = go.Figure()  # Will use the custom theme\n        \"\"\"\n        template = go.layout.Template()\n\n        template.layout.colorway = self.default_colorway\n        template.layout.font = self.font\n        template.layout.paper_bgcolor = self.paper_color\n        template.layout.plot_bgcolor = self.background_color\n        template.layout.xaxis.update(self.xaxis)\n        template.layout.yaxis.update(self.yaxis)\n        template.layout.legend.update(self.legend)\n\n        template.layout.title = dict(x=0.5)\n\n        template.data.bar = [\n            go.Bar(marker=dict(line=dict(width=0)))\n        ]\n\n        if self.watermark_text:\n            template.layout.annotations = [\n                dict(\n                    name='watermark',\n                    text=self.watermark_text,\n                    xref=\"paper\",\n                    yref=\"paper\",\n                    x=self.watermark_position[0],\n                    y=self.watermark_position[1],\n                    showarrow=False,\n                    font=dict(size=50, color=\"black\"),\n                    opacity=self.watermark_opacity,\n                    textangle=0,\n                )\n            ]\n\n        pio.templates[\"custom\"] = template\n        pio.templates.default = \"plotly+custom\"\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/plotly_theme/#mesqual.utils.plotly_utils.plotly_theme.PlotlyTheme.apply","title":"apply","text":"<pre><code>apply() -&gt; None\n</code></pre> <p>Apply the theme settings to Plotly's global template system.</p> <p>Creates a custom template with all specified settings and sets it as the default template for all subsequent figure creation. The template includes styling for layout, axes, legends, and optionally watermarks.</p> Note <p>This method modifies Plotly's global state and affects all figures created after calling this method.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; theme = PlotlyTheme(watermark_text=\"My Project\")\n&gt;&gt;&gt; theme.apply()\n&gt;&gt;&gt; fig = go.Figure()  # Will use the custom theme\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/plotly_theme.py</code> <pre><code>def apply(self) -&gt; None:\n    \"\"\"Apply the theme settings to Plotly's global template system.\n\n    Creates a custom template with all specified settings and sets it as\n    the default template for all subsequent figure creation. The template\n    includes styling for layout, axes, legends, and optionally watermarks.\n\n    Note:\n        This method modifies Plotly's global state and affects all figures\n        created after calling this method.\n\n    Example:\n\n        &gt;&gt;&gt; theme = PlotlyTheme(watermark_text=\"My Project\")\n        &gt;&gt;&gt; theme.apply()\n        &gt;&gt;&gt; fig = go.Figure()  # Will use the custom theme\n    \"\"\"\n    template = go.layout.Template()\n\n    template.layout.colorway = self.default_colorway\n    template.layout.font = self.font\n    template.layout.paper_bgcolor = self.paper_color\n    template.layout.plot_bgcolor = self.background_color\n    template.layout.xaxis.update(self.xaxis)\n    template.layout.yaxis.update(self.yaxis)\n    template.layout.legend.update(self.legend)\n\n    template.layout.title = dict(x=0.5)\n\n    template.data.bar = [\n        go.Bar(marker=dict(line=dict(width=0)))\n    ]\n\n    if self.watermark_text:\n        template.layout.annotations = [\n            dict(\n                name='watermark',\n                text=self.watermark_text,\n                xref=\"paper\",\n                yref=\"paper\",\n                x=self.watermark_position[0],\n                y=self.watermark_position[1],\n                showarrow=False,\n                font=dict(size=50, color=\"black\"),\n                opacity=self.watermark_opacity,\n                textangle=0,\n            )\n        ]\n\n    pio.templates[\"custom\"] = template\n    pio.templates.default = \"plotly+custom\"\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/px_category_utils/","title":"MESQUAL Plotly Util <code>px_category_utils</code>","text":""},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/px_category_utils/#mesqual.utils.plotly_utils.px_category_utils","title":"px_category_utils","text":"<p>Utilities for working with categorical data in Plotly Express subplots and faceted figures.</p> <p>This module provides functions to navigate and manipulate subplot structures created by Plotly Express, particularly when dealing with faceted plots and categorical data. It enables precise positioning of additional elements on specific subplot axes.</p>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/px_category_utils/#mesqual.utils.plotly_utils.px_category_utils.get_x_y_axis_for_category","title":"get_x_y_axis_for_category","text":"<pre><code>get_x_y_axis_for_category(fig: Figure, category_args: dict[str, str]) -&gt; tuple[str, str]\n</code></pre> <p>Find the x and y axis names for a subplot matching specific category values.</p> <p>Searches through figure traces to find one whose hovertemplate contains all specified category key-value pairs, then returns the corresponding axis names.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object containing subplot traces.</p> required <code>category_args</code> <code>dict[str, str]</code> <p>Dictionary mapping category names to their values (e.g., {'sex': 'Male', 'smoker': 'Yes'}).</p> required <p>Returns:</p> Type Description <code>str</code> <p>Tuple containing the x-axis and y-axis names (e.g., ('x', 'y') or</p> <code>str</code> <p>('x2', 'y3')).</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If no trace contains all the specified category key-value pairs in its hovertemplate.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; category_args = {'sex': 'Male', 'smoker': 'Yes'}\n&gt;&gt;&gt; x_axis, y_axis = get_x_y_axis_for_category(fig, category_args)\n&gt;&gt;&gt; print(f\"Found axes: {x_axis}, {y_axis}\")\n    Found axes: x2, y3\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/px_category_utils.py</code> <pre><code>def get_x_y_axis_for_category(fig: go.Figure, category_args: dict[str, str]) -&gt; tuple[str, str]:\n    \"\"\"Find the x and y axis names for a subplot matching specific category values.\n\n    Searches through figure traces to find one whose hovertemplate contains all\n    specified category key-value pairs, then returns the corresponding axis names.\n\n    Args:\n        fig: Plotly figure object containing subplot traces.\n        category_args: Dictionary mapping category names to their values (e.g.,\n            {'sex': 'Male', 'smoker': 'Yes'}).\n\n    Returns:\n        Tuple containing the x-axis and y-axis names (e.g., ('x', 'y') or\n        ('x2', 'y3')).\n\n    Raises:\n        KeyError: If no trace contains all the specified category key-value pairs\n            in its hovertemplate.\n\n    Example:\n\n        &gt;&gt;&gt; category_args = {'sex': 'Male', 'smoker': 'Yes'}\n        &gt;&gt;&gt; x_axis, y_axis = get_x_y_axis_for_category(fig, category_args)\n        &gt;&gt;&gt; print(f\"Found axes: {x_axis}, {y_axis}\")\n            Found axes: x2, y3\n    \"\"\"\n    keys = [f'{k}={i}' for k, i in category_args.items()]\n    for trace in fig.data:\n        if all(k in trace.hovertemplate for k in keys):\n            x_axis = trace.xaxis if 'xaxis' in trace else 'x'\n            y_axis = trace.yaxis if 'yaxis' in trace else 'y'\n            return x_axis, y_axis\n    raise KeyError(f'No trace with matching key: value pairs {keys} found in any hovertemplate.')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/px_category_utils/#mesqual.utils.plotly_utils.px_category_utils.get_all_x_axis_names","title":"get_all_x_axis_names","text":"<pre><code>get_all_x_axis_names(fig: Figure) -&gt; list[str]\n</code></pre> <p>Get all x-axis names from a Plotly figure layout.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of x-axis attribute names found in the figure layout (e.g.,</p> <code>list[str]</code> <p>['xaxis', 'xaxis2', 'xaxis3']).</p> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/px_category_utils.py</code> <pre><code>def get_all_x_axis_names(fig: go.Figure) -&gt; list[str]:\n    \"\"\"Get all x-axis names from a Plotly figure layout.\n\n    Args:\n        fig: Plotly figure object.\n\n    Returns:\n        List of x-axis attribute names found in the figure layout (e.g.,\n        ['xaxis', 'xaxis2', 'xaxis3']).\n    \"\"\"\n    return [attr for attr in fig.layout if attr.startswith('xaxis')]\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/px_category_utils/#mesqual.utils.plotly_utils.px_category_utils.get_all_y_axis_names","title":"get_all_y_axis_names","text":"<pre><code>get_all_y_axis_names(fig: Figure) -&gt; list[str]\n</code></pre> <p>Get all y-axis names from a Plotly figure layout.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of y-axis attribute names found in the figure layout (e.g.,</p> <code>list[str]</code> <p>['yaxis', 'yaxis2', 'yaxis3']).</p> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/px_category_utils.py</code> <pre><code>def get_all_y_axis_names(fig: go.Figure) -&gt; list[str]:\n    \"\"\"Get all y-axis names from a Plotly figure layout.\n\n    Args:\n        fig: Plotly figure object.\n\n    Returns:\n        List of y-axis attribute names found in the figure layout (e.g.,\n        ['yaxis', 'yaxis2', 'yaxis3']).\n    \"\"\"\n    return [attr for attr in fig.layout if attr.startswith('yaxis')]\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/px_category_utils/#mesqual.utils.plotly_utils.px_category_utils.get_row_col_for_x_y_axis","title":"get_row_col_for_x_y_axis","text":"<pre><code>get_row_col_for_x_y_axis(fig: Figure, x_axis: str, y_axis: str) -&gt; tuple[int, int]\n</code></pre> <p>Convert axis names to subplot row and column indices.</p> <p>Determines the subplot grid position by analyzing axis domains within the figure layout. This is useful for adding elements to specific subplots.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object containing subplots.</p> required <code>x_axis</code> <code>str</code> <p>X-axis name (e.g., 'x', 'x2', 'xaxis', 'xaxis2').</p> required <code>y_axis</code> <code>str</code> <p>Y-axis name (e.g., 'y', 'y2', 'yaxis', 'yaxis2').</p> required <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Tuple of (row, col) indices for the subplot (1-indexed).</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the specified axis names are not found in the figure layout.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; row, col = get_row_col_for_x_y_axis(fig, 'x2', 'y3')\n&gt;&gt;&gt; print(f\"Subplot at row {row}, column {col}\")\n    Subplot at row 2, column 3\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/px_category_utils.py</code> <pre><code>def get_row_col_for_x_y_axis(fig: go.Figure, x_axis: str, y_axis: str) -&gt; tuple[int, int]:\n    \"\"\"Convert axis names to subplot row and column indices.\n\n    Determines the subplot grid position by analyzing axis domains within the\n    figure layout. This is useful for adding elements to specific subplots.\n\n    Args:\n        fig: Plotly figure object containing subplots.\n        x_axis: X-axis name (e.g., 'x', 'x2', 'xaxis', 'xaxis2').\n        y_axis: Y-axis name (e.g., 'y', 'y2', 'yaxis', 'yaxis2').\n\n    Returns:\n        Tuple of (row, col) indices for the subplot (1-indexed).\n\n    Raises:\n        KeyError: If the specified axis names are not found in the figure layout.\n\n    Example:\n\n        &gt;&gt;&gt; row, col = get_row_col_for_x_y_axis(fig, 'x2', 'y3')\n        &gt;&gt;&gt; print(f\"Subplot at row {row}, column {col}\")\n            Subplot at row 2, column 3\n    \"\"\"\n    all_x_axis_names = get_all_x_axis_names(fig)\n    all_y_axis_names = get_all_y_axis_names(fig)\n\n    x_domains = list(sorted(set([tuple(fig.layout[x].domain) for x in all_x_axis_names])))\n    y_domains = list(sorted(set([tuple(fig.layout[y].domain) for y in all_y_axis_names])))\n\n    if 'axis' not in x_axis:\n        x_axis = f'xaxis{x_axis[1:]}'\n    if 'axis' not in y_axis:\n        y_axis = f'yaxis{y_axis[1:]}'\n\n    if x_axis not in all_x_axis_names:\n        raise KeyError(f'No matching axis found for {x_axis}.')\n    if y_axis not in all_y_axis_names:\n        raise KeyError(f'No matching axis found for {y_axis}.')\n\n    col = x_domains.index(tuple(fig.layout[x_axis].domain)) + 1\n    row = y_domains.index(tuple(fig.layout[y_axis].domain)) + 1\n    return row, col\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/px_category_utils/#mesqual.utils.plotly_utils.px_category_utils.get_subplot_row_and_col_for_category","title":"get_subplot_row_and_col_for_category","text":"<pre><code>get_subplot_row_and_col_for_category(fig: Figure, category_args: dict[str, str]) -&gt; tuple[int, int]\n</code></pre> <p>Get subplot row and column for a specific category combination.</p> <p>Combines axis lookup and position conversion to directly find the subplot location for given category values. This is the main convenience function for adding elements to category-specific subplots.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object containing faceted subplots.</p> required <code>category_args</code> <code>dict[str, str]</code> <p>Dictionary mapping category names to their values.</p> required <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Tuple of (row, col) indices for the subplot (1-indexed).</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If no subplot matches the specified category combination or if axis names cannot be found.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; category_args = {'sex': 'Female', 'smoker': 'No'}\n&gt;&gt;&gt; row, col = get_subplot_row_and_col_for_category(fig, category_args)\n&gt;&gt;&gt; fig.add_trace(trace, row=row, col=col)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/px_category_utils.py</code> <pre><code>def get_subplot_row_and_col_for_category(fig: go.Figure, category_args: dict[str, str]) -&gt; tuple[int, int]:\n    \"\"\"Get subplot row and column for a specific category combination.\n\n    Combines axis lookup and position conversion to directly find the subplot\n    location for given category values. This is the main convenience function\n    for adding elements to category-specific subplots.\n\n    Args:\n        fig: Plotly figure object containing faceted subplots.\n        category_args: Dictionary mapping category names to their values.\n\n    Returns:\n        Tuple of (row, col) indices for the subplot (1-indexed).\n\n    Raises:\n        KeyError: If no subplot matches the specified category combination or\n            if axis names cannot be found.\n\n    Example:\n\n        &gt;&gt;&gt; category_args = {'sex': 'Female', 'smoker': 'No'}\n        &gt;&gt;&gt; row, col = get_subplot_row_and_col_for_category(fig, category_args)\n        &gt;&gt;&gt; fig.add_trace(trace, row=row, col=col)\n    \"\"\"\n    x_axis, y_axis = get_x_y_axis_for_category(fig, category_args)\n    row, col = get_row_col_for_x_y_axis(fig, x_axis, y_axis)\n    return row, col\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/utils/plotly_utils/px_category_utils/#mesqual.utils.plotly_utils.px_category_utils.get_index_for_category_on_axis","title":"get_index_for_category_on_axis","text":"<pre><code>get_index_for_category_on_axis(fig: Figure, axis: str, category_value: str) -&gt; int\n</code></pre> <p>Get the numerical index of a categorical value on a specific axis.</p> <p>Converts a categorical string value to its corresponding numerical position on the specified axis. Useful for precise positioning of annotations or additional traces on categorical axes.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Plotly figure object.</p> required <code>axis</code> <code>str</code> <p>Axis name (e.g., 'x', 'x2', 'y', 'y2', 'xaxis', 'yaxis2').</p> required <code>category_value</code> <code>str</code> <p>String value of the category to find.</p> required <p>Returns:</p> Type Description <code>int</code> <p>1-indexed numerical position of the category on the axis.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If category_value is not a string.</p> <code>KeyError</code> <p>If the axis name is invalid or the category value is not found in the axis categoryarray.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; index = get_index_for_category_on_axis(fig, 'x', 'Dinner')\n&gt;&gt;&gt; print(f\"'Dinner' is at position {index}\")\n    'Dinner' is at position 2\n</code></pre> Source code in <code>submodules/mesqual/mesqual/utils/plotly_utils/px_category_utils.py</code> <pre><code>def get_index_for_category_on_axis(fig: go.Figure, axis: str, category_value: str) -&gt; int:\n    \"\"\"Get the numerical index of a categorical value on a specific axis.\n\n    Converts a categorical string value to its corresponding numerical position\n    on the specified axis. Useful for precise positioning of annotations or\n    additional traces on categorical axes.\n\n    Args:\n        fig: Plotly figure object.\n        axis: Axis name (e.g., 'x', 'x2', 'y', 'y2', 'xaxis', 'yaxis2').\n        category_value: String value of the category to find.\n\n    Returns:\n        1-indexed numerical position of the category on the axis.\n\n    Raises:\n        TypeError: If category_value is not a string.\n        KeyError: If the axis name is invalid or the category value is not\n            found in the axis categoryarray.\n\n    Example:\n\n        &gt;&gt;&gt; index = get_index_for_category_on_axis(fig, 'x', 'Dinner')\n        &gt;&gt;&gt; print(f\"'Dinner' is at position {index}\")\n            'Dinner' is at position 2\n    \"\"\"\n    if not isinstance(category_value, str):\n        raise TypeError(\n            'Method only works with string categories. '\n            'Sure you need this? In case you already have an int / float, just use the value as an index directly.'\n        )\n    if 'axis' not in axis:\n        if axis.startswith('x'):\n            axis = f'xaxis{axis[1:]}'\n        elif axis.startswith('y'):\n            axis = f'yaxis{axis[1:]}'\n        else:\n            raise KeyError(f'Unknown axis {axis}')\n    cat_array = fig.layout[axis].categoryarray\n    if category_value not in cat_array:\n        raise KeyError(f'Unknown category {category_value}. Recognised categories for axis: {cat_array}')\n    index = cat_array.index(category_value) + 1\n    return index\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/","title":"MESQUAL Visualization Modules","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/#mesqual.visualizations","title":"visualizations","text":"<p>MESQUAL Visualizations Package.</p> <p>This package provides comprehensive visualization capabilities for energy systems analysis, including interactive maps, time series dashboards, and data export functionality.</p> <p>The visualizations package supports multiple output formats and interactive components designed specifically for multi-scenario energy modeling analysis and comparison.</p> <p>Modules:</p> Name Description <code>- value_mapping_system</code> <p>Data value mapping and color scaling utilities</p> <code>- folium_viz_system</code> <p>Interactive map visualization and Legend system for Folium maps</p> <p>Classes:</p> Name Description <code>- TimeSeriesDashboardGenerator</code> <p>Creates interactive Plotly time series dashboards</p> <code>- HTMLDashboard</code> <p>Generates comprehensive HTML analysis dashboards</p> <code>- HTMLTable</code> <p>Creates formatted HTML tables for data presentation</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from mesqual.visualizations import HTMLDashboard, TimeSeriesDashboardGenerator\n&gt;&gt;&gt; dashboard = HTMLDashboard()\n&gt;&gt;&gt; ts_gen = TimeSeriesDashboardGenerator()\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/html_dashboard/","title":"MESQUAL HTML Dashboard","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/html_dashboard/#mesqual.visualizations.html_dashboard.HTMLDashboard","title":"HTMLDashboard","text":"<p>A dashboard builder for creating HTML reports with multiple visualizations.</p> <p>This class provides a flexible way to combine Plotly figures, Folium maps, HTML tables, custom HTML content, and section dividers into a single HTML dashboard file. Elements are stored with unique names and can be ordered when saving the final dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The dashboard title. Defaults to 'HTML Dashboard'.</p> <code>None</code> <code>font_family</code> <code>str</code> <p>CSS font family specification for the dashboard. Defaults to \"Arial, sans-serif\".</p> <code>'Arial, sans-serif'</code> <p>Attributes:</p> Name Type Description <code>name</code> <p>The dashboard title.</p> <code>content</code> <code>Dict[str, HTMLDashboardElement]</code> <p>Dictionary mapping element names to HTMLDashboardElement objects.</p> <code>font_family</code> <p>The CSS font family specification.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import plotly.express as px\n&gt;&gt;&gt; dashboard = HTMLDashboard(name=\"My Analysis\")\n&gt;&gt;&gt; fig = px.scatter(px.data.iris(), x=\"sepal_width\", y=\"sepal_length\")\n&gt;&gt;&gt; dashboard.add_plotly_figure(fig, name=\"iris_scatter\")\n&gt;&gt;&gt; dashboard.save(\"analysis.html\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/html_dashboard.py</code> <pre><code>class HTMLDashboard:\n    \"\"\"A dashboard builder for creating HTML reports with multiple visualizations.\n\n    This class provides a flexible way to combine Plotly figures, Folium maps,\n    HTML tables, custom HTML content, and section dividers into a single HTML\n    dashboard file. Elements are stored with unique names and can be ordered\n    when saving the final dashboard.\n\n    Args:\n        name: The dashboard title. Defaults to 'HTML Dashboard'.\n        font_family: CSS font family specification for the dashboard.\n            Defaults to \"Arial, sans-serif\".\n\n    Attributes:\n        name: The dashboard title.\n        content: Dictionary mapping element names to HTMLDashboardElement objects.\n        font_family: The CSS font family specification.\n\n    Example:\n\n        &gt;&gt;&gt; import plotly.express as px\n        &gt;&gt;&gt; dashboard = HTMLDashboard(name=\"My Analysis\")\n        &gt;&gt;&gt; fig = px.scatter(px.data.iris(), x=\"sepal_width\", y=\"sepal_length\")\n        &gt;&gt;&gt; dashboard.add_plotly_figure(fig, name=\"iris_scatter\")\n        &gt;&gt;&gt; dashboard.save(\"analysis.html\")\n    \"\"\"\n    def __init__(self, name: str = None, font_family: str = \"Arial, sans-serif\"):\n        self.name = name if name else 'HTML Dashboard'\n        self.content: Dict[str, HTMLDashboardElement] = dict()\n        self.font_family = font_family\n\n    def add_plotly_figure(self, fig: go.Figure, height: str = '100%', name: str = None):\n        \"\"\"Add a Plotly figure to the dashboard.\n\n        Args:\n            fig: The Plotly figure to add.\n            height: CSS height specification for the figure. Defaults to '100%'.\n            name: Unique identifier for the figure. If None, auto-generates.\n\n        Example:\n\n            &gt;&gt;&gt; import plotly.express as px\n            &gt;&gt;&gt; fig = px.bar(x=[\"A\", \"B\", \"C\"], y=[1, 3, 2])\n            &gt;&gt;&gt; dashboard.add_plotly_figure(fig, height=\"400px\", name=\"my_bar_chart\")\n        \"\"\"\n        element = HTMLDashboardElement(fig, height, name)\n        self.content[element.name] = element\n\n    def add_html(self, html_string: str, name: str = None):\n        \"\"\"Add custom HTML content to the dashboard.\n\n        Args:\n            html_string: The HTML content to add.\n            name: Unique identifier for the HTML content. If None, auto-generates.\n\n        Example:\n\n            &gt;&gt;&gt; html = \"&lt;div&gt;&lt;h2&gt;Custom Section&lt;/h2&gt;&lt;p&gt;Some content here.&lt;/p&gt;&lt;/div&gt;\"\n            &gt;&gt;&gt; dashboard.add_html(html, name=\"custom_section\")\n        \"\"\"\n        element = HTMLDashboardElement(html_string, name=name)\n        self.content[element.name] = element\n\n    def add_folium_map(\n            self,\n            folium_map: 'folium.Map',\n            name: str = None,\n    ):\n        \"\"\"Add a Folium map to the dashboard.\n\n        Args:\n            folium_map: The Folium map object to add.\n            name: Unique identifier for the map. If None, auto-generates\n                as \"folium_map_{index}\".\n\n        Returns:\n            str: The name assigned to the map element.\n\n        Example:\n\n            &gt;&gt;&gt; import folium\n            &gt;&gt;&gt; m = folium.Map(location=[45.5236, -122.6750], zoom_start=13)\n            &gt;&gt;&gt; map_name = dashboard.add_folium_map(m, name=\"portland_map\")\n        \"\"\"\n        map_html = folium_map._repr_html_()\n\n        if name is None:\n            name = f\"folium_map_{len([k for k in self.content.keys() if 'folium_map' in k])}\"\n\n        wrapped_map_html = f'&lt;div&gt;{map_html}&lt;/div&gt;'\n\n        self.add_html(wrapped_map_html, name=name)\n\n        return name\n\n    def add_table(\n            self,\n            table: 'HTMLTable',\n            name: str = None,\n            include_dependencies: bool = True\n    ) -&gt; str:\n        \"\"\"Add an HTML table to the dashboard.\n\n        Args:\n            table: The HTMLTable object to add.\n            name: Unique identifier for the table. If None, derives from table title\n                or uses table_id.\n            include_dependencies: Whether to include CSS/JS dependencies in the\n                table HTML. Defaults to True.\n\n        Returns:\n            str: The name assigned to the table element.\n\n        Raises:\n            ValueError: If the table cannot be converted to HTML.\n\n        Example:\n\n            &gt;&gt;&gt; from mesqual.visualizations.html_table import HTMLTable\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n            &gt;&gt;&gt; table = HTMLTable(df, title=\"Sample Data\")\n            &gt;&gt;&gt; table_name = dashboard.add_table(table, name=\"sample_table\")\n        \"\"\"\n        try:\n            table_html = table.get_html(include_dependencies=include_dependencies)\n\n            if name is None:\n                name = f\"table_{table.title.lower().replace(' ', '_')}\" if table.title else table.table_id\n\n            self.add_html(table_html, name=name)\n            return name\n\n        except Exception as e:\n            raise ValueError(f\"Failed to add table to dashboard: {str(e)}\") from e\n\n    def add_section_divider(\n            self,\n            title: str,\n            subtitle: str = None,\n            name: str = None,\n            background_color: str = \"#f9f9f9\",\n            title_color: str = \"#333\",\n            subtitle_color: str = \"#666\",\n            padding: str = \"20px\",\n            margin: str = \"20px 0\",\n            text_align: str = \"center\",\n            title_font_size: str = \"24px\",\n            subtitle_font_size: str = \"16px\",\n            border_radius: str = \"0px\",\n            border: str = \"none\",\n            **kwargs\n    ) -&gt; str:\n        \"\"\"Add a styled section divider with title and optional subtitle.\n\n        Creates a formatted section header that can be used to organize dashboard\n        content into logical groups. Supports extensive CSS customization through\n        parameters and keyword arguments.\n\n        Args:\n            title: The main section title.\n            subtitle: Optional subtitle text.\n            name: Unique identifier for the divider. If None, derives from title.\n            background_color: CSS background color. Defaults to \"#f9f9f9\".\n            title_color: CSS color for the title text. Defaults to \"#333\".\n            subtitle_color: CSS color for the subtitle text. Defaults to \"#666\".\n            padding: CSS padding specification. Defaults to \"20px\".\n            margin: CSS margin specification. Defaults to \"20px 0\".\n            text_align: CSS text alignment. Defaults to \"center\".\n            title_font_size: CSS font size for title. Defaults to \"24px\".\n            subtitle_font_size: CSS font size for subtitle. Defaults to \"16px\".\n            border_radius: CSS border radius. Defaults to \"0px\".\n            border: CSS border specification. Defaults to \"none\".\n            **kwargs: Additional CSS properties. Underscores in keys are converted\n                to camelCase (e.g., box_shadow becomes boxShadow).\n\n        Returns:\n            str: The name assigned to the section divider element.\n\n        Example:\n\n            &gt;&gt;&gt; divider_name = dashboard.add_section_divider(\n            ...     title=\"Data Analysis Results\",\n            ...     subtitle=\"Generated on 2024-01-01\",\n            ...     background_color=\"#e3f2fd\",\n            ...     border=\"1px solid #2196f3\"\n            ... )\n        \"\"\"\n        base_style = f\"background-color: {background_color}; padding: {padding}; margin: {margin}; text-align: {text_align}; border-radius: {border_radius}; border: {border};\"\n\n        for key, value in kwargs.items():\n            css_key = ''.join(word.capitalize() if i &gt; 0 else word for i, word in enumerate(key.split('_')))\n            base_style += f\" {css_key}: {value};\"\n\n        html = f'&lt;div style=\"{base_style}\"&gt;\\n'\n        html += f'    &lt;h2 style=\"color: {title_color}; font-size: {title_font_size};\"&gt;{title}&lt;/h2&gt;\\n'\n\n        if subtitle:\n            html += f'    &lt;p style=\"color: {subtitle_color}; font-size: {subtitle_font_size};\"&gt;{subtitle}&lt;/p&gt;\\n'\n\n        html += '&lt;/div&gt;'\n\n        if name is None:\n            name = f\"section_{title.lower().replace(' ', '_')}\"\n\n        self.add_html(html, name=name)\n\n        return name\n\n    def save(self, save_to_path, content_order=None):\n        \"\"\"Save the dashboard as an HTML file.\n\n        Generates a complete HTML document containing all dashboard elements.\n        Automatically handles Plotly.js inclusion (only includes once for efficiency)\n        and creates output directories as needed.\n\n        Args:\n            save_to_path: File path where the HTML dashboard will be saved.\n            content_order: Optional list specifying the order of elements in the\n                dashboard. If None, uses the order elements were added. Must\n                contain only valid element names.\n\n        Raises:\n            KeyError: If content_order contains names not found in the dashboard.\n            TypeError: If an element has an unexpected type (internal error).\n\n        Example:\n\n            &gt;&gt;&gt; dashboard.save(\"my_dashboard.html\")\n            &gt;&gt;&gt; # Custom ordering\n            &gt;&gt;&gt; dashboard.save(\"ordered_dashboard.html\",\n            ...               content_order=[\"intro_section\", \"chart1\", \"table1\"])\n        \"\"\"\n        if not os.path.exists(os.path.dirname(save_to_path)):\n            os.makedirs(os.path.dirname(save_to_path))\n\n        if content_order is None:\n            content_order = list(self.content.keys())\n        else:\n            unrecognized = [k for k in content_order if k not in self.content.keys()]\n            if unrecognized:\n                raise KeyError(f'Unrecognized content names: {unrecognized}. Allowed: {self.content.keys()}')\n\n        content = []\n        plotly_js_included = False\n        for key in content_order:\n            v = self.content[key]\n            if isinstance(v.element, go.Figure):\n                html_text = v.element.to_html(\n                    include_plotlyjs=True if not plotly_js_included else False,\n                    full_html=False,\n                    default_height=v.height\n                )\n                content.append(html_text)\n                plotly_js_included = True\n            elif isinstance(v.element, str):\n                content.append(v.element)\n            else:\n                TypeError(f'Unexpected element type: {type(v.element)}')\n\n        with open(save_to_path, 'w', encoding='utf-8') as dashboard:\n            dashboard.write(\"&lt;html&gt;&lt;head&gt;\\n\")\n            dashboard.write(\"&lt;meta charset='UTF-8'&gt;\\n\")\n            dashboard.write(f\"&lt;title&gt;{self.name}&lt;/title&gt;\\n\")\n            dashboard.write(f\"&lt;style&gt;\\n  body, * {{ font-family: {self.font_family}; }}\\n&lt;/style&gt;\\n\")\n            dashboard.write(\"&lt;/head&gt;&lt;body&gt;\\n\")\n\n            for item in content:\n                dashboard.write(item + \"\\n\")\n\n            dashboard.write(\"&lt;/body&gt;&lt;/html&gt;\\n\")\n\n    def show(self, width: str = \"100%\", height: str = \"600\"):\n        \"\"\"Display the dashboard inline in a Jupyter notebook.\n\n        Creates a temporary HTML file and displays it using an IPython IFrame.\n        This method is designed for use within Jupyter notebooks to provide\n        inline dashboard previews.\n\n        Args:\n            width: CSS width specification for the iframe. Defaults to \"100%\".\n            height: CSS height specification for the iframe. Defaults to \"600\".\n\n        Note:\n            This method requires IPython and is intended for Jupyter notebook use.\n            The temporary file is created in the system temp directory.\n\n        Example:\n\n            &gt;&gt;&gt; # In a Jupyter notebook cell\n            &gt;&gt;&gt; dashboard.show(width=\"100%\", height=\"800px\")\n        \"\"\"\n        import tempfile\n        from pathlib import Path\n        from IPython.display import IFrame, display\n\n        tmp_dir = Path(tempfile.mkdtemp())\n        html_path = tmp_dir / \"dashboard.html\"\n        self.save(html_path)\n        display(IFrame(src=str(html_path.resolve()), width=width, height=height))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/html_dashboard/#mesqual.visualizations.html_dashboard.HTMLDashboard.add_plotly_figure","title":"add_plotly_figure","text":"<pre><code>add_plotly_figure(fig: Figure, height: str = '100%', name: str = None)\n</code></pre> <p>Add a Plotly figure to the dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>The Plotly figure to add.</p> required <code>height</code> <code>str</code> <p>CSS height specification for the figure. Defaults to '100%'.</p> <code>'100%'</code> <code>name</code> <code>str</code> <p>Unique identifier for the figure. If None, auto-generates.</p> <code>None</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; import plotly.express as px\n&gt;&gt;&gt; fig = px.bar(x=[\"A\", \"B\", \"C\"], y=[1, 3, 2])\n&gt;&gt;&gt; dashboard.add_plotly_figure(fig, height=\"400px\", name=\"my_bar_chart\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/html_dashboard.py</code> <pre><code>def add_plotly_figure(self, fig: go.Figure, height: str = '100%', name: str = None):\n    \"\"\"Add a Plotly figure to the dashboard.\n\n    Args:\n        fig: The Plotly figure to add.\n        height: CSS height specification for the figure. Defaults to '100%'.\n        name: Unique identifier for the figure. If None, auto-generates.\n\n    Example:\n\n        &gt;&gt;&gt; import plotly.express as px\n        &gt;&gt;&gt; fig = px.bar(x=[\"A\", \"B\", \"C\"], y=[1, 3, 2])\n        &gt;&gt;&gt; dashboard.add_plotly_figure(fig, height=\"400px\", name=\"my_bar_chart\")\n    \"\"\"\n    element = HTMLDashboardElement(fig, height, name)\n    self.content[element.name] = element\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/html_dashboard/#mesqual.visualizations.html_dashboard.HTMLDashboard.add_html","title":"add_html","text":"<pre><code>add_html(html_string: str, name: str = None)\n</code></pre> <p>Add custom HTML content to the dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>html_string</code> <code>str</code> <p>The HTML content to add.</p> required <code>name</code> <code>str</code> <p>Unique identifier for the HTML content. If None, auto-generates.</p> <code>None</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; html = \"&lt;div&gt;&lt;h2&gt;Custom Section&lt;/h2&gt;&lt;p&gt;Some content here.&lt;/p&gt;&lt;/div&gt;\"\n&gt;&gt;&gt; dashboard.add_html(html, name=\"custom_section\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/html_dashboard.py</code> <pre><code>def add_html(self, html_string: str, name: str = None):\n    \"\"\"Add custom HTML content to the dashboard.\n\n    Args:\n        html_string: The HTML content to add.\n        name: Unique identifier for the HTML content. If None, auto-generates.\n\n    Example:\n\n        &gt;&gt;&gt; html = \"&lt;div&gt;&lt;h2&gt;Custom Section&lt;/h2&gt;&lt;p&gt;Some content here.&lt;/p&gt;&lt;/div&gt;\"\n        &gt;&gt;&gt; dashboard.add_html(html, name=\"custom_section\")\n    \"\"\"\n    element = HTMLDashboardElement(html_string, name=name)\n    self.content[element.name] = element\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/html_dashboard/#mesqual.visualizations.html_dashboard.HTMLDashboard.add_folium_map","title":"add_folium_map","text":"<pre><code>add_folium_map(folium_map: Map, name: str = None)\n</code></pre> <p>Add a Folium map to the dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>folium_map</code> <code>Map</code> <p>The Folium map object to add.</p> required <code>name</code> <code>str</code> <p>Unique identifier for the map. If None, auto-generates as \"folium_map_{index}\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The name assigned to the map element.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import folium\n&gt;&gt;&gt; m = folium.Map(location=[45.5236, -122.6750], zoom_start=13)\n&gt;&gt;&gt; map_name = dashboard.add_folium_map(m, name=\"portland_map\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/html_dashboard.py</code> <pre><code>def add_folium_map(\n        self,\n        folium_map: 'folium.Map',\n        name: str = None,\n):\n    \"\"\"Add a Folium map to the dashboard.\n\n    Args:\n        folium_map: The Folium map object to add.\n        name: Unique identifier for the map. If None, auto-generates\n            as \"folium_map_{index}\".\n\n    Returns:\n        str: The name assigned to the map element.\n\n    Example:\n\n        &gt;&gt;&gt; import folium\n        &gt;&gt;&gt; m = folium.Map(location=[45.5236, -122.6750], zoom_start=13)\n        &gt;&gt;&gt; map_name = dashboard.add_folium_map(m, name=\"portland_map\")\n    \"\"\"\n    map_html = folium_map._repr_html_()\n\n    if name is None:\n        name = f\"folium_map_{len([k for k in self.content.keys() if 'folium_map' in k])}\"\n\n    wrapped_map_html = f'&lt;div&gt;{map_html}&lt;/div&gt;'\n\n    self.add_html(wrapped_map_html, name=name)\n\n    return name\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/html_dashboard/#mesqual.visualizations.html_dashboard.HTMLDashboard.add_table","title":"add_table","text":"<pre><code>add_table(table: HTMLTable, name: str = None, include_dependencies: bool = True) -&gt; str\n</code></pre> <p>Add an HTML table to the dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>HTMLTable</code> <p>The HTMLTable object to add.</p> required <code>name</code> <code>str</code> <p>Unique identifier for the table. If None, derives from table title or uses table_id.</p> <code>None</code> <code>include_dependencies</code> <code>bool</code> <p>Whether to include CSS/JS dependencies in the table HTML. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name assigned to the table element.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the table cannot be converted to HTML.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from mesqual.visualizations.html_table import HTMLTable\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n&gt;&gt;&gt; table = HTMLTable(df, title=\"Sample Data\")\n&gt;&gt;&gt; table_name = dashboard.add_table(table, name=\"sample_table\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/html_dashboard.py</code> <pre><code>def add_table(\n        self,\n        table: 'HTMLTable',\n        name: str = None,\n        include_dependencies: bool = True\n) -&gt; str:\n    \"\"\"Add an HTML table to the dashboard.\n\n    Args:\n        table: The HTMLTable object to add.\n        name: Unique identifier for the table. If None, derives from table title\n            or uses table_id.\n        include_dependencies: Whether to include CSS/JS dependencies in the\n            table HTML. Defaults to True.\n\n    Returns:\n        str: The name assigned to the table element.\n\n    Raises:\n        ValueError: If the table cannot be converted to HTML.\n\n    Example:\n\n        &gt;&gt;&gt; from mesqual.visualizations.html_table import HTMLTable\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n        &gt;&gt;&gt; table = HTMLTable(df, title=\"Sample Data\")\n        &gt;&gt;&gt; table_name = dashboard.add_table(table, name=\"sample_table\")\n    \"\"\"\n    try:\n        table_html = table.get_html(include_dependencies=include_dependencies)\n\n        if name is None:\n            name = f\"table_{table.title.lower().replace(' ', '_')}\" if table.title else table.table_id\n\n        self.add_html(table_html, name=name)\n        return name\n\n    except Exception as e:\n        raise ValueError(f\"Failed to add table to dashboard: {str(e)}\") from e\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/html_dashboard/#mesqual.visualizations.html_dashboard.HTMLDashboard.add_section_divider","title":"add_section_divider","text":"<pre><code>add_section_divider(title: str, subtitle: str = None, name: str = None, background_color: str = '#f9f9f9', title_color: str = '#333', subtitle_color: str = '#666', padding: str = '20px', margin: str = '20px 0', text_align: str = 'center', title_font_size: str = '24px', subtitle_font_size: str = '16px', border_radius: str = '0px', border: str = 'none', **kwargs) -&gt; str\n</code></pre> <p>Add a styled section divider with title and optional subtitle.</p> <p>Creates a formatted section header that can be used to organize dashboard content into logical groups. Supports extensive CSS customization through parameters and keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>The main section title.</p> required <code>subtitle</code> <code>str</code> <p>Optional subtitle text.</p> <code>None</code> <code>name</code> <code>str</code> <p>Unique identifier for the divider. If None, derives from title.</p> <code>None</code> <code>background_color</code> <code>str</code> <p>CSS background color. Defaults to \"#f9f9f9\".</p> <code>'#f9f9f9'</code> <code>title_color</code> <code>str</code> <p>CSS color for the title text. Defaults to \"#333\".</p> <code>'#333'</code> <code>subtitle_color</code> <code>str</code> <p>CSS color for the subtitle text. Defaults to \"#666\".</p> <code>'#666'</code> <code>padding</code> <code>str</code> <p>CSS padding specification. Defaults to \"20px\".</p> <code>'20px'</code> <code>margin</code> <code>str</code> <p>CSS margin specification. Defaults to \"20px 0\".</p> <code>'20px 0'</code> <code>text_align</code> <code>str</code> <p>CSS text alignment. Defaults to \"center\".</p> <code>'center'</code> <code>title_font_size</code> <code>str</code> <p>CSS font size for title. Defaults to \"24px\".</p> <code>'24px'</code> <code>subtitle_font_size</code> <code>str</code> <p>CSS font size for subtitle. Defaults to \"16px\".</p> <code>'16px'</code> <code>border_radius</code> <code>str</code> <p>CSS border radius. Defaults to \"0px\".</p> <code>'0px'</code> <code>border</code> <code>str</code> <p>CSS border specification. Defaults to \"none\".</p> <code>'none'</code> <code>**kwargs</code> <p>Additional CSS properties. Underscores in keys are converted to camelCase (e.g., box_shadow becomes boxShadow).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name assigned to the section divider element.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; divider_name = dashboard.add_section_divider(\n...     title=\"Data Analysis Results\",\n...     subtitle=\"Generated on 2024-01-01\",\n...     background_color=\"#e3f2fd\",\n...     border=\"1px solid #2196f3\"\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/html_dashboard.py</code> <pre><code>def add_section_divider(\n        self,\n        title: str,\n        subtitle: str = None,\n        name: str = None,\n        background_color: str = \"#f9f9f9\",\n        title_color: str = \"#333\",\n        subtitle_color: str = \"#666\",\n        padding: str = \"20px\",\n        margin: str = \"20px 0\",\n        text_align: str = \"center\",\n        title_font_size: str = \"24px\",\n        subtitle_font_size: str = \"16px\",\n        border_radius: str = \"0px\",\n        border: str = \"none\",\n        **kwargs\n) -&gt; str:\n    \"\"\"Add a styled section divider with title and optional subtitle.\n\n    Creates a formatted section header that can be used to organize dashboard\n    content into logical groups. Supports extensive CSS customization through\n    parameters and keyword arguments.\n\n    Args:\n        title: The main section title.\n        subtitle: Optional subtitle text.\n        name: Unique identifier for the divider. If None, derives from title.\n        background_color: CSS background color. Defaults to \"#f9f9f9\".\n        title_color: CSS color for the title text. Defaults to \"#333\".\n        subtitle_color: CSS color for the subtitle text. Defaults to \"#666\".\n        padding: CSS padding specification. Defaults to \"20px\".\n        margin: CSS margin specification. Defaults to \"20px 0\".\n        text_align: CSS text alignment. Defaults to \"center\".\n        title_font_size: CSS font size for title. Defaults to \"24px\".\n        subtitle_font_size: CSS font size for subtitle. Defaults to \"16px\".\n        border_radius: CSS border radius. Defaults to \"0px\".\n        border: CSS border specification. Defaults to \"none\".\n        **kwargs: Additional CSS properties. Underscores in keys are converted\n            to camelCase (e.g., box_shadow becomes boxShadow).\n\n    Returns:\n        str: The name assigned to the section divider element.\n\n    Example:\n\n        &gt;&gt;&gt; divider_name = dashboard.add_section_divider(\n        ...     title=\"Data Analysis Results\",\n        ...     subtitle=\"Generated on 2024-01-01\",\n        ...     background_color=\"#e3f2fd\",\n        ...     border=\"1px solid #2196f3\"\n        ... )\n    \"\"\"\n    base_style = f\"background-color: {background_color}; padding: {padding}; margin: {margin}; text-align: {text_align}; border-radius: {border_radius}; border: {border};\"\n\n    for key, value in kwargs.items():\n        css_key = ''.join(word.capitalize() if i &gt; 0 else word for i, word in enumerate(key.split('_')))\n        base_style += f\" {css_key}: {value};\"\n\n    html = f'&lt;div style=\"{base_style}\"&gt;\\n'\n    html += f'    &lt;h2 style=\"color: {title_color}; font-size: {title_font_size};\"&gt;{title}&lt;/h2&gt;\\n'\n\n    if subtitle:\n        html += f'    &lt;p style=\"color: {subtitle_color}; font-size: {subtitle_font_size};\"&gt;{subtitle}&lt;/p&gt;\\n'\n\n    html += '&lt;/div&gt;'\n\n    if name is None:\n        name = f\"section_{title.lower().replace(' ', '_')}\"\n\n    self.add_html(html, name=name)\n\n    return name\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/html_dashboard/#mesqual.visualizations.html_dashboard.HTMLDashboard.save","title":"save","text":"<pre><code>save(save_to_path, content_order=None)\n</code></pre> <p>Save the dashboard as an HTML file.</p> <p>Generates a complete HTML document containing all dashboard elements. Automatically handles Plotly.js inclusion (only includes once for efficiency) and creates output directories as needed.</p> <p>Parameters:</p> Name Type Description Default <code>save_to_path</code> <p>File path where the HTML dashboard will be saved.</p> required <code>content_order</code> <p>Optional list specifying the order of elements in the dashboard. If None, uses the order elements were added. Must contain only valid element names.</p> <code>None</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If content_order contains names not found in the dashboard.</p> <code>TypeError</code> <p>If an element has an unexpected type (internal error).</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; dashboard.save(\"my_dashboard.html\")\n&gt;&gt;&gt; # Custom ordering\n&gt;&gt;&gt; dashboard.save(\"ordered_dashboard.html\",\n...               content_order=[\"intro_section\", \"chart1\", \"table1\"])\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/html_dashboard.py</code> <pre><code>def save(self, save_to_path, content_order=None):\n    \"\"\"Save the dashboard as an HTML file.\n\n    Generates a complete HTML document containing all dashboard elements.\n    Automatically handles Plotly.js inclusion (only includes once for efficiency)\n    and creates output directories as needed.\n\n    Args:\n        save_to_path: File path where the HTML dashboard will be saved.\n        content_order: Optional list specifying the order of elements in the\n            dashboard. If None, uses the order elements were added. Must\n            contain only valid element names.\n\n    Raises:\n        KeyError: If content_order contains names not found in the dashboard.\n        TypeError: If an element has an unexpected type (internal error).\n\n    Example:\n\n        &gt;&gt;&gt; dashboard.save(\"my_dashboard.html\")\n        &gt;&gt;&gt; # Custom ordering\n        &gt;&gt;&gt; dashboard.save(\"ordered_dashboard.html\",\n        ...               content_order=[\"intro_section\", \"chart1\", \"table1\"])\n    \"\"\"\n    if not os.path.exists(os.path.dirname(save_to_path)):\n        os.makedirs(os.path.dirname(save_to_path))\n\n    if content_order is None:\n        content_order = list(self.content.keys())\n    else:\n        unrecognized = [k for k in content_order if k not in self.content.keys()]\n        if unrecognized:\n            raise KeyError(f'Unrecognized content names: {unrecognized}. Allowed: {self.content.keys()}')\n\n    content = []\n    plotly_js_included = False\n    for key in content_order:\n        v = self.content[key]\n        if isinstance(v.element, go.Figure):\n            html_text = v.element.to_html(\n                include_plotlyjs=True if not plotly_js_included else False,\n                full_html=False,\n                default_height=v.height\n            )\n            content.append(html_text)\n            plotly_js_included = True\n        elif isinstance(v.element, str):\n            content.append(v.element)\n        else:\n            TypeError(f'Unexpected element type: {type(v.element)}')\n\n    with open(save_to_path, 'w', encoding='utf-8') as dashboard:\n        dashboard.write(\"&lt;html&gt;&lt;head&gt;\\n\")\n        dashboard.write(\"&lt;meta charset='UTF-8'&gt;\\n\")\n        dashboard.write(f\"&lt;title&gt;{self.name}&lt;/title&gt;\\n\")\n        dashboard.write(f\"&lt;style&gt;\\n  body, * {{ font-family: {self.font_family}; }}\\n&lt;/style&gt;\\n\")\n        dashboard.write(\"&lt;/head&gt;&lt;body&gt;\\n\")\n\n        for item in content:\n            dashboard.write(item + \"\\n\")\n\n        dashboard.write(\"&lt;/body&gt;&lt;/html&gt;\\n\")\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/html_dashboard/#mesqual.visualizations.html_dashboard.HTMLDashboard.show","title":"show","text":"<pre><code>show(width: str = '100%', height: str = '600')\n</code></pre> <p>Display the dashboard inline in a Jupyter notebook.</p> <p>Creates a temporary HTML file and displays it using an IPython IFrame. This method is designed for use within Jupyter notebooks to provide inline dashboard previews.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>str</code> <p>CSS width specification for the iframe. Defaults to \"100%\".</p> <code>'100%'</code> <code>height</code> <code>str</code> <p>CSS height specification for the iframe. Defaults to \"600\".</p> <code>'600'</code> Note <p>This method requires IPython and is intended for Jupyter notebook use. The temporary file is created in the system temp directory.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # In a Jupyter notebook cell\n&gt;&gt;&gt; dashboard.show(width=\"100%\", height=\"800px\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/html_dashboard.py</code> <pre><code>def show(self, width: str = \"100%\", height: str = \"600\"):\n    \"\"\"Display the dashboard inline in a Jupyter notebook.\n\n    Creates a temporary HTML file and displays it using an IPython IFrame.\n    This method is designed for use within Jupyter notebooks to provide\n    inline dashboard previews.\n\n    Args:\n        width: CSS width specification for the iframe. Defaults to \"100%\".\n        height: CSS height specification for the iframe. Defaults to \"600\".\n\n    Note:\n        This method requires IPython and is intended for Jupyter notebook use.\n        The temporary file is created in the system temp directory.\n\n    Example:\n\n        &gt;&gt;&gt; # In a Jupyter notebook cell\n        &gt;&gt;&gt; dashboard.show(width=\"100%\", height=\"800px\")\n    \"\"\"\n    import tempfile\n    from pathlib import Path\n    from IPython.display import IFrame, display\n\n    tmp_dir = Path(tempfile.mkdtemp())\n    html_path = tmp_dir / \"dashboard.html\"\n    self.save(html_path)\n    display(IFrame(src=str(html_path.resolve()), width=width, height=height))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/html_table/","title":"MESQUAL HTML Table","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/html_table/#mesqual.visualizations.html_table.HTMLTable","title":"HTMLTable","text":"<p>A class to create interactive Tabulator tables from pandas DataFrames.</p> <p>Tabulator (http://tabulator.info/) is a feature-rich interactive table library that provides sorting, filtering, formatting, and editing capabilities.</p> <p>This class automatically detects column types and applies appropriate formatters, sorters, and filters. It supports customization through column configuration and provides methods to display tables in Jupyter notebooks or save as HTML files.</p> <p>Attributes:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame to display.</p> <code>title</code> <code>Optional[str]</code> <p>Optional title for the table.</p> <code>height</code> <code>str</code> <p>Height of the table container.</p> <code>layout</code> <code>str</code> <p>Tabulator layout mode.</p> <code>theme</code> <code>str</code> <p>Visual theme for the table.</p> <code>pagination</code> <code>Union[str, bool]</code> <p>Pagination settings.</p> <code>page_size</code> <code>int</code> <p>Number of rows per page when pagination is enabled.</p> <code>movable_columns</code> <code>bool</code> <p>Whether columns can be moved.</p> <code>resizable_columns</code> <code>bool</code> <p>Whether columns can be resized.</p> <code>column_config</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>Custom column configurations.</p> <code>responsive</code> <code>bool</code> <p>Whether table should be responsive.</p> <code>selectable</code> <code>bool</code> <p>Whether rows can be selected.</p> <code>table_id</code> <code>Optional[str]</code> <p>Unique identifier for the table.</p> <code>container_style</code> <code>Optional[Dict[str, str]]</code> <p>Custom CSS styles for container.</p> <code>default_float_precision</code> <code>int | None</code> <p>Default decimal places for float columns.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/html_table.py</code> <pre><code>class HTMLTable:\n    \"\"\"A class to create interactive Tabulator tables from pandas DataFrames.\n\n    Tabulator (http://tabulator.info/) is a feature-rich interactive table library\n    that provides sorting, filtering, formatting, and editing capabilities.\n\n    This class automatically detects column types and applies appropriate formatters,\n    sorters, and filters. It supports customization through column configuration\n    and provides methods to display tables in Jupyter notebooks or save as HTML files.\n\n    Attributes:\n        df (pd.DataFrame): The pandas DataFrame to display.\n        title (Optional[str]): Optional title for the table.\n        height (str): Height of the table container.\n        layout (str): Tabulator layout mode.\n        theme (str): Visual theme for the table.\n        pagination (Union[str, bool]): Pagination settings.\n        page_size (int): Number of rows per page when pagination is enabled.\n        movable_columns (bool): Whether columns can be moved.\n        resizable_columns (bool): Whether columns can be resized.\n        column_config (Optional[Dict[str, Dict[str, Any]]]): Custom column configurations.\n        responsive (bool): Whether table should be responsive.\n        selectable (bool): Whether rows can be selected.\n        table_id (Optional[str]): Unique identifier for the table.\n        container_style (Optional[Dict[str, str]]): Custom CSS styles for container.\n        default_float_precision (int | None): Default decimal places for float columns.\n    \"\"\"\n\n    def __init__(\n            self,\n            df: pd.DataFrame,\n            title: Optional[str] = None,\n            height: str = \"400px\",\n            layout: str = \"fitColumns\",  # fitColumns, fitData, fitDataFill\n            theme: str = \"simple\",  # simple, bootstrap, midnight, modern, etc.\n            pagination: Union[str, bool] = \"local\",  # local, remote, or False\n            page_size: int = 10,\n            movable_columns: bool = True,\n            resizable_columns: bool = True,\n            column_config: Optional[Dict[str, Dict[str, Any]]] = None,\n            responsive: bool = True,\n            selectable: bool = False,\n            table_id: Optional[str] = None,\n            container_style: Optional[Dict[str, str]] = None,\n            default_float_precision: int | None = 2,\n    ):\n        \"\"\"Initialize the HTMLTable with configuration options.\n\n        Args:\n            df: The pandas DataFrame to display in the table.\n            title: Optional title to display above the table.\n            height: CSS height value for the table container (default: \"400px\").\n            layout: Tabulator layout mode - \"fitColumns\", \"fitData\", or \"fitDataFill\".\n            theme: Visual theme - \"simple\", \"bootstrap\", \"midnight\", \"modern\", etc.\n            pagination: Pagination mode - \"local\", \"remote\", or False to disable.\n            page_size: Number of rows to display per page when pagination is enabled.\n            movable_columns: Whether users can drag columns to reorder them.\n            resizable_columns: Whether users can resize column widths.\n            column_config: Dictionary mapping column names to Tabulator column definitions\n                for custom formatting, validation, or behavior.\n            responsive: Whether the table should adapt to different screen sizes.\n            selectable: Whether users can select table rows.\n            table_id: Custom HTML ID for the table element. If None, generates unique ID.\n            container_style: CSS styles to apply to the table container as key-value pairs.\n            default_float_precision: Number of decimal places for float columns.\n                Set to None to disable automatic formatting.\n\n        Example:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; df = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Score': [95.5, 87.2]})\n            &gt;&gt;&gt; table = HTMLTable(df, title=\"Student Scores\", height=\"300px\")\n        \"\"\"\n        self.df = df\n        self.title = title\n        self.height = height\n        self.layout = layout\n        self.theme = theme\n        self.pagination = pagination\n        self.page_size = page_size\n        self.movable_columns = movable_columns\n        self.resizable_columns = resizable_columns\n        self.column_config = column_config or {}\n        self.responsive = responsive\n        self.selectable = selectable\n        self.table_id = table_id or f\"tabulator_{str(uuid.uuid4()).replace('-', '')[:8]}\"\n        self.container_style = container_style or {}\n        self.default_float_precision = default_float_precision\n\n    def _get_column_definitions(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Generate column definitions for Tabulator based on DataFrame columns and types.\n\n        Automatically detects column data types and applies appropriate Tabulator\n        configurations including sorters, formatters, and filters. Custom configurations\n        from column_config override automatic detection.\n\n        Returns:\n            List of column definition dictionaries compatible with Tabulator.\n        \"\"\"\n        column_defs = []\n\n        for col in self.df.columns:\n            # Start with default configuration\n            col_def = {\n                \"title\": str(col),\n                \"field\": str(col),\n                \"headerFilter\": \"input\",\n            }\n\n            if col in self.column_config:\n                col_def.update(self.column_config[col])\n            else:\n                sample_data = self.df[col].dropna().iloc[0] if not self.df[col].dropna().empty else None\n\n                if pd.api.types.is_numeric_dtype(self.df[col]):\n                    col_def[\"sorter\"] = \"number\"\n                    col_def[\"headerFilter\"] = \"number\"\n                    col_def[\"hozAlign\"] = \"right\"\n\n                    if pd.api.types.is_float_dtype(self.df[col]) and (self.default_float_precision is not None):\n                        if col not in self.column_config or \"formatter\" not in self.column_config[col]:\n                            col_def[\"formatter\"] = \"money\"\n                            col_def[\"formatterParams\"] = {\n                                \"symbol\": \"\",\n                                \"precision\": str(self.default_float_precision),\n                                \"thousand\": \",\",\n                                \"decimal\": \".\"\n                            }\n                elif pd.api.types.is_datetime64_dtype(self.df[col]):\n                    col_def[\"sorter\"] = \"date\"\n                    col_def[\"headerFilter\"] = \"input\"\n                    col_def[\"formatter\"] = \"datetime\"\n                    col_def[\"formatterParams\"] = {\"outputFormat\": \"YYYY-MM-DD HH:mm:ss\"}\n                elif isinstance(sample_data, bool) or pd.api.types.is_bool_dtype(self.df[col]):\n                    col_def[\"sorter\"] = \"boolean\"\n                    col_def[\"formatter\"] = \"tickCross\"\n                    col_def[\"headerFilter\"] = \"tickCross\"\n                    col_def[\"hozAlign\"] = \"center\"\n                else:\n                    col_def[\"sorter\"] = \"string\"\n\n            column_defs.append(col_def)\n\n        return column_defs\n\n    def _process_dataframe(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Process DataFrame to handle NaN values and convert to JSON records.\n\n        Converts pandas DataFrame to a format suitable for Tabulator by handling\n        NaN values, ensuring numeric columns are properly typed, and converting\n        to JSON-serializable records.\n\n        Returns:\n            List of dictionaries representing table rows.\n        \"\"\"\n        processed_df = self.df.copy()\n\n        for col in processed_df.columns:\n            if pd.api.types.is_numeric_dtype(processed_df[col]):\n                processed_df[col] = processed_df[col].astype(float)\n            processed_df[col] = processed_df[col].where(~pd.isna(processed_df[col]), None)\n\n        records = processed_df.to_dict(orient='records')\n        return records\n\n    def _get_container_style_string(self) -&gt; str:\n        \"\"\"Convert container style dictionary to CSS string.\n\n        Transforms Python-style CSS property names (snake_case) to CSS format\n        (camelCase) and combines with base styling.\n\n        Returns:\n            CSS style string for the table container.\n        \"\"\"\n        base_style = \"padding: 10px; margin: 15px 0;\"\n\n        for key, value in self.container_style.items():\n            css_key = ''.join(word.capitalize() if i &gt; 0 else word\n                              for i, word in enumerate(key.split('_')))\n            base_style += f\" {css_key}: {value};\"\n\n        return base_style\n\n    def get_html(self, include_dependencies: bool = True) -&gt; str:\n        \"\"\"Generate HTML representation of the interactive table.\n\n        Creates the complete HTML markup including JavaScript initialization\n        for the Tabulator table with all configured options.\n\n        Args:\n            include_dependencies: Whether to include Tabulator CSS/JS dependencies\n                in the output. Set to False when embedding in documents that already\n                include these dependencies.\n\n        Returns:\n            Complete HTML string ready for display or embedding.\n\n        Example:\n\n            &gt;&gt;&gt; table = HTMLTable(df, title=\"My Data\")\n            &gt;&gt;&gt; html_output = table.get_html()\n            &gt;&gt;&gt; # Save to file or display in web application\n        \"\"\"\n        processed_data = self._process_dataframe()\n        column_defs = self._get_column_definitions()\n        container_style = self._get_container_style_string()\n\n        html_parts = []\n\n        if include_dependencies:\n            html_parts.append(\n                \"\"\"\n                &lt;link href=\"https://unpkg.com/tabulator-tables@5.4.4/dist/css/tabulator.min.css\" rel=\"stylesheet\"&gt;\n                &lt;script type=\"text/javascript\" src=\"https://unpkg.com/tabulator-tables@5.4.4/dist/js/tabulator.min.js\"&gt;&lt;/script&gt;\n                \"\"\"\n            )\n\n        html_parts.append(f'&lt;div style=\"{container_style}\"&gt;')\n\n        if self.title:\n            html_parts.append(f'&lt;h3 style=\"margin-bottom: 10px;\"&gt;{self.title}&lt;/h3&gt;')\n\n        html_parts.append(f'&lt;div id=\"{self.table_id}\" style=\"height: {self.height};\"&gt;&lt;/div&gt;')\n\n        html_parts.append(\n            f\"\"\"\n            &lt;script type=\"text/javascript\"&gt;\n                document.addEventListener('DOMContentLoaded', function() {{\n                    var tabledata = {json.dumps(processed_data)};\n\n                    var table = new Tabulator(\"#{self.table_id}\", {{\n                        data: tabledata,\n                        columns: {json.dumps(column_defs)},\n                        layout: \"{self.layout}\",\n                        responsiveLayout: {json.dumps(self.responsive)},\n                        movableColumns: {json.dumps(self.movable_columns)},\n                        resizableColumns: {json.dumps(self.resizable_columns)},\n                        selectable: {json.dumps(self.selectable)},\n                    \"\"\"\n        )\n\n        if self.pagination:\n            html_parts.append(f\"\"\"\n            pagination: \"{self.pagination}\",\n            paginationSize: {self.page_size},\n            paginationSizeSelector: [10, 25, 50, 100, true],\n            \"\"\")\n\n        if self.theme:\n            html_parts.append(f'    theme: \"{self.theme}\",')\n\n        html_parts.append(\n            \"\"\"\n                    });\n                });\n            &lt;/script&gt;\n            \"\"\"\n        )\n        html_parts.append(\"&lt;/div&gt;\")\n\n        return \"\\n\".join(html_parts)\n\n    def save_html(self, filepath: str, title: str = \"Tabulator Table\") -&gt; str:\n        \"\"\"Save the table as a standalone HTML file.\n\n        Creates a complete HTML document with embedded Tabulator dependencies\n        and saves it to the specified path. The resulting file can be opened\n        directly in any web browser.\n\n        Args:\n            filepath: Path where to save the HTML file. Parent directories\n                will be created if they don't exist.\n            title: Title for the HTML document head section.\n\n        Returns:\n            The filepath of the saved file (same as input parameter).\n\n        Example:\n\n            &gt;&gt;&gt; table = HTMLTable(df, title=\"Sales Report\")\n            &gt;&gt;&gt; saved_path = table.save_html(\"reports/sales_table.html\")\n            &gt;&gt;&gt; print(f\"Table saved to: {saved_path}\")\n        \"\"\"\n        os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n\n        html = f\"\"\"&lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;meta charset=\"UTF-8\"&gt;\n            &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n            &lt;title&gt;{title}&lt;/title&gt;\n            &lt;link href=\"https://unpkg.com/tabulator-tables@5.4.4/dist/css/tabulator.min.css\" rel=\"stylesheet\"&gt;\n            &lt;script type=\"text/javascript\" src=\"https://unpkg.com/tabulator-tables@5.4.4/dist/js/tabulator.min.js\"&gt;&lt;/script&gt;\n            &lt;style&gt;\n                body {{\n                    font-family: Arial, sans-serif;\n                    margin: 20px;\n                    background-color: #f5f5f5;\n                }}\n                .container {{\n                    background-color: white;\n                    padding: 20px;\n                    border-radius: 5px;\n                    box-shadow: 0 2px 10px rgba(0,0,0,0.05);\n                }}\n            &lt;/style&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;div class=\"container\"&gt;\n                {self.get_html(include_dependencies=False)}\n            &lt;/div&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n        \"\"\"\n\n        with open(filepath, 'w', encoding='utf-8') as f:\n            f.write(html)\n\n        return filepath\n\n    def show(self, width: str = \"100%\", height: str = \"600\"):\n        \"\"\"Display the interactive Tabulator table inline in a Jupyter notebook.\n\n        Creates a temporary HTML file and displays it using an IPython IFrame.\n        This method is designed for use within Jupyter notebook environments.\n\n        Args:\n            width: Width of the display frame (CSS units or percentage).\n            height: Height of the display frame in pixels (as string).\n\n        Note:\n            This method requires IPython to be available and will only work\n            in Jupyter notebook environments.\n\n        Example:\n\n            &gt;&gt;&gt; # In a Jupyter notebook cell:\n            &gt;&gt;&gt; table = HTMLTable(df, title=\"Interactive Data\")\n            &gt;&gt;&gt; table.show(width=\"800px\", height=\"400\")\n        \"\"\"\n        import tempfile\n        from pathlib import Path\n        from IPython.display import IFrame, display\n\n        tmp_dir = Path(tempfile.mkdtemp())\n        html_path = tmp_dir / f\"{self.table_id}.html\"\n        self.save_html(str(html_path))\n        display(IFrame(src=str(html_path.resolve()), width=width, height=height))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/html_table/#mesqual.visualizations.html_table.HTMLTable.__init__","title":"__init__","text":"<pre><code>__init__(df: DataFrame, title: Optional[str] = None, height: str = '400px', layout: str = 'fitColumns', theme: str = 'simple', pagination: Union[str, bool] = 'local', page_size: int = 10, movable_columns: bool = True, resizable_columns: bool = True, column_config: Optional[Dict[str, Dict[str, Any]]] = None, responsive: bool = True, selectable: bool = False, table_id: Optional[str] = None, container_style: Optional[Dict[str, str]] = None, default_float_precision: int | None = 2)\n</code></pre> <p>Initialize the HTMLTable with configuration options.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame to display in the table.</p> required <code>title</code> <code>Optional[str]</code> <p>Optional title to display above the table.</p> <code>None</code> <code>height</code> <code>str</code> <p>CSS height value for the table container (default: \"400px\").</p> <code>'400px'</code> <code>layout</code> <code>str</code> <p>Tabulator layout mode - \"fitColumns\", \"fitData\", or \"fitDataFill\".</p> <code>'fitColumns'</code> <code>theme</code> <code>str</code> <p>Visual theme - \"simple\", \"bootstrap\", \"midnight\", \"modern\", etc.</p> <code>'simple'</code> <code>pagination</code> <code>Union[str, bool]</code> <p>Pagination mode - \"local\", \"remote\", or False to disable.</p> <code>'local'</code> <code>page_size</code> <code>int</code> <p>Number of rows to display per page when pagination is enabled.</p> <code>10</code> <code>movable_columns</code> <code>bool</code> <p>Whether users can drag columns to reorder them.</p> <code>True</code> <code>resizable_columns</code> <code>bool</code> <p>Whether users can resize column widths.</p> <code>True</code> <code>column_config</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>Dictionary mapping column names to Tabulator column definitions for custom formatting, validation, or behavior.</p> <code>None</code> <code>responsive</code> <code>bool</code> <p>Whether the table should adapt to different screen sizes.</p> <code>True</code> <code>selectable</code> <code>bool</code> <p>Whether users can select table rows.</p> <code>False</code> <code>table_id</code> <code>Optional[str]</code> <p>Custom HTML ID for the table element. If None, generates unique ID.</p> <code>None</code> <code>container_style</code> <code>Optional[Dict[str, str]]</code> <p>CSS styles to apply to the table container as key-value pairs.</p> <code>None</code> <code>default_float_precision</code> <code>int | None</code> <p>Number of decimal places for float columns. Set to None to disable automatic formatting.</p> <code>2</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Score': [95.5, 87.2]})\n&gt;&gt;&gt; table = HTMLTable(df, title=\"Student Scores\", height=\"300px\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/html_table.py</code> <pre><code>def __init__(\n        self,\n        df: pd.DataFrame,\n        title: Optional[str] = None,\n        height: str = \"400px\",\n        layout: str = \"fitColumns\",  # fitColumns, fitData, fitDataFill\n        theme: str = \"simple\",  # simple, bootstrap, midnight, modern, etc.\n        pagination: Union[str, bool] = \"local\",  # local, remote, or False\n        page_size: int = 10,\n        movable_columns: bool = True,\n        resizable_columns: bool = True,\n        column_config: Optional[Dict[str, Dict[str, Any]]] = None,\n        responsive: bool = True,\n        selectable: bool = False,\n        table_id: Optional[str] = None,\n        container_style: Optional[Dict[str, str]] = None,\n        default_float_precision: int | None = 2,\n):\n    \"\"\"Initialize the HTMLTable with configuration options.\n\n    Args:\n        df: The pandas DataFrame to display in the table.\n        title: Optional title to display above the table.\n        height: CSS height value for the table container (default: \"400px\").\n        layout: Tabulator layout mode - \"fitColumns\", \"fitData\", or \"fitDataFill\".\n        theme: Visual theme - \"simple\", \"bootstrap\", \"midnight\", \"modern\", etc.\n        pagination: Pagination mode - \"local\", \"remote\", or False to disable.\n        page_size: Number of rows to display per page when pagination is enabled.\n        movable_columns: Whether users can drag columns to reorder them.\n        resizable_columns: Whether users can resize column widths.\n        column_config: Dictionary mapping column names to Tabulator column definitions\n            for custom formatting, validation, or behavior.\n        responsive: Whether the table should adapt to different screen sizes.\n        selectable: Whether users can select table rows.\n        table_id: Custom HTML ID for the table element. If None, generates unique ID.\n        container_style: CSS styles to apply to the table container as key-value pairs.\n        default_float_precision: Number of decimal places for float columns.\n            Set to None to disable automatic formatting.\n\n    Example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Score': [95.5, 87.2]})\n        &gt;&gt;&gt; table = HTMLTable(df, title=\"Student Scores\", height=\"300px\")\n    \"\"\"\n    self.df = df\n    self.title = title\n    self.height = height\n    self.layout = layout\n    self.theme = theme\n    self.pagination = pagination\n    self.page_size = page_size\n    self.movable_columns = movable_columns\n    self.resizable_columns = resizable_columns\n    self.column_config = column_config or {}\n    self.responsive = responsive\n    self.selectable = selectable\n    self.table_id = table_id or f\"tabulator_{str(uuid.uuid4()).replace('-', '')[:8]}\"\n    self.container_style = container_style or {}\n    self.default_float_precision = default_float_precision\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/html_table/#mesqual.visualizations.html_table.HTMLTable.get_html","title":"get_html","text":"<pre><code>get_html(include_dependencies: bool = True) -&gt; str\n</code></pre> <p>Generate HTML representation of the interactive table.</p> <p>Creates the complete HTML markup including JavaScript initialization for the Tabulator table with all configured options.</p> <p>Parameters:</p> Name Type Description Default <code>include_dependencies</code> <code>bool</code> <p>Whether to include Tabulator CSS/JS dependencies in the output. Set to False when embedding in documents that already include these dependencies.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Complete HTML string ready for display or embedding.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; table = HTMLTable(df, title=\"My Data\")\n&gt;&gt;&gt; html_output = table.get_html()\n&gt;&gt;&gt; # Save to file or display in web application\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/html_table.py</code> <pre><code>def get_html(self, include_dependencies: bool = True) -&gt; str:\n    \"\"\"Generate HTML representation of the interactive table.\n\n    Creates the complete HTML markup including JavaScript initialization\n    for the Tabulator table with all configured options.\n\n    Args:\n        include_dependencies: Whether to include Tabulator CSS/JS dependencies\n            in the output. Set to False when embedding in documents that already\n            include these dependencies.\n\n    Returns:\n        Complete HTML string ready for display or embedding.\n\n    Example:\n\n        &gt;&gt;&gt; table = HTMLTable(df, title=\"My Data\")\n        &gt;&gt;&gt; html_output = table.get_html()\n        &gt;&gt;&gt; # Save to file or display in web application\n    \"\"\"\n    processed_data = self._process_dataframe()\n    column_defs = self._get_column_definitions()\n    container_style = self._get_container_style_string()\n\n    html_parts = []\n\n    if include_dependencies:\n        html_parts.append(\n            \"\"\"\n            &lt;link href=\"https://unpkg.com/tabulator-tables@5.4.4/dist/css/tabulator.min.css\" rel=\"stylesheet\"&gt;\n            &lt;script type=\"text/javascript\" src=\"https://unpkg.com/tabulator-tables@5.4.4/dist/js/tabulator.min.js\"&gt;&lt;/script&gt;\n            \"\"\"\n        )\n\n    html_parts.append(f'&lt;div style=\"{container_style}\"&gt;')\n\n    if self.title:\n        html_parts.append(f'&lt;h3 style=\"margin-bottom: 10px;\"&gt;{self.title}&lt;/h3&gt;')\n\n    html_parts.append(f'&lt;div id=\"{self.table_id}\" style=\"height: {self.height};\"&gt;&lt;/div&gt;')\n\n    html_parts.append(\n        f\"\"\"\n        &lt;script type=\"text/javascript\"&gt;\n            document.addEventListener('DOMContentLoaded', function() {{\n                var tabledata = {json.dumps(processed_data)};\n\n                var table = new Tabulator(\"#{self.table_id}\", {{\n                    data: tabledata,\n                    columns: {json.dumps(column_defs)},\n                    layout: \"{self.layout}\",\n                    responsiveLayout: {json.dumps(self.responsive)},\n                    movableColumns: {json.dumps(self.movable_columns)},\n                    resizableColumns: {json.dumps(self.resizable_columns)},\n                    selectable: {json.dumps(self.selectable)},\n                \"\"\"\n    )\n\n    if self.pagination:\n        html_parts.append(f\"\"\"\n        pagination: \"{self.pagination}\",\n        paginationSize: {self.page_size},\n        paginationSizeSelector: [10, 25, 50, 100, true],\n        \"\"\")\n\n    if self.theme:\n        html_parts.append(f'    theme: \"{self.theme}\",')\n\n    html_parts.append(\n        \"\"\"\n                });\n            });\n        &lt;/script&gt;\n        \"\"\"\n    )\n    html_parts.append(\"&lt;/div&gt;\")\n\n    return \"\\n\".join(html_parts)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/html_table/#mesqual.visualizations.html_table.HTMLTable.save_html","title":"save_html","text":"<pre><code>save_html(filepath: str, title: str = 'Tabulator Table') -&gt; str\n</code></pre> <p>Save the table as a standalone HTML file.</p> <p>Creates a complete HTML document with embedded Tabulator dependencies and saves it to the specified path. The resulting file can be opened directly in any web browser.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path where to save the HTML file. Parent directories will be created if they don't exist.</p> required <code>title</code> <code>str</code> <p>Title for the HTML document head section.</p> <code>'Tabulator Table'</code> <p>Returns:</p> Type Description <code>str</code> <p>The filepath of the saved file (same as input parameter).</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; table = HTMLTable(df, title=\"Sales Report\")\n&gt;&gt;&gt; saved_path = table.save_html(\"reports/sales_table.html\")\n&gt;&gt;&gt; print(f\"Table saved to: {saved_path}\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/html_table.py</code> <pre><code>def save_html(self, filepath: str, title: str = \"Tabulator Table\") -&gt; str:\n    \"\"\"Save the table as a standalone HTML file.\n\n    Creates a complete HTML document with embedded Tabulator dependencies\n    and saves it to the specified path. The resulting file can be opened\n    directly in any web browser.\n\n    Args:\n        filepath: Path where to save the HTML file. Parent directories\n            will be created if they don't exist.\n        title: Title for the HTML document head section.\n\n    Returns:\n        The filepath of the saved file (same as input parameter).\n\n    Example:\n\n        &gt;&gt;&gt; table = HTMLTable(df, title=\"Sales Report\")\n        &gt;&gt;&gt; saved_path = table.save_html(\"reports/sales_table.html\")\n        &gt;&gt;&gt; print(f\"Table saved to: {saved_path}\")\n    \"\"\"\n    os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n\n    html = f\"\"\"&lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n        &lt;meta charset=\"UTF-8\"&gt;\n        &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n        &lt;title&gt;{title}&lt;/title&gt;\n        &lt;link href=\"https://unpkg.com/tabulator-tables@5.4.4/dist/css/tabulator.min.css\" rel=\"stylesheet\"&gt;\n        &lt;script type=\"text/javascript\" src=\"https://unpkg.com/tabulator-tables@5.4.4/dist/js/tabulator.min.js\"&gt;&lt;/script&gt;\n        &lt;style&gt;\n            body {{\n                font-family: Arial, sans-serif;\n                margin: 20px;\n                background-color: #f5f5f5;\n            }}\n            .container {{\n                background-color: white;\n                padding: 20px;\n                border-radius: 5px;\n                box-shadow: 0 2px 10px rgba(0,0,0,0.05);\n            }}\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div class=\"container\"&gt;\n            {self.get_html(include_dependencies=False)}\n        &lt;/div&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n\n    with open(filepath, 'w', encoding='utf-8') as f:\n        f.write(html)\n\n    return filepath\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/html_table/#mesqual.visualizations.html_table.HTMLTable.show","title":"show","text":"<pre><code>show(width: str = '100%', height: str = '600')\n</code></pre> <p>Display the interactive Tabulator table inline in a Jupyter notebook.</p> <p>Creates a temporary HTML file and displays it using an IPython IFrame. This method is designed for use within Jupyter notebook environments.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>str</code> <p>Width of the display frame (CSS units or percentage).</p> <code>'100%'</code> <code>height</code> <code>str</code> <p>Height of the display frame in pixels (as string).</p> <code>'600'</code> Note <p>This method requires IPython to be available and will only work in Jupyter notebook environments.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # In a Jupyter notebook cell:\n&gt;&gt;&gt; table = HTMLTable(df, title=\"Interactive Data\")\n&gt;&gt;&gt; table.show(width=\"800px\", height=\"400\")\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/html_table.py</code> <pre><code>def show(self, width: str = \"100%\", height: str = \"600\"):\n    \"\"\"Display the interactive Tabulator table inline in a Jupyter notebook.\n\n    Creates a temporary HTML file and displays it using an IPython IFrame.\n    This method is designed for use within Jupyter notebook environments.\n\n    Args:\n        width: Width of the display frame (CSS units or percentage).\n        height: Height of the display frame in pixels (as string).\n\n    Note:\n        This method requires IPython to be available and will only work\n        in Jupyter notebook environments.\n\n    Example:\n\n        &gt;&gt;&gt; # In a Jupyter notebook cell:\n        &gt;&gt;&gt; table = HTMLTable(df, title=\"Interactive Data\")\n        &gt;&gt;&gt; table.show(width=\"800px\", height=\"400\")\n    \"\"\"\n    import tempfile\n    from pathlib import Path\n    from IPython.display import IFrame, display\n\n    tmp_dir = Path(tempfile.mkdtemp())\n    html_path = tmp_dir / f\"{self.table_id}.html\"\n    self.save_html(str(html_path))\n    display(IFrame(src=str(html_path.resolve()), width=width, height=height))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/","title":"TimeSeries Dashboard (Plotly Figure)","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/#mesqual.visualizations.plotly_figures.timeseries_dashboard.TimeSeriesDashboardGenerator","title":"TimeSeriesDashboardGenerator","text":"<p>Main class for generating timeseries heatmap dashboards.</p> <p>Creates comprehensive dashboards that visualize timeseries data as heatmaps with hour-of-day on y-axis and time aggregations on x-axis. Supports faceting by data columns (MultiIndex) or analysis parameters, customizable KPI statistics, and flexible color schemes including per-facet colorscales.</p> <p>The dashboard displays heatmaps alongside statistical summaries and supports various time aggregations (daily, weekly, monthly) with configurable grouping functions (mean, sum, min, max, etc.).</p> The input data must be a pandas DataFrame or Series with: <ul> <li>DateTime index (required for time-based aggregations)</li> <li>For faceting: MultiIndex columns with named levels</li> </ul> <p>Examples:</p> <p>Basic usage with single variable:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from datetime import datetime, timedelta\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Create sample timeseries data\n&gt;&gt;&gt; dates = pd.date_range('2023-01-01', periods=8760, freq='H')\n&gt;&gt;&gt; data = pd.Series(np.random.randn(8760), index=dates, name='power')\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Generate basic dashboard\n&gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(x_axis='date')\n&gt;&gt;&gt; fig = generator.get_figure(data)\n&gt;&gt;&gt; fig.show()\n</code></pre> <p>Multi-variable with faceting:</p> <pre><code>&gt;&gt;&gt; # Create multi-column data with proper MultiIndex\n&gt;&gt;&gt; variables = ['solar', 'wind', 'load']\n&gt;&gt;&gt; scenarios = ['base', 'high', 'low']\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Method 1: Using pd.concat to create MultiIndex\n&gt;&gt;&gt; data_dict = {}\n&gt;&gt;&gt; for scenario in scenarios:\n&gt;&gt;&gt;     scenario_data = pd.DataFrame({\n&gt;&gt;&gt;         var: np.random.randn(8760) for var in variables\n&gt;&gt;&gt;     }, index=dates)\n&gt;&gt;&gt;     data_dict[scenario] = scenario_data\n&gt;&gt;&gt; \n&gt;&gt;&gt; data_multi = pd.concat(data_dict, axis=1, names=['scenario', 'variable'])\n&gt;&gt;&gt; print(data_multi)\n    scenario             base  ...   low\n    variable            solar  wind  load  ...  load\n    datetime\n    2023-01-01 00:00:00 -0.95 -1.57  0.89  ...  0.06\n    2023-01-01 01:00:00  1.18  0.88 -0.62  ...  1.18\n    2023-01-01 02:00:00  0.25  0.31  0.12  ...  0.24\n    2023-01-01 03:00:00 -2.02 -0.59 -0.92  ...  0.45\n    2023-01-01 04:00:00  1.13  0.73 -1.04  ... -0.05\n    ...                   ...   ...   ...  ...   ...\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Generate dashboard with row and column facets\n&gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n&gt;&gt;&gt;     x_axis='date',\n&gt;&gt;&gt;     facet_row='variable',      # First level of MultiIndex\n&gt;&gt;&gt;     facet_col='scenario',      # Second level of MultiIndex\n&gt;&gt;&gt;     facet_row_order=['solar', 'wind', 'load'],\n&gt;&gt;&gt;     facet_col_order=['base', 'high', 'low']\n&gt;&gt;&gt; )\n&gt;&gt;&gt; fig = generator.get_figure(data_multi)\n</code></pre> <p>Custom KPI statistics:</p> <pre><code>&gt;&gt;&gt; # Define custom aggregation functions\n&gt;&gt;&gt; custom_stats = {\n&gt;&gt;&gt;     'Peak': lambda x: x.max(),\n&gt;&gt;&gt;     'Valley': lambda x: x.min(),\n&gt;&gt;&gt;     'Peak-Valley': lambda x: x.max() - x.min(),\n&gt;&gt;&gt;     'Above 50%': lambda x: (x &gt; x.quantile(0.5)).sum() / len(x) * 100,\n&gt;&gt;&gt;     'Volatility': lambda x: x.std() / x.mean() * 100\n&gt;&gt;&gt; }\n&gt;&gt;&gt; \n&gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n&gt;&gt;&gt;     x_axis='week',\n&gt;&gt;&gt;     stat_aggs=custom_stats,\n&gt;&gt;&gt;     facet_row='variable'\n&gt;&gt;&gt; )\n&gt;&gt;&gt; fig = generator.get_figure(data_multi)\n</code></pre> <p>Parameter-based faceting (multiple x-axis or aggregations):</p> <pre><code>&gt;&gt;&gt; # Compare different time aggregations\n&gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n&gt;&gt;&gt;     x_axis=['date', 'week', 'month'],    # Multiple x-axis types\n&gt;&gt;&gt;     facet_col='x_axis',                  # Facet by x_axis parameter\n&gt;&gt;&gt;     facet_row='variable'\n&gt;&gt;&gt; )\n&gt;&gt;&gt; fig = generator.get_figure(data_multi)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Compare different aggregation methods\n&gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n&gt;&gt;&gt;     x_axis='month',\n&gt;&gt;&gt;     groupby_aggregation=['min', 'mean', 'max'],  # Multiple agg methods\n&gt;&gt;&gt;     facet_col='groupby_aggregation',             # Facet by aggregation\n&gt;&gt;&gt;     facet_row='variable'\n&gt;&gt;&gt; )\n&gt;&gt;&gt; fig = generator.get_figure(data_multi)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>class TimeSeriesDashboardGenerator:\n    \"\"\"Main class for generating timeseries heatmap dashboards.\n\n    Creates comprehensive dashboards that visualize timeseries data as heatmaps\n    with hour-of-day on y-axis and time aggregations on x-axis. Supports faceting\n    by data columns (MultiIndex) or analysis parameters, customizable KPI statistics, and\n    flexible color schemes including per-facet colorscales.\n\n    The dashboard displays heatmaps alongside statistical summaries and supports\n    various time aggregations (daily, weekly, monthly) with configurable grouping\n    functions (mean, sum, min, max, etc.).\n\n    Expected Data Format: The input data must be a pandas DataFrame or Series with:\n        - DateTime index (required for time-based aggregations)\n        - For faceting: MultiIndex columns with named levels\n\n    Examples:\n        Basic usage with single variable:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from datetime import datetime, timedelta\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Create sample timeseries data\n        &gt;&gt;&gt; dates = pd.date_range('2023-01-01', periods=8760, freq='H')\n        &gt;&gt;&gt; data = pd.Series(np.random.randn(8760), index=dates, name='power')\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Generate basic dashboard\n        &gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(x_axis='date')\n        &gt;&gt;&gt; fig = generator.get_figure(data)\n        &gt;&gt;&gt; fig.show()\n\n        Multi-variable with faceting:\n\n        &gt;&gt;&gt; # Create multi-column data with proper MultiIndex\n        &gt;&gt;&gt; variables = ['solar', 'wind', 'load']\n        &gt;&gt;&gt; scenarios = ['base', 'high', 'low']\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Method 1: Using pd.concat to create MultiIndex\n        &gt;&gt;&gt; data_dict = {}\n        &gt;&gt;&gt; for scenario in scenarios:\n        &gt;&gt;&gt;     scenario_data = pd.DataFrame({\n        &gt;&gt;&gt;         var: np.random.randn(8760) for var in variables\n        &gt;&gt;&gt;     }, index=dates)\n        &gt;&gt;&gt;     data_dict[scenario] = scenario_data\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; data_multi = pd.concat(data_dict, axis=1, names=['scenario', 'variable'])\n        &gt;&gt;&gt; print(data_multi)\n            scenario             base  ...   low\n            variable            solar  wind  load  ...  load\n            datetime\n            2023-01-01 00:00:00 -0.95 -1.57  0.89  ...  0.06\n            2023-01-01 01:00:00  1.18  0.88 -0.62  ...  1.18\n            2023-01-01 02:00:00  0.25  0.31  0.12  ...  0.24\n            2023-01-01 03:00:00 -2.02 -0.59 -0.92  ...  0.45\n            2023-01-01 04:00:00  1.13  0.73 -1.04  ... -0.05\n            ...                   ...   ...   ...  ...   ...\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Generate dashboard with row and column facets\n        &gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n        &gt;&gt;&gt;     x_axis='date',\n        &gt;&gt;&gt;     facet_row='variable',      # First level of MultiIndex\n        &gt;&gt;&gt;     facet_col='scenario',      # Second level of MultiIndex\n        &gt;&gt;&gt;     facet_row_order=['solar', 'wind', 'load'],\n        &gt;&gt;&gt;     facet_col_order=['base', 'high', 'low']\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; fig = generator.get_figure(data_multi)\n\n        Custom KPI statistics:\n\n        &gt;&gt;&gt; # Define custom aggregation functions\n        &gt;&gt;&gt; custom_stats = {\n        &gt;&gt;&gt;     'Peak': lambda x: x.max(),\n        &gt;&gt;&gt;     'Valley': lambda x: x.min(),\n        &gt;&gt;&gt;     'Peak-Valley': lambda x: x.max() - x.min(),\n        &gt;&gt;&gt;     'Above 50%': lambda x: (x &gt; x.quantile(0.5)).sum() / len(x) * 100,\n        &gt;&gt;&gt;     'Volatility': lambda x: x.std() / x.mean() * 100\n        &gt;&gt;&gt; }\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n        &gt;&gt;&gt;     x_axis='week',\n        &gt;&gt;&gt;     stat_aggs=custom_stats,\n        &gt;&gt;&gt;     facet_row='variable'\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; fig = generator.get_figure(data_multi)\n\n        Parameter-based faceting (multiple x-axis or aggregations):\n\n        &gt;&gt;&gt; # Compare different time aggregations\n        &gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n        &gt;&gt;&gt;     x_axis=['date', 'week', 'month'],    # Multiple x-axis types\n        &gt;&gt;&gt;     facet_col='x_axis',                  # Facet by x_axis parameter\n        &gt;&gt;&gt;     facet_row='variable'\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; fig = generator.get_figure(data_multi)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Compare different aggregation methods\n        &gt;&gt;&gt; generator = TimeSeriesDashboardGenerator(\n        &gt;&gt;&gt;     x_axis='month',\n        &gt;&gt;&gt;     groupby_aggregation=['min', 'mean', 'max'],  # Multiple agg methods\n        &gt;&gt;&gt;     facet_col='groupby_aggregation',             # Facet by aggregation\n        &gt;&gt;&gt;     facet_row='variable'\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; fig = generator.get_figure(data_multi)\n    \"\"\"\n    def __init__(\n            self,\n            x_axis: X_AXIS_TYPES = 'date',\n            facet_col: str = None,\n            facet_row: str = None,\n            facet_col_wrap: int = None,\n            facet_col_order: list[str] = None,\n            facet_row_order: list[str] = None,\n            ratio_of_stat_col: float = 0.1,\n            stat_aggs: Dict[str, Callable[[pd.Series], float | int]] = None,\n            groupby_aggregation: GROUPBY_AGG_TYPES = 'mean',\n            title: str = None,\n            color_continuous_scale: str | list[str] | list[tuple[float, str]] = None,\n            color_continuous_midpoint: int | float = None,\n            range_color: tuple[float, float] | list[int | float] = None,\n            per_facet_col_colorscale: bool = False,\n            per_facet_row_colorscale: bool = False,\n            facet_row_color_settings: dict = None,\n            facet_col_color_settings: dict = None,\n            subplots_vertical_spacing: float = None,\n            subplots_horizontal_spacing: float = None,\n            time_series_figure_kwargs: dict = None,\n            stat_figure_kwargs: dict = None,\n            universal_figure_kwargs: dict = None,\n            use_string_for_axis: bool = False,\n            config_cls: type[DashboardConfig] = DashboardConfig,\n            data_processor_cls: type[DataProcessor] = DataProcessor,\n            color_manager_cls: type[ColorManager] = ColorManager,\n            trace_generator_cls: type[TraceGenerator] = TraceGenerator,\n            **figure_kwargs\n    ):\n        \"\"\"Initialize the timeseries dashboard generator.\n\n        Args:\n            x_axis: Time aggregation for x-axis or list for faceting.\n            facet_col: Column faceting specification.\n            facet_row: Row faceting specification.\n            facet_col_wrap: Maximum columns per row in faceted layout.\n            facet_col_order: Custom ordering for column facets.\n            facet_row_order: Custom ordering for row facets.\n            ratio_of_stat_col: Width ratio of statistics column to heatmap.\n            stat_aggs: Custom KPI aggregation functions.\n            groupby_aggregation: Data aggregation method or list for faceting.\n            title: Dashboard title.\n            color_continuous_scale: Plotly colorscale specification.\n            color_continuous_midpoint: Midpoint for diverging colorscales.\n            range_color: Fixed color range for heatmaps.\n            per_facet_col_colorscale: Enable separate colorscales per column facet.\n            per_facet_row_colorscale: Enable separate colorscales per row facet.\n            facet_row_color_settings: Custom color settings per row facet.\n            facet_col_color_settings: Custom color settings per column facet.\n            subplots_vertical_spacing: Vertical spacing between subplots.\n            subplots_horizontal_spacing: Horizontal spacing between subplots.\n            time_series_figure_kwargs: Additional heatmap trace parameters.\n            stat_figure_kwargs: Additional statistics trace parameters.\n            universal_figure_kwargs: Parameters applied to all traces.\n            use_string_for_axis: Convert axis values to strings.\n            config_cls: Configuration class for dependency injection.\n            data_processor_cls: Data processor class for dependency injection.\n            color_manager_cls: Color manager class for dependency injection.\n            trace_generator_cls: Trace generator class for dependency injection.\n            **figure_kwargs: Additional figure-level parameters.\n        \"\"\"\n        self.config = config_cls(\n            x_axis=x_axis,\n            facet_col=facet_col,\n            facet_row=facet_row,\n            facet_col_wrap=facet_col_wrap,\n            facet_col_order=facet_col_order,\n            facet_row_order=facet_row_order,\n            ratio_of_stat_col=ratio_of_stat_col,\n            stat_aggs=stat_aggs,\n            groupby_aggregation=groupby_aggregation,\n            title=title,\n            color_continuous_scale=color_continuous_scale,\n            color_continuous_midpoint=color_continuous_midpoint,\n            range_color=range_color,\n            per_facet_col_colorscale=per_facet_col_colorscale,\n            per_facet_row_colorscale=per_facet_row_colorscale,\n            facet_row_color_settings=facet_row_color_settings,\n            facet_col_color_settings=facet_col_color_settings,\n            subplots_vertical_spacing=subplots_vertical_spacing,\n            subplots_horizontal_spacing=subplots_horizontal_spacing,\n            time_series_figure_kwargs=time_series_figure_kwargs,\n            stat_figure_kwargs=stat_figure_kwargs,\n            universal_figure_kwargs=universal_figure_kwargs,\n            use_string_for_axis=use_string_for_axis,\n            ** figure_kwargs\n        )\n        self.data_processor_cls = data_processor_cls\n        self.color_manager_cls = color_manager_cls\n        self.trace_generator_cls = trace_generator_cls\n\n\n    def get_figure(self, data: pd.DataFrame, **kwargs):\n        \"\"\"Generate a complete dashboard figure from timeseries data.\n\n        Creates a plotly figure containing heatmaps with associated statistics,\n        properly formatted axes, and optional faceting. Applies all configured\n        styling, color schemes, and layout settings.\n\n        Args:\n            data: Input timeseries DataFrame or Series with datetime index.\n            **kwargs: Runtime configuration overrides.\n\n        Returns:\n            Plotly Figure object containing the complete dashboard visualization.\n        \"\"\"\n        original_config = copy.deepcopy(self.config)\n\n        for key, value in kwargs.items():\n            if hasattr(self.config, key):\n                setattr(self.config, key, value)\n\n        if not kwargs.get('_skip_validation', False):\n            self.data_processor_cls.validate_input_data_and_config(data, self.config)\n        data = self.data_processor_cls.prepare_dataframe_for_facet(data.copy(), self.config)\n        data = self.data_processor_cls.ensure_df_has_two_column_levels(data, self.config)\n        self.data_processor_cls.update_facet_config(data, self.config)\n\n        fig = self._create_figure_layout_with_subplots(data)\n\n        self._add_heatmap_and_stat_traces_to_figure(data, fig)\n\n        if self.config.per_facet_col_colorscale:\n            self._add_column_colorscales(data, fig)\n            fig.update_traces(showlegend=False)\n        elif self.config.per_facet_row_colorscale:\n            self._add_row_colorscales(data, fig)\n            fig.update_traces(showlegend=False)\n\n        if self.config.title:\n            fig.update_layout(\n                title=f'&lt;b&gt;{self.config.title}&lt;/b&gt;',\n                title_x=0.5,\n            )\n\n        self.config = original_config\n\n        return fig\n\n    def get_figures_chunked(\n            self,\n            data: pd.DataFrame,\n            max_n_rows_per_figure: int = None,\n            n_figures: int = None,\n            chunk_title_suffix: bool = True,\n            **kwargs\n    ) -&gt; list[go.Figure]:\n        \"\"\"Generate multiple figures by splitting facet rows into chunks.\n\n        Useful for handling large datasets with many row facets by creating\n        multiple smaller figures instead of one large figure.\n\n        Args:\n            data: Input timeseries DataFrame with datetime index.\n            max_n_rows_per_figure: Maximum number of row facets per figure.\n            n_figures: Total number of figures to create (alternative to max_n_rows_per_figure).\n            chunk_title_suffix: Whether to add \"(Part X/Y)\" suffix to titles.\n            **kwargs: Runtime configuration overrides.\n\n        Returns:\n            List of plotly Figure objects, each containing a subset of row facets.\n\n        Raises:\n            ValueError: If both or neither of max_n_rows_per_figure and n_figures are provided.\n        \"\"\"\n        original_config = copy.deepcopy(self.config)\n\n        if sum(x is not None for x in [max_n_rows_per_figure, n_figures]) != 1:\n            raise ValueError(\"Provide exactly one of: max_n_rows_per_figure or n_figures\")\n\n        for key, value in kwargs.items():\n            if hasattr(self.config, key):\n                setattr(self.config, key, value)\n\n        data = self.data_processor_cls.prepare_dataframe_for_facet(data, self.config)\n        data = self.data_processor_cls.ensure_df_has_two_column_levels(data, self.config)\n        self.data_processor_cls.update_facet_config(data, self.config)\n\n        if self.config.facet_row is None:\n            return [self.get_figure(data, **kwargs)]\n\n        total_rows = len(self.config.facet_row_order)\n\n        if max_n_rows_per_figure:\n            n_chunks = math.ceil(total_rows / max_n_rows_per_figure)\n            chunk_size = max_n_rows_per_figure\n        else:\n            n_chunks = n_figures\n            chunk_size = math.ceil(total_rows / n_figures)\n\n        figures = []\n        original_title = self.config.title\n\n        for i in range(n_chunks):\n            start_idx = i * chunk_size\n            end_idx = min(start_idx + chunk_size, total_rows)\n            chunk_rows = self.config.facet_row_order[start_idx:end_idx]\n\n            if not chunk_rows:\n                continue\n\n            chunk_kwargs = kwargs.copy()\n            chunk_kwargs['facet_row_order'] = chunk_rows\n\n            if chunk_title_suffix and original_title:\n                chunk_kwargs['title'] = f\"{original_title} (Part {i + 1}/{n_chunks})\"\n            elif chunk_title_suffix:\n                chunk_kwargs['title'] = f\"Part {i + 1}/{n_chunks}\"\n\n            cols_in_chunk = [\n                c for c, facet_row_category in\n                zip(data.columns, data.columns.get_level_values(self.config.facet_row))\n                if facet_row_category in chunk_rows\n            ]\n            data_chunk = data[cols_in_chunk]\n\n            fig = self.get_figure(data_chunk, **chunk_kwargs, _skip_validation=True)\n            figures.append(fig)\n\n        self.config = original_config\n\n        return figures\n\n    def _create_figure_layout_with_subplots(self, data: pd.DataFrame) -&gt; go.Figure:\n        facet_col_wrap = max([1, self.config.facet_col_wrap])\n        ratio_of_stat_col = self.config.ratio_of_stat_col\n\n        has_colorscale_col = self.config.per_facet_row_colorscale\n        has_colorscale_row = self.config.per_facet_col_colorscale\n\n        num_facet_rows = max([1, len(self.config.facet_row_order)])\n        num_facet_cols = max([1, len(self.config.facet_col_order)])\n\n        num_rows = math.ceil(num_facet_cols / facet_col_wrap) * num_facet_rows\n        num_cols = facet_col_wrap * 2  # Each facet gets a heatmap + stats column\n\n        if has_colorscale_col:\n            num_cols += 1\n        if has_colorscale_row:\n            num_rows += 1\n\n        subplot_titles = self._generate_subplot_titles(has_colorscale_col, has_colorscale_row)\n        column_widths = self._get_column_widths(facet_col_wrap, has_colorscale_col, ratio_of_stat_col)\n        row_heights = self._get_row_heights(has_colorscale_row, num_rows)\n        specs = [[{} for _ in range(num_cols)] for _ in range(num_rows)]\n\n        fig = make_subplots(\n            rows=num_rows,\n            cols=num_cols,\n            subplot_titles=subplot_titles,\n            column_widths=column_widths,\n            row_heights=row_heights,\n            specs=specs,\n            vertical_spacing=self.config.subplots_vertical_spacing,\n            horizontal_spacing=self.config.subplots_horizontal_spacing,\n        )\n        fig.update_layout(\n            plot_bgcolor='rgba(0, 0, 0, 0)',\n            margin=dict(t=50, b=50)\n        )\n\n        return fig\n\n    def _get_row_heights(self, has_colorscale_row: bool, num_rows: int) -&gt; list[float]:\n        row_heights = None\n        if has_colorscale_row:\n            regular_height = 1.0\n            colorscale_height = 0.15\n\n            total_regular_rows = num_rows - 1\n            total_height = total_regular_rows * regular_height + colorscale_height\n            norm_regular = regular_height / total_height\n            norm_colorscale = colorscale_height / total_height\n\n            row_heights = [norm_regular] * total_regular_rows + [norm_colorscale]\n        return row_heights\n\n    def _get_column_widths(self, facet_col_wrap, has_colorscale_col, ratio_of_stat_col) -&gt; list[float]:\n        if has_colorscale_col:\n            colorscale_width = 0.03\n            adjusted_width = 1 - colorscale_width\n            column_widths = []\n\n            for _ in range(facet_col_wrap):\n                heatmap_width = (adjusted_width - ratio_of_stat_col) / facet_col_wrap\n                stats_width = ratio_of_stat_col / facet_col_wrap\n                column_widths.extend([heatmap_width, stats_width])\n\n            column_widths.append(colorscale_width)\n        else:\n            column_widths = [(1 - ratio_of_stat_col) / facet_col_wrap, ratio_of_stat_col / facet_col_wrap] * facet_col_wrap\n        return column_widths\n\n    def _generate_subplot_titles(self, has_colorscale_col, has_colorscale_row):\n        subplot_titles = []\n        for row_name in self.config.facet_row_order:\n            for col_name in self.config.facet_col_order:\n                if row_name and col_name:\n                    title = f'{row_name} - {col_name}'\n                else:\n                    title = row_name or col_name\n                subplot_titles.append(title)  # Title for heatmap\n                subplot_titles.append(None)  # Title for stats\n\n            if has_colorscale_col:\n                subplot_titles.append(row_name)\n\n        if has_colorscale_row:\n            for col_name in self.config.facet_col_order:\n                subplot_titles.append(col_name)\n                subplot_titles.append(None)\n        return subplot_titles\n\n    def _add_heatmap_and_stat_traces_to_figure(self, data, fig):\n        facet_col_wrap = self.config.facet_col_wrap\n\n        disable_main_colorbars = self.config.per_facet_col_colorscale or self.config.per_facet_row_colorscale\n        if disable_main_colorbars:\n            self.config.time_series_figure_kwargs['showscale'] = False\n\n        global_color_params = {}\n        if not (self.config.per_facet_col_colorscale or self.config.per_facet_row_colorscale):\n            global_color_params = self.color_manager_cls.compute_color_params(data, self.config)\n\n        current_row = 1\n        row_offset = 0\n\n        for row_idx, row_key in enumerate(self.config.facet_row_order):\n            col_offset = 0\n            for col_idx, col_key in enumerate(self.config.facet_col_order):\n                facet_pos = col_idx % facet_col_wrap\n                if facet_pos == 0 and col_idx &gt; 0:\n                    row_offset += 1\n\n                fig_row = current_row + row_offset\n                fig_col = col_offset + facet_pos * 2 + 1  # +1 because plotly indexing starts at 1\n\n                data_col = facet_key = (row_key, col_key)\n                if data_col not in data.columns:\n                    continue\n                series = data[data_col]\n\n                x_axis = self._get_effective_param_for_data_col('x_axis', data_col)\n                groupby_aggregation = self._get_effective_param_for_data_col('groupby_aggregation', data_col)\n\n                self._set_hovertemplates(x_axis)\n\n                grouped_data = self.data_processor_cls.get_grouped_data(series, x_axis, groupby_aggregation)\n\n                color_params = self._get_color_params_for_facet(data, facet_key, global_color_params)\n\n                show_colorbar = False\n                if not disable_main_colorbars:\n                    show_colorbar = (row_idx == 0 and col_idx == 0)\n\n                heatmap_trace = self.trace_generator_cls.get_heatmap_trace(\n                    grouped_data,\n                    self.config.time_series_figure_kwargs,\n                    color_params,\n                    showscale=show_colorbar,\n                    use_string_for_axis=self.config.use_string_for_axis,\n                )\n\n                fig.add_trace(heatmap_trace, row=fig_row, col=fig_col)\n\n                fig.update_yaxes(\n                    tickvals=[time(hour=h, minute=0) for h in [0, 6, 12, 18]] + [max(grouped_data.index)],\n                    ticktext=['0', '6', '12', '18', '24'],\n                    row=fig_row,\n                    col=fig_col,\n                    autorange='reversed',\n                )\n\n                if x_axis == 'year_week':\n                    fig.update_xaxes(dtick=8, row=fig_row, col=fig_col)\n\n                stats_trace = self.trace_generator_cls.get_stats_trace(\n                    series,\n                    self.config.stat_aggs,\n                    self.config.stat_figure_kwargs,\n                    color_params\n                )\n\n                fig.add_trace(stats_trace, row=fig_row, col=fig_col + 1)\n\n                fig.update_xaxes(showgrid=False, row=fig_row, col=fig_col + 1)\n                fig.update_yaxes(showgrid=False, autorange='reversed', row=fig_row, col=fig_col + 1)\n\n            if col_offset == 0:\n                current_row += math.ceil(len(self.config.facet_col_order) / facet_col_wrap)\n\n    def _get_color_params_for_facet(self, data: pd.DataFrame, facet_key: tuple[str, str], global_color_params: dict) -&gt; dict:\n        if self.config.per_facet_col_colorscale or self.config.per_facet_row_colorscale:\n            color_params = self.color_manager_cls.compute_color_params(data, self.config, facet_key)\n        else:\n            color_params = global_color_params\n        return color_params\n\n    def _add_row_colorscales(self, data, fig):\n        colorscale_col = self.config.facet_col_wrap * 2 + 1  # Column after all heatmaps and stats\n\n        for row_idx, row_key in enumerate(self.config.facet_row_order):\n            row_pos = row_idx * math.ceil(len(self.config.facet_col_order) / self.config.facet_col_wrap) + 1\n\n            facet_key = (row_key, self.config.facet_col_order[0])\n            colorscale, z_max, z_min = self._get_color_settings_for_category(data, facet_key)\n            colorscale_trace = self.trace_generator_cls.create_colorscale_trace(\n                z_min, z_max, colorscale, 'v', row_key\n            )\n\n            fig.add_trace(colorscale_trace, row=row_pos, col=colorscale_col)\n            fig.update_xaxes(showticklabels=False, showgrid=False, row=row_pos, col=colorscale_col)\n            fig.update_yaxes(showticklabels=True, showgrid=False, row=row_pos, col=colorscale_col, side='right')\n\n    def _get_color_settings_for_category(self, data, facet_key):\n        color_params = self.color_manager_cls.compute_color_params(data, self.config, facet_key)\n        colorscale = color_params.get('colorscale', 'viridis')\n        z_min = color_params.get('zmin', 0)\n        z_max = color_params.get('zmax', 1)\n        return colorscale, z_max, z_min\n\n    def _add_column_colorscales(self, data, fig):\n        colorscale_row = math.ceil(len(self.config.facet_col_order) / self.config.facet_col_wrap) * len(\n            self.config.facet_row_order) + 1\n\n        for col_idx, col_key in enumerate(self.config.facet_col_order):\n            col_pos = (col_idx % self.config.facet_col_wrap) * 2 + 1\n\n            facet_key = (self.config.facet_row_order[0], col_key)\n\n            colorscale, z_max, z_min = self._get_color_settings_for_category(data, facet_key)\n\n            colorscale_trace = self.trace_generator_cls.create_colorscale_trace(\n                z_min, z_max, colorscale, 'h', col_key\n            )\n\n            fig.add_trace(colorscale_trace, row=colorscale_row, col=col_pos)\n            fig.update_xaxes(showticklabels=True, showgrid=False, row=colorscale_row, col=col_pos)\n            fig.update_yaxes(showticklabels=False, showgrid=False, row=colorscale_row, col=col_pos)\n\n    def _get_effective_param_for_data_col(self, param_name, data_col):\n        param_value = getattr(self.config, param_name)\n        if not isinstance(param_value, list):\n            return param_value\n        else:\n            return list(set(param_value).intersection(list(data_col)))[0]\n\n    def _set_hovertemplates(self, x_axis):\n        ts_kwargs = self.config.time_series_figure_kwargs\n        stat_kwargs = self.config.stat_figure_kwargs\n\n        ts_kwargs['hovertemplate'] = f\"{x_axis}: %{{x}}&lt;br&gt;Hour of day: %{{y}}&lt;br&gt;Value: %{{z}}&lt;extra&gt;&lt;/extra&gt;\"\n        stat_kwargs['hovertemplate'] = f\"aggregation: %{{y}}&lt;br&gt;Value: %{{z}}&lt;extra&gt;&lt;/extra&gt;\"\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/#mesqual.visualizations.plotly_figures.timeseries_dashboard.TimeSeriesDashboardGenerator.__init__","title":"__init__","text":"<pre><code>__init__(x_axis: X_AXIS_TYPES = 'date', facet_col: str = None, facet_row: str = None, facet_col_wrap: int = None, facet_col_order: list[str] = None, facet_row_order: list[str] = None, ratio_of_stat_col: float = 0.1, stat_aggs: Dict[str, Callable[[Series], float | int]] = None, groupby_aggregation: GROUPBY_AGG_TYPES = 'mean', title: str = None, color_continuous_scale: str | list[str] | list[tuple[float, str]] = None, color_continuous_midpoint: int | float = None, range_color: tuple[float, float] | list[int | float] = None, per_facet_col_colorscale: bool = False, per_facet_row_colorscale: bool = False, facet_row_color_settings: dict = None, facet_col_color_settings: dict = None, subplots_vertical_spacing: float = None, subplots_horizontal_spacing: float = None, time_series_figure_kwargs: dict = None, stat_figure_kwargs: dict = None, universal_figure_kwargs: dict = None, use_string_for_axis: bool = False, config_cls: type[DashboardConfig] = DashboardConfig, data_processor_cls: type[DataProcessor] = DataProcessor, color_manager_cls: type[ColorManager] = ColorManager, trace_generator_cls: type[TraceGenerator] = TraceGenerator, **figure_kwargs)\n</code></pre> <p>Initialize the timeseries dashboard generator.</p> <p>Parameters:</p> Name Type Description Default <code>x_axis</code> <code>X_AXIS_TYPES</code> <p>Time aggregation for x-axis or list for faceting.</p> <code>'date'</code> <code>facet_col</code> <code>str</code> <p>Column faceting specification.</p> <code>None</code> <code>facet_row</code> <code>str</code> <p>Row faceting specification.</p> <code>None</code> <code>facet_col_wrap</code> <code>int</code> <p>Maximum columns per row in faceted layout.</p> <code>None</code> <code>facet_col_order</code> <code>list[str]</code> <p>Custom ordering for column facets.</p> <code>None</code> <code>facet_row_order</code> <code>list[str]</code> <p>Custom ordering for row facets.</p> <code>None</code> <code>ratio_of_stat_col</code> <code>float</code> <p>Width ratio of statistics column to heatmap.</p> <code>0.1</code> <code>stat_aggs</code> <code>Dict[str, Callable[[Series], float | int]]</code> <p>Custom KPI aggregation functions.</p> <code>None</code> <code>groupby_aggregation</code> <code>GROUPBY_AGG_TYPES</code> <p>Data aggregation method or list for faceting.</p> <code>'mean'</code> <code>title</code> <code>str</code> <p>Dashboard title.</p> <code>None</code> <code>color_continuous_scale</code> <code>str | list[str] | list[tuple[float, str]]</code> <p>Plotly colorscale specification.</p> <code>None</code> <code>color_continuous_midpoint</code> <code>int | float</code> <p>Midpoint for diverging colorscales.</p> <code>None</code> <code>range_color</code> <code>tuple[float, float] | list[int | float]</code> <p>Fixed color range for heatmaps.</p> <code>None</code> <code>per_facet_col_colorscale</code> <code>bool</code> <p>Enable separate colorscales per column facet.</p> <code>False</code> <code>per_facet_row_colorscale</code> <code>bool</code> <p>Enable separate colorscales per row facet.</p> <code>False</code> <code>facet_row_color_settings</code> <code>dict</code> <p>Custom color settings per row facet.</p> <code>None</code> <code>facet_col_color_settings</code> <code>dict</code> <p>Custom color settings per column facet.</p> <code>None</code> <code>subplots_vertical_spacing</code> <code>float</code> <p>Vertical spacing between subplots.</p> <code>None</code> <code>subplots_horizontal_spacing</code> <code>float</code> <p>Horizontal spacing between subplots.</p> <code>None</code> <code>time_series_figure_kwargs</code> <code>dict</code> <p>Additional heatmap trace parameters.</p> <code>None</code> <code>stat_figure_kwargs</code> <code>dict</code> <p>Additional statistics trace parameters.</p> <code>None</code> <code>universal_figure_kwargs</code> <code>dict</code> <p>Parameters applied to all traces.</p> <code>None</code> <code>use_string_for_axis</code> <code>bool</code> <p>Convert axis values to strings.</p> <code>False</code> <code>config_cls</code> <code>type[DashboardConfig]</code> <p>Configuration class for dependency injection.</p> <code>DashboardConfig</code> <code>data_processor_cls</code> <code>type[DataProcessor]</code> <p>Data processor class for dependency injection.</p> <code>DataProcessor</code> <code>color_manager_cls</code> <code>type[ColorManager]</code> <p>Color manager class for dependency injection.</p> <code>ColorManager</code> <code>trace_generator_cls</code> <code>type[TraceGenerator]</code> <p>Trace generator class for dependency injection.</p> <code>TraceGenerator</code> <code>**figure_kwargs</code> <p>Additional figure-level parameters.</p> <code>{}</code> Source code in <code>submodules/mesqual/mesqual/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>def __init__(\n        self,\n        x_axis: X_AXIS_TYPES = 'date',\n        facet_col: str = None,\n        facet_row: str = None,\n        facet_col_wrap: int = None,\n        facet_col_order: list[str] = None,\n        facet_row_order: list[str] = None,\n        ratio_of_stat_col: float = 0.1,\n        stat_aggs: Dict[str, Callable[[pd.Series], float | int]] = None,\n        groupby_aggregation: GROUPBY_AGG_TYPES = 'mean',\n        title: str = None,\n        color_continuous_scale: str | list[str] | list[tuple[float, str]] = None,\n        color_continuous_midpoint: int | float = None,\n        range_color: tuple[float, float] | list[int | float] = None,\n        per_facet_col_colorscale: bool = False,\n        per_facet_row_colorscale: bool = False,\n        facet_row_color_settings: dict = None,\n        facet_col_color_settings: dict = None,\n        subplots_vertical_spacing: float = None,\n        subplots_horizontal_spacing: float = None,\n        time_series_figure_kwargs: dict = None,\n        stat_figure_kwargs: dict = None,\n        universal_figure_kwargs: dict = None,\n        use_string_for_axis: bool = False,\n        config_cls: type[DashboardConfig] = DashboardConfig,\n        data_processor_cls: type[DataProcessor] = DataProcessor,\n        color_manager_cls: type[ColorManager] = ColorManager,\n        trace_generator_cls: type[TraceGenerator] = TraceGenerator,\n        **figure_kwargs\n):\n    \"\"\"Initialize the timeseries dashboard generator.\n\n    Args:\n        x_axis: Time aggregation for x-axis or list for faceting.\n        facet_col: Column faceting specification.\n        facet_row: Row faceting specification.\n        facet_col_wrap: Maximum columns per row in faceted layout.\n        facet_col_order: Custom ordering for column facets.\n        facet_row_order: Custom ordering for row facets.\n        ratio_of_stat_col: Width ratio of statistics column to heatmap.\n        stat_aggs: Custom KPI aggregation functions.\n        groupby_aggregation: Data aggregation method or list for faceting.\n        title: Dashboard title.\n        color_continuous_scale: Plotly colorscale specification.\n        color_continuous_midpoint: Midpoint for diverging colorscales.\n        range_color: Fixed color range for heatmaps.\n        per_facet_col_colorscale: Enable separate colorscales per column facet.\n        per_facet_row_colorscale: Enable separate colorscales per row facet.\n        facet_row_color_settings: Custom color settings per row facet.\n        facet_col_color_settings: Custom color settings per column facet.\n        subplots_vertical_spacing: Vertical spacing between subplots.\n        subplots_horizontal_spacing: Horizontal spacing between subplots.\n        time_series_figure_kwargs: Additional heatmap trace parameters.\n        stat_figure_kwargs: Additional statistics trace parameters.\n        universal_figure_kwargs: Parameters applied to all traces.\n        use_string_for_axis: Convert axis values to strings.\n        config_cls: Configuration class for dependency injection.\n        data_processor_cls: Data processor class for dependency injection.\n        color_manager_cls: Color manager class for dependency injection.\n        trace_generator_cls: Trace generator class for dependency injection.\n        **figure_kwargs: Additional figure-level parameters.\n    \"\"\"\n    self.config = config_cls(\n        x_axis=x_axis,\n        facet_col=facet_col,\n        facet_row=facet_row,\n        facet_col_wrap=facet_col_wrap,\n        facet_col_order=facet_col_order,\n        facet_row_order=facet_row_order,\n        ratio_of_stat_col=ratio_of_stat_col,\n        stat_aggs=stat_aggs,\n        groupby_aggregation=groupby_aggregation,\n        title=title,\n        color_continuous_scale=color_continuous_scale,\n        color_continuous_midpoint=color_continuous_midpoint,\n        range_color=range_color,\n        per_facet_col_colorscale=per_facet_col_colorscale,\n        per_facet_row_colorscale=per_facet_row_colorscale,\n        facet_row_color_settings=facet_row_color_settings,\n        facet_col_color_settings=facet_col_color_settings,\n        subplots_vertical_spacing=subplots_vertical_spacing,\n        subplots_horizontal_spacing=subplots_horizontal_spacing,\n        time_series_figure_kwargs=time_series_figure_kwargs,\n        stat_figure_kwargs=stat_figure_kwargs,\n        universal_figure_kwargs=universal_figure_kwargs,\n        use_string_for_axis=use_string_for_axis,\n        ** figure_kwargs\n    )\n    self.data_processor_cls = data_processor_cls\n    self.color_manager_cls = color_manager_cls\n    self.trace_generator_cls = trace_generator_cls\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/#mesqual.visualizations.plotly_figures.timeseries_dashboard.TimeSeriesDashboardGenerator.get_figure","title":"get_figure","text":"<pre><code>get_figure(data: DataFrame, **kwargs)\n</code></pre> <p>Generate a complete dashboard figure from timeseries data.</p> <p>Creates a plotly figure containing heatmaps with associated statistics, properly formatted axes, and optional faceting. Applies all configured styling, color schemes, and layout settings.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input timeseries DataFrame or Series with datetime index.</p> required <code>**kwargs</code> <p>Runtime configuration overrides.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Plotly Figure object containing the complete dashboard visualization.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>def get_figure(self, data: pd.DataFrame, **kwargs):\n    \"\"\"Generate a complete dashboard figure from timeseries data.\n\n    Creates a plotly figure containing heatmaps with associated statistics,\n    properly formatted axes, and optional faceting. Applies all configured\n    styling, color schemes, and layout settings.\n\n    Args:\n        data: Input timeseries DataFrame or Series with datetime index.\n        **kwargs: Runtime configuration overrides.\n\n    Returns:\n        Plotly Figure object containing the complete dashboard visualization.\n    \"\"\"\n    original_config = copy.deepcopy(self.config)\n\n    for key, value in kwargs.items():\n        if hasattr(self.config, key):\n            setattr(self.config, key, value)\n\n    if not kwargs.get('_skip_validation', False):\n        self.data_processor_cls.validate_input_data_and_config(data, self.config)\n    data = self.data_processor_cls.prepare_dataframe_for_facet(data.copy(), self.config)\n    data = self.data_processor_cls.ensure_df_has_two_column_levels(data, self.config)\n    self.data_processor_cls.update_facet_config(data, self.config)\n\n    fig = self._create_figure_layout_with_subplots(data)\n\n    self._add_heatmap_and_stat_traces_to_figure(data, fig)\n\n    if self.config.per_facet_col_colorscale:\n        self._add_column_colorscales(data, fig)\n        fig.update_traces(showlegend=False)\n    elif self.config.per_facet_row_colorscale:\n        self._add_row_colorscales(data, fig)\n        fig.update_traces(showlegend=False)\n\n    if self.config.title:\n        fig.update_layout(\n            title=f'&lt;b&gt;{self.config.title}&lt;/b&gt;',\n            title_x=0.5,\n        )\n\n    self.config = original_config\n\n    return fig\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/#mesqual.visualizations.plotly_figures.timeseries_dashboard.TimeSeriesDashboardGenerator.get_figures_chunked","title":"get_figures_chunked","text":"<pre><code>get_figures_chunked(data: DataFrame, max_n_rows_per_figure: int = None, n_figures: int = None, chunk_title_suffix: bool = True, **kwargs) -&gt; list[Figure]\n</code></pre> <p>Generate multiple figures by splitting facet rows into chunks.</p> <p>Useful for handling large datasets with many row facets by creating multiple smaller figures instead of one large figure.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input timeseries DataFrame with datetime index.</p> required <code>max_n_rows_per_figure</code> <code>int</code> <p>Maximum number of row facets per figure.</p> <code>None</code> <code>n_figures</code> <code>int</code> <p>Total number of figures to create (alternative to max_n_rows_per_figure).</p> <code>None</code> <code>chunk_title_suffix</code> <code>bool</code> <p>Whether to add \"(Part X/Y)\" suffix to titles.</p> <code>True</code> <code>**kwargs</code> <p>Runtime configuration overrides.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Figure]</code> <p>List of plotly Figure objects, each containing a subset of row facets.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both or neither of max_n_rows_per_figure and n_figures are provided.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>def get_figures_chunked(\n        self,\n        data: pd.DataFrame,\n        max_n_rows_per_figure: int = None,\n        n_figures: int = None,\n        chunk_title_suffix: bool = True,\n        **kwargs\n) -&gt; list[go.Figure]:\n    \"\"\"Generate multiple figures by splitting facet rows into chunks.\n\n    Useful for handling large datasets with many row facets by creating\n    multiple smaller figures instead of one large figure.\n\n    Args:\n        data: Input timeseries DataFrame with datetime index.\n        max_n_rows_per_figure: Maximum number of row facets per figure.\n        n_figures: Total number of figures to create (alternative to max_n_rows_per_figure).\n        chunk_title_suffix: Whether to add \"(Part X/Y)\" suffix to titles.\n        **kwargs: Runtime configuration overrides.\n\n    Returns:\n        List of plotly Figure objects, each containing a subset of row facets.\n\n    Raises:\n        ValueError: If both or neither of max_n_rows_per_figure and n_figures are provided.\n    \"\"\"\n    original_config = copy.deepcopy(self.config)\n\n    if sum(x is not None for x in [max_n_rows_per_figure, n_figures]) != 1:\n        raise ValueError(\"Provide exactly one of: max_n_rows_per_figure or n_figures\")\n\n    for key, value in kwargs.items():\n        if hasattr(self.config, key):\n            setattr(self.config, key, value)\n\n    data = self.data_processor_cls.prepare_dataframe_for_facet(data, self.config)\n    data = self.data_processor_cls.ensure_df_has_two_column_levels(data, self.config)\n    self.data_processor_cls.update_facet_config(data, self.config)\n\n    if self.config.facet_row is None:\n        return [self.get_figure(data, **kwargs)]\n\n    total_rows = len(self.config.facet_row_order)\n\n    if max_n_rows_per_figure:\n        n_chunks = math.ceil(total_rows / max_n_rows_per_figure)\n        chunk_size = max_n_rows_per_figure\n    else:\n        n_chunks = n_figures\n        chunk_size = math.ceil(total_rows / n_figures)\n\n    figures = []\n    original_title = self.config.title\n\n    for i in range(n_chunks):\n        start_idx = i * chunk_size\n        end_idx = min(start_idx + chunk_size, total_rows)\n        chunk_rows = self.config.facet_row_order[start_idx:end_idx]\n\n        if not chunk_rows:\n            continue\n\n        chunk_kwargs = kwargs.copy()\n        chunk_kwargs['facet_row_order'] = chunk_rows\n\n        if chunk_title_suffix and original_title:\n            chunk_kwargs['title'] = f\"{original_title} (Part {i + 1}/{n_chunks})\"\n        elif chunk_title_suffix:\n            chunk_kwargs['title'] = f\"Part {i + 1}/{n_chunks}\"\n\n        cols_in_chunk = [\n            c for c, facet_row_category in\n            zip(data.columns, data.columns.get_level_values(self.config.facet_row))\n            if facet_row_category in chunk_rows\n        ]\n        data_chunk = data[cols_in_chunk]\n\n        fig = self.get_figure(data_chunk, **chunk_kwargs, _skip_validation=True)\n        figures.append(fig)\n\n    self.config = original_config\n\n    return figures\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/#mesqual.visualizations.plotly_figures.timeseries_dashboard.DashboardConfig","title":"DashboardConfig","text":"<p>Configuration class for timeseries dashboard visualization.</p> <p>Manages all configuration parameters for generating heatmap-based timeseries dashboards with customizable statistics, faceting, and color schemes.</p> Custom KPI Statistics <p>You can define custom KPIs by providing a dictionary of functions that operate on pandas Series. Each function should take a Series and return a single numeric value.</p> <p>KPI Customization Example:</p> <pre><code>&gt;&gt;&gt; custom_kpis = {\n...     'Peak Load': lambda x: x.max(),\n...     'Capacity Factor': lambda x: x.mean() / x.max() * 100,\n...     'Ramp Rate': lambda x: x.diff().abs().max(),\n...     'Hours Above Mean': lambda x: (x &gt; x.mean()).sum(),\n...     'Volatility': lambda x: x.std() / x.mean() * 100 if x.mean() != 0 else 0\n... }\n</code></pre> <p>Available built-in statistics are provided in DEFAULT_STATISTICS and STATISTICS_LIBRARY class attributes.</p> Data Format Requirements <p>Input data must be a pandas DataFrame or Series with: - DatetimeIndex (hourly or sub-hourly recommended) - For faceting: MultiIndex columns with named levels - Column names will be used as facet category labels</p> <p>MultiIndex structure for faceting:</p> <pre><code>&gt;&gt;&gt; # Two-level MultiIndex example\n&gt;&gt;&gt; data.columns = pd.MultiIndex.from_tuples([\n&gt;&gt;&gt;     ('scenario1', 'solar'), ('scenario1', 'wind'),\n&gt;&gt;&gt;     ('scenario2', 'solar'), ('scenario2', 'wind')\n&gt;&gt;&gt; ], names=['scenario', 'technology'])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use column level names for faceting\n&gt;&gt;&gt; config = DashboardConfig(\n...     facet_row='technology',  # Use 'technology' level\n...     facet_col='scenario'     # Use 'scenario' level\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>class DashboardConfig:\n    \"\"\"Configuration class for timeseries dashboard visualization.\n\n    Manages all configuration parameters for generating heatmap-based timeseries\n    dashboards with customizable statistics, faceting, and color schemes.\n\n    Custom KPI Statistics:\n        You can define custom KPIs by providing a dictionary of functions that\n        operate on pandas Series. Each function should take a Series and return\n        a single numeric value.\n\n        KPI Customization Example:\n\n            &gt;&gt;&gt; custom_kpis = {\n            ...     'Peak Load': lambda x: x.max(),\n            ...     'Capacity Factor': lambda x: x.mean() / x.max() * 100,\n            ...     'Ramp Rate': lambda x: x.diff().abs().max(),\n            ...     'Hours Above Mean': lambda x: (x &gt; x.mean()).sum(),\n            ...     'Volatility': lambda x: x.std() / x.mean() * 100 if x.mean() != 0 else 0\n            ... }\n\n        Available built-in statistics are provided in DEFAULT_STATISTICS and\n        STATISTICS_LIBRARY class attributes.\n\n    Data Format Requirements:\n        Input data must be a pandas DataFrame or Series with:\n        - DatetimeIndex (hourly or sub-hourly recommended)\n        - For faceting: MultiIndex columns with named levels\n        - Column names will be used as facet category labels\n\n        MultiIndex structure for faceting:\n\n            &gt;&gt;&gt; # Two-level MultiIndex example\n            &gt;&gt;&gt; data.columns = pd.MultiIndex.from_tuples([\n            &gt;&gt;&gt;     ('scenario1', 'solar'), ('scenario1', 'wind'),\n            &gt;&gt;&gt;     ('scenario2', 'solar'), ('scenario2', 'wind')\n            &gt;&gt;&gt; ], names=['scenario', 'technology'])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Use column level names for faceting\n            &gt;&gt;&gt; config = DashboardConfig(\n            ...     facet_row='technology',  # Use 'technology' level\n            ...     facet_col='scenario'     # Use 'scenario' level\n            ... )\n    \"\"\"\n    DEFAULT_STATISTICS = {\n        'Datums': lambda x: len(x),\n        'Abs max': lambda x: x.abs().max(),\n        'Abs mean': lambda x: x.abs().mean(),\n        'Max': lambda x: x.max(),\n        'Mean': lambda x: x.mean(),\n        'Min': lambda x: x.min(),\n    }\n\n    STATISTICS_LIBRARY = {\n        '# Values': lambda x: (~x.isna()).sum(),\n        '# NaNs': lambda x: x.isna().sum(),\n        '% == 0': lambda x: (x.round(2) == 0).sum() / (~x.isna()).sum() * 100,\n        '% != 0': lambda x: ((x.round(2) != 0) &amp; (~x.isna())).sum() / (~x.isna()).sum() * 100,\n        '% &gt; 0': lambda x: (x.round(2) &gt; 0).sum() / (~x.isna()).sum() * 100,\n        '% &lt; 0': lambda x: (x.round(2) &lt; 0).sum() / (~x.isna()).sum() * 100,\n        'Mean of v&gt;0': lambda x: x.where(x &gt; 0, np.nan).mean(),\n        'Mean of v&lt;0': lambda x: x.where(x &lt; 0, np.nan).mean(),\n        'Median': lambda x: x.median(),\n        'Q0.99': lambda x: x.quantile(0.99),\n        'Q0.95': lambda x: x.quantile(0.95),\n        'Q0.05': lambda x: x.quantile(0.05),\n        'Q0.01': lambda x: x.quantile(0.01),\n        'Std': lambda x: x.std(),\n    }\n\n    def __init__(\n            self,\n            x_axis: X_AXIS_TYPES = 'date',\n            facet_col: str = None,\n            facet_row: str = None,\n            facet_col_wrap: int = None,\n            facet_col_order: list[str] = None,\n            facet_row_order: list[str] = None,\n            ratio_of_stat_col: float = 0.1,\n            stat_aggs: Dict[str, Callable[[pd.Series], float | int]] = None,\n            groupby_aggregation: GROUPBY_AGG_TYPES = 'mean',\n            title: str = None,\n            color_continuous_scale: str | list[str] | list[tuple[float, str]] = 'Turbo',\n            color_continuous_midpoint: int | float = None,\n            range_color: list[int | float] = None,\n            per_facet_col_colorscale: bool = False,\n            per_facet_row_colorscale: bool = False,\n            facet_row_color_settings: dict = None,\n            facet_col_color_settings: dict = None,\n            subplots_vertical_spacing: float = None,\n            subplots_horizontal_spacing: float = None,\n            time_series_figure_kwargs: dict = None,\n            stat_figure_kwargs: dict = None,\n            universal_figure_kwargs: dict = None,\n            use_string_for_axis: bool = False,\n            **figure_kwargs\n    ):\n        \"\"\"Initialize dashboard configuration.\n\n        Args:\n            x_axis: X-axis aggregation type ('date', 'year_month', 'year_week', 'week', 'month', 'year')\n                   or list of aggregation types for faceting.\n            facet_col: Column name to use for column faceting, or 'x_axis'/'groupby_aggregation'\n                      for parameter-based faceting.\n            facet_row: Column name to use for row faceting, or 'x_axis'/'groupby_aggregation'\n                      for parameter-based faceting.\n            facet_col_wrap: Maximum number of columns per row when using column faceting.\n            facet_col_order: Custom ordering for column facets.\n            facet_row_order: Custom ordering for row facets.\n            ratio_of_stat_col: Width ratio of statistics column relative to heatmap column.\n            stat_aggs: Dictionary of statistic names to aggregation functions for KPI calculation.\n            groupby_aggregation: Aggregation method for grouping data ('mean', 'sum', etc.) or list\n                               of methods for faceting.\n            title: Dashboard title.\n            color_continuous_scale: Plotly colorscale name or custom colorscale.\n            color_continuous_midpoint: Midpoint value for diverging colorscales.\n            range_color: Fixed color range [min, max] for heatmaps.\n            per_facet_col_colorscale: Whether to use separate colorscales per column facet.\n            per_facet_row_colorscale: Whether to use separate colorscales per row facet.\n            facet_row_color_settings: Custom color settings per row facet category.\n            facet_col_color_settings: Custom color settings per column facet category.\n            subplots_vertical_spacing: Vertical spacing between subplots.\n            subplots_horizontal_spacing: Horizontal spacing between subplots.\n            time_series_figure_kwargs: Additional kwargs for heatmap traces.\n            stat_figure_kwargs: Additional kwargs for statistics traces.\n            universal_figure_kwargs: kwargs applied to all traces.\n            use_string_for_axis: Whether to convert axis values to strings.\n            **figure_kwargs: Additional figure-level kwargs.\n        \"\"\"\n        self.x_axis = x_axis\n        self.facet_col = facet_col\n        self.facet_row = facet_row\n        self.facet_col_wrap = facet_col_wrap\n        self.facet_col_order = facet_col_order\n        self.facet_row_order = facet_row_order\n        self.ratio_of_stat_col = ratio_of_stat_col\n        self.stat_aggs = stat_aggs or self.DEFAULT_STATISTICS\n        self.groupby_aggregation = groupby_aggregation\n        self.title = title\n\n        self.per_facet_col_colorscale = per_facet_col_colorscale\n        self.per_facet_row_colorscale = per_facet_row_colorscale\n\n        if per_facet_col_colorscale and per_facet_row_colorscale:\n            raise ValueError(\"Cannot use both per_facet_col_colorscale and per_facet_row_colorscale simultaneously\")\n        if facet_row_color_settings and not per_facet_row_colorscale:\n            raise ValueError(\"Set per_facet_row_colorscale to True in order to use facet_row_color_settings.\")\n        if facet_col_color_settings and not per_facet_col_colorscale:\n            raise ValueError(\"Set per_facet_col_colorscale to True in order to use facet_col_color_settings.\")\n\n        self.facet_row_color_settings = facet_row_color_settings or {}\n        self.facet_col_color_settings = facet_col_color_settings or {}\n\n        self.time_series_figure_kwargs = time_series_figure_kwargs or {}\n        self.stat_figure_kwargs = stat_figure_kwargs or {}\n\n        self.subplots_vertical_spacing = subplots_vertical_spacing\n        self.subplots_horizontal_spacing = subplots_horizontal_spacing\n\n        universal_figure_kwargs = universal_figure_kwargs or {}\n\n        self.figure_kwargs = {\n            'color_continuous_scale': color_continuous_scale,\n            'color_continuous_midpoint': color_continuous_midpoint,\n            'range_color': range_color,\n            **universal_figure_kwargs,\n            **figure_kwargs,\n        }\n        self.use_string_for_axis = use_string_for_axis\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/#mesqual.visualizations.plotly_figures.timeseries_dashboard.DashboardConfig.__init__","title":"__init__","text":"<pre><code>__init__(x_axis: X_AXIS_TYPES = 'date', facet_col: str = None, facet_row: str = None, facet_col_wrap: int = None, facet_col_order: list[str] = None, facet_row_order: list[str] = None, ratio_of_stat_col: float = 0.1, stat_aggs: Dict[str, Callable[[Series], float | int]] = None, groupby_aggregation: GROUPBY_AGG_TYPES = 'mean', title: str = None, color_continuous_scale: str | list[str] | list[tuple[float, str]] = 'Turbo', color_continuous_midpoint: int | float = None, range_color: list[int | float] = None, per_facet_col_colorscale: bool = False, per_facet_row_colorscale: bool = False, facet_row_color_settings: dict = None, facet_col_color_settings: dict = None, subplots_vertical_spacing: float = None, subplots_horizontal_spacing: float = None, time_series_figure_kwargs: dict = None, stat_figure_kwargs: dict = None, universal_figure_kwargs: dict = None, use_string_for_axis: bool = False, **figure_kwargs)\n</code></pre> <p>Initialize dashboard configuration.</p> <p>Parameters:</p> Name Type Description Default <code>x_axis</code> <code>X_AXIS_TYPES</code> <p>X-axis aggregation type ('date', 'year_month', 'year_week', 'week', 'month', 'year')    or list of aggregation types for faceting.</p> <code>'date'</code> <code>facet_col</code> <code>str</code> <p>Column name to use for column faceting, or 'x_axis'/'groupby_aggregation'       for parameter-based faceting.</p> <code>None</code> <code>facet_row</code> <code>str</code> <p>Column name to use for row faceting, or 'x_axis'/'groupby_aggregation'       for parameter-based faceting.</p> <code>None</code> <code>facet_col_wrap</code> <code>int</code> <p>Maximum number of columns per row when using column faceting.</p> <code>None</code> <code>facet_col_order</code> <code>list[str]</code> <p>Custom ordering for column facets.</p> <code>None</code> <code>facet_row_order</code> <code>list[str]</code> <p>Custom ordering for row facets.</p> <code>None</code> <code>ratio_of_stat_col</code> <code>float</code> <p>Width ratio of statistics column relative to heatmap column.</p> <code>0.1</code> <code>stat_aggs</code> <code>Dict[str, Callable[[Series], float | int]]</code> <p>Dictionary of statistic names to aggregation functions for KPI calculation.</p> <code>None</code> <code>groupby_aggregation</code> <code>GROUPBY_AGG_TYPES</code> <p>Aggregation method for grouping data ('mean', 'sum', etc.) or list                of methods for faceting.</p> <code>'mean'</code> <code>title</code> <code>str</code> <p>Dashboard title.</p> <code>None</code> <code>color_continuous_scale</code> <code>str | list[str] | list[tuple[float, str]]</code> <p>Plotly colorscale name or custom colorscale.</p> <code>'Turbo'</code> <code>color_continuous_midpoint</code> <code>int | float</code> <p>Midpoint value for diverging colorscales.</p> <code>None</code> <code>range_color</code> <code>list[int | float]</code> <p>Fixed color range [min, max] for heatmaps.</p> <code>None</code> <code>per_facet_col_colorscale</code> <code>bool</code> <p>Whether to use separate colorscales per column facet.</p> <code>False</code> <code>per_facet_row_colorscale</code> <code>bool</code> <p>Whether to use separate colorscales per row facet.</p> <code>False</code> <code>facet_row_color_settings</code> <code>dict</code> <p>Custom color settings per row facet category.</p> <code>None</code> <code>facet_col_color_settings</code> <code>dict</code> <p>Custom color settings per column facet category.</p> <code>None</code> <code>subplots_vertical_spacing</code> <code>float</code> <p>Vertical spacing between subplots.</p> <code>None</code> <code>subplots_horizontal_spacing</code> <code>float</code> <p>Horizontal spacing between subplots.</p> <code>None</code> <code>time_series_figure_kwargs</code> <code>dict</code> <p>Additional kwargs for heatmap traces.</p> <code>None</code> <code>stat_figure_kwargs</code> <code>dict</code> <p>Additional kwargs for statistics traces.</p> <code>None</code> <code>universal_figure_kwargs</code> <code>dict</code> <p>kwargs applied to all traces.</p> <code>None</code> <code>use_string_for_axis</code> <code>bool</code> <p>Whether to convert axis values to strings.</p> <code>False</code> <code>**figure_kwargs</code> <p>Additional figure-level kwargs.</p> <code>{}</code> Source code in <code>submodules/mesqual/mesqual/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>def __init__(\n        self,\n        x_axis: X_AXIS_TYPES = 'date',\n        facet_col: str = None,\n        facet_row: str = None,\n        facet_col_wrap: int = None,\n        facet_col_order: list[str] = None,\n        facet_row_order: list[str] = None,\n        ratio_of_stat_col: float = 0.1,\n        stat_aggs: Dict[str, Callable[[pd.Series], float | int]] = None,\n        groupby_aggregation: GROUPBY_AGG_TYPES = 'mean',\n        title: str = None,\n        color_continuous_scale: str | list[str] | list[tuple[float, str]] = 'Turbo',\n        color_continuous_midpoint: int | float = None,\n        range_color: list[int | float] = None,\n        per_facet_col_colorscale: bool = False,\n        per_facet_row_colorscale: bool = False,\n        facet_row_color_settings: dict = None,\n        facet_col_color_settings: dict = None,\n        subplots_vertical_spacing: float = None,\n        subplots_horizontal_spacing: float = None,\n        time_series_figure_kwargs: dict = None,\n        stat_figure_kwargs: dict = None,\n        universal_figure_kwargs: dict = None,\n        use_string_for_axis: bool = False,\n        **figure_kwargs\n):\n    \"\"\"Initialize dashboard configuration.\n\n    Args:\n        x_axis: X-axis aggregation type ('date', 'year_month', 'year_week', 'week', 'month', 'year')\n               or list of aggregation types for faceting.\n        facet_col: Column name to use for column faceting, or 'x_axis'/'groupby_aggregation'\n                  for parameter-based faceting.\n        facet_row: Column name to use for row faceting, or 'x_axis'/'groupby_aggregation'\n                  for parameter-based faceting.\n        facet_col_wrap: Maximum number of columns per row when using column faceting.\n        facet_col_order: Custom ordering for column facets.\n        facet_row_order: Custom ordering for row facets.\n        ratio_of_stat_col: Width ratio of statistics column relative to heatmap column.\n        stat_aggs: Dictionary of statistic names to aggregation functions for KPI calculation.\n        groupby_aggregation: Aggregation method for grouping data ('mean', 'sum', etc.) or list\n                           of methods for faceting.\n        title: Dashboard title.\n        color_continuous_scale: Plotly colorscale name or custom colorscale.\n        color_continuous_midpoint: Midpoint value for diverging colorscales.\n        range_color: Fixed color range [min, max] for heatmaps.\n        per_facet_col_colorscale: Whether to use separate colorscales per column facet.\n        per_facet_row_colorscale: Whether to use separate colorscales per row facet.\n        facet_row_color_settings: Custom color settings per row facet category.\n        facet_col_color_settings: Custom color settings per column facet category.\n        subplots_vertical_spacing: Vertical spacing between subplots.\n        subplots_horizontal_spacing: Horizontal spacing between subplots.\n        time_series_figure_kwargs: Additional kwargs for heatmap traces.\n        stat_figure_kwargs: Additional kwargs for statistics traces.\n        universal_figure_kwargs: kwargs applied to all traces.\n        use_string_for_axis: Whether to convert axis values to strings.\n        **figure_kwargs: Additional figure-level kwargs.\n    \"\"\"\n    self.x_axis = x_axis\n    self.facet_col = facet_col\n    self.facet_row = facet_row\n    self.facet_col_wrap = facet_col_wrap\n    self.facet_col_order = facet_col_order\n    self.facet_row_order = facet_row_order\n    self.ratio_of_stat_col = ratio_of_stat_col\n    self.stat_aggs = stat_aggs or self.DEFAULT_STATISTICS\n    self.groupby_aggregation = groupby_aggregation\n    self.title = title\n\n    self.per_facet_col_colorscale = per_facet_col_colorscale\n    self.per_facet_row_colorscale = per_facet_row_colorscale\n\n    if per_facet_col_colorscale and per_facet_row_colorscale:\n        raise ValueError(\"Cannot use both per_facet_col_colorscale and per_facet_row_colorscale simultaneously\")\n    if facet_row_color_settings and not per_facet_row_colorscale:\n        raise ValueError(\"Set per_facet_row_colorscale to True in order to use facet_row_color_settings.\")\n    if facet_col_color_settings and not per_facet_col_colorscale:\n        raise ValueError(\"Set per_facet_col_colorscale to True in order to use facet_col_color_settings.\")\n\n    self.facet_row_color_settings = facet_row_color_settings or {}\n    self.facet_col_color_settings = facet_col_color_settings or {}\n\n    self.time_series_figure_kwargs = time_series_figure_kwargs or {}\n    self.stat_figure_kwargs = stat_figure_kwargs or {}\n\n    self.subplots_vertical_spacing = subplots_vertical_spacing\n    self.subplots_horizontal_spacing = subplots_horizontal_spacing\n\n    universal_figure_kwargs = universal_figure_kwargs or {}\n\n    self.figure_kwargs = {\n        'color_continuous_scale': color_continuous_scale,\n        'color_continuous_midpoint': color_continuous_midpoint,\n        'range_color': range_color,\n        **universal_figure_kwargs,\n        **figure_kwargs,\n    }\n    self.use_string_for_axis = use_string_for_axis\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/#mesqual.visualizations.plotly_figures.timeseries_dashboard.DataProcessor","title":"DataProcessor","text":"Source code in <code>submodules/mesqual/mesqual/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>class DataProcessor:\n    @staticmethod\n    def validate_input_data_and_config(data: pd.DataFrame, config: DashboardConfig) -&gt; None:\n        x_axis = config.x_axis\n        groupby_aggregation = config.groupby_aggregation\n        facet_col = config.facet_col\n        facet_row = config.facet_row\n        facet_col_wrap = config.facet_col_wrap\n\n        if facet_col_wrap is not None and facet_row is not None:\n            raise ValueError('You cannot set facet_row if you are setting a facet_col_wrap')\n\n        if isinstance(data, pd.Series):\n            if sum(facet not in [None, 'x_axis', 'groupby_aggregation'] for facet in [facet_col, facet_row]):\n                raise ValueError('You can not define facet_col or facet_row if you just have a pd.Series')\n        elif data.columns.nlevels &gt; 2:\n            raise ValueError('Your data must not have more than 2 column index levels.')\n        elif data.columns.nlevels == 2:\n            if (facet_col is None) and (facet_row is None):\n                raise ValueError('If you have two column levels, you must define both, facet_col and facet_row.')\n            if isinstance(x_axis, list) or isinstance(groupby_aggregation, list):\n                raise ValueError(\n                    'You cannot set x_axis or groupby_aggregation to a list if your data already has 2 levels.'\n                )\n        elif data.columns.nlevels == 1:\n            if sum(facet not in [None, 'x_axis', 'groupby_aggregation'] for facet in [facet_col, facet_row]) &gt; 1:\n                raise ValueError('You only have 1 column level. You can only define facet_col or facet_row')\n            if isinstance(x_axis, list) and isinstance(groupby_aggregation, list):\n                raise ValueError(\n                    'You cannot set x_axis and groupby_aggregation to a list if your data already has 1 level.'\n                )\n\n        if isinstance(x_axis, list):\n            if not any('x_axis' == facet for facet in [facet_col, facet_row]):\n                raise ValueError(\n                    \"x_axis must be either 'facet_col' or 'facet_row' when provided as a list.\"\n                )\n        else:\n            if any('x_axis' == facet for facet in [facet_col, facet_row]):\n                raise ValueError(\n                    \"You provided a str for x_axis, \"\n                    \"but set facet_col or facet_row to 'x_axis'. This is not allowed! \\n\"\n                    \"You must provide a List[str] and in order to use facet_row / facet_col \"\n                    \"for different x_axis.\"\n                )\n\n        if isinstance(groupby_aggregation, list):\n            if not any('groupby_aggregation' == facet for facet in [facet_col, facet_row]):\n                raise ValueError(\n                    \"groupby_aggregation must be either 'facet_col' or 'facet_row' when provided as a list.\"\n                )\n        else:\n            if any('groupby_aggregation' == facet for facet in [facet_col, facet_row]):\n                raise ValueError(\n                    \"You provided a str for groupby_aggregation, \"\n                    \"but set facet_col or facet_row to 'groupby_aggregation'. This is not allowed! \\n\"\n                    \"You must provide a List[str] and in order to use facet_row / facet_col \"\n                    \"for different groupby_aggregation.\"\n                )\n\n    @staticmethod\n    def prepare_dataframe_for_facet(data: pd.DataFrame, config: DashboardConfig) -&gt; pd.DataFrame:\n        for k in ['x_axis', 'groupby_aggregation']:\n            config_value = getattr(config, k)\n            if isinstance(config_value, list):\n                data = pd.concat(\n                    {i: data.copy(deep=True) for i in config_value},\n                    axis=1,\n                    names=[k],\n                )\n        return data\n\n    @classmethod\n    def ensure_df_has_two_column_levels(cls, data: pd.DataFrame, config: DashboardConfig) -&gt; pd.DataFrame:\n        if isinstance(data, pd.Series):\n            data = data.to_frame(data.name or 'Time series')\n\n        if data.columns.nlevels == 1:\n            data.columns.name = data.columns.name or 'variable'\n            data = cls._insert_empty_column_index_level(data)\n\n        if config.facet_col in [data.columns.names[0]]:\n            data.columns = data.columns.reorder_levels([1, 0])\n\n        return data\n\n    @staticmethod\n    def update_facet_config(data: pd.DataFrame, config: DashboardConfig) -&gt; None:\n        unique_facet_col_keys = data.columns.get_level_values(config.facet_col).unique().to_list()\n        if config.facet_col_order is None:\n            config.facet_col_order = unique_facet_col_keys\n        else:\n            config.facet_col_order += [c for c in unique_facet_col_keys if c not in config.facet_col_order]\n\n        unique_facet_row_keys = data.columns.get_level_values(config.facet_row).unique().to_list()\n        if config.facet_row_order is None:\n            config.facet_row_order = unique_facet_row_keys\n        else:\n            config.facet_row_order += [c for c in unique_facet_row_keys if c not in config.facet_row_order]\n\n        if config.facet_col_wrap is None:\n            config.facet_col_wrap = len(config.facet_col_order)\n\n    @staticmethod\n    def get_grouped_data(series: pd.Series, x_axis: str, groupby_aggregation: str) -&gt; pd.DataFrame:\n        \"\"\"Group and aggregate time series data into heatmap format.\n\n        Transforms timeseries data into a matrix suitable for heatmap visualization\n        with hour-of-day on y-axis and specified time aggregation on x-axis.\n\n        Args:\n            series: Input timeseries data with datetime index.\n            x_axis: Time aggregation method ('date', 'week', 'month', etc.).\n            groupby_aggregation: Aggregation function name ('mean', 'sum', etc.).\n\n        Returns:\n            DataFrame with time categories as columns and hour-of-day as rows.\n        \"\"\"\n        temp = series.to_frame('value')\n        temp.loc[:, 'time'] = temp.index.time\n        temp.loc[:, 'minute'] = temp.index.minute\n        temp.loc[:, 'hour'] = temp.index.hour + 1\n        temp.loc[:, 'date'] = temp.index.date\n        temp.loc[:, 'month'] = temp.index.month\n        temp.loc[:, 'week'] = temp.index.isocalendar().week\n        temp.loc[:, 'year_month'] = temp.index.strftime('%Y-%m')\n        temp.loc[:, 'year_week'] = temp.index.strftime('%Y-CW%U')\n\n        y_axis = 'time'\n        groupby = [y_axis, x_axis]\n        temp = temp.groupby(groupby)['value'].agg(groupby_aggregation)\n        temp = temp.unstack(x_axis)\n        temp_data = temp.sort_index(ascending=False)\n        return temp_data\n\n    @staticmethod\n    def _insert_empty_column_index_level(df: pd.DataFrame, level_name: str = None) -&gt; pd.DataFrame:\n        level_value = ''\n        return pd.concat({level_value: df}, axis=1, names=[level_name])\n\n    @staticmethod\n    def _prepend_empty_row(df: pd.DataFrame) -&gt; pd.DataFrame:\n        empty_row = pd.DataFrame([[np.nan] * len(df.columns)], index=[' '], columns=df.columns)\n        return pd.concat([empty_row, df])\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/#mesqual.visualizations.plotly_figures.timeseries_dashboard.DataProcessor.get_grouped_data","title":"get_grouped_data  <code>staticmethod</code>","text":"<pre><code>get_grouped_data(series: Series, x_axis: str, groupby_aggregation: str) -&gt; DataFrame\n</code></pre> <p>Group and aggregate time series data into heatmap format.</p> <p>Transforms timeseries data into a matrix suitable for heatmap visualization with hour-of-day on y-axis and specified time aggregation on x-axis.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series</code> <p>Input timeseries data with datetime index.</p> required <code>x_axis</code> <code>str</code> <p>Time aggregation method ('date', 'week', 'month', etc.).</p> required <code>groupby_aggregation</code> <code>str</code> <p>Aggregation function name ('mean', 'sum', etc.).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with time categories as columns and hour-of-day as rows.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>@staticmethod\ndef get_grouped_data(series: pd.Series, x_axis: str, groupby_aggregation: str) -&gt; pd.DataFrame:\n    \"\"\"Group and aggregate time series data into heatmap format.\n\n    Transforms timeseries data into a matrix suitable for heatmap visualization\n    with hour-of-day on y-axis and specified time aggregation on x-axis.\n\n    Args:\n        series: Input timeseries data with datetime index.\n        x_axis: Time aggregation method ('date', 'week', 'month', etc.).\n        groupby_aggregation: Aggregation function name ('mean', 'sum', etc.).\n\n    Returns:\n        DataFrame with time categories as columns and hour-of-day as rows.\n    \"\"\"\n    temp = series.to_frame('value')\n    temp.loc[:, 'time'] = temp.index.time\n    temp.loc[:, 'minute'] = temp.index.minute\n    temp.loc[:, 'hour'] = temp.index.hour + 1\n    temp.loc[:, 'date'] = temp.index.date\n    temp.loc[:, 'month'] = temp.index.month\n    temp.loc[:, 'week'] = temp.index.isocalendar().week\n    temp.loc[:, 'year_month'] = temp.index.strftime('%Y-%m')\n    temp.loc[:, 'year_week'] = temp.index.strftime('%Y-CW%U')\n\n    y_axis = 'time'\n    groupby = [y_axis, x_axis]\n    temp = temp.groupby(groupby)['value'].agg(groupby_aggregation)\n    temp = temp.unstack(x_axis)\n    temp_data = temp.sort_index(ascending=False)\n    return temp_data\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/#mesqual.visualizations.plotly_figures.timeseries_dashboard.ColorManager","title":"ColorManager","text":"<p>Manages color settings and scale computation for dashboard visualizations.</p> <p>Handles colorscale selection, range computation, and facet-specific color customization for heatmap and statistics traces.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>class ColorManager:\n    \"\"\"Manages color settings and scale computation for dashboard visualizations.\n\n    Handles colorscale selection, range computation, and facet-specific color\n    customization for heatmap and statistics traces.\n    \"\"\"\n    @staticmethod\n    def get_color_settings_for_facet_category(config: DashboardConfig, facet_key: tuple[str, str]):\n        \"\"\"Get color settings for a specific facet category.\n\n        Args:\n            config: Dashboard configuration object.\n            facet_key: Tuple of (row_key, col_key) identifying the facet.\n\n        Returns:\n            Dictionary of color settings for the specified facet category.\n        \"\"\"\n        row_key, col_key = facet_key\n        settings = {\n            'color_continuous_scale': config.figure_kwargs.get('color_continuous_scale'),\n            'color_continuous_midpoint': config.figure_kwargs.get('color_continuous_midpoint'),\n            'range_color': config.figure_kwargs.get('range_color')\n        }\n\n        if config.per_facet_row_colorscale and row_key in config.facet_row_color_settings:\n            settings.update(config.facet_row_color_settings.get(row_key, {}))\n        elif config.per_facet_col_colorscale and col_key in config.facet_col_color_settings:\n            settings.update(config.facet_col_color_settings.get(col_key, {}))\n\n        return settings\n\n    @staticmethod\n    def compute_color_params(data, config: DashboardConfig, facet_key: tuple[str, str] = None):\n        \"\"\"Compute color parameters for heatmap traces.\n\n        Calculates colorscale, min/max values, and other color-related parameters\n        based on data range and configuration settings.\n\n        Args:\n            data: Input data for color range calculation.\n            config: Dashboard configuration object.\n            facet_key: Optional facet identifier for per-facet colorscales.\n\n        Returns:\n            Dictionary of color parameters for plotly traces.\n        \"\"\"\n        if facet_key is not None:\n            settings = ColorManager.get_color_settings_for_facet_category(config, facet_key)\n        else:\n            settings = config.figure_kwargs\n\n        if facet_key is not None:\n            if config.per_facet_row_colorscale:\n                row_key, _ = facet_key\n                filtered_data = data.loc[:, (row_key, slice(None))]\n            elif config.per_facet_col_colorscale:\n                _, col_key = facet_key\n                filtered_data = data.loc[:, (slice(None), col_key)]\n            else:\n                filtered_data = data\n        else:\n            filtered_data = data\n\n        color_continuous_scale = settings.get('color_continuous_scale')\n        color_continuous_midpoint = settings.get('color_continuous_midpoint')\n        range_color = settings.get('range_color')\n\n        result = {}\n        if color_continuous_scale:\n            result['colorscale'] = color_continuous_scale\n\n        if range_color:\n            result['zmin'] = range_color[0]\n            result['zmax'] = range_color[1]\n        elif color_continuous_midpoint == 0:\n            _absmax = filtered_data.abs().max().max()\n            result['zmin'] = -_absmax\n            result['zmax'] = _absmax\n        elif color_continuous_midpoint:\n            raise NotImplementedError(\"color_continuous_midpoint other than 0 is not implemented\")\n        else:\n            result['zmin'] = filtered_data.min().min()\n            result['zmax'] = filtered_data.max().max()\n\n        return result\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/#mesqual.visualizations.plotly_figures.timeseries_dashboard.ColorManager.get_color_settings_for_facet_category","title":"get_color_settings_for_facet_category  <code>staticmethod</code>","text":"<pre><code>get_color_settings_for_facet_category(config: DashboardConfig, facet_key: tuple[str, str])\n</code></pre> <p>Get color settings for a specific facet category.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DashboardConfig</code> <p>Dashboard configuration object.</p> required <code>facet_key</code> <code>tuple[str, str]</code> <p>Tuple of (row_key, col_key) identifying the facet.</p> required <p>Returns:</p> Type Description <p>Dictionary of color settings for the specified facet category.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>@staticmethod\ndef get_color_settings_for_facet_category(config: DashboardConfig, facet_key: tuple[str, str]):\n    \"\"\"Get color settings for a specific facet category.\n\n    Args:\n        config: Dashboard configuration object.\n        facet_key: Tuple of (row_key, col_key) identifying the facet.\n\n    Returns:\n        Dictionary of color settings for the specified facet category.\n    \"\"\"\n    row_key, col_key = facet_key\n    settings = {\n        'color_continuous_scale': config.figure_kwargs.get('color_continuous_scale'),\n        'color_continuous_midpoint': config.figure_kwargs.get('color_continuous_midpoint'),\n        'range_color': config.figure_kwargs.get('range_color')\n    }\n\n    if config.per_facet_row_colorscale and row_key in config.facet_row_color_settings:\n        settings.update(config.facet_row_color_settings.get(row_key, {}))\n    elif config.per_facet_col_colorscale and col_key in config.facet_col_color_settings:\n        settings.update(config.facet_col_color_settings.get(col_key, {}))\n\n    return settings\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/#mesqual.visualizations.plotly_figures.timeseries_dashboard.ColorManager.compute_color_params","title":"compute_color_params  <code>staticmethod</code>","text":"<pre><code>compute_color_params(data, config: DashboardConfig, facet_key: tuple[str, str] = None)\n</code></pre> <p>Compute color parameters for heatmap traces.</p> <p>Calculates colorscale, min/max values, and other color-related parameters based on data range and configuration settings.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>Input data for color range calculation.</p> required <code>config</code> <code>DashboardConfig</code> <p>Dashboard configuration object.</p> required <code>facet_key</code> <code>tuple[str, str]</code> <p>Optional facet identifier for per-facet colorscales.</p> <code>None</code> <p>Returns:</p> Type Description <p>Dictionary of color parameters for plotly traces.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>@staticmethod\ndef compute_color_params(data, config: DashboardConfig, facet_key: tuple[str, str] = None):\n    \"\"\"Compute color parameters for heatmap traces.\n\n    Calculates colorscale, min/max values, and other color-related parameters\n    based on data range and configuration settings.\n\n    Args:\n        data: Input data for color range calculation.\n        config: Dashboard configuration object.\n        facet_key: Optional facet identifier for per-facet colorscales.\n\n    Returns:\n        Dictionary of color parameters for plotly traces.\n    \"\"\"\n    if facet_key is not None:\n        settings = ColorManager.get_color_settings_for_facet_category(config, facet_key)\n    else:\n        settings = config.figure_kwargs\n\n    if facet_key is not None:\n        if config.per_facet_row_colorscale:\n            row_key, _ = facet_key\n            filtered_data = data.loc[:, (row_key, slice(None))]\n        elif config.per_facet_col_colorscale:\n            _, col_key = facet_key\n            filtered_data = data.loc[:, (slice(None), col_key)]\n        else:\n            filtered_data = data\n    else:\n        filtered_data = data\n\n    color_continuous_scale = settings.get('color_continuous_scale')\n    color_continuous_midpoint = settings.get('color_continuous_midpoint')\n    range_color = settings.get('range_color')\n\n    result = {}\n    if color_continuous_scale:\n        result['colorscale'] = color_continuous_scale\n\n    if range_color:\n        result['zmin'] = range_color[0]\n        result['zmax'] = range_color[1]\n    elif color_continuous_midpoint == 0:\n        _absmax = filtered_data.abs().max().max()\n        result['zmin'] = -_absmax\n        result['zmax'] = _absmax\n    elif color_continuous_midpoint:\n        raise NotImplementedError(\"color_continuous_midpoint other than 0 is not implemented\")\n    else:\n        result['zmin'] = filtered_data.min().min()\n        result['zmax'] = filtered_data.max().max()\n\n    return result\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/#mesqual.visualizations.plotly_figures.timeseries_dashboard.TraceGenerator","title":"TraceGenerator","text":"<p>Generates plotly trace objects for dashboard visualization.</p> <p>Creates heatmap traces for timeseries data, statistics traces for KPI display, and colorscale traces for custom color legends.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>class TraceGenerator:\n    \"\"\"Generates plotly trace objects for dashboard visualization.\n\n    Creates heatmap traces for timeseries data, statistics traces for KPI display,\n    and colorscale traces for custom color legends.\n    \"\"\"\n\n    @classmethod\n    def get_heatmap_trace(cls, data: pd.DataFrame, ts_kwargs, color_kwargs, use_string_for_axis, **kwargs):\n        \"\"\"Create a heatmap trace for timeseries data visualization.\n\n        Args:\n            data: DataFrame with time categories as columns and hour-of-day as rows.\n            ts_kwargs: Additional kwargs for the heatmap trace.\n            color_kwargs: Color-related parameters (colorscale, zmin, zmax).\n            use_string_for_axis: Whether to convert axis values to strings.\n            **kwargs: Additional plotly Heatmap parameters.\n\n        Returns:\n            Plotly Heatmap trace object for timeseries visualization.\n        \"\"\"\n        if set(data.columns).issubset(list(range(1, 13))):\n            x = [calendar.month_abbr[m] for m in range(1, 13)]\n        else:\n            if use_string_for_axis:\n                x = [str(i).replace('-', '_') for i in data.columns]\n            else:\n                x = data.columns\n\n        trace_kwargs = {**color_kwargs, **ts_kwargs, **kwargs}\n\n        assert 'colorscale' in trace_kwargs and 'zmin' in trace_kwargs and 'zmax' in trace_kwargs\n\n        trace_heatmap = go.Heatmap(x=x, z=data.values, y=data.index, **trace_kwargs)\n        return trace_heatmap\n\n    @classmethod\n    def get_stats_trace(cls, series: pd.Series, stat_aggs, stat_kwargs, color_kwargs, **kwargs):\n        \"\"\"Create a heatmap trace for displaying KPI statistics.\n\n        Args:\n            series: Input timeseries data for statistics calculation.\n            stat_aggs: Dictionary of statistic names to aggregation functions.\n            stat_kwargs: Additional kwargs for the statistics trace.\n            color_kwargs: Color-related parameters (colorscale, zmin, zmax).\n            **kwargs: Additional plotly Heatmap parameters.\n\n        Returns:\n            Plotly Heatmap trace object displaying calculated statistics.\n        \"\"\"\n        data_stats = pd.Series({agg: func(series) for agg, func in stat_aggs.items()})\n        data_stats = data_stats.to_frame('stats')\n        data_stats = DataProcessor._prepend_empty_row(data_stats)\n\n        if 'ygap' not in stat_kwargs:\n            stat_kwargs['ygap'] = 5\n\n        text_data = data_stats.map(lambda x: f'{x:.0f}')\n        text_data = text_data.replace('nan', '').replace('null', '')\n\n        trace_kwargs = {**color_kwargs, **stat_kwargs, **kwargs}\n        trace_kwargs['showscale'] = False  # Stats should never have a colorbar\n\n        assert 'colorscale' in trace_kwargs and 'zmin' in trace_kwargs and 'zmax' in trace_kwargs\n\n        trace_stats = go.Heatmap(\n            z=data_stats.values,\n            x=data_stats.columns,\n            y=data_stats.index,\n            text=text_data.values,\n            texttemplate=\"%{text}\",\n            **trace_kwargs\n        )\n        return trace_stats\n\n    @staticmethod\n    def create_colorscale_trace(z_min, z_max, colorscale, orientation='v', title=None):\n        \"\"\"Create a colorscale trace for custom color legend display.\n\n        Args:\n            z_min: Minimum value for colorscale range.\n            z_max: Maximum value for colorscale range.\n            colorscale: Plotly colorscale specification.\n            orientation: Colorscale orientation ('v' for vertical, 'h' for horizontal).\n            title: Optional title for the colorscale.\n\n        Returns:\n            Plotly Heatmap trace object representing the colorscale legend.\n        \"\"\"\n        if orientation == 'v':\n            z_vals = np.linspace(z_min, z_max, 100).reshape(-1, 1)\n        else:\n            z_vals = np.linspace(z_min, z_max, 100).reshape(1, -1)\n\n        axis_vals = np.linspace(z_min, z_max, 100)\n\n        colorbar_settings = {\n            'thickness': 15,\n            'title': title or ''\n        }\n\n        if orientation == 'h':\n            colorbar_settings.update({\n                'orientation': 'h',\n                'y': -0.15,\n                'xanchor': 'center',\n                'x': 0.5\n            })\n            x = axis_vals\n            y = None\n        else:\n            x = None\n            y = axis_vals\n\n        return go.Heatmap(\n            x=x,\n            y=y,\n            z=z_vals,\n            colorscale=colorscale,\n            showscale=False,\n            zmin=z_min,\n            zmax=z_max,\n            colorbar=colorbar_settings\n        )\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/#mesqual.visualizations.plotly_figures.timeseries_dashboard.TraceGenerator.get_heatmap_trace","title":"get_heatmap_trace  <code>classmethod</code>","text":"<pre><code>get_heatmap_trace(data: DataFrame, ts_kwargs, color_kwargs, use_string_for_axis, **kwargs)\n</code></pre> <p>Create a heatmap trace for timeseries data visualization.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame with time categories as columns and hour-of-day as rows.</p> required <code>ts_kwargs</code> <p>Additional kwargs for the heatmap trace.</p> required <code>color_kwargs</code> <p>Color-related parameters (colorscale, zmin, zmax).</p> required <code>use_string_for_axis</code> <p>Whether to convert axis values to strings.</p> required <code>**kwargs</code> <p>Additional plotly Heatmap parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Plotly Heatmap trace object for timeseries visualization.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>@classmethod\ndef get_heatmap_trace(cls, data: pd.DataFrame, ts_kwargs, color_kwargs, use_string_for_axis, **kwargs):\n    \"\"\"Create a heatmap trace for timeseries data visualization.\n\n    Args:\n        data: DataFrame with time categories as columns and hour-of-day as rows.\n        ts_kwargs: Additional kwargs for the heatmap trace.\n        color_kwargs: Color-related parameters (colorscale, zmin, zmax).\n        use_string_for_axis: Whether to convert axis values to strings.\n        **kwargs: Additional plotly Heatmap parameters.\n\n    Returns:\n        Plotly Heatmap trace object for timeseries visualization.\n    \"\"\"\n    if set(data.columns).issubset(list(range(1, 13))):\n        x = [calendar.month_abbr[m] for m in range(1, 13)]\n    else:\n        if use_string_for_axis:\n            x = [str(i).replace('-', '_') for i in data.columns]\n        else:\n            x = data.columns\n\n    trace_kwargs = {**color_kwargs, **ts_kwargs, **kwargs}\n\n    assert 'colorscale' in trace_kwargs and 'zmin' in trace_kwargs and 'zmax' in trace_kwargs\n\n    trace_heatmap = go.Heatmap(x=x, z=data.values, y=data.index, **trace_kwargs)\n    return trace_heatmap\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/#mesqual.visualizations.plotly_figures.timeseries_dashboard.TraceGenerator.get_stats_trace","title":"get_stats_trace  <code>classmethod</code>","text":"<pre><code>get_stats_trace(series: Series, stat_aggs, stat_kwargs, color_kwargs, **kwargs)\n</code></pre> <p>Create a heatmap trace for displaying KPI statistics.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series</code> <p>Input timeseries data for statistics calculation.</p> required <code>stat_aggs</code> <p>Dictionary of statistic names to aggregation functions.</p> required <code>stat_kwargs</code> <p>Additional kwargs for the statistics trace.</p> required <code>color_kwargs</code> <p>Color-related parameters (colorscale, zmin, zmax).</p> required <code>**kwargs</code> <p>Additional plotly Heatmap parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Plotly Heatmap trace object displaying calculated statistics.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>@classmethod\ndef get_stats_trace(cls, series: pd.Series, stat_aggs, stat_kwargs, color_kwargs, **kwargs):\n    \"\"\"Create a heatmap trace for displaying KPI statistics.\n\n    Args:\n        series: Input timeseries data for statistics calculation.\n        stat_aggs: Dictionary of statistic names to aggregation functions.\n        stat_kwargs: Additional kwargs for the statistics trace.\n        color_kwargs: Color-related parameters (colorscale, zmin, zmax).\n        **kwargs: Additional plotly Heatmap parameters.\n\n    Returns:\n        Plotly Heatmap trace object displaying calculated statistics.\n    \"\"\"\n    data_stats = pd.Series({agg: func(series) for agg, func in stat_aggs.items()})\n    data_stats = data_stats.to_frame('stats')\n    data_stats = DataProcessor._prepend_empty_row(data_stats)\n\n    if 'ygap' not in stat_kwargs:\n        stat_kwargs['ygap'] = 5\n\n    text_data = data_stats.map(lambda x: f'{x:.0f}')\n    text_data = text_data.replace('nan', '').replace('null', '')\n\n    trace_kwargs = {**color_kwargs, **stat_kwargs, **kwargs}\n    trace_kwargs['showscale'] = False  # Stats should never have a colorbar\n\n    assert 'colorscale' in trace_kwargs and 'zmin' in trace_kwargs and 'zmax' in trace_kwargs\n\n    trace_stats = go.Heatmap(\n        z=data_stats.values,\n        x=data_stats.columns,\n        y=data_stats.index,\n        text=text_data.values,\n        texttemplate=\"%{text}\",\n        **trace_kwargs\n    )\n    return trace_stats\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/time_series_dashboard/#mesqual.visualizations.plotly_figures.timeseries_dashboard.TraceGenerator.create_colorscale_trace","title":"create_colorscale_trace  <code>staticmethod</code>","text":"<pre><code>create_colorscale_trace(z_min, z_max, colorscale, orientation='v', title=None)\n</code></pre> <p>Create a colorscale trace for custom color legend display.</p> <p>Parameters:</p> Name Type Description Default <code>z_min</code> <p>Minimum value for colorscale range.</p> required <code>z_max</code> <p>Maximum value for colorscale range.</p> required <code>colorscale</code> <p>Plotly colorscale specification.</p> required <code>orientation</code> <p>Colorscale orientation ('v' for vertical, 'h' for horizontal).</p> <code>'v'</code> <code>title</code> <p>Optional title for the colorscale.</p> <code>None</code> <p>Returns:</p> Type Description <p>Plotly Heatmap trace object representing the colorscale legend.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/plotly_figures/timeseries_dashboard.py</code> <pre><code>@staticmethod\ndef create_colorscale_trace(z_min, z_max, colorscale, orientation='v', title=None):\n    \"\"\"Create a colorscale trace for custom color legend display.\n\n    Args:\n        z_min: Minimum value for colorscale range.\n        z_max: Maximum value for colorscale range.\n        colorscale: Plotly colorscale specification.\n        orientation: Colorscale orientation ('v' for vertical, 'h' for horizontal).\n        title: Optional title for the colorscale.\n\n    Returns:\n        Plotly Heatmap trace object representing the colorscale legend.\n    \"\"\"\n    if orientation == 'v':\n        z_vals = np.linspace(z_min, z_max, 100).reshape(-1, 1)\n    else:\n        z_vals = np.linspace(z_min, z_max, 100).reshape(1, -1)\n\n    axis_vals = np.linspace(z_min, z_max, 100)\n\n    colorbar_settings = {\n        'thickness': 15,\n        'title': title or ''\n    }\n\n    if orientation == 'h':\n        colorbar_settings.update({\n            'orientation': 'h',\n            'y': -0.15,\n            'xanchor': 'center',\n            'x': 0.5\n        })\n        x = axis_vals\n        y = None\n    else:\n        x = None\n        y = axis_vals\n\n    return go.Heatmap(\n        x=x,\n        y=y,\n        z=z_vals,\n        colorscale=colorscale,\n        showscale=False,\n        zmin=z_min,\n        zmax=z_max,\n        colorbar=colorbar_settings\n    )\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/","title":"MESQUAL Folium Visualization System","text":"<p>Folium-based visualization system for MESQUAL energy system analysis.</p> <p>This module provides a comprehensive visualization framework built on Folium for creating interactive maps and geospatial visualizations of energy system data. The system supports multiple visualization types including areas, lines, markers, icons, and text overlays, with automatic feature resolution and generation capabilities.</p> <p>The visualization system follows a modular architecture with: - Base visualization components for property mapping and feature resolution - Specialized visualizers for KPI collections and data management - Multiple visualization feature types (areas, lines, markers, overlays) - Data item abstractions for model and KPI data integration</p> <p>Example:</p> <pre><code>Basic usage for creating area visualizations:\n&gt;&gt;&gt; from mesqual.visualizations.folium_viz_system import AreaGenerator, AreaFeatureResolver\n&gt;&gt;&gt; area_resolver = AreaFeatureResolver(property_mapper)\n&gt;&gt;&gt; area_generator = AreaGenerator()\n&gt;&gt;&gt; area_features = area_resolver.resolve_features(data_items)\n&gt;&gt;&gt; folium_map = area_generator.add_features_to_map(folium_map, area_features)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/areas/","title":"MESQUAL Folium Area Visualization System","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/areas/#mesqual.visualizations.folium_viz_system.viz_areas","title":"viz_areas","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/areas/#mesqual.visualizations.folium_viz_system.viz_areas.ResolvedAreaFeature","title":"ResolvedAreaFeature  <code>dataclass</code>","text":"<p>               Bases: <code>ResolvedFeature</code></p> <p>Resolved visual properties for area/polygon map elements.</p> <p>Container for all computed styling properties of polygon visualizations, including fill colors, border styles, and interactive behaviors. Used by AreaGenerator to create folium GeoJson polygons.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_areas.py</code> <pre><code>@dataclass\nclass ResolvedAreaFeature(ResolvedFeature):\n    \"\"\"\n    Resolved visual properties for area/polygon map elements.\n\n    Container for all computed styling properties of polygon visualizations,\n    including fill colors, border styles, and interactive behaviors.\n    Used by AreaGenerator to create folium GeoJson polygons.\n    \"\"\"\n\n    @property\n    def geometry(self) -&gt; Polygon | MultiPolygon:\n        return self.get('geometry')\n\n    @property\n    def fill_color(self) -&gt; str:\n        return self.get('fill_color')\n\n    @property\n    def border_color(self) -&gt; str:\n        return self.get('border_color')\n\n    @property\n    def border_width(self) -&gt; float:\n        return self.get('border_width')\n\n    @property\n    def fill_opacity(self) -&gt; float:\n        return min(self.get('fill_opacity'), 1.0)\n\n    @property\n    def highlight_border_width(self) -&gt; float:\n        highlight_border_width = self.get('highlight_border_width')\n        if highlight_border_width is None:\n            return self.border_width\n        return highlight_border_width\n\n    @property\n    def highlight_fill_opacity(self) -&gt; float:\n        return min(self.get('highlight_fill_opacity'), 1.0)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/areas/#mesqual.visualizations.folium_viz_system.viz_areas.AreaFeatureResolver","title":"AreaFeatureResolver","text":"<p>               Bases: <code>FeatureResolver[ResolvedAreaFeature]</code></p> <p>Resolves visual properties for polygon/area map elements.</p> <p>Specialized feature resolver for area visualizations that handles polygon geometries, fill colors, border styling, and opacity settings. Commonly used for visualizing bidding zones, geographic regions, or any spatial areas with associated data values.</p> <p>Parameters:</p> Name Type Description Default <code>fill_color</code> <code>PropertyMapper | str</code> <p>Area fill color (static value or PropertyMapper)</p> <code>'#D2D2D2'</code> <code>border_color</code> <code>PropertyMapper | str</code> <p>Border/stroke color (static value or PropertyMapper)  </p> <code>'white'</code> <code>border_width</code> <code>PropertyMapper | float</code> <p>Border width in pixels (static value or PropertyMapper)</p> <code>2.0</code> <code>fill_opacity</code> <code>PropertyMapper | float</code> <p>Fill transparency 0-1 (static value or PropertyMapper)</p> <code>0.8</code> <code>highlight_border_width</code> <code>PropertyMapper | float</code> <p>Border width on hover (defaults to border_width)</p> <code>None</code> <code>highlight_fill_opacity</code> <code>PropertyMapper | float</code> <p>Fill opacity on hover (defaults to 1.0)</p> <code>1.0</code> <code>tooltip</code> <code>PropertyMapper | str | bool</code> <p>Tooltip content (True for auto-generated, False for none)</p> <code>True</code> <code>popup</code> <code>PropertyMapper | Popup | bool</code> <p>Popup content (True/False/PropertyMapper)</p> <code>False</code> <code>geometry</code> <code>PropertyMapper | Polygon</code> <p>Polygon geometry (defaults to 'geometry' attribute)</p> <code>None</code> <code>**property_mappers</code> <code>PropertyMapper | Any</code> <p>Additional custom property mappings</p> <code>{}</code> <p>Examples:</p> <p>Basic area visualization:</p> <pre><code>&gt;&gt;&gt; resolver = AreaFeatureResolver(\n...     fill_color='#FF0000',\n...     border_color='white',\n...     fill_opacity=0.8\n... )\n</code></pre> <p>Data-driven coloring:</p> <pre><code>&gt;&gt;&gt; color_scale = SegmentedContinuousColorscale(...)\n&gt;&gt;&gt; resolver = AreaFeatureResolver(\n...     fill_color=PropertyMapper.from_kpi_value(color_scale),\n...     fill_opacity=PropertyMapper.from_item_attr('confidence', lambda c: 0.3 + 0.6 * c)\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_areas.py</code> <pre><code>class AreaFeatureResolver(FeatureResolver[ResolvedAreaFeature]):\n    \"\"\"\n    Resolves visual properties for polygon/area map elements.\n\n    Specialized feature resolver for area visualizations that handles polygon\n    geometries, fill colors, border styling, and opacity settings. Commonly\n    used for visualizing bidding zones, geographic regions, or any spatial\n    areas with associated data values.\n\n    Args:\n        fill_color: Area fill color (static value or PropertyMapper)\n        border_color: Border/stroke color (static value or PropertyMapper)  \n        border_width: Border width in pixels (static value or PropertyMapper)\n        fill_opacity: Fill transparency 0-1 (static value or PropertyMapper)\n        highlight_border_width: Border width on hover (defaults to border_width)\n        highlight_fill_opacity: Fill opacity on hover (defaults to 1.0)\n        tooltip: Tooltip content (True for auto-generated, False for none)\n        popup: Popup content (True/False/PropertyMapper)\n        geometry: Polygon geometry (defaults to 'geometry' attribute)\n        **property_mappers: Additional custom property mappings\n\n    Examples:\n        Basic area visualization:\n        &gt;&gt;&gt; resolver = AreaFeatureResolver(\n        ...     fill_color='#FF0000',\n        ...     border_color='white',\n        ...     fill_opacity=0.8\n        ... )\n\n        Data-driven coloring:\n        &gt;&gt;&gt; color_scale = SegmentedContinuousColorscale(...)\n        &gt;&gt;&gt; resolver = AreaFeatureResolver(\n        ...     fill_color=PropertyMapper.from_kpi_value(color_scale),\n        ...     fill_opacity=PropertyMapper.from_item_attr('confidence', lambda c: 0.3 + 0.6 * c)\n        ... )\n    \"\"\"\n    def __init__(\n            self,\n            fill_color: PropertyMapper | str = '#D2D2D2',\n            border_color: PropertyMapper | str = 'white',\n            border_width: PropertyMapper | float = 2.0,\n            fill_opacity: PropertyMapper | float = 0.8,\n            highlight_border_width: PropertyMapper | float = None,\n            highlight_fill_opacity: PropertyMapper | float = 1.0,\n            tooltip: PropertyMapper | str | bool = True,\n            popup: PropertyMapper | folium.Popup | bool = False,\n            geometry: PropertyMapper | Polygon = None,\n            **property_mappers: PropertyMapper | Any,\n    ):\n        mappers = dict(\n            fill_color=fill_color,\n            border_color=border_color,\n            border_width=border_width,\n            fill_opacity=fill_opacity,\n            highlight_border_width=highlight_border_width,\n            highlight_fill_opacity=highlight_fill_opacity,\n            tooltip=tooltip,\n            popup=popup,\n            geometry=self._explicit_or_fallback(geometry, self._default_geometry_mapper()),\n            **property_mappers\n        )\n        super().__init__(feature_type=ResolvedAreaFeature, **mappers)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/areas/#mesqual.visualizations.folium_viz_system.viz_areas.AreaGenerator","title":"AreaGenerator","text":"<p>               Bases: <code>FoliumObjectGenerator[AreaFeatureResolver]</code></p> <p>Generates folium GeoJson polygon objects for area visualizations.</p> <p>Creates interactive map polygons from data items with computed styling properties. Handles polygon and multipolygon geometries, applies styling, and adds tooltips/popups for user interaction.</p> <p>Commonly used for visualizing: - Bidding zones colored by electricity prices - Geographic regions sized by population or economic data - Network areas colored by KPI values - Administrative boundaries with associated statistics</p> <p>Examples:</p> <p>Basic usage pattern:</p> <pre><code>&gt;&gt;&gt; color_scale = SegmentedContinuousColorscale(...)\n&gt;&gt;&gt; generator = AreaGenerator(\n...     AreaFeatureResolver(\n...         fill_color=PropertyMapper.from_kpi_value(color_scale),\n...         tooltip=True\n...     )\n... )\n&gt;&gt;&gt; fg = folium.FeatureGroup('Bidding Zones')\n&gt;&gt;&gt; generator.generate_objects_for_kpi_collection(price_kpis, fg)\n&gt;&gt;&gt; fg.add_to(map)\n</code></pre> <p>Combined with other generators:</p> <pre><code>&gt;&gt;&gt; # Areas for zones, lines for interconnectors\n&gt;&gt;&gt; area_gen = AreaGenerator(...)\n&gt;&gt;&gt; line_gen = LineGenerator(...)\n&gt;&gt;&gt; \n&gt;&gt;&gt; area_gen.generate_objects_for_model_df(zones_df, feature_group)\n&gt;&gt;&gt; line_gen.generate_objects_for_model_df(borders_df, feature_group)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_areas.py</code> <pre><code>class AreaGenerator(FoliumObjectGenerator[AreaFeatureResolver]):\n    \"\"\"\n    Generates folium GeoJson polygon objects for area visualizations.\n\n    Creates interactive map polygons from data items with computed styling\n    properties. Handles polygon and multipolygon geometries, applies styling,\n    and adds tooltips/popups for user interaction.\n\n    Commonly used for visualizing:\n    - Bidding zones colored by electricity prices\n    - Geographic regions sized by population or economic data  \n    - Network areas colored by KPI values\n    - Administrative boundaries with associated statistics\n\n    Examples:\n        Basic usage pattern:\n        &gt;&gt;&gt; color_scale = SegmentedContinuousColorscale(...)\n        &gt;&gt;&gt; generator = AreaGenerator(\n        ...     AreaFeatureResolver(\n        ...         fill_color=PropertyMapper.from_kpi_value(color_scale),\n        ...         tooltip=True\n        ...     )\n        ... )\n        &gt;&gt;&gt; fg = folium.FeatureGroup('Bidding Zones')\n        &gt;&gt;&gt; generator.generate_objects_for_kpi_collection(price_kpis, fg)\n        &gt;&gt;&gt; fg.add_to(map)\n\n        Combined with other generators:\n        &gt;&gt;&gt; # Areas for zones, lines for interconnectors\n        &gt;&gt;&gt; area_gen = AreaGenerator(...)\n        &gt;&gt;&gt; line_gen = LineGenerator(...)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; area_gen.generate_objects_for_model_df(zones_df, feature_group)\n        &gt;&gt;&gt; line_gen.generate_objects_for_model_df(borders_df, feature_group)\n    \"\"\"\n    \"\"\"Generates folium GeoJson objects for area geometries.\"\"\"\n\n    def _feature_resolver_type(self) -&gt; Type[AreaFeatureResolver]:\n        return AreaFeatureResolver\n\n    def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n        \"\"\"\n        Generate and add a folium GeoJson polygon to the feature group.\n\n        Args:\n            data_item: Data item containing polygon geometry and associated data\n            feature_group: Folium feature group to add the polygon to\n        \"\"\"\n        style = self.feature_resolver.resolve_feature(data_item)\n\n        geometry = style.geometry\n        if not isinstance(geometry, (Polygon, MultiPolygon)):\n            return\n\n        style_dict = {\n            'fillColor': style.fill_color,\n            'color': style.border_color,\n            'weight': style.border_width,\n            'fillOpacity': style.fill_opacity\n        }\n\n        highlight_dict = style_dict.copy()\n        highlight_dict['weight'] = style.highlight_border_width\n        highlight_dict['fillOpacity'] = style.highlight_fill_opacity\n\n        geojson_data = {\n            \"type\": \"Feature\",\n            \"geometry\": geometry.__geo_interface__,\n            \"properties\": {\"tooltip\": style.tooltip}\n        }\n\n        folium.GeoJson(\n            geojson_data,\n            style_function=lambda x, s=style_dict: s,\n            highlight_function=lambda x, h=highlight_dict: h,\n            tooltip=folium.GeoJsonTooltip(fields=['tooltip'], aliases=[''], sticky=True) if style.tooltip else None,\n            popup=style.popup\n        ).add_to(feature_group)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/areas/#mesqual.visualizations.folium_viz_system.viz_areas.AreaGenerator.generate","title":"generate","text":"<pre><code>generate(data_item: VisualizableDataItem, feature_group: FeatureGroup) -&gt; None\n</code></pre> <p>Generate and add a folium GeoJson polygon to the feature group.</p> <p>Parameters:</p> Name Type Description Default <code>data_item</code> <code>VisualizableDataItem</code> <p>Data item containing polygon geometry and associated data</p> required <code>feature_group</code> <code>FeatureGroup</code> <p>Folium feature group to add the polygon to</p> required Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_areas.py</code> <pre><code>def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n    \"\"\"\n    Generate and add a folium GeoJson polygon to the feature group.\n\n    Args:\n        data_item: Data item containing polygon geometry and associated data\n        feature_group: Folium feature group to add the polygon to\n    \"\"\"\n    style = self.feature_resolver.resolve_feature(data_item)\n\n    geometry = style.geometry\n    if not isinstance(geometry, (Polygon, MultiPolygon)):\n        return\n\n    style_dict = {\n        'fillColor': style.fill_color,\n        'color': style.border_color,\n        'weight': style.border_width,\n        'fillOpacity': style.fill_opacity\n    }\n\n    highlight_dict = style_dict.copy()\n    highlight_dict['weight'] = style.highlight_border_width\n    highlight_dict['fillOpacity'] = style.highlight_fill_opacity\n\n    geojson_data = {\n        \"type\": \"Feature\",\n        \"geometry\": geometry.__geo_interface__,\n        \"properties\": {\"tooltip\": style.tooltip}\n    }\n\n    folium.GeoJson(\n        geojson_data,\n        style_function=lambda x, s=style_dict: s,\n        highlight_function=lambda x, h=highlight_dict: h,\n        tooltip=folium.GeoJsonTooltip(fields=['tooltip'], aliases=[''], sticky=True) if style.tooltip else None,\n        popup=style.popup\n    ).add_to(feature_group)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/arrow_icon/","title":"MESQUAL Folium Area Visualization System","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/arrow_icon/#mesqual.visualizations.folium_viz_system.viz_arrow_icon","title":"viz_arrow_icon","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/arrow_icon/#mesqual.visualizations.folium_viz_system.viz_arrow_icon.ResolvedArrowIconFeature","title":"ResolvedArrowIconFeature  <code>dataclass</code>","text":"<p>               Bases: <code>ResolvedFeature</code></p> <p>Resolved visual properties for animated arrow icon elements.</p> <p>Container for all computed styling properties of arrow icon visualizations, including position, orientation, colors, animation settings, and size. Used by ArrowIconGenerator to create folium markers with SVG arrow icons.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_arrow_icon.py</code> <pre><code>@dataclass\nclass ResolvedArrowIconFeature(ResolvedFeature):\n    \"\"\"\n    Resolved visual properties for animated arrow icon elements.\n\n    Container for all computed styling properties of arrow icon visualizations,\n    including position, orientation, colors, animation settings, and size.\n    Used by ArrowIconGenerator to create folium markers with SVG arrow icons.\n    \"\"\"\n    @property\n    def location(self) -&gt; Point:\n        return self.get('location')\n\n    @property\n    def offset(self) -&gt; tuple[float, float]:\n        return self.get('offset')\n\n    @property\n    def azimuth_angle(self) -&gt; float:\n        return self.get('azimuth_angle')\n\n    @property\n    def arrow_type(self) -&gt; 'ArrowTypeEnum':\n        return self.get('arrow_type')\n\n    @property\n    def color(self) -&gt; str:\n        return self.get('color')\n\n    @property\n    def stroke_width(self) -&gt; int:\n        return self.get('stroke_width')\n\n    @property\n    def width(self) -&gt; int:\n        return self.get('width')\n\n    @property\n    def height(self) -&gt; int:\n        return self.get('height')\n\n    @property\n    def speed_in_px_per_second(self) -&gt; float:\n        return self.get('speed_in_px_per_second')\n\n    @property\n    def speed_in_duration_seconds(self) -&gt; float:\n        return self.get('speed_in_duration_seconds')\n\n    @property\n    def num_arrows(self) -&gt; int:\n        return self.get('num_arrows')\n\n    @property\n    def opacity(self) -&gt; float:\n        return self.get('opacity')\n\n    @property\n    def reverse_direction(self) -&gt; bool:\n        return self.get('reverse_direction')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/arrow_icon/#mesqual.visualizations.folium_viz_system.viz_arrow_icon.ArrowIconFeatureResolver","title":"ArrowIconFeatureResolver","text":"<p>               Bases: <code>FeatureResolver[ResolvedArrowIconFeature]</code></p> <p>Resolves visual properties for animated arrow icon elements.</p> <p>Specialized feature resolver for arrow icon visualizations that handles point locations, arrow styling, animation parameters, and directional indicators. Commonly used for visualizing directional flows, network connections, or any point-based data with directional significance.</p> <p>Integrates with the captain_arro library to provide various arrow types and animation effects for dynamic flow visualization.</p> <p>Parameters:</p> Name Type Description Default <code>arrow_type</code> <code>Union[PropertyMapper, ArrowTypeEnum]</code> <p>Type of arrow from ArrowTypeEnum (static value or PropertyMapper)</p> <code>None</code> <code>color</code> <code>PropertyMapper | str</code> <p>Arrow color (static value or PropertyMapper)</p> <code>'#2563eb'</code> <code>stroke_width</code> <code>PropertyMapper | int</code> <p>Arrow stroke width in pixels (static value or PropertyMapper)</p> <code>8</code> <code>width</code> <code>PropertyMapper | int</code> <p>Arrow icon width in pixels (static value or PropertyMapper)</p> <code>60</code> <code>height</code> <code>PropertyMapper | int</code> <p>Arrow icon height in pixels (static value or PropertyMapper)</p> <code>60</code> <code>speed_in_px_per_second</code> <code>PropertyMapper | float | None</code> <p>Animation speed in pixels/second (static value or PropertyMapper)</p> <code>20.0</code> <code>speed_in_duration_seconds</code> <code>PropertyMapper | float | None</code> <p>Animation duration in seconds (static value or PropertyMapper)</p> <code>None</code> <code>num_arrows</code> <code>PropertyMapper | int</code> <p>Number of animated arrows (static value or PropertyMapper)</p> <code>3</code> <code>opacity</code> <code>PropertyMapper | float</code> <p>Icon opacity 0-1 (static value or PropertyMapper)</p> <code>0.8</code> <code>reverse_direction</code> <code>PropertyMapper | bool</code> <p>Reverse arrow direction (static value or PropertyMapper)</p> <code>False</code> <code>tooltip</code> <code>PropertyMapper | str | bool</code> <p>Tooltip content (True for auto-generated, False for none)</p> <code>True</code> <code>popup</code> <code>PropertyMapper | Popup | bool</code> <p>Popup content (True/False/PropertyMapper)</p> <code>False</code> <code>location</code> <code>PropertyMapper | Point</code> <p>Point location (defaults to smart location detection)</p> <code>None</code> <code>rotation_angle</code> <p>Arrow rotation angle in degrees (static value or PropertyMapper)</p> required <code>**property_mappers</code> <code>PropertyMapper | Any</code> <p>Additional custom property mappings</p> <code>{}</code> <p>Examples:</p> <pre><code>Basic directional flow arrows:\n&gt;&gt;&gt; from captain_arro import ArrowTypeEnum\n&gt;&gt;&gt; resolver = ArrowIconFeatureResolver(\n...     arrow_type=ArrowTypeEnum.MOVING_FLOW_ARROW,\n...     color='#FF0000',\n...     width=50,\n...     height=30\n... )\n\nData-driven flow visualization:\n&gt;&gt;&gt; flow_color_scale = SegmentedContinuousColorscale(...)\n&gt;&gt;&gt; size_scale = SegmentedContinuousInputToContinuousOutputMapping(...)\n&gt;&gt;&gt; resolver = ArrowIconFeatureResolver(\n...     arrow_type=PropertyMapper.from_kpi_value(\n...         lambda v: ArrowTypeEnum.SPOTLIGHT_FLOW_ARROW if abs(v) &gt; 100 \n...                   else ArrowTypeEnum.BOUNCING_SPREAD_ARROW\n...     ),\n...     color=PropertyMapper.from_kpi_value(flow_color_scale),\n...     width=PropertyMapper.from_kpi_value(size_scale),\n...     reverse_direction=PropertyMapper.from_kpi_value(lambda v: v &lt; 0),\n...     rotation_angle=PropertyMapper.from_item_attr('azimuth_angle')\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_arrow_icon.py</code> <pre><code>class ArrowIconFeatureResolver(FeatureResolver[ResolvedArrowIconFeature]):\n    \"\"\"\n    Resolves visual properties for animated arrow icon elements.\n\n    Specialized feature resolver for arrow icon visualizations that handles point\n    locations, arrow styling, animation parameters, and directional indicators.\n    Commonly used for visualizing directional flows, network connections, or\n    any point-based data with directional significance.\n\n    Integrates with the captain_arro library to provide various arrow types\n    and animation effects for dynamic flow visualization.\n\n    Args:\n        arrow_type: Type of arrow from ArrowTypeEnum (static value or PropertyMapper)\n        color: Arrow color (static value or PropertyMapper)\n        stroke_width: Arrow stroke width in pixels (static value or PropertyMapper)\n        width: Arrow icon width in pixels (static value or PropertyMapper)\n        height: Arrow icon height in pixels (static value or PropertyMapper)\n        speed_in_px_per_second: Animation speed in pixels/second (static value or PropertyMapper)\n        speed_in_duration_seconds: Animation duration in seconds (static value or PropertyMapper)\n        num_arrows: Number of animated arrows (static value or PropertyMapper)\n        opacity: Icon opacity 0-1 (static value or PropertyMapper)\n        reverse_direction: Reverse arrow direction (static value or PropertyMapper)\n        tooltip: Tooltip content (True for auto-generated, False for none)\n        popup: Popup content (True/False/PropertyMapper)\n        location: Point location (defaults to smart location detection)\n        rotation_angle: Arrow rotation angle in degrees (static value or PropertyMapper)\n        **property_mappers: Additional custom property mappings\n\n    Examples:\n\n        Basic directional flow arrows:\n        &gt;&gt;&gt; from captain_arro import ArrowTypeEnum\n        &gt;&gt;&gt; resolver = ArrowIconFeatureResolver(\n        ...     arrow_type=ArrowTypeEnum.MOVING_FLOW_ARROW,\n        ...     color='#FF0000',\n        ...     width=50,\n        ...     height=30\n        ... )\n\n        Data-driven flow visualization:\n        &gt;&gt;&gt; flow_color_scale = SegmentedContinuousColorscale(...)\n        &gt;&gt;&gt; size_scale = SegmentedContinuousInputToContinuousOutputMapping(...)\n        &gt;&gt;&gt; resolver = ArrowIconFeatureResolver(\n        ...     arrow_type=PropertyMapper.from_kpi_value(\n        ...         lambda v: ArrowTypeEnum.SPOTLIGHT_FLOW_ARROW if abs(v) &gt; 100 \n        ...                   else ArrowTypeEnum.BOUNCING_SPREAD_ARROW\n        ...     ),\n        ...     color=PropertyMapper.from_kpi_value(flow_color_scale),\n        ...     width=PropertyMapper.from_kpi_value(size_scale),\n        ...     reverse_direction=PropertyMapper.from_kpi_value(lambda v: v &lt; 0),\n        ...     rotation_angle=PropertyMapper.from_item_attr('azimuth_angle')\n        ... )\n    \"\"\"\n    def __init__(\n            self,\n            arrow_type: Union[PropertyMapper, 'ArrowTypeEnum'] = None,\n            color: PropertyMapper | str = '#2563eb',\n            stroke_width: PropertyMapper | int = 8,\n            width: PropertyMapper | int = 60,\n            height: PropertyMapper | int = 60,\n            speed_in_px_per_second: PropertyMapper | float | None = 20.0,\n            speed_in_duration_seconds: PropertyMapper | float | None = None,\n            num_arrows: PropertyMapper | int = 3,\n            opacity: PropertyMapper | float = 0.8,\n            reverse_direction: PropertyMapper | bool = False,\n            tooltip: PropertyMapper | str | bool = True,\n            popup: PropertyMapper | folium.Popup | bool = False,\n            location: PropertyMapper | Point = None,\n            offset: PropertyMapper | tuple[float, float] = (0, 0),\n            azimuth_angle: PropertyMapper | float = None,\n            **property_mappers: PropertyMapper | Any,\n    ):\n        from captain_arro import ArrowTypeEnum\n        mappers = dict(\n            arrow_type=arrow_type or ArrowTypeEnum.MOVING_FLOW_ARROW,\n            color=color,\n            stroke_width=stroke_width,\n            width=width,\n            height=height,\n            speed_in_px_per_second=speed_in_px_per_second,\n            speed_in_duration_seconds=speed_in_duration_seconds,\n            num_arrows=num_arrows,\n            opacity=opacity,\n            reverse_direction=reverse_direction,\n            tooltip=tooltip,\n            popup=popup,\n            location=self._explicit_or_fallback(location, self._default_location_mapper()),\n            offset=offset,\n            azimuth_angle=self._explicit_or_fallback(azimuth_angle, self._default_rotation_angle_mapper()),\n            **property_mappers\n        )\n        super().__init__(feature_type=ResolvedArrowIconFeature, **mappers)\n\n    @staticmethod\n    def _default_rotation_angle_mapper() -&gt; PropertyMapper:\n\n        def get_rotation_angle(data_item: VisualizableDataItem) -&gt; float | None:\n            for k in ['azimuth_angle', 'rotation_angle', 'projection_angle']:\n                if data_item.object_has_attribute(k):\n                    angle = data_item.get_object_attribute(k)\n                    return angle\n            return None\n\n        return PropertyMapper(get_rotation_angle)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/arrow_icon/#mesqual.visualizations.folium_viz_system.viz_arrow_icon.ArrowIconGenerator","title":"ArrowIconGenerator","text":"<p>               Bases: <code>FoliumObjectGenerator[ArrowIconFeatureResolver]</code></p> <p>Generates folium Marker objects with animated SVG arrow icons.</p> <p>Creates interactive map markers featuring animated arrow icons from the captain_arro library. Handles SVG generation, base64 encoding, rotation, and integration with folium's marker system.</p> <p>Commonly used for visualizing: - Directional border flow directions with magnitude-based styling - Border price-spread with magnitude-based styling - Any point-based directional data with animation needs</p> <p>Examples:</p> <p>Power flow visualization:</p> <pre><code>&gt;&gt;&gt; from captain_arro import ArrowTypeEnum\n&gt;&gt;&gt; flow_color_scale = SegmentedContinuousColorscale(...)\n&gt;&gt;&gt; size_mapping = SegmentedContinuousInputToContinuousOutputMapping(...)\n&gt;&gt;&gt; \n&gt;&gt;&gt; generator = ArrowIconGenerator(\n...     ArrowIconFeatureResolver(\n...         arrow_type=PropertyMapper.from_kpi_value(\n...             lambda v: ArrowTypeEnum.MOVING_FLOW_ARROW if abs(v) &gt; 50\n...                       else ArrowTypeEnum.BOUNCING_SPREAD_ARROW\n...         ),\n...         color=PropertyMapper.from_kpi_value(lambda v: flow_color_scale(abs(v))),\n...         width=PropertyMapper.from_kpi_value(lambda v: size_mapping(abs(v))),\n...         reverse_direction=PropertyMapper.from_kpi_value(lambda v: v &lt; 0),\n...         rotation_angle=PropertyMapper.from_item_attr('azimuth_angle'),\n...         speed_in_duration_seconds=4\n...     )\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; fg = folium.FeatureGroup('Flow Arrows')\n&gt;&gt;&gt; generator.generate_objects_for_kpi_collection(flow_kpis, fg)\n&gt;&gt;&gt; fg.add_to(map)\n</code></pre> <p>Border flow indicators:</p> <pre><code>&gt;&gt;&gt; generator.generate_objects_for_model_df(border_df, feature_group)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_arrow_icon.py</code> <pre><code>class ArrowIconGenerator(FoliumObjectGenerator[ArrowIconFeatureResolver]):\n    \"\"\"\n    Generates folium Marker objects with animated SVG arrow icons.\n\n    Creates interactive map markers featuring animated arrow icons from the\n    captain_arro library. Handles SVG generation, base64 encoding, rotation,\n    and integration with folium's marker system.\n\n    Commonly used for visualizing:\n    - Directional border flow directions with magnitude-based styling\n    - Border price-spread with magnitude-based styling\n    - Any point-based directional data with animation needs\n\n    Examples:\n        Power flow visualization:\n        &gt;&gt;&gt; from captain_arro import ArrowTypeEnum\n        &gt;&gt;&gt; flow_color_scale = SegmentedContinuousColorscale(...)\n        &gt;&gt;&gt; size_mapping = SegmentedContinuousInputToContinuousOutputMapping(...)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; generator = ArrowIconGenerator(\n        ...     ArrowIconFeatureResolver(\n        ...         arrow_type=PropertyMapper.from_kpi_value(\n        ...             lambda v: ArrowTypeEnum.MOVING_FLOW_ARROW if abs(v) &gt; 50\n        ...                       else ArrowTypeEnum.BOUNCING_SPREAD_ARROW\n        ...         ),\n        ...         color=PropertyMapper.from_kpi_value(lambda v: flow_color_scale(abs(v))),\n        ...         width=PropertyMapper.from_kpi_value(lambda v: size_mapping(abs(v))),\n        ...         reverse_direction=PropertyMapper.from_kpi_value(lambda v: v &lt; 0),\n        ...         rotation_angle=PropertyMapper.from_item_attr('azimuth_angle'),\n        ...         speed_in_duration_seconds=4\n        ...     )\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; fg = folium.FeatureGroup('Flow Arrows')\n        &gt;&gt;&gt; generator.generate_objects_for_kpi_collection(flow_kpis, fg)\n        &gt;&gt;&gt; fg.add_to(map)\n\n        Border flow indicators:\n        &gt;&gt;&gt; generator.generate_objects_for_model_df(border_df, feature_group)\n    \"\"\"\n    def _feature_resolver_type(self) -&gt; Type[ArrowIconFeatureResolver]:\n        return ArrowIconFeatureResolver\n\n    def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n        \"\"\"\n        Generate and add a folium Marker with animated SVG arrow icon.\n\n        Args:\n            data_item: Data item containing point location and associated data\n            feature_group: Folium feature group to add the arrow marker to\n        \"\"\"\n        style = self.feature_resolver.resolve_feature(data_item)\n        if style.location is None:\n            return\n\n        svg_content = self._generate_arrow_svg(style, data_item)\n        encoded_svg = base64.b64encode(svg_content.encode()).decode()\n\n        angle = float(style.azimuth_angle) - 90  # Folium counts clockwise from right-pointing direction; normal convention is CCW\n        x_offset, y_offset = style.offset\n\n        icon_html = f'''\n            &lt;div style=\"\n                position: absolute;\n                left: 50%;\n                top: 50%;\n                transform: translate(-50%, -50%) translate({x_offset}px, {-y_offset}px) rotate({angle}deg);\n                opacity: {style.opacity};\n            \"&gt;\n                &lt;img src=\"data:image/svg+xml;base64,{encoded_svg}\" \n                     width=\"{style.width}\" \n                     height=\"{style.height}\"&gt;\n                &lt;/img&gt;\n            &lt;/div&gt;\n        '''\n\n        icon = folium.DivIcon(\n            html=icon_html,\n            icon_size=(style.width, style.height),\n            icon_anchor=(style.width // 2, style.height // 2)\n        )\n        folium.Marker(\n            location=(style.location.y, style.location.x),\n            icon=icon,\n            tooltip=style.tooltip,\n            popup=style.popup\n        ).add_to(feature_group)\n\n    def _generate_arrow_svg(self, style: ResolvedArrowIconFeature, data_item: VisualizableDataItem) -&gt; str:\n        from inspect import signature\n        from captain_arro import get_generator_for_arrow_type\n\n        def safe_init(cls: type, kwargs: dict):\n            init_params = signature(cls.__init__).parameters\n            accepted_keys = {\n                k for k in init_params\n                if k != 'self' and init_params[k].kind in (\n                init_params[k].POSITIONAL_OR_KEYWORD, init_params[k].KEYWORD_ONLY)\n            }\n            filtered_kwargs = {k: v for k, v in kwargs.items() if k in accepted_keys}\n            return cls(**filtered_kwargs)\n\n        arrow_type = style.arrow_type\n        generator_class = get_generator_for_arrow_type(arrow_type)\n        arrow_style_kwargs = style.to_dict()\n        if 'direction' not in arrow_style_kwargs:\n            arrow_style_kwargs['direction'] = 'left' if style.reverse_direction else 'right'\n        return safe_init(generator_class, arrow_style_kwargs).generate_svg(unique_id=True)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/arrow_icon/#mesqual.visualizations.folium_viz_system.viz_arrow_icon.ArrowIconGenerator.generate","title":"generate","text":"<pre><code>generate(data_item: VisualizableDataItem, feature_group: FeatureGroup) -&gt; None\n</code></pre> <p>Generate and add a folium Marker with animated SVG arrow icon.</p> <p>Parameters:</p> Name Type Description Default <code>data_item</code> <code>VisualizableDataItem</code> <p>Data item containing point location and associated data</p> required <code>feature_group</code> <code>FeatureGroup</code> <p>Folium feature group to add the arrow marker to</p> required Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_arrow_icon.py</code> <pre><code>def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n    \"\"\"\n    Generate and add a folium Marker with animated SVG arrow icon.\n\n    Args:\n        data_item: Data item containing point location and associated data\n        feature_group: Folium feature group to add the arrow marker to\n    \"\"\"\n    style = self.feature_resolver.resolve_feature(data_item)\n    if style.location is None:\n        return\n\n    svg_content = self._generate_arrow_svg(style, data_item)\n    encoded_svg = base64.b64encode(svg_content.encode()).decode()\n\n    angle = float(style.azimuth_angle) - 90  # Folium counts clockwise from right-pointing direction; normal convention is CCW\n    x_offset, y_offset = style.offset\n\n    icon_html = f'''\n        &lt;div style=\"\n            position: absolute;\n            left: 50%;\n            top: 50%;\n            transform: translate(-50%, -50%) translate({x_offset}px, {-y_offset}px) rotate({angle}deg);\n            opacity: {style.opacity};\n        \"&gt;\n            &lt;img src=\"data:image/svg+xml;base64,{encoded_svg}\" \n                 width=\"{style.width}\" \n                 height=\"{style.height}\"&gt;\n            &lt;/img&gt;\n        &lt;/div&gt;\n    '''\n\n    icon = folium.DivIcon(\n        html=icon_html,\n        icon_size=(style.width, style.height),\n        icon_anchor=(style.width // 2, style.height // 2)\n    )\n    folium.Marker(\n        location=(style.location.y, style.location.x),\n        icon=icon,\n        tooltip=style.tooltip,\n        popup=style.popup\n    ).add_to(feature_group)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/circle_marker/","title":"MESQUAL Folium CircleMarker Visualization System","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/circle_marker/#mesqual.visualizations.folium_viz_system.viz_circle_marker","title":"viz_circle_marker","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/circle_marker/#mesqual.visualizations.folium_viz_system.viz_circle_marker.ResolvedCircleMarkerFeature","title":"ResolvedCircleMarkerFeature  <code>dataclass</code>","text":"<p>               Bases: <code>ResolvedFeature</code></p> <p>Specialized style container for circle marker visualizations.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_circle_marker.py</code> <pre><code>@dataclass\nclass ResolvedCircleMarkerFeature(ResolvedFeature):\n    \"\"\"Specialized style container for circle marker visualizations.\"\"\"\n\n    @property\n    def location(self) -&gt; Point:\n        return self.get('location')\n\n    @property\n    def fill_color(self) -&gt; str:\n        return self.get('fill_color')\n\n    @property\n    def border_color(self) -&gt; str:\n        return self.get('border_color')\n\n    @property\n    def radius(self) -&gt; float:\n        return self.get('radius')\n\n    @property\n    def border_width(self) -&gt; float:\n        return self.get('border_width')\n\n    @property\n    def fill_opacity(self) -&gt; float:\n        return self.get('fill_opacity')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/circle_marker/#mesqual.visualizations.folium_viz_system.viz_circle_marker.CircleMarkerGenerator","title":"CircleMarkerGenerator","text":"<p>               Bases: <code>FoliumObjectGenerator[CircleMarkerFeatureResolver]</code></p> <p>Generates folium CircleMarker objects for point geometries.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_circle_marker.py</code> <pre><code>class CircleMarkerGenerator(FoliumObjectGenerator[CircleMarkerFeatureResolver]):\n    \"\"\"Generates folium CircleMarker objects for point geometries.\"\"\"\n\n    def _feature_resolver_type(self) -&gt; Type[CircleMarkerFeatureResolver]:\n        return CircleMarkerFeatureResolver\n\n    def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n        style = self.feature_resolver.resolve_feature(data_item)\n        if style.location is None:\n            return\n\n        folium.CircleMarker(\n            location=(style.location.y, style.location.x),\n            tooltip=style.tooltip,\n            popup=style.popup,\n            radius=style.radius,\n            color=style.border_color,\n            fillColor=style.fill_color,\n            fillOpacity=style.fill_opacity,\n            weight=style.border_width,\n        ).add_to(feature_group)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/data_item/","title":"MESQUAL Folium-Visualizable Data Item","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mesqual.visualizations.folium_viz_system.visualizable_data_item","title":"visualizable_data_item","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mesqual.visualizations.folium_viz_system.visualizable_data_item.VisualizableDataItem","title":"VisualizableDataItem","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract interface for data items that can be visualized on maps.</p> <p>Defines the contract for all data items that can be processed by the folium visualization system. Provides attribute access, tooltip data generation, and text representation for map elements.</p> <p>This abstraction enables polymorphic handling of different data sources (model DataFrames, KPI objects, custom data) within the visualization pipeline while maintaining consistent interface expectations.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>class VisualizableDataItem(ABC):\n    \"\"\"\n    Abstract interface for data items that can be visualized on maps.\n\n    Defines the contract for all data items that can be processed by the\n    folium visualization system. Provides attribute access, tooltip data\n    generation, and text representation for map elements.\n\n    This abstraction enables polymorphic handling of different data sources\n    (model DataFrames, KPI objects, custom data) within the visualization\n    pipeline while maintaining consistent interface expectations.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n    @abstractmethod\n    def get_name(self) -&gt; str:\n        \"\"\"Get a representative name for the object.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_text_representation(self) -&gt; str:\n        \"\"\"Get a representative text for the object.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_tooltip_data(self, **kwargs) -&gt; dict:\n        \"\"\"Get data for tooltip display.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_object_attribute(self, attribute: str) -&gt; Any:\n        \"\"\"Get value for styling from specified column.\"\"\"\n        pass\n\n    @abstractmethod\n    def object_has_attribute(self, attribute: str) -&gt; bool:\n        pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mesqual.visualizations.folium_viz_system.visualizable_data_item.VisualizableDataItem.get_name","title":"get_name  <code>abstractmethod</code>","text":"<pre><code>get_name() -&gt; str\n</code></pre> <p>Get a representative name for the object.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>@abstractmethod\ndef get_name(self) -&gt; str:\n    \"\"\"Get a representative name for the object.\"\"\"\n    pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mesqual.visualizations.folium_viz_system.visualizable_data_item.VisualizableDataItem.get_text_representation","title":"get_text_representation  <code>abstractmethod</code>","text":"<pre><code>get_text_representation() -&gt; str\n</code></pre> <p>Get a representative text for the object.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>@abstractmethod\ndef get_text_representation(self) -&gt; str:\n    \"\"\"Get a representative text for the object.\"\"\"\n    pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mesqual.visualizations.folium_viz_system.visualizable_data_item.VisualizableDataItem.get_tooltip_data","title":"get_tooltip_data  <code>abstractmethod</code>","text":"<pre><code>get_tooltip_data(**kwargs) -&gt; dict\n</code></pre> <p>Get data for tooltip display.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>@abstractmethod\ndef get_tooltip_data(self, **kwargs) -&gt; dict:\n    \"\"\"Get data for tooltip display.\"\"\"\n    pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mesqual.visualizations.folium_viz_system.visualizable_data_item.VisualizableDataItem.get_object_attribute","title":"get_object_attribute  <code>abstractmethod</code>","text":"<pre><code>get_object_attribute(attribute: str) -&gt; Any\n</code></pre> <p>Get value for styling from specified column.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>@abstractmethod\ndef get_object_attribute(self, attribute: str) -&gt; Any:\n    \"\"\"Get value for styling from specified column.\"\"\"\n    pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mesqual.visualizations.folium_viz_system.visualizable_data_item.ModelDataItem","title":"ModelDataItem","text":"<p>               Bases: <code>VisualizableDataItem</code></p> <p>Visualizable data item for model DataFrame rows.</p> <p>Wraps pandas Series objects (DataFrame rows) to provide the VisualizableDataItem interface. Commonly used for visualizing static model data like network topology, geographic boundaries, or reference datasets.</p> <p>Handles attribute access from DataFrame columns, provides meaningful object naming, and generates informative tooltips from available data.</p> <p>Parameters:</p> Name Type Description Default <code>object_data</code> <code>Series</code> <p>Pandas Series representing a DataFrame row</p> required <code>object_type</code> <code>str | Any</code> <p>Optional type identifier for the object</p> <code>None</code> <code>**kwargs</code> <p>Additional attributes to set on the data item</p> <code>{}</code> <p>Examples:</p> <p>Typical usage in generators:</p> <pre><code>&gt;&gt;&gt; for _, row in model_df.iterrows():\n...     data_item = ModelDataItem(row, object_type='BiddingZone')\n...     generator.generate(data_item, feature_group)\n</code></pre> <p>Access pattern:</p> <pre><code>&gt;&gt;&gt; data_item.get_object_attribute('geometry')  # From DataFrame column\n&gt;&gt;&gt; data_item.get_name()  # Object identifier\n&gt;&gt;&gt; data_item.get_tooltip_data()  # All available data for tooltip\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>class ModelDataItem(VisualizableDataItem):\n    \"\"\"\n    Visualizable data item for model DataFrame rows.\n\n    Wraps pandas Series objects (DataFrame rows) to provide the VisualizableDataItem\n    interface. Commonly used for visualizing static model data like network\n    topology, geographic boundaries, or reference datasets.\n\n    Handles attribute access from DataFrame columns, provides meaningful object\n    naming, and generates informative tooltips from available data.\n\n    Args:\n        object_data: Pandas Series representing a DataFrame row\n        object_type: Optional type identifier for the object\n        **kwargs: Additional attributes to set on the data item\n\n    Examples:\n        Typical usage in generators:\n        &gt;&gt;&gt; for _, row in model_df.iterrows():\n        ...     data_item = ModelDataItem(row, object_type='BiddingZone')\n        ...     generator.generate(data_item, feature_group)\n\n        Access pattern:\n        &gt;&gt;&gt; data_item.get_object_attribute('geometry')  # From DataFrame column\n        &gt;&gt;&gt; data_item.get_name()  # Object identifier\n        &gt;&gt;&gt; data_item.get_tooltip_data()  # All available data for tooltip\n    \"\"\"\n    OBJECT_NAME_COLUMNS = ['name', 'object_id', 'index', 'object_name']\n\n    def __init__(self, object_data: pd.Series, object_type: str | Any = None, **kwargs):\n        self.object_data = object_data\n        self.object_id = object_data.name\n        self.object_type = object_type\n        self.name_attributes = self.OBJECT_NAME_COLUMNS + [self.object_type]\n        super().__init__(**kwargs)\n\n    def get_name(self) -&gt; str:\n        return str(self.object_id)\n\n    def get_text_representation(self) -&gt; str:\n        return self.get_name()\n\n    def get_tooltip_data(self, **kwargs) -&gt; dict:\n        data = {'ID': self.object_id}\n        for col, value in self.object_data.items():\n            if pd.notna(value):\n                value_str = str(value)\n                if len(value_str) &gt; 50:\n                    value_str = value_str[:47] + \"...\"\n                data[col] = value_str\n        return data\n\n    def get_object_attribute(self, attribute: str) -&gt; Any:\n        if (attribute in self.name_attributes) and (attribute not in self.object_data):\n            return self.get_name()\n        return self.object_data.get(attribute)\n\n    def object_has_attribute(self, attribute: str) -&gt; bool:\n        return (attribute in self.object_data) or (attribute in self.name_attributes)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mesqual.visualizations.folium_viz_system.visualizable_data_item.KPIDataItem","title":"KPIDataItem","text":"<p>               Bases: <code>VisualizableDataItem</code></p> <p>Visualizable data item for KPI objects.</p> <p>Wraps MESQUAL KPI objects to provide the VisualizableDataItem interface. Combines KPI values with associated model object information for rich map visualization of computed energy system metrics.</p> <p>Handles attribute access from both KPI values and underlying model objects, provides formatted value representations, and generates enhanced tooltips showing both KPI information and object details.</p> <p>Parameters:</p> Name Type Description Default <code>kpi</code> <code>KPI</code> <p>MESQUAL KPI object with computed value and metadata</p> required <code>kpi_collection</code> <code>KPICollection</code> <p>Optional KPI collection for context</p> <code>None</code> <code>**kwargs</code> <p>Additional attributes to set on the data item</p> <code>{}</code> <p>Examples:</p> <p>Typical usage in visualizers:</p> <pre><code>&gt;&gt;&gt; for kpi in kpi_collection:\n...     data_item = KPIDataItem(kpi, kpi_collection)\n...     generator.generate(data_item, feature_group)\n</code></pre> <p>Access patterns:</p> <pre><code>&gt;&gt;&gt; data_item.kpi.value  # Direct KPI value access\n&gt;&gt;&gt; data_item.get_object_attribute('geometry')  # From model object\n&gt;&gt;&gt; data_item.get_object_attribute('kpi_value')  # Alias for KPI value\n&gt;&gt;&gt; data_item.get_text_representation()  # Formatted value string\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>class KPIDataItem(VisualizableDataItem):\n    \"\"\"\n    Visualizable data item for KPI objects.\n\n    Wraps MESQUAL KPI objects to provide the VisualizableDataItem interface.\n    Combines KPI values with associated model object information for rich\n    map visualization of computed energy system metrics.\n\n    Handles attribute access from both KPI values and underlying model objects,\n    provides formatted value representations, and generates enhanced tooltips\n    showing both KPI information and object details.\n\n    Args:\n        kpi: MESQUAL KPI object with computed value and metadata\n        kpi_collection: Optional KPI collection for context\n        **kwargs: Additional attributes to set on the data item\n\n    Examples:\n        Typical usage in visualizers:\n        &gt;&gt;&gt; for kpi in kpi_collection:\n        ...     data_item = KPIDataItem(kpi, kpi_collection)\n        ...     generator.generate(data_item, feature_group)\n\n        Access patterns:\n        &gt;&gt;&gt; data_item.kpi.value  # Direct KPI value access\n        &gt;&gt;&gt; data_item.get_object_attribute('geometry')  # From model object\n        &gt;&gt;&gt; data_item.get_object_attribute('kpi_value')  # Alias for KPI value\n        &gt;&gt;&gt; data_item.get_text_representation()  # Formatted value string\n    \"\"\"\n\n    KPI_VALUE_COLUMNS = ['kpi_value', 'value', 'kpi']\n\n    def __init__(self, kpi: KPI, kpi_collection: KPICollection = None, **kwargs):\n        self.kpi = kpi\n        self.kpi_collection = kpi_collection\n        self._object_info = kpi.get_object_info_from_model()\n        self._model_item = ModelDataItem(self._object_info)\n        super().__init__(**kwargs)\n\n    def get_name(self) -&gt; str:\n        return str(self.kpi.name)\n\n    def get_text_representation(self, converter: QuantityToTextConverter = None) -&gt; str:\n        \"\"\"\n        Get formatted text representation of the KPI value.\n\n        Args:\n            converter: Optional QuantityToTextConverter for custom formatting\n\n        Returns:\n            Formatted string representation of the KPI value\n        \"\"\"\n\n        if converter is not None:\n            return converter.convert(self.kpi.quantity)\n\n        q = Units.get_quantity_in_pretty_unit(self.kpi.quantity)\n        text = Units.get_pretty_text_for_quantity(q)\n        return text\n\n    def get_tooltip_data(self, converter: QuantityToTextConverter = None) -&gt; dict:\n        kpi_data = {\n            'KPI': self.kpi.get_kpi_name_with_dataset_name(),\n            'Value': self.get_text_representation(converter),\n        }\n        _MAX_MODEL_DATA_LEN = 3\n        model_data = self._model_item.get_tooltip_data()\n        model_data = dict(list(model_data.items())[:_MAX_MODEL_DATA_LEN])\n        if len(model_data) &gt; _MAX_MODEL_DATA_LEN:\n            model_data['...'] = '...'\n        return {**kpi_data, **model_data}\n\n    def get_object_attribute(self, attribute: str) -&gt; Any:\n        if (attribute in self.KPI_VALUE_COLUMNS) and (not self._model_item.object_has_attribute(attribute)):\n            return self.kpi.value\n        return self._model_item.get_object_attribute(attribute)\n\n    def object_has_attribute(self, attribute: str) -&gt; bool:\n        if attribute in self.KPI_VALUE_COLUMNS:\n            return True\n        return self._model_item.object_has_attribute(attribute)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/data_item/#mesqual.visualizations.folium_viz_system.visualizable_data_item.KPIDataItem.get_text_representation","title":"get_text_representation","text":"<pre><code>get_text_representation(converter: QuantityToTextConverter = None) -&gt; str\n</code></pre> <p>Get formatted text representation of the KPI value.</p> <p>Parameters:</p> Name Type Description Default <code>converter</code> <code>QuantityToTextConverter</code> <p>Optional QuantityToTextConverter for custom formatting</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted string representation of the KPI value</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/visualizable_data_item.py</code> <pre><code>def get_text_representation(self, converter: QuantityToTextConverter = None) -&gt; str:\n    \"\"\"\n    Get formatted text representation of the KPI value.\n\n    Args:\n        converter: Optional QuantityToTextConverter for custom formatting\n\n    Returns:\n        Formatted string representation of the KPI value\n    \"\"\"\n\n    if converter is not None:\n        return converter.convert(self.kpi.quantity)\n\n    q = Units.get_quantity_in_pretty_unit(self.kpi.quantity)\n    text = Units.get_pretty_text_for_quantity(q)\n    return text\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/kpi_collection/","title":"MESQUAL Folium KPICollection Visualization System","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/kpi_collection/#mesqual.visualizations.folium_viz_system.kpi_collection_map_visualizer.KPICollectionMapVisualizer","title":"KPICollectionMapVisualizer","text":"<p>High-level KPI collection map visualizer for energy system analysis.</p> <p>Main orchestrator for converting KPI collections into organized folium map visualizations. Handles KPI grouping, feature group creation, tooltip enhancement, and progress tracking.</p> <p>Supports multiple generators for complex visualizations (e.g., areas with text overlays, lines with arrow indicators) and provides sophisticated KPI organization and related KPI discovery for enhanced user experience.</p> <p>Parameters:</p> Name Type Description Default <code>generators</code> <code>FoliumObjectGenerator | List[FoliumObjectGenerator]</code> <p>Single generator or list of generators for visualization</p> required <code>study_manager</code> <code>StudyManager</code> <p>StudyManager for enhanced KPI relationships and tooltips</p> <code>None</code> <code>include_related_kpis_in_tooltip</code> <code>bool</code> <p>Add related KPIs to tooltip display</p> <code>False</code> <code>kpi_grouping_manager</code> <code>KPIGroupingManager</code> <p>Custom grouping manager (optional)</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to data items</p> <code>{}</code> <p>Examples:</p> <pre><code>Basic area visualization:\n&gt;&gt;&gt; visualizer = KPICollectionMapVisualizer(\n...     generators=[\n...         AreaGenerator(\n...             AreaFeatureResolver(\n...                 fill_color=PropertyMapper.from_kpi_value(color_scale),\n...                 tooltip=True\n...             )\n...         )\n...     ],\n...     study_manager=study\n... )\n&gt;&gt;&gt; feature_groups = visualizer.get_feature_groups(price_kpis)\n&gt;&gt;&gt; for fg in feature_groups:\n...     fg.add_to(folium_map)\n\nComplex multi-layer visualization:\n&gt;&gt;&gt; visualizer = KPICollectionMapVisualizer(\n...     generators=[\n...         AreaGenerator(AreaFeatureResolver(...)),\n...         TextOverlayGenerator(TextOverlayFeatureResolver(...))\n...     ],\n...     include_related_kpis_in_tooltip=True,\n...     study_manager=study\n... )\n&gt;&gt;&gt; visualizer.generate_and_add_feature_groups_to_map(\n...     kpi_collection, folium_map, show='first'\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/kpi_collection_map_visualizer.py</code> <pre><code>class KPICollectionMapVisualizer:\n    \"\"\"\n    High-level KPI collection map visualizer for energy system analysis.\n\n    Main orchestrator for converting KPI collections into organized folium map\n    visualizations. Handles KPI grouping, feature group creation, tooltip\n    enhancement, and progress tracking.\n\n    Supports multiple generators for complex visualizations (e.g., areas with\n    text overlays, lines with arrow indicators) and provides sophisticated\n    KPI organization and related KPI discovery for enhanced user experience.\n\n    Args:\n        generators: Single generator or list of generators for visualization\n        study_manager: StudyManager for enhanced KPI relationships and tooltips\n        include_related_kpis_in_tooltip: Add related KPIs to tooltip display\n        kpi_grouping_manager: Custom grouping manager (optional)\n        **kwargs: Additional arguments passed to data items\n\n    Examples:\n\n        Basic area visualization:\n        &gt;&gt;&gt; visualizer = KPICollectionMapVisualizer(\n        ...     generators=[\n        ...         AreaGenerator(\n        ...             AreaFeatureResolver(\n        ...                 fill_color=PropertyMapper.from_kpi_value(color_scale),\n        ...                 tooltip=True\n        ...             )\n        ...         )\n        ...     ],\n        ...     study_manager=study\n        ... )\n        &gt;&gt;&gt; feature_groups = visualizer.get_feature_groups(price_kpis)\n        &gt;&gt;&gt; for fg in feature_groups:\n        ...     fg.add_to(folium_map)\n\n        Complex multi-layer visualization:\n        &gt;&gt;&gt; visualizer = KPICollectionMapVisualizer(\n        ...     generators=[\n        ...         AreaGenerator(AreaFeatureResolver(...)),\n        ...         TextOverlayGenerator(TextOverlayFeatureResolver(...))\n        ...     ],\n        ...     include_related_kpis_in_tooltip=True,\n        ...     study_manager=study\n        ... )\n        &gt;&gt;&gt; visualizer.generate_and_add_feature_groups_to_map(\n        ...     kpi_collection, folium_map, show='first'\n        ... )\n    \"\"\"\n\n    def __init__(\n            self,\n            generators: FoliumObjectGenerator | List[FoliumObjectGenerator],\n            study_manager: 'StudyManager' = None,\n            include_related_kpis_in_tooltip: bool = False,\n            kpi_grouping_manager: KPIGroupingManager = None,\n            value_formatting: VALUE_FORMATTING_OPTIONS = None,\n            **kwargs\n    ):\n        self.generators: List[FoliumObjectGenerator] = generators if isinstance(generators, list) else [generators]\n        self.study_manager = study_manager\n        self.include_related_kpis_in_tooltip = include_related_kpis_in_tooltip\n        self.value_formatting = value_formatting\n\n        self.grouping_manager = kpi_grouping_manager or KPIGroupingManager()\n        self.kwargs = kwargs\n\n    def generate_and_add_feature_groups_to_map(\n            self,\n            kpi_collection: KPICollection,\n            folium_map: folium.Map,\n            show: SHOW_OPTIONS = 'none',\n            overlay: bool = False,\n    ) -&gt; list[folium.FeatureGroup]:\n        \"\"\"Generate feature groups and add them to map.\"\"\"\n        fgs = self.get_feature_groups(kpi_collection, show=show, overlay=overlay)\n        for fg in fgs:\n            folium_map.add_child(fg)\n        return fgs\n\n    def get_feature_groups(\n            self,\n            kpi_collection: KPICollection,\n            show: SHOW_OPTIONS = 'none',\n            overlay: bool = False\n    ) -&gt; list[folium.FeatureGroup]:\n        \"\"\"\n        Create feature groups for KPI collection with organized grouping.\n\n        Main method that processes KPI collection through grouping, creates\n        folium FeatureGroups, and applies all configured generators to create\n        a complete map visualization.\n\n        Args:\n            kpi_collection: Collection of KPIs to visualize\n            show: Which feature groups to show initially ('first', 'last', 'none')\n            overlay: Whether feature groups should be overlay controls\n\n        Returns:\n            List of folium FeatureGroup objects ready to add to map\n        \"\"\"\n        from tqdm import tqdm\n        from mesqual.utils.logging import get_logger\n\n        logger = get_logger(__name__)\n        feature_groups = []\n        failed = []\n\n        kpi_to_text_converter_resolver = KPIQuantityToTextConverterResolver(self.value_formatting, kpi_collection)\n\n        pbar = tqdm(kpi_collection, total=kpi_collection.size, desc=f'{self.__class__.__name__}')\n        with pbar:\n            kpi_groups = self.grouping_manager.get_kpi_groups(kpi_collection)\n\n            for kpi_group in kpi_groups:\n                group_name = self.grouping_manager.get_feature_group_name(kpi_group)\n\n                if show == 'first':\n                    show_fg = kpi_group == kpi_groups[0]\n                elif show == 'last':\n                    show_fg = kpi_group == kpi_groups[-1]\n                else:\n                    show_fg = False\n\n                fg = folium.FeatureGroup(name=group_name, overlay=overlay, show=show_fg)\n                converter = kpi_to_text_converter_resolver.get_converter_for_kpi(group_name, kpi_group)\n\n                _generators = [\n                    self._create_generator_with_updated_converters_if_needed(converter, generator)\n                    for generator in self.generators\n                ]\n\n                for kpi in kpi_group:\n                    data_item = KPIDataItem(kpi, kpi_collection, study_manager=self.study_manager, **self.kwargs)\n                    for gen in _generators:\n                        try:\n                            gen.generate(data_item, fg)\n                        except Exception as e:\n                            failed.append((kpi.name, group_name, e))\n\n                    pbar.update(1)\n\n                feature_groups.append(fg)\n\n        if failed:\n            logger.warning(\n                f'Exception while trying to add {len(failed)} KPIs: {failed[:3]}'\n            )\n        return feature_groups\n\n    def _create_generator_with_updated_converters_if_needed(self, converter, generator):\n        \"\"\"Create generator copy with converter-specific mappers if needed.\"\"\"\n        import copy\n\n        # Check if we need to modify any mappers\n        needs_modification = (\n            self.include_related_kpis_in_tooltip or\n            'text_print_content' in generator.feature_resolver.property_mappers\n        )\n\n        if not needs_modification:\n            return generator\n\n        # Create shallow copies\n        gen = copy.copy(generator)\n        gen.feature_resolver = copy.copy(generator.feature_resolver)\n        gen.feature_resolver.property_mappers = generator.feature_resolver.property_mappers.copy()\n\n        if self.include_related_kpis_in_tooltip:\n            gen.feature_resolver.property_mappers['tooltip'] = self._create_enhanced_tooltip_generator(converter)\n\n        if (self.value_formatting is not None) and ('text_print_content' in gen.feature_resolver.property_mappers):\n            gen.feature_resolver.property_mappers['text_print_content'] = self._create_text_print_generator(converter)\n\n        return gen\n\n    def _create_text_print_generator(self, converter: QuantityToTextConverter) -&gt; PropertyMapper:\n        \"\"\"\n        Create text print generator with fixed converter.\n\n        Args:\n            converter: QuantityToTextConverter to use for formatting\n\n        Returns:\n            PropertyMapper with converter baked into the closure\n        \"\"\"\n        def get_text(data_item: VisualizableDataItem) -&gt; str:\n            return data_item.get_text_representation(converter)\n\n        return PropertyMapper(get_text)\n\n    @staticmethod\n    def _create_default_tooltip_generator(converter: QuantityToTextConverter) -&gt; PropertyMapper:\n        \"\"\"\n        Create default tooltip generator showing data item information.\n\n        Args:\n            converter: QuantityToTextConverter to use for formatting all values\n\n        Returns:\n            PropertyMapper that generates HTML table tooltips with data item attributes\n        \"\"\"\n\n        def get_tooltip(data_item: VisualizableDataItem) -&gt; str:\n            tooltip_data = data_item.get_tooltip_data(converter=converter)\n\n            html = '&lt;table style=\"border-collapse: collapse;\"&gt;\\n'\n            for key, value in tooltip_data.items():\n                html += f'  &lt;tr&gt;&lt;td style=\"padding: 4px 8px;\"&gt;&lt;strong&gt;{key}&lt;/strong&gt;&lt;/td&gt;' \\\n                        f'&lt;td style=\"text-align: right; padding: 4px 8px;\"&gt;{value}&lt;/td&gt;&lt;/tr&gt;\\n'\n            html += '&lt;/table&gt;'\n\n            return html\n\n        return PropertyMapper(get_tooltip)\n\n    def _create_enhanced_tooltip_generator(self, converter: QuantityToTextConverter) -&gt; PropertyMapper:\n        \"\"\"\n        Create tooltip generator that includes related KPIs with fixed converter.\n\n        Args:\n            converter: QuantityToTextConverter to use for formatting all values\n\n        Returns:\n            PropertyMapper with converter baked into the closure\n        \"\"\"\n\n        def generate_tooltip(data_item: KPIDataItem) -&gt; str:\n            kpi = data_item.kpi\n            kpi_name = kpi.get_kpi_name_with_dataset_name()\n\n            kpi_text = converter.convert(kpi.quantity)\n\n            html = '&lt;table style=\"border-collapse: collapse;\"&gt;\\n'\n            html += f'  &lt;tr&gt;&lt;td style=\"padding: 4px 8px;\"&gt;&lt;strong&gt;{kpi_name}&lt;/strong&gt;&lt;/td&gt;' \\\n                    f'&lt;td style=\"text-align: right; padding: 4px 8px;\"&gt;{kpi_text}&lt;/td&gt;&lt;/tr&gt;\\n'\n\n            if self.include_related_kpis_in_tooltip and self.study_manager:\n                related_groups = self.grouping_manager.get_related_kpi_groups(\n                    kpi, self.study_manager\n                )\n\n                if any(not g.empty for g in related_groups.values()):\n                    for name, group in related_groups.items():\n                        if group.empty:\n                            continue\n                        html += \"&lt;tr&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;/tr&gt;\"\n                        html += f'  &lt;tr&gt;&lt;th colspan=\"2\" style=\"text-align: left; padding: 8px;\"&gt;{name}&lt;/th&gt;&lt;/tr&gt;\\n'\n                        for related_kpi in group:\n                            related_kpi_name = related_kpi.get_kpi_name_with_dataset_name()\n                            related_kpi_value_text = converter.convert(related_kpi.quantity)\n                            html += f'  &lt;tr&gt;&lt;td style=\"padding: 4px 8px;\"&gt;{related_kpi_name}&lt;/td&gt;' \\\n                                    f'&lt;td style=\"text-align: right; padding: 4px 8px;\"&gt;{related_kpi_value_text}&lt;/td&gt;&lt;/tr&gt;\\n'\n\n            html += '&lt;br&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;/table&gt;'\n            return html\n\n        return PropertyMapper(generate_tooltip)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/kpi_collection/#mesqual.visualizations.folium_viz_system.kpi_collection_map_visualizer.KPICollectionMapVisualizer.generate_and_add_feature_groups_to_map","title":"generate_and_add_feature_groups_to_map","text":"<pre><code>generate_and_add_feature_groups_to_map(kpi_collection: KPICollection, folium_map: Map, show: SHOW_OPTIONS = 'none', overlay: bool = False) -&gt; list[FeatureGroup]\n</code></pre> <p>Generate feature groups and add them to map.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/kpi_collection_map_visualizer.py</code> <pre><code>def generate_and_add_feature_groups_to_map(\n        self,\n        kpi_collection: KPICollection,\n        folium_map: folium.Map,\n        show: SHOW_OPTIONS = 'none',\n        overlay: bool = False,\n) -&gt; list[folium.FeatureGroup]:\n    \"\"\"Generate feature groups and add them to map.\"\"\"\n    fgs = self.get_feature_groups(kpi_collection, show=show, overlay=overlay)\n    for fg in fgs:\n        folium_map.add_child(fg)\n    return fgs\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/kpi_collection/#mesqual.visualizations.folium_viz_system.kpi_collection_map_visualizer.KPICollectionMapVisualizer.get_feature_groups","title":"get_feature_groups","text":"<pre><code>get_feature_groups(kpi_collection: KPICollection, show: SHOW_OPTIONS = 'none', overlay: bool = False) -&gt; list[FeatureGroup]\n</code></pre> <p>Create feature groups for KPI collection with organized grouping.</p> <p>Main method that processes KPI collection through grouping, creates folium FeatureGroups, and applies all configured generators to create a complete map visualization.</p> <p>Parameters:</p> Name Type Description Default <code>kpi_collection</code> <code>KPICollection</code> <p>Collection of KPIs to visualize</p> required <code>show</code> <code>SHOW_OPTIONS</code> <p>Which feature groups to show initially ('first', 'last', 'none')</p> <code>'none'</code> <code>overlay</code> <code>bool</code> <p>Whether feature groups should be overlay controls</p> <code>False</code> <p>Returns:</p> Type Description <code>list[FeatureGroup]</code> <p>List of folium FeatureGroup objects ready to add to map</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/kpi_collection_map_visualizer.py</code> <pre><code>def get_feature_groups(\n        self,\n        kpi_collection: KPICollection,\n        show: SHOW_OPTIONS = 'none',\n        overlay: bool = False\n) -&gt; list[folium.FeatureGroup]:\n    \"\"\"\n    Create feature groups for KPI collection with organized grouping.\n\n    Main method that processes KPI collection through grouping, creates\n    folium FeatureGroups, and applies all configured generators to create\n    a complete map visualization.\n\n    Args:\n        kpi_collection: Collection of KPIs to visualize\n        show: Which feature groups to show initially ('first', 'last', 'none')\n        overlay: Whether feature groups should be overlay controls\n\n    Returns:\n        List of folium FeatureGroup objects ready to add to map\n    \"\"\"\n    from tqdm import tqdm\n    from mesqual.utils.logging import get_logger\n\n    logger = get_logger(__name__)\n    feature_groups = []\n    failed = []\n\n    kpi_to_text_converter_resolver = KPIQuantityToTextConverterResolver(self.value_formatting, kpi_collection)\n\n    pbar = tqdm(kpi_collection, total=kpi_collection.size, desc=f'{self.__class__.__name__}')\n    with pbar:\n        kpi_groups = self.grouping_manager.get_kpi_groups(kpi_collection)\n\n        for kpi_group in kpi_groups:\n            group_name = self.grouping_manager.get_feature_group_name(kpi_group)\n\n            if show == 'first':\n                show_fg = kpi_group == kpi_groups[0]\n            elif show == 'last':\n                show_fg = kpi_group == kpi_groups[-1]\n            else:\n                show_fg = False\n\n            fg = folium.FeatureGroup(name=group_name, overlay=overlay, show=show_fg)\n            converter = kpi_to_text_converter_resolver.get_converter_for_kpi(group_name, kpi_group)\n\n            _generators = [\n                self._create_generator_with_updated_converters_if_needed(converter, generator)\n                for generator in self.generators\n            ]\n\n            for kpi in kpi_group:\n                data_item = KPIDataItem(kpi, kpi_collection, study_manager=self.study_manager, **self.kwargs)\n                for gen in _generators:\n                    try:\n                        gen.generate(data_item, fg)\n                    except Exception as e:\n                        failed.append((kpi.name, group_name, e))\n\n                pbar.update(1)\n\n            feature_groups.append(fg)\n\n    if failed:\n        logger.warning(\n            f'Exception while trying to add {len(failed)} KPIs: {failed[:3]}'\n        )\n    return feature_groups\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/kpi_collection/#mesqual.visualizations.folium_viz_system.kpi_collection_map_visualizer.KPIGroupingManager","title":"KPIGroupingManager","text":"<p>Manages sophisticated KPI grouping and organization for map visualization.</p> <p>Handles the complex logic of grouping KPIs by their attributes, creating meaningful feature group names, and finding related KPIs for enhanced tooltips. Supports custom sorting orders and category hierarchies.</p> <p>The grouping system is designed to create logical visual organization of energy system KPIs, where related metrics (same flag, different datasets) are grouped together and presented with consistent naming.</p> <p>Parameters:</p> Name Type Description Default <code>kpi_attribute_category_orders</code> <code>dict[str, list[str]]</code> <p>Custom ordering for specific attribute values</p> <code>None</code> <code>kpi_attribute_keys_to_exclude_from_grouping</code> <code>list[str]</code> <p>Attributes to ignore during grouping</p> <code>None</code> <code>kpi_attribute_sort_order</code> <code>list[str]</code> <p>Order of attributes for group sorting</p> <code>None</code> <p>Examples:</p> <pre><code>Custom grouping configuration:\n&gt;&gt;&gt; manager = KPIGroupingManager(\n...     kpi_attribute_category_orders={\n...         'dataset_name': ['reference', 'scenario_1', 'scenario_2'],\n...         'aggregation': ['sum', 'mean', 'max']\n...     },\n...     kpi_attribute_keys_to_exclude_from_grouping=['object_name']\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/kpi_collection_map_visualizer.py</code> <pre><code>class KPIGroupingManager:\n    \"\"\"\n    Manages sophisticated KPI grouping and organization for map visualization.\n\n    Handles the complex logic of grouping KPIs by their attributes, creating\n    meaningful feature group names, and finding related KPIs for enhanced\n    tooltips. Supports custom sorting orders and category hierarchies.\n\n    The grouping system is designed to create logical visual organization\n    of energy system KPIs, where related metrics (same flag, different datasets)\n    are grouped together and presented with consistent naming.\n\n    Args:\n        kpi_attribute_category_orders: Custom ordering for specific attribute values\n        kpi_attribute_keys_to_exclude_from_grouping: Attributes to ignore during grouping\n        kpi_attribute_sort_order: Order of attributes for group sorting\n\n    Examples:\n\n        Custom grouping configuration:\n        &gt;&gt;&gt; manager = KPIGroupingManager(\n        ...     kpi_attribute_category_orders={\n        ...         'dataset_name': ['reference', 'scenario_1', 'scenario_2'],\n        ...         'aggregation': ['sum', 'mean', 'max']\n        ...     },\n        ...     kpi_attribute_keys_to_exclude_from_grouping=['object_name']\n        ... )\n    \"\"\"\n\n    DEFAULT_EXCLUDE_FROM_GROUPING = ['name', 'object_name', 'column_subset', 'custom_name',\n                                      'name_prefix', 'name_suffix', 'unit', 'target_unit']\n    DEFAULT_SORT_ORDER = [\n        'name_prefix', 'model_flag', 'flag', 'aggregation',\n        'reference_dataset_name', 'variation_dataset_name', 'dataset_name',\n        'value_comparison', 'arithmetic_operation', 'name_suffix'\n    ]\n    DEFAULT_INCLUDE_ATTRIBUTES = ['arithmetic_operation', 'aggregation', 'flag', 'dataset_name', 'unit']\n    DEFAULT_EXCLUDE_ATTRIBUTES = ['variation_dataset_name', 'reference_dataset_name', 'model_flag',\n                                   'dataset_type', 'target_unit', 'dataset_attributes']\n\n    def __init__(\n            self,\n            kpi_attribute_category_orders: dict[str, list[str]] = None,\n            kpi_attribute_keys_to_exclude_from_grouping: list[str] = None,\n            kpi_attribute_sort_order: list[str] = None\n    ):\n        self.kpi_attribute_category_orders = kpi_attribute_category_orders or {}\n        self.kpi_attribute_keys_to_exclude_from_grouping = (\n                kpi_attribute_keys_to_exclude_from_grouping or self.DEFAULT_EXCLUDE_FROM_GROUPING.copy()\n        )\n        self.kpi_attribute_sort_order = (\n                kpi_attribute_sort_order or self.DEFAULT_SORT_ORDER.copy()\n        )\n\n    def get_kpi_groups(self, kpi_collection: KPICollection) -&gt; list[KPICollection]:\n        \"\"\"\n        Group KPIs by attributes with sophisticated sorting.\n\n        Creates logical groups of KPIs based on their attributes, excluding\n        specified attributes from grouping and applying custom sort orders.\n\n        Args:\n            kpi_collection: Collection of KPIs to group\n\n        Returns:\n            List of KPICollection objects, each representing a logical group\n        \"\"\"\n        from mesqual.utils.dict_combinations import dict_combination_iterator\n\n        attribute_sets = kpi_collection.get_all_kpi_attributes_and_value_sets(primitive_values=True)\n        relevant_attribute_sets = {\n            k: v for k, v in attribute_sets.items()\n            if k not in self.kpi_attribute_keys_to_exclude_from_grouping\n        }\n\n        ordered_keys = [k for k in self.kpi_attribute_sort_order if k in relevant_attribute_sets]\n\n        # Build attribute value rankings\n        attribute_value_rank: dict[str, dict[str, int]] = {}\n        for attr in ordered_keys:\n            existing_values = set(relevant_attribute_sets.get(attr, []))\n            manual_order = [v for v in self.kpi_attribute_category_orders.get(attr, []) if v in existing_values]\n            remaining = list(existing_values - set(manual_order))\n            try:\n                remaining = list(sorted(remaining))\n            except TypeError:\n                pass\n            full_order = manual_order + remaining\n            attribute_value_rank[attr] = {val: idx for idx, val in enumerate(full_order)}\n\n        def sorting_index(group_kwargs: dict[str, str]) -&gt; tuple:\n            return tuple(\n                attribute_value_rank[attr].get(group_kwargs.get(attr), float(\"inf\"))\n                for attr in ordered_keys\n            )\n\n        # Create and sort groups\n        group_kwargs_list = list(dict_combination_iterator(relevant_attribute_sets))\n        group_kwargs_list.sort(key=sorting_index)\n\n        groups: list[KPICollection] = []\n        for group_kwargs in group_kwargs_list:\n            g = kpi_collection.filter(**group_kwargs)\n            if not g.empty:\n                groups.append(g)\n\n        return groups\n\n    def get_feature_group_name(self, kpi_group: KPICollection) -&gt; str:\n        \"\"\"\n        Generate meaningful feature group name from KPI group.\n\n        Creates human-readable names for map feature groups based on\n        common KPI attributes, prioritizing important attributes.\n\n        Args:\n            kpi_group: KPI group to generate name for\n\n        Returns:\n            Human-readable feature group name\n        \"\"\"\n        attributes = kpi_group.get_in_common_kpi_attributes(primitive_values=True)\n\n        for k in self.DEFAULT_EXCLUDE_ATTRIBUTES:\n            attributes.pop(k, None)\n\n        components = []\n        include_attrs = self.DEFAULT_INCLUDE_ATTRIBUTES + [\n            k for k in attributes.keys() if k not in self.DEFAULT_INCLUDE_ATTRIBUTES\n        ]\n        for k in include_attrs:\n            value = attributes.pop(k, None)\n            if value is not None:\n                components.append(str(value))\n\n        return ' '.join(components)\n\n    def get_related_kpi_groups(self, kpi: KPI, study_manager) -&gt; dict[str, KPICollection]:\n        \"\"\"\n        Get related KPIs grouped by relationship type (FIXED VERSION).\n\n        Finds KPIs related to the given KPI across different dimensions:\n        - Same object/flag, different aggregations\n        - Same object/flag/aggregation, different datasets\n        - Same object/flag/aggregation, different comparisons/operations\n\n        This is a corrected version that properly separates different relationship types.\n\n        Args:\n            kpi: Source KPI to find relatives for\n            study_manager: StudyManager for accessing merged KPI collection\n\n        Returns:\n            Dict mapping relationship type to KPICollection of related KPIs\n        \"\"\"\n        groups = {\n            'Different Comparisons / Operations': KPICollection(),\n            'Different Aggregations': KPICollection(),\n            'Different Datasets': KPICollection(),\n        }\n\n        if not study_manager:\n            return groups\n\n        # Get reference KPI attributes\n        object_name = kpi.attributes.object_name\n        flag = kpi.attributes.flag\n        model_flag = kpi.attributes.model_flag\n        aggregation = kpi.attributes.aggregation\n        dataset_name = kpi.attributes.dataset_name\n        value_comparison = kpi.attributes.value_comparison\n        arithmetic_operation = kpi.attributes.arithmetic_operation\n\n        if not flag or aggregation is None:\n            return groups\n\n        try:\n            # Get all KPIs for same object/flag/model_flag\n            all_related = study_manager.scen_comp.get_merged_kpi_collection()\n            pre_filtered = all_related.filter(\n                object_name=object_name,\n                flag=flag,\n                model_flag=model_flag\n            )\n        except:\n            return groups\n\n        # Determine if main KPI has comparison/operation\n        main_has_comparison = value_comparison is not None or arithmetic_operation is not None\n\n        for potential in pre_filtered:\n            # Skip self\n            if potential is kpi:\n                continue\n\n            pot_agg = potential.attributes.aggregation\n            pot_dataset = potential.attributes.dataset_name\n            pot_comparison = potential.attributes.value_comparison\n            pot_operation = potential.attributes.arithmetic_operation\n            pot_has_comparison = pot_comparison is not None or pot_operation is not None\n\n            # Category 1: Different Aggregations\n            # Same dataset, same comparison status, different aggregation\n            if (pot_dataset == dataset_name and\n                pot_agg != aggregation and\n                pot_comparison == value_comparison and\n                pot_operation == arithmetic_operation):\n                groups['Different Aggregations'].add(potential)\n                continue\n\n            # Category 2: Different Datasets\n            # Same aggregation, same comparison status, different dataset\n            if (pot_agg == aggregation and\n                pot_dataset != dataset_name and\n                pot_comparison == value_comparison and\n                pot_operation == arithmetic_operation):\n                groups['Different Datasets'].add(potential)\n                continue\n\n            # Category 3: Different Comparisons/Operations\n            # Same dataset, same aggregation, different comparison/operation\n            if (pot_dataset == dataset_name and\n                pot_agg == aggregation and\n                (pot_comparison != value_comparison or pot_operation != arithmetic_operation)):\n                groups['Different Comparisons / Operations'].add(potential)\n                continue\n\n        return groups\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/kpi_collection/#mesqual.visualizations.folium_viz_system.kpi_collection_map_visualizer.KPIGroupingManager.get_kpi_groups","title":"get_kpi_groups","text":"<pre><code>get_kpi_groups(kpi_collection: KPICollection) -&gt; list[KPICollection]\n</code></pre> <p>Group KPIs by attributes with sophisticated sorting.</p> <p>Creates logical groups of KPIs based on their attributes, excluding specified attributes from grouping and applying custom sort orders.</p> <p>Parameters:</p> Name Type Description Default <code>kpi_collection</code> <code>KPICollection</code> <p>Collection of KPIs to group</p> required <p>Returns:</p> Type Description <code>list[KPICollection]</code> <p>List of KPICollection objects, each representing a logical group</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/kpi_collection_map_visualizer.py</code> <pre><code>def get_kpi_groups(self, kpi_collection: KPICollection) -&gt; list[KPICollection]:\n    \"\"\"\n    Group KPIs by attributes with sophisticated sorting.\n\n    Creates logical groups of KPIs based on their attributes, excluding\n    specified attributes from grouping and applying custom sort orders.\n\n    Args:\n        kpi_collection: Collection of KPIs to group\n\n    Returns:\n        List of KPICollection objects, each representing a logical group\n    \"\"\"\n    from mesqual.utils.dict_combinations import dict_combination_iterator\n\n    attribute_sets = kpi_collection.get_all_kpi_attributes_and_value_sets(primitive_values=True)\n    relevant_attribute_sets = {\n        k: v for k, v in attribute_sets.items()\n        if k not in self.kpi_attribute_keys_to_exclude_from_grouping\n    }\n\n    ordered_keys = [k for k in self.kpi_attribute_sort_order if k in relevant_attribute_sets]\n\n    # Build attribute value rankings\n    attribute_value_rank: dict[str, dict[str, int]] = {}\n    for attr in ordered_keys:\n        existing_values = set(relevant_attribute_sets.get(attr, []))\n        manual_order = [v for v in self.kpi_attribute_category_orders.get(attr, []) if v in existing_values]\n        remaining = list(existing_values - set(manual_order))\n        try:\n            remaining = list(sorted(remaining))\n        except TypeError:\n            pass\n        full_order = manual_order + remaining\n        attribute_value_rank[attr] = {val: idx for idx, val in enumerate(full_order)}\n\n    def sorting_index(group_kwargs: dict[str, str]) -&gt; tuple:\n        return tuple(\n            attribute_value_rank[attr].get(group_kwargs.get(attr), float(\"inf\"))\n            for attr in ordered_keys\n        )\n\n    # Create and sort groups\n    group_kwargs_list = list(dict_combination_iterator(relevant_attribute_sets))\n    group_kwargs_list.sort(key=sorting_index)\n\n    groups: list[KPICollection] = []\n    for group_kwargs in group_kwargs_list:\n        g = kpi_collection.filter(**group_kwargs)\n        if not g.empty:\n            groups.append(g)\n\n    return groups\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/kpi_collection/#mesqual.visualizations.folium_viz_system.kpi_collection_map_visualizer.KPIGroupingManager.get_feature_group_name","title":"get_feature_group_name","text":"<pre><code>get_feature_group_name(kpi_group: KPICollection) -&gt; str\n</code></pre> <p>Generate meaningful feature group name from KPI group.</p> <p>Creates human-readable names for map feature groups based on common KPI attributes, prioritizing important attributes.</p> <p>Parameters:</p> Name Type Description Default <code>kpi_group</code> <code>KPICollection</code> <p>KPI group to generate name for</p> required <p>Returns:</p> Type Description <code>str</code> <p>Human-readable feature group name</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/kpi_collection_map_visualizer.py</code> <pre><code>def get_feature_group_name(self, kpi_group: KPICollection) -&gt; str:\n    \"\"\"\n    Generate meaningful feature group name from KPI group.\n\n    Creates human-readable names for map feature groups based on\n    common KPI attributes, prioritizing important attributes.\n\n    Args:\n        kpi_group: KPI group to generate name for\n\n    Returns:\n        Human-readable feature group name\n    \"\"\"\n    attributes = kpi_group.get_in_common_kpi_attributes(primitive_values=True)\n\n    for k in self.DEFAULT_EXCLUDE_ATTRIBUTES:\n        attributes.pop(k, None)\n\n    components = []\n    include_attrs = self.DEFAULT_INCLUDE_ATTRIBUTES + [\n        k for k in attributes.keys() if k not in self.DEFAULT_INCLUDE_ATTRIBUTES\n    ]\n    for k in include_attrs:\n        value = attributes.pop(k, None)\n        if value is not None:\n            components.append(str(value))\n\n    return ' '.join(components)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/kpi_collection/#mesqual.visualizations.folium_viz_system.kpi_collection_map_visualizer.KPIGroupingManager.get_related_kpi_groups","title":"get_related_kpi_groups","text":"<pre><code>get_related_kpi_groups(kpi: KPI, study_manager) -&gt; dict[str, KPICollection]\n</code></pre> <p>Get related KPIs grouped by relationship type (FIXED VERSION).</p> <p>Finds KPIs related to the given KPI across different dimensions: - Same object/flag, different aggregations - Same object/flag/aggregation, different datasets - Same object/flag/aggregation, different comparisons/operations</p> <p>This is a corrected version that properly separates different relationship types.</p> <p>Parameters:</p> Name Type Description Default <code>kpi</code> <code>KPI</code> <p>Source KPI to find relatives for</p> required <code>study_manager</code> <p>StudyManager for accessing merged KPI collection</p> required <p>Returns:</p> Type Description <code>dict[str, KPICollection]</code> <p>Dict mapping relationship type to KPICollection of related KPIs</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/kpi_collection_map_visualizer.py</code> <pre><code>def get_related_kpi_groups(self, kpi: KPI, study_manager) -&gt; dict[str, KPICollection]:\n    \"\"\"\n    Get related KPIs grouped by relationship type (FIXED VERSION).\n\n    Finds KPIs related to the given KPI across different dimensions:\n    - Same object/flag, different aggregations\n    - Same object/flag/aggregation, different datasets\n    - Same object/flag/aggregation, different comparisons/operations\n\n    This is a corrected version that properly separates different relationship types.\n\n    Args:\n        kpi: Source KPI to find relatives for\n        study_manager: StudyManager for accessing merged KPI collection\n\n    Returns:\n        Dict mapping relationship type to KPICollection of related KPIs\n    \"\"\"\n    groups = {\n        'Different Comparisons / Operations': KPICollection(),\n        'Different Aggregations': KPICollection(),\n        'Different Datasets': KPICollection(),\n    }\n\n    if not study_manager:\n        return groups\n\n    # Get reference KPI attributes\n    object_name = kpi.attributes.object_name\n    flag = kpi.attributes.flag\n    model_flag = kpi.attributes.model_flag\n    aggregation = kpi.attributes.aggregation\n    dataset_name = kpi.attributes.dataset_name\n    value_comparison = kpi.attributes.value_comparison\n    arithmetic_operation = kpi.attributes.arithmetic_operation\n\n    if not flag or aggregation is None:\n        return groups\n\n    try:\n        # Get all KPIs for same object/flag/model_flag\n        all_related = study_manager.scen_comp.get_merged_kpi_collection()\n        pre_filtered = all_related.filter(\n            object_name=object_name,\n            flag=flag,\n            model_flag=model_flag\n        )\n    except:\n        return groups\n\n    # Determine if main KPI has comparison/operation\n    main_has_comparison = value_comparison is not None or arithmetic_operation is not None\n\n    for potential in pre_filtered:\n        # Skip self\n        if potential is kpi:\n            continue\n\n        pot_agg = potential.attributes.aggregation\n        pot_dataset = potential.attributes.dataset_name\n        pot_comparison = potential.attributes.value_comparison\n        pot_operation = potential.attributes.arithmetic_operation\n        pot_has_comparison = pot_comparison is not None or pot_operation is not None\n\n        # Category 1: Different Aggregations\n        # Same dataset, same comparison status, different aggregation\n        if (pot_dataset == dataset_name and\n            pot_agg != aggregation and\n            pot_comparison == value_comparison and\n            pot_operation == arithmetic_operation):\n            groups['Different Aggregations'].add(potential)\n            continue\n\n        # Category 2: Different Datasets\n        # Same aggregation, same comparison status, different dataset\n        if (pot_agg == aggregation and\n            pot_dataset != dataset_name and\n            pot_comparison == value_comparison and\n            pot_operation == arithmetic_operation):\n            groups['Different Datasets'].add(potential)\n            continue\n\n        # Category 3: Different Comparisons/Operations\n        # Same dataset, same aggregation, different comparison/operation\n        if (pot_dataset == dataset_name and\n            pot_agg == aggregation and\n            (pot_comparison != value_comparison or pot_operation != arithmetic_operation)):\n            groups['Different Comparisons / Operations'].add(potential)\n            continue\n\n    return groups\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/legends/","title":"Legends","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/legends/#mesqual.visualizations.folium_viz_system.legends","title":"legends","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/legends/#mesqual.visualizations.folium_viz_system.legends.ContinuousColorscaleLegend","title":"ContinuousColorscaleLegend","text":"<p>               Bases: <code>ContinuousLegendBase[SegmentedContinuousColorscale]</code></p> <p>Legend for continuous color mappings</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/legends/continuous_color.py</code> <pre><code>class ContinuousColorscaleLegend(ContinuousLegendBase[SegmentedContinuousColorscale]):\n    \"\"\"Legend for continuous color mappings\"\"\"\n\n    def specific_visual_styles(self) -&gt; str:\n        # no extra CSS beyond base\n        return \"\"\n\n    def create_segment_visual(self, start: float, end: float, colors: Any) -&gt; str:\n        if isinstance(colors, list):\n            if len(colors) == 1:\n                gradient = f\"background: {colors[0]};\"\n            else:\n                stops = [\n                    f\"{color} {100 * i / (len(colors) - 1):.1f}%\"\n                    for i, color in enumerate(colors)\n                ]\n                gradient = f\"background: linear-gradient(to right, {', '.join(stops)});\"\n        else:\n            gradient = f\"background: {colors};\"\n        return f'&lt;div class=\"segment-visual\" style=\"{gradient}\"&gt;&lt;/div&gt;'\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/legends/#mesqual.visualizations.folium_viz_system.legends.DiscreteColorLegend","title":"DiscreteColorLegend","text":"<p>               Bases: <code>DiscreteLegendBase[DiscreteColorMapping]</code></p> <p>Legend for discrete color mappings</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/legends/discrete_color.py</code> <pre><code>class DiscreteColorLegend(DiscreteLegendBase[DiscreteColorMapping]):\n    \"\"\"Legend for discrete color mappings\"\"\"\n\n    def __init__(\n            self,\n            mapping: DiscreteColorMapping,\n            swatch_size: int = 20,\n            **kwargs\n    ):\n        super().__init__(mapping, **kwargs)\n        self.swatch_size = swatch_size\n\n    def specific_visual_styles(self) -&gt; str:\n        id_selector = f\"#{self.get_name()}\"\n        return f\"\"\"\n            {id_selector} .color-swatch {{\n                width: {self.swatch_size}px;\n                height: {self.swatch_size}px;\n                border: 1px solid #ccc;\n                border-radius: 2px;\n            }}\n        \"\"\"\n\n    def create_visual_element(self, color: str) -&gt; str:\n        return f'&lt;div class=\"color-swatch\" style=\"background-color: {color};\"&gt;&lt;/div&gt;'\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/legends/#mesqual.visualizations.folium_viz_system.legends.DiscreteLineDashLegend","title":"DiscreteLineDashLegend","text":"<p>               Bases: <code>DiscreteLegendBase[DiscreteLineDashPatternMapping]</code></p> <p>Legend for line dash pattern mappings</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/legends/discrete_line_dash.py</code> <pre><code>class DiscreteLineDashLegend(DiscreteLegendBase[DiscreteLineDashPatternMapping]):\n    \"\"\"Legend for line dash pattern mappings\"\"\"\n\n    def __init__(\n            self,\n            mapping: DiscreteLineDashPatternMapping,\n            line_color: str = \"#000000\",\n            line_width: int = 2,\n            **kwargs\n    ):\n        super().__init__(mapping, **kwargs)\n        self.line_color = line_color\n        self.line_width = line_width\n\n    def specific_visual_styles(self) -&gt; str:\n        id_selector = f\"#{self.get_name()}\"\n        return f\"\"\"\n            {id_selector} svg {{\n                /* you can put shared SVG styling here if needed */\n            }}\n        \"\"\"\n\n    def create_visual_element(self, pattern: str) -&gt; str:\n        dash_array = f'stroke-dasharray=\"{pattern}\"' if pattern else \"\"\n        return f\"\"\"\n            &lt;svg width=\"{self.visual_column_width}\" height=\"{self.line_width * 2}\"&gt;\n                &lt;line x1=\"0\" y1=\"{self.line_width}\" x2=\"{self.visual_column_width}\" y2=\"{self.line_width}\"\n                      stroke=\"{self.line_color}\" stroke-width=\"{self.line_width}\" {dash_array}/&gt;\n            &lt;/svg&gt;\n        \"\"\"\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/legends/#mesqual.visualizations.folium_viz_system.legends.DiscreteLineWidthLegend","title":"DiscreteLineWidthLegend","text":"<p>               Bases: <code>DiscreteLegendBase[DiscreteLineWidthMapping]</code></p> <p>Legend for discrete line width mappings</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/legends/discrete_line_width.py</code> <pre><code>class DiscreteLineWidthLegend(DiscreteLegendBase[DiscreteLineWidthMapping]):\n    \"\"\"Legend for discrete line width mappings\"\"\"\n\n    def __init__(\n            self,\n            mapping: DiscreteLineWidthMapping,\n            line_color: str = \"#000000\",\n            show_pixel_values: bool = False,\n            **kwargs\n    ):\n        super().__init__(mapping, **kwargs)\n        self.line_color = line_color\n        self.show_pixel_values = show_pixel_values\n\n    def specific_visual_styles(self) -&gt; str:\n        id_selector = f\"#{self.get_name()}\"\n        return f\"\"\"\n            {id_selector} .pixel-value {{\n                font-size: {self.label_font_size - 2}px;\n                color: {self.title_color};\n                opacity: 0.7;\n                margin-left: 5px;\n            }}\n        \"\"\"\n\n    def create_visual_element(self, width: float) -&gt; str:\n        height = max(10, width * 2)\n        return f\"\"\"\n            &lt;svg width=\"{self.visual_column_width}\" height=\"{height}\"&gt;\n                &lt;line x1=\"0\" y1=\"{height / 2}\" x2=\"{self.visual_column_width}\" y2=\"{height / 2}\"\n                      stroke=\"{self.line_color}\" stroke-width=\"{width}\"/&gt;\n            &lt;/svg&gt;\n        \"\"\"\n\n    def render_content(self) -&gt; str:\n        items = []\n\n        # Override to add pixel values\n        for key, width in sorted(self.mapping.mapping.items()):\n            visual = self.create_visual_element(width)\n            label = self._format_value(key)\n            if self.show_pixel_values:\n                label += f'&lt;span class=\"pixel-value\"&gt;({width}px)&lt;/span&gt;'\n\n            items.append(f\"\"\"\n                &lt;div class=\"discrete-item\"&gt;\n                    &lt;div class=\"visual-column\"&gt;{visual}&lt;/div&gt;\n                    &lt;div class=\"label-column\"&gt;{label}&lt;/div&gt;\n                &lt;/div&gt;\n            \"\"\")\n\n        if self.mapping.default_output is not None:\n            width = self.mapping.default_output\n            visual = self.create_visual_element(width)\n            label = \"Other\"\n            if self.show_pixel_values:\n                label += f'&lt;span class=\"pixel-value\"&gt;({width}px)&lt;/span&gt;'\n\n            items.append(f\"\"\"\n                &lt;div class=\"discrete-item\"&gt;\n                    &lt;div class=\"visual-column\"&gt;{visual}&lt;/div&gt;\n                    &lt;div class=\"label-column\"&gt;{label}&lt;/div&gt;\n                &lt;/div&gt;\n            \"\"\")\n\n        return ''.join(items)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/line_text_overlay/","title":"MESQUAL Folium Line-Text-Overlay Visualization System","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/line_text_overlay/#mesqual.visualizations.folium_viz_system.viz_line_text_overlay","title":"viz_line_text_overlay","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/lines/","title":"MESQUAL Folium Text-Overlay Visualization System","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/lines/#mesqual.visualizations.folium_viz_system.viz_text_overlay","title":"viz_text_overlay","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/lines/#mesqual.visualizations.folium_viz_system.viz_text_overlay.ResolvedTextOverlayFeature","title":"ResolvedTextOverlayFeature  <code>dataclass</code>","text":"<p>               Bases: <code>ResolvedFeature</code></p> <p>Resolved visual properties for text overlay elements.</p> <p>Container for all computed styling properties of text overlay visualizations, including font styling, positioning, colors, and shadow effects. Used by TextOverlayGenerator to create folium markers with styled text content.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>@dataclass\nclass ResolvedTextOverlayFeature(ResolvedFeature):\n    \"\"\"\n    Resolved visual properties for text overlay elements.\n\n    Container for all computed styling properties of text overlay visualizations,\n    including font styling, positioning, colors, and shadow effects.\n    Used by TextOverlayGenerator to create folium markers with styled text content.\n    \"\"\"\n\n    @property\n    def location(self) -&gt; Point:\n        return self.get('location')\n\n    @property\n    def offset(self) -&gt; tuple[float, float]:\n        return self.get('offset')\n\n    @property\n    def azimuth_angle(self) -&gt; float:\n        return self.get('azimuth_angle')\n\n    @property\n    def text_color(self) -&gt; str:\n        return self.get('text_color')\n\n    @property\n    def font_size(self) -&gt; str:\n        return self.get('font_size')\n\n    @property\n    def font_weight(self) -&gt; str:\n        return self.get('font_weight')\n\n    @property\n    def background_color(self) -&gt; str:\n        return self.get('background_color')\n\n    @property\n    def shadow_size(self) -&gt; float:\n        return self.get('shadow_size')\n\n    @property\n    def shadow_color(self) -&gt; str:\n        return self.get('shadow_color')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/lines/#mesqual.visualizations.folium_viz_system.viz_text_overlay.TextOverlayFeatureResolver","title":"TextOverlayFeatureResolver","text":"<p>               Bases: <code>FeatureResolver[ResolvedTextOverlayFeature]</code></p> <p>Resolves visual properties for text overlay elements.</p> <p>Specialized feature resolver for text overlay visualizations that handles text content, font styling, positioning, and visual effects. Commonly used for adding data labels, value displays, or annotations to map elements.</p> <p>Parameters:</p> Name Type Description Default <code>text_color</code> <code>PropertyMapper | str</code> <p>Text color (static value or PropertyMapper)</p> <code>'#3A3A3A'</code> <code>font_size</code> <code>PropertyMapper | str</code> <p>Font size with units like '10pt' (static value or PropertyMapper)</p> <code>'10pt'</code> <code>font_weight</code> <code>PropertyMapper | str</code> <p>Font weight like 'bold', 'normal' (static value or PropertyMapper)</p> <code>'bold'</code> <code>background_color</code> <code>PropertyMapper | str</code> <p>Background color for text (static value or PropertyMapper)</p> <code>None</code> <code>shadow_size</code> <code>PropertyMapper | str</code> <p>Text shadow size like '0.5px' (static value or PropertyMapper)</p> <code>'0.5px'</code> <code>shadow_color</code> <code>PropertyMapper | str</code> <p>Text shadow color (static value or PropertyMapper)</p> <code>'#F2F2F2'</code> <code>text_print_content</code> <code>PropertyMapper | str | bool</code> <p>Text content to display (True for auto-generated)</p> <code>True</code> <code>tooltip</code> <code>PropertyMapper | str | bool</code> <p>Tooltip content (True for auto-generated, False for none)</p> <code>True</code> <code>popup</code> <code>PropertyMapper | Popup | bool</code> <p>Popup content (True/False/PropertyMapper)</p> <code>False</code> <code>location</code> <code>PropertyMapper | Point</code> <p>Point location (defaults to smart location detection)</p> <code>None</code> <code>**property_mappers</code> <code>PropertyMapper | Any</code> <p>Additional custom property mappings</p> <code>{}</code> <p>Examples:</p> <p>Basic value labels:</p> <pre><code>&gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n...     text_color='#000000',\n...     font_size='12pt',\n...     font_weight='bold'\n... )\n</code></pre> <p>Data-driven text styling:</p> <pre><code>&gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n...     text_color=PropertyMapper.from_kpi_value(\n...         lambda v: '#FF0000' if v &gt; 100 else '#000000'\n...     ),\n...     font_size=PropertyMapper.from_kpi_value(\n...         lambda v: f'{min(max(8 + v/50, 8), 20)}pt'\n...     ),\n...     text_print_content=PropertyMapper.from_kpi_value(\n...         lambda v: f'{v:.0f} MW'\n...     )\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>class TextOverlayFeatureResolver(FeatureResolver[ResolvedTextOverlayFeature]):\n    \"\"\"\n    Resolves visual properties for text overlay elements.\n\n    Specialized feature resolver for text overlay visualizations that handles text\n    content, font styling, positioning, and visual effects. Commonly used for\n    adding data labels, value displays, or annotations to map elements.\n\n    Args:\n        text_color: Text color (static value or PropertyMapper)\n        font_size: Font size with units like '10pt' (static value or PropertyMapper)\n        font_weight: Font weight like 'bold', 'normal' (static value or PropertyMapper)\n        background_color: Background color for text (static value or PropertyMapper)\n        shadow_size: Text shadow size like '0.5px' (static value or PropertyMapper)\n        shadow_color: Text shadow color (static value or PropertyMapper)\n        text_print_content: Text content to display (True for auto-generated)\n        tooltip: Tooltip content (True for auto-generated, False for none)\n        popup: Popup content (True/False/PropertyMapper)\n        location: Point location (defaults to smart location detection)\n        **property_mappers: Additional custom property mappings\n\n    Examples:\n        Basic value labels:\n        &gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n        ...     text_color='#000000',\n        ...     font_size='12pt',\n        ...     font_weight='bold'\n        ... )\n\n        Data-driven text styling:\n        &gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n        ...     text_color=PropertyMapper.from_kpi_value(\n        ...         lambda v: '#FF0000' if v &gt; 100 else '#000000'\n        ...     ),\n        ...     font_size=PropertyMapper.from_kpi_value(\n        ...         lambda v: f'{min(max(8 + v/50, 8), 20)}pt'\n        ...     ),\n        ...     text_print_content=PropertyMapper.from_kpi_value(\n        ...         lambda v: f'{v:.0f} MW'\n        ...     )\n        ... )\n    \"\"\"\n    def __init__(\n            self,\n            text_color: PropertyMapper | str = '#3A3A3A',\n            font_size: PropertyMapper | str = '10pt',\n            font_weight: PropertyMapper | str = 'bold',\n            background_color: PropertyMapper | str = None,\n            shadow_size: PropertyMapper | str = '0.5px',\n            shadow_color: PropertyMapper | str = '#F2F2F2',\n            text_print_content: PropertyMapper | str | bool = True,\n            tooltip: PropertyMapper | str | bool = True,\n            popup: PropertyMapper | folium.Popup | bool = False,\n            location: PropertyMapper | Point = None,\n            offset: PropertyMapper | tuple[float, float] = (0, 0),\n            azimuth_angle: PropertyMapper | float = 90,\n            **property_mappers: PropertyMapper | Any,\n    ):\n        mappers = dict(\n            text_color=text_color,\n            font_size=font_size,\n            font_weight=font_weight,\n            background_color=background_color,\n            shadow_size=shadow_size,\n            shadow_color=shadow_color,\n            text_print_content=text_print_content,\n            tooltip=tooltip,\n            popup=popup,\n            location=self._explicit_or_fallback(location, self._default_location_mapper()),\n            offset=offset,\n            azimuth_angle=azimuth_angle,\n            **property_mappers\n        )\n        super().__init__(feature_type=ResolvedTextOverlayFeature, **mappers)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/lines/#mesqual.visualizations.folium_viz_system.viz_text_overlay.TextOverlayGenerator","title":"TextOverlayGenerator","text":"<p>               Bases: <code>FoliumObjectGenerator[TextOverlayFeatureResolver]</code></p> <p>Generates text overlays for map data items.</p> <p>Creates folium Marker objects with styled HTML text content overlaid on the map. Handles text formatting, positioning, shadow effects, and responsive styling based on data values.</p> <p>Commonly used for displaying: - KPI values directly on map elements (power flows, prices, etc.) - Data labels for areas, lines, or points - Dynamic text that changes based on underlying data - Status indicators or categorical labels</p> <p>Examples:</p> <p>Value display on bidding zones:</p> <pre><code>&gt;&gt;&gt; from mesqual.units import Units\n&gt;&gt;&gt; text_gen = TextOverlayGenerator(\n...     TextOverlayFeatureResolver(\n...         text_print_content=PropertyMapper(\n...             lambda di: Units.get_pretty_text_for_quantity(\n...                 di.kpi.quantity, decimals=0, include_unit=False\n...             )\n...         ),\n...         font_size='10pt',\n...         font_weight='bold'\n...     )\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; fg = folium.FeatureGroup('Price Labels')\n&gt;&gt;&gt; text_gen.generate_objects_for_kpi_collection(price_kpis, fg)\n&gt;&gt;&gt; fg.add_to(map)\n</code></pre> <p>Combined with area visualization:</p> <pre><code>&gt;&gt;&gt; area_gen = AreaGenerator(...)\n&gt;&gt;&gt; text_gen = TextOverlayGenerator(...)\n&gt;&gt;&gt; \n&gt;&gt;&gt; area_gen.generate_objects_for_model_df(zones_df, feature_group)\n&gt;&gt;&gt; text_gen.generate_objects_for_model_df(zones_df, feature_group)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>class TextOverlayGenerator(FoliumObjectGenerator[TextOverlayFeatureResolver]):\n    \"\"\"\n    Generates text overlays for map data items.\n\n    Creates folium Marker objects with styled HTML text content overlaid on\n    the map. Handles text formatting, positioning, shadow effects, and\n    responsive styling based on data values.\n\n    Commonly used for displaying:\n    - KPI values directly on map elements (power flows, prices, etc.)\n    - Data labels for areas, lines, or points\n    - Dynamic text that changes based on underlying data\n    - Status indicators or categorical labels\n\n    Examples:\n        Value display on bidding zones:\n        &gt;&gt;&gt; from mesqual.units import Units\n        &gt;&gt;&gt; text_gen = TextOverlayGenerator(\n        ...     TextOverlayFeatureResolver(\n        ...         text_print_content=PropertyMapper(\n        ...             lambda di: Units.get_pretty_text_for_quantity(\n        ...                 di.kpi.quantity, decimals=0, include_unit=False\n        ...             )\n        ...         ),\n        ...         font_size='10pt',\n        ...         font_weight='bold'\n        ...     )\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; fg = folium.FeatureGroup('Price Labels')\n        &gt;&gt;&gt; text_gen.generate_objects_for_kpi_collection(price_kpis, fg)\n        &gt;&gt;&gt; fg.add_to(map)\n\n        Combined with area visualization:\n        &gt;&gt;&gt; area_gen = AreaGenerator(...)\n        &gt;&gt;&gt; text_gen = TextOverlayGenerator(...)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; area_gen.generate_objects_for_model_df(zones_df, feature_group)\n        &gt;&gt;&gt; text_gen.generate_objects_for_model_df(zones_df, feature_group)\n    \"\"\"\n    \"\"\"Generates text overlays for map data items.\"\"\"\n\n    def _feature_resolver_type(self) -&gt; Type[TextOverlayFeatureResolver]:\n        return TextOverlayFeatureResolver\n\n    def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n        \"\"\"\n        Generate and add a folium Marker with styled text overlay.\n\n        Args:\n            data_item: Data item containing point location and text content\n            feature_group: Folium feature group to add the text marker to\n        \"\"\"\n        style = self.feature_resolver.resolve_feature(data_item)\n        if not isinstance(style.location, Point):\n            return\n\n        if not style.text_print_content:\n            return\n\n        text_content = style.text_print_content\n        text_color = style.text_color\n        font_size = style.font_size\n        font_weight = style.font_weight\n        shadow_size = style.shadow_size\n        shadow_color = style.shadow_color\n\n        angle = float(style.azimuth_angle) - 90  # Folium counts clockwise from right-pointing direction; normal convention is CCW\n        x_offset, y_offset = style.offset\n\n        icon_html = f'''\n            &lt;div style=\"\n                position: absolute;\n                left: 50%;\n                top: 50%;\n                transform: translate(-50%, -50%) translate({x_offset}px, {-y_offset}px) rotate({angle}deg);\n                text-align: center;\n                font-size: {font_size};\n                font-weight: {font_weight};\n                color: {text_color};\n                white-space: nowrap;\n                text-shadow:\n                   -{shadow_size} -{shadow_size} 0 {shadow_color},  \n                    {shadow_size} -{shadow_size} 0 {shadow_color},\n                   -{shadow_size}  {shadow_size} 0 {shadow_color},\n                    {shadow_size}  {shadow_size} 0 {shadow_color};\n            \"&gt;\n                {text_content}\n            &lt;/div&gt;\n        '''\n\n        folium.Marker(\n            location=(style.location.y, style.location.x),\n            icon=folium.DivIcon(html=icon_html),\n            tooltip=style.tooltip,\n            popup=style.popup,\n        ).add_to(feature_group)\n\n    def _get_contrasting_color(self, surface_color: str) -&gt; str:\n        \"\"\"Get contrasting text color for a surface color.\"\"\"\n        if self._is_dark(surface_color):\n            return '#F2F2F2'\n        return '#3A3A3A'\n\n    def _get_shadow_color(self, text_color: str) -&gt; str:\n        \"\"\"Get shadow color for text.\"\"\"\n        if text_color == '#F2F2F2':\n            return '#3A3A3A'\n        return '#F2F2F2'\n\n    @staticmethod\n    def _is_dark(color: str) -&gt; bool:\n        \"\"\"Check if a color is dark.\"\"\"\n        if not color.startswith('#'):\n            return False\n        try:\n            r, g, b = [int(color[i:i + 2], 16) for i in (1, 3, 5)]\n            return (0.299 * r + 0.587 * g + 0.114 * b) &lt; 160\n        except (ValueError, IndexError):\n            return False\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/lines/#mesqual.visualizations.folium_viz_system.viz_text_overlay.TextOverlayGenerator.generate","title":"generate","text":"<pre><code>generate(data_item: VisualizableDataItem, feature_group: FeatureGroup) -&gt; None\n</code></pre> <p>Generate and add a folium Marker with styled text overlay.</p> <p>Parameters:</p> Name Type Description Default <code>data_item</code> <code>VisualizableDataItem</code> <p>Data item containing point location and text content</p> required <code>feature_group</code> <code>FeatureGroup</code> <p>Folium feature group to add the text marker to</p> required Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n    \"\"\"\n    Generate and add a folium Marker with styled text overlay.\n\n    Args:\n        data_item: Data item containing point location and text content\n        feature_group: Folium feature group to add the text marker to\n    \"\"\"\n    style = self.feature_resolver.resolve_feature(data_item)\n    if not isinstance(style.location, Point):\n        return\n\n    if not style.text_print_content:\n        return\n\n    text_content = style.text_print_content\n    text_color = style.text_color\n    font_size = style.font_size\n    font_weight = style.font_weight\n    shadow_size = style.shadow_size\n    shadow_color = style.shadow_color\n\n    angle = float(style.azimuth_angle) - 90  # Folium counts clockwise from right-pointing direction; normal convention is CCW\n    x_offset, y_offset = style.offset\n\n    icon_html = f'''\n        &lt;div style=\"\n            position: absolute;\n            left: 50%;\n            top: 50%;\n            transform: translate(-50%, -50%) translate({x_offset}px, {-y_offset}px) rotate({angle}deg);\n            text-align: center;\n            font-size: {font_size};\n            font-weight: {font_weight};\n            color: {text_color};\n            white-space: nowrap;\n            text-shadow:\n               -{shadow_size} -{shadow_size} 0 {shadow_color},  \n                {shadow_size} -{shadow_size} 0 {shadow_color},\n               -{shadow_size}  {shadow_size} 0 {shadow_color},\n                {shadow_size}  {shadow_size} 0 {shadow_color};\n        \"&gt;\n            {text_content}\n        &lt;/div&gt;\n    '''\n\n    folium.Marker(\n        location=(style.location.y, style.location.x),\n        icon=folium.DivIcon(html=icon_html),\n        tooltip=style.tooltip,\n        popup=style.popup,\n    ).add_to(feature_group)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/system/","title":"MESQUAL Folium Visualization System Base","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/system/#mesqual.visualizations.folium_viz_system.base_viz_system","title":"base_viz_system","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/system/#mesqual.visualizations.folium_viz_system.base_viz_system.PropertyMapper","title":"PropertyMapper","text":"<p>Maps data item attributes to visual properties for folium map visualization.</p> <p>Core abstraction for converting model data, KPI values, or static values into visual properties (colors, sizes, positions, etc.) for map elements. Used throughout the folium visualization system to create dynamic, data-driven map styling.</p> <p>The PropertyMapper encapsulates a transformation function that takes a VisualizableDataItem and returns a styled value. This enables powerful declarative map styling where visual properties are automatically computed from underlying data.</p> <p>Examples:</p> <p>Basic color mapping from KPI values:</p> <pre><code>&gt;&gt;&gt; color_mapper = PropertyMapper.from_kpi_value(lambda v: 'red' if v &gt; 0 else 'blue')\n</code></pre> <p>Size mapping from model attributes:</p> <pre><code>&gt;&gt;&gt; size_mapper = PropertyMapper.from_item_attr('capacity', lambda c: c / 100)\n</code></pre> <p>Static styling:</p> <pre><code>&gt;&gt;&gt; border_mapper = PropertyMapper.from_static_value('#000000')\n</code></pre> <p>Complex conditional styling:</p> <pre><code>&gt;&gt;&gt; def complex_color(data_item: KPIDataItem):\n...     kpi_val = data_item.kpi.value\n...     threshold = data_item.get_object_attribute('threshold')\n...     return 'green' if kpi_val &gt; threshold else 'red'\n&gt;&gt;&gt; mapper = PropertyMapper(complex_color)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>class PropertyMapper:\n    \"\"\"\n    Maps data item attributes to visual properties for folium map visualization.\n\n    Core abstraction for converting model data, KPI values, or static values into\n    visual properties (colors, sizes, positions, etc.) for map elements. Used throughout\n    the folium visualization system to create dynamic, data-driven map styling.\n\n    The PropertyMapper encapsulates a transformation function that takes a VisualizableDataItem\n    and returns a styled value. This enables powerful declarative map styling where\n    visual properties are automatically computed from underlying data.\n\n    Examples:\n        Basic color mapping from KPI values:\n        &gt;&gt;&gt; color_mapper = PropertyMapper.from_kpi_value(lambda v: 'red' if v &gt; 0 else 'blue')\n\n        Size mapping from model attributes:\n        &gt;&gt;&gt; size_mapper = PropertyMapper.from_item_attr('capacity', lambda c: c / 100)\n\n        Static styling:\n        &gt;&gt;&gt; border_mapper = PropertyMapper.from_static_value('#000000')\n\n        Complex conditional styling:\n        &gt;&gt;&gt; def complex_color(data_item: KPIDataItem):\n        ...     kpi_val = data_item.kpi.value\n        ...     threshold = data_item.get_object_attribute('threshold')\n        ...     return 'green' if kpi_val &gt; threshold else 'red'\n        &gt;&gt;&gt; mapper = PropertyMapper(complex_color)\n    \"\"\"\n    def __init__(self, mapping: Callable[[VisualizableDataItem], Any]):\n        self.mapping = mapping\n\n    def map_data_item(self, data_item: VisualizableDataItem) -&gt; Any:\n        \"\"\"\n        Map data item to property value.\n\n        Args:\n            data_item: Data item to map\n\n        Returns:\n            Mapped property value\n        \"\"\"\n        return self.mapping(data_item)\n\n    @classmethod\n    def from_static_value(cls, value: Any) -&gt; 'PropertyMapper':\n        \"\"\"\n        Create mapper that returns the same value for all data items.\n\n        Used for consistent styling across all map elements (e.g., all borders\n        the same color, all markers the same size).\n\n        Args:\n            value: Static value to return for all data items\n\n        Returns:\n            PropertyMapper that always returns the static value\n\n        Examples:\n            &gt;&gt;&gt; border_color = PropertyMapper.from_static_value('#FFFFFF')\n            &gt;&gt;&gt; opacity = PropertyMapper.from_static_value(0.8)\n        \"\"\"\n        return cls(lambda data_item: value)\n\n    @classmethod\n    def from_item_attr(\n            cls,\n            attribute: str,\n            mapping: Callable[[Any], Any] = None,\n    ) -&gt; 'PropertyMapper':\n        \"\"\"\n        Create mapper from model/object attribute with optional transformation.\n\n        Extracts values from model data attributes (geometry, capacity, name, etc.)\n        and optionally applies a transformation function. The attribute is resolved\n        from the underlying model DataFrame or object data.\n\n        Args:\n            attribute: Name of the attribute to extract (e.g., 'geometry', 'capacity'), must be an entry in the object pd.Series\n            mapping: Optional transformation function to apply to the attribute value\n\n        Returns:\n            PropertyMapper that extracts and optionally transforms the attribute\n\n        Examples:\n            &gt;&gt;&gt; # Direct attribute access\n            &gt;&gt;&gt; geom_mapper = PropertyMapper.from_item_attr('geometry')\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # With color scale transformation\n            &gt;&gt;&gt; color_scale = SegmentedContinuousColorscale(...)\n            &gt;&gt;&gt; color_mapper = PropertyMapper.from_item_attr('capacity', color_scale)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # With custom transformation\n            &gt;&gt;&gt; size_mapper = PropertyMapper.from_item_attr('power_mw',\n            ...                                           lambda mw: min(max(mw/10, 5), 50))\n        \"\"\"\n        if mapping is None:\n            mapping = lambda x: x\n        return cls(lambda data_item: mapping(data_item.get_object_attribute(attribute)))\n\n    @classmethod\n    def from_kpi_value(cls, mapping: Callable[[Any], Any]) -&gt; 'PropertyMapper':\n        \"\"\"\n        Create mapper from KPI values with transformation function.\n\n        Specifically designed for KPIDataItem objects, extracts the computed KPI value\n        and applies a transformation. Used for styling based on energy system metrics\n        like power flows, prices, or emissions.\n\n        Args:\n            mapping: Transformation function applied to the KPI value\n\n        Returns:\n            PropertyMapper that transforms KPI values\n\n        Examples:\n            &gt;&gt;&gt; # Color mapping for power flows\n            &gt;&gt;&gt; flow_colors = PropertyMapper.from_kpi_value(\n            ...     lambda v: 'red' if v &gt; 1000 else 'green'\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Size mapping for prices\n            &gt;&gt;&gt; price_sizes = PropertyMapper.from_kpi_value(\n            ...     lambda p: min(max(p * 2, 10), 100)\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Using value mapping system\n            &gt;&gt;&gt; colorscale = SegmentedContinuousColorscale(...)\n            &gt;&gt;&gt; colors = PropertyMapper.from_kpi_value(colorscale)\n        \"\"\"\n        return cls(lambda data_item: mapping(data_item.kpi.value))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/system/#mesqual.visualizations.folium_viz_system.base_viz_system.PropertyMapper.map_data_item","title":"map_data_item","text":"<pre><code>map_data_item(data_item: VisualizableDataItem) -&gt; Any\n</code></pre> <p>Map data item to property value.</p> <p>Parameters:</p> Name Type Description Default <code>data_item</code> <code>VisualizableDataItem</code> <p>Data item to map</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Mapped property value</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>def map_data_item(self, data_item: VisualizableDataItem) -&gt; Any:\n    \"\"\"\n    Map data item to property value.\n\n    Args:\n        data_item: Data item to map\n\n    Returns:\n        Mapped property value\n    \"\"\"\n    return self.mapping(data_item)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/system/#mesqual.visualizations.folium_viz_system.base_viz_system.PropertyMapper.from_static_value","title":"from_static_value  <code>classmethod</code>","text":"<pre><code>from_static_value(value: Any) -&gt; PropertyMapper\n</code></pre> <p>Create mapper that returns the same value for all data items.</p> <p>Used for consistent styling across all map elements (e.g., all borders the same color, all markers the same size).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Static value to return for all data items</p> required <p>Returns:</p> Type Description <code>PropertyMapper</code> <p>PropertyMapper that always returns the static value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; border_color = PropertyMapper.from_static_value('#FFFFFF')\n&gt;&gt;&gt; opacity = PropertyMapper.from_static_value(0.8)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>@classmethod\ndef from_static_value(cls, value: Any) -&gt; 'PropertyMapper':\n    \"\"\"\n    Create mapper that returns the same value for all data items.\n\n    Used for consistent styling across all map elements (e.g., all borders\n    the same color, all markers the same size).\n\n    Args:\n        value: Static value to return for all data items\n\n    Returns:\n        PropertyMapper that always returns the static value\n\n    Examples:\n        &gt;&gt;&gt; border_color = PropertyMapper.from_static_value('#FFFFFF')\n        &gt;&gt;&gt; opacity = PropertyMapper.from_static_value(0.8)\n    \"\"\"\n    return cls(lambda data_item: value)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/system/#mesqual.visualizations.folium_viz_system.base_viz_system.PropertyMapper.from_item_attr","title":"from_item_attr  <code>classmethod</code>","text":"<pre><code>from_item_attr(attribute: str, mapping: Callable[[Any], Any] = None) -&gt; PropertyMapper\n</code></pre> <p>Create mapper from model/object attribute with optional transformation.</p> <p>Extracts values from model data attributes (geometry, capacity, name, etc.) and optionally applies a transformation function. The attribute is resolved from the underlying model DataFrame or object data.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>Name of the attribute to extract (e.g., 'geometry', 'capacity'), must be an entry in the object pd.Series</p> required <code>mapping</code> <code>Callable[[Any], Any]</code> <p>Optional transformation function to apply to the attribute value</p> <code>None</code> <p>Returns:</p> Type Description <code>PropertyMapper</code> <p>PropertyMapper that extracts and optionally transforms the attribute</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Direct attribute access\n&gt;&gt;&gt; geom_mapper = PropertyMapper.from_item_attr('geometry')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With color scale transformation\n&gt;&gt;&gt; color_scale = SegmentedContinuousColorscale(...)\n&gt;&gt;&gt; color_mapper = PropertyMapper.from_item_attr('capacity', color_scale)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With custom transformation\n&gt;&gt;&gt; size_mapper = PropertyMapper.from_item_attr('power_mw',\n...                                           lambda mw: min(max(mw/10, 5), 50))\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>@classmethod\ndef from_item_attr(\n        cls,\n        attribute: str,\n        mapping: Callable[[Any], Any] = None,\n) -&gt; 'PropertyMapper':\n    \"\"\"\n    Create mapper from model/object attribute with optional transformation.\n\n    Extracts values from model data attributes (geometry, capacity, name, etc.)\n    and optionally applies a transformation function. The attribute is resolved\n    from the underlying model DataFrame or object data.\n\n    Args:\n        attribute: Name of the attribute to extract (e.g., 'geometry', 'capacity'), must be an entry in the object pd.Series\n        mapping: Optional transformation function to apply to the attribute value\n\n    Returns:\n        PropertyMapper that extracts and optionally transforms the attribute\n\n    Examples:\n        &gt;&gt;&gt; # Direct attribute access\n        &gt;&gt;&gt; geom_mapper = PropertyMapper.from_item_attr('geometry')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With color scale transformation\n        &gt;&gt;&gt; color_scale = SegmentedContinuousColorscale(...)\n        &gt;&gt;&gt; color_mapper = PropertyMapper.from_item_attr('capacity', color_scale)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With custom transformation\n        &gt;&gt;&gt; size_mapper = PropertyMapper.from_item_attr('power_mw',\n        ...                                           lambda mw: min(max(mw/10, 5), 50))\n    \"\"\"\n    if mapping is None:\n        mapping = lambda x: x\n    return cls(lambda data_item: mapping(data_item.get_object_attribute(attribute)))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/system/#mesqual.visualizations.folium_viz_system.base_viz_system.PropertyMapper.from_kpi_value","title":"from_kpi_value  <code>classmethod</code>","text":"<pre><code>from_kpi_value(mapping: Callable[[Any], Any]) -&gt; PropertyMapper\n</code></pre> <p>Create mapper from KPI values with transformation function.</p> <p>Specifically designed for KPIDataItem objects, extracts the computed KPI value and applies a transformation. Used for styling based on energy system metrics like power flows, prices, or emissions.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Callable[[Any], Any]</code> <p>Transformation function applied to the KPI value</p> required <p>Returns:</p> Type Description <code>PropertyMapper</code> <p>PropertyMapper that transforms KPI values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Color mapping for power flows\n&gt;&gt;&gt; flow_colors = PropertyMapper.from_kpi_value(\n...     lambda v: 'red' if v &gt; 1000 else 'green'\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Size mapping for prices\n&gt;&gt;&gt; price_sizes = PropertyMapper.from_kpi_value(\n...     lambda p: min(max(p * 2, 10), 100)\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Using value mapping system\n&gt;&gt;&gt; colorscale = SegmentedContinuousColorscale(...)\n&gt;&gt;&gt; colors = PropertyMapper.from_kpi_value(colorscale)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>@classmethod\ndef from_kpi_value(cls, mapping: Callable[[Any], Any]) -&gt; 'PropertyMapper':\n    \"\"\"\n    Create mapper from KPI values with transformation function.\n\n    Specifically designed for KPIDataItem objects, extracts the computed KPI value\n    and applies a transformation. Used for styling based on energy system metrics\n    like power flows, prices, or emissions.\n\n    Args:\n        mapping: Transformation function applied to the KPI value\n\n    Returns:\n        PropertyMapper that transforms KPI values\n\n    Examples:\n        &gt;&gt;&gt; # Color mapping for power flows\n        &gt;&gt;&gt; flow_colors = PropertyMapper.from_kpi_value(\n        ...     lambda v: 'red' if v &gt; 1000 else 'green'\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Size mapping for prices\n        &gt;&gt;&gt; price_sizes = PropertyMapper.from_kpi_value(\n        ...     lambda p: min(max(p * 2, 10), 100)\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Using value mapping system\n        &gt;&gt;&gt; colorscale = SegmentedContinuousColorscale(...)\n        &gt;&gt;&gt; colors = PropertyMapper.from_kpi_value(colorscale)\n    \"\"\"\n    return cls(lambda data_item: mapping(data_item.kpi.value))\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/system/#mesqual.visualizations.folium_viz_system.base_viz_system.ResolvedFeature","title":"ResolvedFeature  <code>dataclass</code>","text":"<p>Container for resolved feature properties.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>@dataclass\nclass ResolvedFeature:\n    \"\"\"Container for resolved feature properties.\"\"\"\n    properties: dict = field(default_factory=dict)\n    tooltip: str = None\n    popup: folium.Popup = None\n    text_print_content: str = None\n\n    def get(self, property: str, default=None):\n        return self.properties.get(property, default)\n\n    def __getitem__(self, key):\n        return self.properties[key]\n\n    def __setitem__(self, key, value):\n        self.properties[key] = value\n\n    def __contains__(self, key):\n        return key in self.properties\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        out = dict(self.properties)\n        for name in dir(self.__class__):\n            attr = getattr(self.__class__, name)\n            if isinstance(attr, property):\n                out[name] = getattr(self, name)\n        return out\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/system/#mesqual.visualizations.folium_viz_system.base_viz_system.FeatureResolver","title":"FeatureResolver","text":"<p>               Bases: <code>Generic[ResolvedFeatureType]</code></p> <p>Resolves visual feature properties from data items using PropertyMappers.</p> <p>Central orchestrator for map element styling that takes a VisualizableDataItem and a collection of PropertyMappers, then produces a ResolvedFeature containing all computed visual properties. Handles default values, tooltip generation, and property normalization.</p> <p>The FeatureResolver acts as a bridge between data and visualization, converting raw data items into styled features ready for folium map rendering. It supports automatic tooltip/popup generation and flexible property mapping.</p> <p>          Class Type Parameters:        </p> Name Bound or Constraints Description Default <code>ResolvedFeatureType</code> <p>The specific resolved feature type (e.g., ResolvedAreaFeature)</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; resolver = AreaFeatureResolver(\n...     fill_color=PropertyMapper.from_kpi_value(color_scale),\n...     fill_opacity=PropertyMapper.from_static_value(0.8),\n...     tooltip=True  # Auto-generate tooltip\n... )\n&gt;&gt;&gt; resolved = resolver.resolve_feature(kpi_data_item)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>class FeatureResolver(Generic[ResolvedFeatureType]):\n    \"\"\"\n    Resolves visual feature properties from data items using PropertyMappers.\n\n    Central orchestrator for map element styling that takes a VisualizableDataItem\n    and a collection of PropertyMappers, then produces a ResolvedFeature containing\n    all computed visual properties. Handles default values, tooltip generation,\n    and property normalization.\n\n    The FeatureResolver acts as a bridge between data and visualization, converting\n    raw data items into styled features ready for folium map rendering. It supports\n    automatic tooltip/popup generation and flexible property mapping.\n\n    Type Parameters:\n        ResolvedFeatureType: The specific resolved feature type (e.g., ResolvedAreaFeature)\n\n    Examples:\n        &gt;&gt;&gt; resolver = AreaFeatureResolver(\n        ...     fill_color=PropertyMapper.from_kpi_value(color_scale),\n        ...     fill_opacity=PropertyMapper.from_static_value(0.8),\n        ...     tooltip=True  # Auto-generate tooltip\n        ... )\n        &gt;&gt;&gt; resolved = resolver.resolve_feature(kpi_data_item)\n    \"\"\"\n    def __init__(\n        self,\n        feature_type: Type[ResolvedFeatureType] = None,\n        **property_mappers: PropertyMapper | Any\n    ):\n        self.feature_type: Type[ResolvedFeatureType] = feature_type or ResolvedFeatureType.__constraints__[0]\n\n        _defaults_if_true = dict(\n            tooltip=self._default_tooltip_generator,\n            popup=self._default_popup_generator,\n            text_print_content=self._default_text_print_generator,\n        )\n        for k, mapper_factory in _defaults_if_true.items():\n            if property_mappers.get(k, None) is True:\n                property_mappers[k] = mapper_factory()\n            elif property_mappers.get(k, None) is False:\n                property_mappers[k] = None\n\n        self.property_mappers: dict[str, PropertyMapper] = self._normalize_property_mappers(property_mappers)\n\n    def resolve_feature(self, data_item: VisualizableDataItem) -&gt; ResolvedFeatureType:\n        \"\"\"\n        Resolve feature properties from data item.\n\n        Args:\n            data_item: Data item to resolve\n\n        Returns:\n            Resolved feature with all computed properties\n        \"\"\"\n        resolved = self.feature_type()\n        for prop, mapper in self.property_mappers.items():\n            resolved[prop] = mapper.map_data_item(data_item)\n            if prop in ['tooltip', 'popup', 'text_print_content']:\n                setattr(resolved, prop, mapper.map_data_item(data_item))\n        return resolved\n\n    @staticmethod\n    def _normalize_property_mappers(mappers: dict[str, PropertyMapper | Any]) -&gt; dict[str, PropertyMapper]:\n        return {\n            key: mapper if isinstance(mapper, PropertyMapper) else PropertyMapper.from_static_value(mapper)\n            for key, mapper in mappers.items()\n        }\n\n    @staticmethod\n    def _explicit_or_fallback(explicit: Any, fallback: PropertyMapper = None) -&gt; PropertyMapper:\n        if explicit is not None:\n            return explicit\n        return fallback\n\n    @staticmethod\n    def _default_tooltip_generator() -&gt; PropertyMapper:\n        \"\"\"\n        Create default tooltip generator showing data item information.\n\n        Returns:\n            PropertyMapper that generates HTML table tooltips with data item attributes\n        \"\"\"\n\n        def get_tooltip(data_item: VisualizableDataItem) -&gt; str:\n            tooltip_data = data_item.get_tooltip_data()\n\n            html = '&lt;table style=\"border-collapse: collapse;\"&gt;\\n'\n            for key, value in tooltip_data.items():\n                html += f'  &lt;tr&gt;&lt;td style=\"padding: 4px 8px;\"&gt;&lt;strong&gt;{key}&lt;/strong&gt;&lt;/td&gt;' \\\n                        f'&lt;td style=\"text-align: right; padding: 4px 8px;\"&gt;{value}&lt;/td&gt;&lt;/tr&gt;\\n'\n            html += '&lt;/table&gt;'\n\n            return html\n\n        return PropertyMapper(get_tooltip)\n\n    @staticmethod\n    def _default_popup_generator() -&gt; PropertyMapper:\n        \"\"\"\n        Create default popup generator with formatted data item information.\n\n        Returns:\n            PropertyMapper that generates folium.Popup objects with data tables\n        \"\"\"\n\n        def get_popup(data_item: VisualizableDataItem) -&gt; folium.Popup:\n            tooltip_data = data_item.get_tooltip_data()\n\n            html = '&lt;table style=\"border-collapse: collapse;\"&gt;\\n'\n            for key, value in tooltip_data.items():\n                html += f'  &lt;tr&gt;&lt;td style=\"padding: 4px 8px;\"&gt;&lt;strong&gt;{key}&lt;/strong&gt;&lt;/td&gt;' \\\n                        f'&lt;td style=\"text-align: right; padding: 4px 8px;\"&gt;{value}&lt;/td&gt;&lt;/tr&gt;\\n'\n            html += '&lt;/table&gt;'\n\n            return folium.Popup(html, max_width=300)\n\n        return PropertyMapper(get_popup)\n\n    @staticmethod\n    def _default_text_print_generator() -&gt; PropertyMapper:\n        \"\"\"\n        Create default text content generator for overlay labels.\n\n        Returns:\n            PropertyMapper that returns data item text representation\n        \"\"\"\n        def get_text(data_item: VisualizableDataItem) -&gt; str:\n            return data_item.get_text_representation()\n\n        return PropertyMapper(get_text)\n\n    @staticmethod\n    def _default_geometry_mapper() -&gt; PropertyMapper:\n        \"\"\"\n        Create default geometry mapper that extracts geometric objects.\n\n        Returns:\n            PropertyMapper that extracts 'geometry' attribute from data items\n        \"\"\"\n\n        def get_geometry(data_item: VisualizableDataItem) -&gt; Polygon | None:\n            if data_item.object_has_attribute('geometry'):\n                return data_item.get_object_attribute('geometry')\n            return None\n\n        return PropertyMapper(get_geometry)\n\n    @staticmethod\n    def _default_location_mapper() -&gt; PropertyMapper:\n        \"\"\"\n        Create smart location mapper with multiple fallback strategies.\n\n        Attempts to extract Point locations from data items using various\n        attribute names and geometric calculations. Handles common location\n        attribute names and derives locations from complex geometries.\n\n        Returns:\n            PropertyMapper that intelligently extracts Point locations\n        \"\"\"\n\n        def get_location(data_item: VisualizableDataItem) -&gt; Point | None:\n            for k in ['location', 'projection_point', 'centroid', 'midpoint']:\n                if data_item.object_has_attribute(k):\n                    location = data_item.get_object_attribute(k)\n                    if isinstance(location, Point):\n                        return location\n\n            for lat, lon in [('lat', 'lon'), ('latitude', 'longitude')]:\n                if data_item.object_has_attribute(lat) and data_item.object_has_attribute(lon):\n                    lat_value = data_item.get_object_attribute(lat)\n                    lon_value = data_item.get_object_attribute(lon)\n                    if all(isinstance(v, (int, float)) for v in [lat_value, lon_value]):\n                        return Point([lon_value, lat_value])\n\n            if data_item.object_has_attribute('geometry'):\n                geometry = data_item.get_object_attribute('geometry')\n                if isinstance(geometry, Point):\n                    return geometry\n                elif isinstance(geometry, (Polygon, MultiPolygon)):\n                    return geometry.representative_point()\n                elif isinstance(geometry, (LineString, MultiLineString)):\n                    return geometry.interpolate(0.5, normalized=True)\n            return None\n\n        return PropertyMapper(get_location)\n\n    @staticmethod\n    def _default_line_string_mapper() -&gt; PropertyMapper:\n        \"\"\"\n        Create default LineString geometry mapper for line visualizations.\n\n        Returns:\n            PropertyMapper that extracts LineString geometries from data items\n        \"\"\"\n\n        def get_line_string(data_item: VisualizableDataItem) -&gt; LineString | None:\n            for k in ['geometry', 'line_string']:\n                if data_item.object_has_attribute(k):\n                    line_string = data_item.get_object_attribute(k)\n                    if isinstance(line_string, (LineString, MultiLineString)):\n                        return line_string\n            return None\n\n        return PropertyMapper(get_line_string)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/system/#mesqual.visualizations.folium_viz_system.base_viz_system.FeatureResolver.resolve_feature","title":"resolve_feature","text":"<pre><code>resolve_feature(data_item: VisualizableDataItem) -&gt; ResolvedFeatureType\n</code></pre> <p>Resolve feature properties from data item.</p> <p>Parameters:</p> Name Type Description Default <code>data_item</code> <code>VisualizableDataItem</code> <p>Data item to resolve</p> required <p>Returns:</p> Type Description <code>ResolvedFeatureType</code> <p>Resolved feature with all computed properties</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>def resolve_feature(self, data_item: VisualizableDataItem) -&gt; ResolvedFeatureType:\n    \"\"\"\n    Resolve feature properties from data item.\n\n    Args:\n        data_item: Data item to resolve\n\n    Returns:\n        Resolved feature with all computed properties\n    \"\"\"\n    resolved = self.feature_type()\n    for prop, mapper in self.property_mappers.items():\n        resolved[prop] = mapper.map_data_item(data_item)\n        if prop in ['tooltip', 'popup', 'text_print_content']:\n            setattr(resolved, prop, mapper.map_data_item(data_item))\n    return resolved\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/system/#mesqual.visualizations.folium_viz_system.base_viz_system.FoliumObjectGenerator","title":"FoliumObjectGenerator","text":"<p>               Bases: <code>Generic[FeatureResolverType]</code>, <code>ABC</code></p> <p>Abstract base class for generating folium map objects from data items.</p> <p>Defines the interface for converting VisualizableDataItems into folium map elements (areas, lines, markers, etc.). Each generator type handles a specific kind of map visualization and uses a corresponding FeatureResolver to compute visual properties.</p> <p>The generator pattern enables modular, composable map building where different visualization types can be combined within the same map. Generators can process both model DataFrames and KPI collections.</p> <p>          Class Type Parameters:        </p> Name Bound or Constraints Description Default <code>FeatureResolverType</code> <p>The specific feature resolver type used by this generator</p> required <p>Examples:</p> <p>Typical usage in map building:</p> <pre><code>&gt;&gt;&gt; area_gen = AreaGenerator(AreaFeatureResolver(fill_color=...))\n&gt;&gt;&gt; line_gen = LineGenerator(LineFeatureResolver(line_color=...))\n&gt;&gt;&gt;\n&gt;&gt;&gt; fg = folium.FeatureGroup('My Data')\n&gt;&gt;&gt; area_gen.generate_objects_for_model_df(model_df, fg)\n&gt;&gt;&gt; line_gen.generate_objects_for_kpi_collection(kpi_collection, fg)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>class FoliumObjectGenerator(Generic[FeatureResolverType], ABC):\n    \"\"\"\n    Abstract base class for generating folium map objects from data items.\n\n    Defines the interface for converting VisualizableDataItems into folium\n    map elements (areas, lines, markers, etc.). Each generator type handles\n    a specific kind of map visualization and uses a corresponding FeatureResolver\n    to compute visual properties.\n\n    The generator pattern enables modular, composable map building where different\n    visualization types can be combined within the same map. Generators can process\n    both model DataFrames and KPI collections.\n\n    Type Parameters:\n        FeatureResolverType: The specific feature resolver type used by this generator\n\n    Examples:\n        Typical usage in map building:\n        &gt;&gt;&gt; area_gen = AreaGenerator(AreaFeatureResolver(fill_color=...))\n        &gt;&gt;&gt; line_gen = LineGenerator(LineFeatureResolver(line_color=...))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; fg = folium.FeatureGroup('My Data')\n        &gt;&gt;&gt; area_gen.generate_objects_for_model_df(model_df, fg)\n        &gt;&gt;&gt; line_gen.generate_objects_for_kpi_collection(kpi_collection, fg)\n    \"\"\"\n\n    def __init__(\n            self,\n            feature_resolver: FeatureResolverType = None,\n    ):\n        self.feature_resolver: FeatureResolverType = feature_resolver or self._feature_resolver_type()()\n\n    @abstractmethod\n    def _feature_resolver_type(self) -&gt; Type[FeatureResolverType]:\n        return FeatureResolver\n\n    @abstractmethod\n    def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n        \"\"\"\n        Generate folium object and add it to the feature group.\n\n        Args:\n            data_item: Data item to visualize\n            feature_group: Feature group to add to\n        \"\"\"\n        pass\n\n    def generate_objects_for_model_df(\n            self,\n            model_df: pd.DataFrame,\n            feature_group: folium.FeatureGroup,\n            **kwargs\n    ) -&gt; folium.FeatureGroup:\n        \"\"\"Add model DataFrame data to the map.\"\"\"\n        model_dff = add_index_as_column(model_df)\n        object_type = model_dff.index.name if isinstance(model_dff.index.name, str) else None\n        for _, row in model_dff.iterrows():\n            data_item = ModelDataItem(row, object_type=object_type, **kwargs)\n            self.generate(data_item, feature_group)\n        return feature_group\n\n    def generate_objects_for_kpi_collection(\n            self,\n            kpi_collection: 'KPICollection',\n            feature_group: folium.FeatureGroup,\n            **kwargs\n    ) -&gt; folium.FeatureGroup:\n        \"\"\"Add KPI data to the map.\"\"\"\n        for kpi in kpi_collection:\n            data_item = KPIDataItem(kpi, kpi_collection, **kwargs)\n            self.generate(data_item, feature_group)\n        return feature_group\n\n    def generate_object_for_single_kpi(\n            self,\n            kpi: 'KPI',\n            feature_group: folium.FeatureGroup,\n            kpi_collection: 'KPICollection' = None,\n            **kwargs\n    ) -&gt; folium.FeatureGroup:\n        \"\"\"Add a single KPI to the map with optional context.\"\"\"\n        data_item = KPIDataItem(kpi, kpi_collection, **kwargs)\n        self.generate(data_item, feature_group)\n        return feature_group\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/system/#mesqual.visualizations.folium_viz_system.base_viz_system.FoliumObjectGenerator.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(data_item: VisualizableDataItem, feature_group: FeatureGroup) -&gt; None\n</code></pre> <p>Generate folium object and add it to the feature group.</p> <p>Parameters:</p> Name Type Description Default <code>data_item</code> <code>VisualizableDataItem</code> <p>Data item to visualize</p> required <code>feature_group</code> <code>FeatureGroup</code> <p>Feature group to add to</p> required Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>@abstractmethod\ndef generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n    \"\"\"\n    Generate folium object and add it to the feature group.\n\n    Args:\n        data_item: Data item to visualize\n        feature_group: Feature group to add to\n    \"\"\"\n    pass\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/system/#mesqual.visualizations.folium_viz_system.base_viz_system.FoliumObjectGenerator.generate_objects_for_model_df","title":"generate_objects_for_model_df","text":"<pre><code>generate_objects_for_model_df(model_df: DataFrame, feature_group: FeatureGroup, **kwargs) -&gt; FeatureGroup\n</code></pre> <p>Add model DataFrame data to the map.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>def generate_objects_for_model_df(\n        self,\n        model_df: pd.DataFrame,\n        feature_group: folium.FeatureGroup,\n        **kwargs\n) -&gt; folium.FeatureGroup:\n    \"\"\"Add model DataFrame data to the map.\"\"\"\n    model_dff = add_index_as_column(model_df)\n    object_type = model_dff.index.name if isinstance(model_dff.index.name, str) else None\n    for _, row in model_dff.iterrows():\n        data_item = ModelDataItem(row, object_type=object_type, **kwargs)\n        self.generate(data_item, feature_group)\n    return feature_group\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/system/#mesqual.visualizations.folium_viz_system.base_viz_system.FoliumObjectGenerator.generate_objects_for_kpi_collection","title":"generate_objects_for_kpi_collection","text":"<pre><code>generate_objects_for_kpi_collection(kpi_collection: KPICollection, feature_group: FeatureGroup, **kwargs) -&gt; FeatureGroup\n</code></pre> <p>Add KPI data to the map.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>def generate_objects_for_kpi_collection(\n        self,\n        kpi_collection: 'KPICollection',\n        feature_group: folium.FeatureGroup,\n        **kwargs\n) -&gt; folium.FeatureGroup:\n    \"\"\"Add KPI data to the map.\"\"\"\n    for kpi in kpi_collection:\n        data_item = KPIDataItem(kpi, kpi_collection, **kwargs)\n        self.generate(data_item, feature_group)\n    return feature_group\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/system/#mesqual.visualizations.folium_viz_system.base_viz_system.FoliumObjectGenerator.generate_object_for_single_kpi","title":"generate_object_for_single_kpi","text":"<pre><code>generate_object_for_single_kpi(kpi: KPI, feature_group: FeatureGroup, kpi_collection: KPICollection = None, **kwargs) -&gt; FeatureGroup\n</code></pre> <p>Add a single KPI to the map with optional context.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/base_viz_system.py</code> <pre><code>def generate_object_for_single_kpi(\n        self,\n        kpi: 'KPI',\n        feature_group: folium.FeatureGroup,\n        kpi_collection: 'KPICollection' = None,\n        **kwargs\n) -&gt; folium.FeatureGroup:\n    \"\"\"Add a single KPI to the map with optional context.\"\"\"\n    data_item = KPIDataItem(kpi, kpi_collection, **kwargs)\n    self.generate(data_item, feature_group)\n    return feature_group\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/text_overlay/","title":"MESQUAL Folium Text-Overlay Visualization System","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/text_overlay/#mesqual.visualizations.folium_viz_system.viz_text_overlay","title":"viz_text_overlay","text":""},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/text_overlay/#mesqual.visualizations.folium_viz_system.viz_text_overlay.ResolvedTextOverlayFeature","title":"ResolvedTextOverlayFeature  <code>dataclass</code>","text":"<p>               Bases: <code>ResolvedFeature</code></p> <p>Resolved visual properties for text overlay elements.</p> <p>Container for all computed styling properties of text overlay visualizations, including font styling, positioning, colors, and shadow effects. Used by TextOverlayGenerator to create folium markers with styled text content.</p> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>@dataclass\nclass ResolvedTextOverlayFeature(ResolvedFeature):\n    \"\"\"\n    Resolved visual properties for text overlay elements.\n\n    Container for all computed styling properties of text overlay visualizations,\n    including font styling, positioning, colors, and shadow effects.\n    Used by TextOverlayGenerator to create folium markers with styled text content.\n    \"\"\"\n\n    @property\n    def location(self) -&gt; Point:\n        return self.get('location')\n\n    @property\n    def offset(self) -&gt; tuple[float, float]:\n        return self.get('offset')\n\n    @property\n    def azimuth_angle(self) -&gt; float:\n        return self.get('azimuth_angle')\n\n    @property\n    def text_color(self) -&gt; str:\n        return self.get('text_color')\n\n    @property\n    def font_size(self) -&gt; str:\n        return self.get('font_size')\n\n    @property\n    def font_weight(self) -&gt; str:\n        return self.get('font_weight')\n\n    @property\n    def background_color(self) -&gt; str:\n        return self.get('background_color')\n\n    @property\n    def shadow_size(self) -&gt; float:\n        return self.get('shadow_size')\n\n    @property\n    def shadow_color(self) -&gt; str:\n        return self.get('shadow_color')\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/text_overlay/#mesqual.visualizations.folium_viz_system.viz_text_overlay.TextOverlayFeatureResolver","title":"TextOverlayFeatureResolver","text":"<p>               Bases: <code>FeatureResolver[ResolvedTextOverlayFeature]</code></p> <p>Resolves visual properties for text overlay elements.</p> <p>Specialized feature resolver for text overlay visualizations that handles text content, font styling, positioning, and visual effects. Commonly used for adding data labels, value displays, or annotations to map elements.</p> <p>Parameters:</p> Name Type Description Default <code>text_color</code> <code>PropertyMapper | str</code> <p>Text color (static value or PropertyMapper)</p> <code>'#3A3A3A'</code> <code>font_size</code> <code>PropertyMapper | str</code> <p>Font size with units like '10pt' (static value or PropertyMapper)</p> <code>'10pt'</code> <code>font_weight</code> <code>PropertyMapper | str</code> <p>Font weight like 'bold', 'normal' (static value or PropertyMapper)</p> <code>'bold'</code> <code>background_color</code> <code>PropertyMapper | str</code> <p>Background color for text (static value or PropertyMapper)</p> <code>None</code> <code>shadow_size</code> <code>PropertyMapper | str</code> <p>Text shadow size like '0.5px' (static value or PropertyMapper)</p> <code>'0.5px'</code> <code>shadow_color</code> <code>PropertyMapper | str</code> <p>Text shadow color (static value or PropertyMapper)</p> <code>'#F2F2F2'</code> <code>text_print_content</code> <code>PropertyMapper | str | bool</code> <p>Text content to display (True for auto-generated)</p> <code>True</code> <code>tooltip</code> <code>PropertyMapper | str | bool</code> <p>Tooltip content (True for auto-generated, False for none)</p> <code>True</code> <code>popup</code> <code>PropertyMapper | Popup | bool</code> <p>Popup content (True/False/PropertyMapper)</p> <code>False</code> <code>location</code> <code>PropertyMapper | Point</code> <p>Point location (defaults to smart location detection)</p> <code>None</code> <code>**property_mappers</code> <code>PropertyMapper | Any</code> <p>Additional custom property mappings</p> <code>{}</code> <p>Examples:</p> <p>Basic value labels:</p> <pre><code>&gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n...     text_color='#000000',\n...     font_size='12pt',\n...     font_weight='bold'\n... )\n</code></pre> <p>Data-driven text styling:</p> <pre><code>&gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n...     text_color=PropertyMapper.from_kpi_value(\n...         lambda v: '#FF0000' if v &gt; 100 else '#000000'\n...     ),\n...     font_size=PropertyMapper.from_kpi_value(\n...         lambda v: f'{min(max(8 + v/50, 8), 20)}pt'\n...     ),\n...     text_print_content=PropertyMapper.from_kpi_value(\n...         lambda v: f'{v:.0f} MW'\n...     )\n... )\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>class TextOverlayFeatureResolver(FeatureResolver[ResolvedTextOverlayFeature]):\n    \"\"\"\n    Resolves visual properties for text overlay elements.\n\n    Specialized feature resolver for text overlay visualizations that handles text\n    content, font styling, positioning, and visual effects. Commonly used for\n    adding data labels, value displays, or annotations to map elements.\n\n    Args:\n        text_color: Text color (static value or PropertyMapper)\n        font_size: Font size with units like '10pt' (static value or PropertyMapper)\n        font_weight: Font weight like 'bold', 'normal' (static value or PropertyMapper)\n        background_color: Background color for text (static value or PropertyMapper)\n        shadow_size: Text shadow size like '0.5px' (static value or PropertyMapper)\n        shadow_color: Text shadow color (static value or PropertyMapper)\n        text_print_content: Text content to display (True for auto-generated)\n        tooltip: Tooltip content (True for auto-generated, False for none)\n        popup: Popup content (True/False/PropertyMapper)\n        location: Point location (defaults to smart location detection)\n        **property_mappers: Additional custom property mappings\n\n    Examples:\n        Basic value labels:\n        &gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n        ...     text_color='#000000',\n        ...     font_size='12pt',\n        ...     font_weight='bold'\n        ... )\n\n        Data-driven text styling:\n        &gt;&gt;&gt; resolver = TextOverlayFeatureResolver(\n        ...     text_color=PropertyMapper.from_kpi_value(\n        ...         lambda v: '#FF0000' if v &gt; 100 else '#000000'\n        ...     ),\n        ...     font_size=PropertyMapper.from_kpi_value(\n        ...         lambda v: f'{min(max(8 + v/50, 8), 20)}pt'\n        ...     ),\n        ...     text_print_content=PropertyMapper.from_kpi_value(\n        ...         lambda v: f'{v:.0f} MW'\n        ...     )\n        ... )\n    \"\"\"\n    def __init__(\n            self,\n            text_color: PropertyMapper | str = '#3A3A3A',\n            font_size: PropertyMapper | str = '10pt',\n            font_weight: PropertyMapper | str = 'bold',\n            background_color: PropertyMapper | str = None,\n            shadow_size: PropertyMapper | str = '0.5px',\n            shadow_color: PropertyMapper | str = '#F2F2F2',\n            text_print_content: PropertyMapper | str | bool = True,\n            tooltip: PropertyMapper | str | bool = True,\n            popup: PropertyMapper | folium.Popup | bool = False,\n            location: PropertyMapper | Point = None,\n            offset: PropertyMapper | tuple[float, float] = (0, 0),\n            azimuth_angle: PropertyMapper | float = 90,\n            **property_mappers: PropertyMapper | Any,\n    ):\n        mappers = dict(\n            text_color=text_color,\n            font_size=font_size,\n            font_weight=font_weight,\n            background_color=background_color,\n            shadow_size=shadow_size,\n            shadow_color=shadow_color,\n            text_print_content=text_print_content,\n            tooltip=tooltip,\n            popup=popup,\n            location=self._explicit_or_fallback(location, self._default_location_mapper()),\n            offset=offset,\n            azimuth_angle=azimuth_angle,\n            **property_mappers\n        )\n        super().__init__(feature_type=ResolvedTextOverlayFeature, **mappers)\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/text_overlay/#mesqual.visualizations.folium_viz_system.viz_text_overlay.TextOverlayGenerator","title":"TextOverlayGenerator","text":"<p>               Bases: <code>FoliumObjectGenerator[TextOverlayFeatureResolver]</code></p> <p>Generates text overlays for map data items.</p> <p>Creates folium Marker objects with styled HTML text content overlaid on the map. Handles text formatting, positioning, shadow effects, and responsive styling based on data values.</p> <p>Commonly used for displaying: - KPI values directly on map elements (power flows, prices, etc.) - Data labels for areas, lines, or points - Dynamic text that changes based on underlying data - Status indicators or categorical labels</p> <p>Examples:</p> <p>Value display on bidding zones:</p> <pre><code>&gt;&gt;&gt; from mesqual.units import Units\n&gt;&gt;&gt; text_gen = TextOverlayGenerator(\n...     TextOverlayFeatureResolver(\n...         text_print_content=PropertyMapper(\n...             lambda di: Units.get_pretty_text_for_quantity(\n...                 di.kpi.quantity, decimals=0, include_unit=False\n...             )\n...         ),\n...         font_size='10pt',\n...         font_weight='bold'\n...     )\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; fg = folium.FeatureGroup('Price Labels')\n&gt;&gt;&gt; text_gen.generate_objects_for_kpi_collection(price_kpis, fg)\n&gt;&gt;&gt; fg.add_to(map)\n</code></pre> <p>Combined with area visualization:</p> <pre><code>&gt;&gt;&gt; area_gen = AreaGenerator(...)\n&gt;&gt;&gt; text_gen = TextOverlayGenerator(...)\n&gt;&gt;&gt; \n&gt;&gt;&gt; area_gen.generate_objects_for_model_df(zones_df, feature_group)\n&gt;&gt;&gt; text_gen.generate_objects_for_model_df(zones_df, feature_group)\n</code></pre> Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>class TextOverlayGenerator(FoliumObjectGenerator[TextOverlayFeatureResolver]):\n    \"\"\"\n    Generates text overlays for map data items.\n\n    Creates folium Marker objects with styled HTML text content overlaid on\n    the map. Handles text formatting, positioning, shadow effects, and\n    responsive styling based on data values.\n\n    Commonly used for displaying:\n    - KPI values directly on map elements (power flows, prices, etc.)\n    - Data labels for areas, lines, or points\n    - Dynamic text that changes based on underlying data\n    - Status indicators or categorical labels\n\n    Examples:\n        Value display on bidding zones:\n        &gt;&gt;&gt; from mesqual.units import Units\n        &gt;&gt;&gt; text_gen = TextOverlayGenerator(\n        ...     TextOverlayFeatureResolver(\n        ...         text_print_content=PropertyMapper(\n        ...             lambda di: Units.get_pretty_text_for_quantity(\n        ...                 di.kpi.quantity, decimals=0, include_unit=False\n        ...             )\n        ...         ),\n        ...         font_size='10pt',\n        ...         font_weight='bold'\n        ...     )\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; fg = folium.FeatureGroup('Price Labels')\n        &gt;&gt;&gt; text_gen.generate_objects_for_kpi_collection(price_kpis, fg)\n        &gt;&gt;&gt; fg.add_to(map)\n\n        Combined with area visualization:\n        &gt;&gt;&gt; area_gen = AreaGenerator(...)\n        &gt;&gt;&gt; text_gen = TextOverlayGenerator(...)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; area_gen.generate_objects_for_model_df(zones_df, feature_group)\n        &gt;&gt;&gt; text_gen.generate_objects_for_model_df(zones_df, feature_group)\n    \"\"\"\n    \"\"\"Generates text overlays for map data items.\"\"\"\n\n    def _feature_resolver_type(self) -&gt; Type[TextOverlayFeatureResolver]:\n        return TextOverlayFeatureResolver\n\n    def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n        \"\"\"\n        Generate and add a folium Marker with styled text overlay.\n\n        Args:\n            data_item: Data item containing point location and text content\n            feature_group: Folium feature group to add the text marker to\n        \"\"\"\n        style = self.feature_resolver.resolve_feature(data_item)\n        if not isinstance(style.location, Point):\n            return\n\n        if not style.text_print_content:\n            return\n\n        text_content = style.text_print_content\n        text_color = style.text_color\n        font_size = style.font_size\n        font_weight = style.font_weight\n        shadow_size = style.shadow_size\n        shadow_color = style.shadow_color\n\n        angle = float(style.azimuth_angle) - 90  # Folium counts clockwise from right-pointing direction; normal convention is CCW\n        x_offset, y_offset = style.offset\n\n        icon_html = f'''\n            &lt;div style=\"\n                position: absolute;\n                left: 50%;\n                top: 50%;\n                transform: translate(-50%, -50%) translate({x_offset}px, {-y_offset}px) rotate({angle}deg);\n                text-align: center;\n                font-size: {font_size};\n                font-weight: {font_weight};\n                color: {text_color};\n                white-space: nowrap;\n                text-shadow:\n                   -{shadow_size} -{shadow_size} 0 {shadow_color},  \n                    {shadow_size} -{shadow_size} 0 {shadow_color},\n                   -{shadow_size}  {shadow_size} 0 {shadow_color},\n                    {shadow_size}  {shadow_size} 0 {shadow_color};\n            \"&gt;\n                {text_content}\n            &lt;/div&gt;\n        '''\n\n        folium.Marker(\n            location=(style.location.y, style.location.x),\n            icon=folium.DivIcon(html=icon_html),\n            tooltip=style.tooltip,\n            popup=style.popup,\n        ).add_to(feature_group)\n\n    def _get_contrasting_color(self, surface_color: str) -&gt; str:\n        \"\"\"Get contrasting text color for a surface color.\"\"\"\n        if self._is_dark(surface_color):\n            return '#F2F2F2'\n        return '#3A3A3A'\n\n    def _get_shadow_color(self, text_color: str) -&gt; str:\n        \"\"\"Get shadow color for text.\"\"\"\n        if text_color == '#F2F2F2':\n            return '#3A3A3A'\n        return '#F2F2F2'\n\n    @staticmethod\n    def _is_dark(color: str) -&gt; bool:\n        \"\"\"Check if a color is dark.\"\"\"\n        if not color.startswith('#'):\n            return False\n        try:\n            r, g, b = [int(color[i:i + 2], 16) for i in (1, 3, 5)]\n            return (0.299 * r + 0.587 * g + 0.114 * b) &lt; 160\n        except (ValueError, IndexError):\n            return False\n</code></pre>"},{"location":"mesqual-package-documentation/api_reference/visualization/folium_viz_system/text_overlay/#mesqual.visualizations.folium_viz_system.viz_text_overlay.TextOverlayGenerator.generate","title":"generate","text":"<pre><code>generate(data_item: VisualizableDataItem, feature_group: FeatureGroup) -&gt; None\n</code></pre> <p>Generate and add a folium Marker with styled text overlay.</p> <p>Parameters:</p> Name Type Description Default <code>data_item</code> <code>VisualizableDataItem</code> <p>Data item containing point location and text content</p> required <code>feature_group</code> <code>FeatureGroup</code> <p>Folium feature group to add the text marker to</p> required Source code in <code>submodules/mesqual/mesqual/visualizations/folium_viz_system/viz_text_overlay.py</code> <pre><code>def generate(self, data_item: VisualizableDataItem, feature_group: folium.FeatureGroup) -&gt; None:\n    \"\"\"\n    Generate and add a folium Marker with styled text overlay.\n\n    Args:\n        data_item: Data item containing point location and text content\n        feature_group: Folium feature group to add the text marker to\n    \"\"\"\n    style = self.feature_resolver.resolve_feature(data_item)\n    if not isinstance(style.location, Point):\n        return\n\n    if not style.text_print_content:\n        return\n\n    text_content = style.text_print_content\n    text_color = style.text_color\n    font_size = style.font_size\n    font_weight = style.font_weight\n    shadow_size = style.shadow_size\n    shadow_color = style.shadow_color\n\n    angle = float(style.azimuth_angle) - 90  # Folium counts clockwise from right-pointing direction; normal convention is CCW\n    x_offset, y_offset = style.offset\n\n    icon_html = f'''\n        &lt;div style=\"\n            position: absolute;\n            left: 50%;\n            top: 50%;\n            transform: translate(-50%, -50%) translate({x_offset}px, {-y_offset}px) rotate({angle}deg);\n            text-align: center;\n            font-size: {font_size};\n            font-weight: {font_weight};\n            color: {text_color};\n            white-space: nowrap;\n            text-shadow:\n               -{shadow_size} -{shadow_size} 0 {shadow_color},  \n                {shadow_size} -{shadow_size} 0 {shadow_color},\n               -{shadow_size}  {shadow_size} 0 {shadow_color},\n                {shadow_size}  {shadow_size} 0 {shadow_color};\n        \"&gt;\n            {text_content}\n        &lt;/div&gt;\n    '''\n\n    folium.Marker(\n        location=(style.location.y, style.location.x),\n        icon=folium.DivIcon(html=icon_html),\n        tooltip=style.tooltip,\n        popup=style.popup,\n    ).add_to(feature_group)\n</code></pre>"},{"location":"mesqual-study-01/","title":"Study 01: Intro to MESQUAL","text":"<p>The intro study primarily uses a PyPSA example network to introduce the MESQUAL modules and framework. The series is structured as follows and will continuously be updated. So stay tuned.</p>"},{"location":"mesqual-study-01/#getting-started-series","title":"Getting Started Series","text":"<ul> <li>mesqual_101_study_manager_and_basic_fetching - Getting started with scenarios and comparisons  </li> <li>mesqual_102_mastering_data_fetching - Mastering the fetch method and data access patterns  </li> <li>mesqual_103_scenario_attributes - Managing and utilizing scenario metadata effectively  </li> </ul>"},{"location":"mesqual-study-01/#kpi-framework-series","title":"KPI Framework Series","text":"<ul> <li>mesqual_201_kpi_framework_and_units - Building a structured KPI system with proper unit handling  </li> <li>mesqual_202_kpi_collections_and_tables - Extracting pretty KPI tables  </li> </ul>"},{"location":"mesqual-study-01/#visualization-series","title":"Visualization Series","text":"<ul> <li>mesqual_301_time_series_dashboard - The best way to visualize, compare and understand time-series  </li> <li>mesqual_302_segmented_colormap - A useful colormap to merge multiple linear segments into a single colormap  </li> <li>mesqual_303_folium_model_df_map - Creating interactive Folium maps visualizing model info  </li> <li>mesqual_304_folium_area_kpi_map - Creating interactive Folium maps with area KPIs for scenarios and comparisons  </li> <li>mesqual_305_area_border_kpi_map - Creating interactive Folium maps with area-border KPIs for scenarios and comparisons</li> <li>mesqual_306_icon_kpi_map - Creating interactive Folium maps with KPIs projected as icons for scenarios and comparisons</li> <li>mesqual_307_country_plotter_util - GeoJSON library of country shapes  </li> <li>mesqual_308_html_dashboards - Multiple HTML plots (e.g. plotly figures) in one share-able html file  </li> </ul>"},{"location":"mesqual-study-01/#advanced-dataset-handling-series","title":"Advanced Dataset Handling Series","text":"<ul> <li>mesqual_401_dataset_collections - Linking, merging and managing multiple data sources</li> <li>mesqual_402_custom_interpreters - Creating study-specific data interpreters and variables</li> <li>mesqual_403_model_timeseries_integration - Combining static model data with time series for richer analysis</li> <li>mesqual_404_dot_notation_api - Using Python's dot notation for elegant data access</li> </ul>"},{"location":"mesqual-study-01/#expert-dataset-handling-series","title":"Expert Dataset Handling Series","text":"<ul> <li>mesqual_501_configuration_hierarchy - Mastering dataset and fetch-level configurations</li> <li>mesqual_502_validation_framework - Ensuring data consistency across scenarios</li> <li>mesqual_503_pickle_database_integration - Intro to simple .pickle DB integration for faster fetching </li> </ul>"},{"location":"mesqual-study-01/#managing-multiple-studies-series","title":"Managing Multiple Studies Series","text":"<ul> <li>mesqual_601_multi_study_architecture - Organizing repositories with multiple studies</li> <li>mesqual_602_color_themes - Managing different color themes across different studies</li> </ul>"},{"location":"mesqual-study-01/#interface-builder-series","title":"Interface Builder Series","text":"<ul> <li>mesqual_701_platform_dataset - An abstract container class for data interpreters to cover all aspects of handling data from a platform</li> <li>mesqual_702_flag_index_system - Understanding relationships between model and time series data</li> <li>mesqual_703_model_enrichment_patterns - Advanced techniques for property and membership enrichment</li> <li>mesqual_704_variable_aggregation_patterns - Advanced techniques for automatic variable aggregations</li> </ul>"},{"location":"mesqual-study-01/#advanced-dataframe-handling-series","title":"Advanced DataFrame Handling Series","text":"<ul> <li>mesqual_801_granularity_analysis_and_conversion - Handling time-series of different granularities</li> <li>mesqual_802_multi_index_utils - xs_df, set_new_column, sort_multi_index</li> </ul>"},{"location":"mesqual-study-01/#references-and-third-party-assets","title":"References and Third-Party Assets","text":"<ul> <li>PyPSA and Scigrid-DE example network:  - [MIT License]</li> <li>GeoJSON of DE control areas:  - [CC BY 4.0]</li> </ul>"},{"location":"mesqual-study-01/non_versioned/_tmp/nbviewer/","title":"Nbviewer","text":"<p>If you are viewing this on github, you will not see the interactive plotly plots. Open the Notebook on nbviewer through the following link to enjoy full interactivity for all plots: https://nbviewer.org/github/helgeesch/mesqual-vanilla-studies/blob/main/studies/study_01_intro_to_mesqual/notebooks/000_intro_to_study_manager.ipynb</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/","title":"MESQUAL 101: StudyManager and Dataset Fundamentals","text":""},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#introduction","title":"Introduction","text":"<p>This notebook demonstrates the core functionality of MESQUAL's StudyManager - the central component for handling multiple scenarios and scenario comparisons in energy system modeling studies. It showcases how MESQUAL's architecture simplifies working with complex multi-scenario analyses through a consistent and powerful interface.</p> <p>Rather than juggling separate data structures for each scenario, MESQUAL provides a unified framework where: - Every data element is accessible through a consistent API - Scenarios and comparisons are handled through the same paradigm - Data relationships are automatically preserved and utilized</p> <p>We'll use PyPSA's Scigrid DE example dataset for this demonstration, but the same principles apply regardless of which modeling platform you use.</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#setup","title":"Setup","text":"<pre><code>import os\nimport logging\nimport warnings\nimport pandas as pd\nimport pypsa\nfrom IPython.display import Image\n\nfrom mesqual import StudyManager\nfrom mesqual.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom mesqual_pypsa import PyPSADataset\n\n# Directory setup\nos.chdir(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))))\n\n# Configuration for cleaner output\nlogging.basicConfig(level=logging.ERROR)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\npd.set_option('display.max_columns', 6)\npd.set_option('display.width', 1000)\nPlotlyTheme().apply()\n</code></pre> <pre><code># Register study-specific interpreters (details on this will be covered in a later notebook)\nfrom studies.study_01_intro_to_mesqual.src.study_specific_model_interpreters import ControlAreaModelInterpreter, ScigridDEBusModelInterpreter\n\nPyPSADataset.register_interpreter(ControlAreaModelInterpreter)\nPyPSADataset.register_interpreter(ScigridDEBusModelInterpreter)\n</code></pre>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#loading-example-data","title":"Loading Example Data","text":"<p>For this demonstration, we use a PyPSA Scigrid DE example with a base network and four scenarios with increased solar and wind capacity. All networks have already been optimized.</p> <pre><code># Loading networks (all have already been optimized, so the results are included)\nstudy_folder = 'studies/study_01_intro_to_mesqual'\nnetworks_folder = os.path.join(study_folder, 'data/networks_scigrid_de')\n\nn_base = pypsa.Network(os.path.join(networks_folder, 'base.nc'))\nn_solar_150 = pypsa.Network(os.path.join(networks_folder, 'solar_150.nc'))\nn_solar_200 = pypsa.Network(os.path.join(networks_folder, 'solar_200.nc'))\nn_wind_150 = pypsa.Network(os.path.join(networks_folder, 'wind_150.nc'))\nn_wind_200 = pypsa.Network(os.path.join(networks_folder, 'wind_200.nc'))\n</code></pre>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#the-studymanager","title":"The StudyManager","text":"<p>The StudyManager is the central component of MESQUAL, organizing all scenarios and scenario comparisons for efficient access and analysis.</p> <pre><code>study = StudyManager.factory_from_scenarios(\n    scenarios=[\n        PyPSADataset(n_base,        name='base'),\n        PyPSADataset(n_solar_150,   name='solar_150'),\n        PyPSADataset(n_solar_200,   name='solar_200'),\n        PyPSADataset(n_wind_150,    name='wind_150'),\n        PyPSADataset(n_wind_200,    name='wind_200'),\n    ],\n    comparisons=[('solar_150', 'base'), ('solar_200', 'base'), ('wind_150', 'base'), ('wind_200', 'base')],\n    export_folder=os.path.join(study_folder, 'non_versioned/output'),\n)\n</code></pre> <p>In just a few lines of code, we've organized all scenarios and defined which comparisons we're interested in (here, comparing each scenario to the base case).</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#the-dataset-concept","title":"The Dataset Concept","text":"<p>The core building block in MESQUAL is the Dataset class. The key insight is that:</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#everything-is-a-dataset","title":"Everything is a Dataset!","text":"<ul> <li>Individual scenarios are Datasets</li> <li>Collections of scenarios are Datasets</li> <li>Scenario comparisons are Datasets</li> <li>Collections of comparisons are Datasets</li> </ul> <p>This means you interact with all entities through a consistent interface, regardless of whether you're working with a single scenario or a complex collection of scenario comparisons.</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#working-with-datasets","title":"Working with Datasets","text":"<p>Let's explore the fundamental operations with Datasets:</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#accessing-a-single-dataset","title":"Accessing a Single Dataset","text":"<pre><code>ds_base = study.scen.get_dataset('base')\n</code></pre>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#fetching-data","title":"Fetching Data","text":"<p>The primary method for interacting with Datasets is the <code>fetch()</code> method:</p> <pre><code>df_price_base = ds_base.fetch('buses_t.marginal_price')\nprint(df_price_base.head())\n</code></pre> <pre><code>Bus                         1        10        100  ...        98         99   99_220kV\nsnapshot                                            ...                                \n2011-01-01 00:00:00 -0.439753  5.772135  23.120287  ...  1.890824  23.723792  23.685443\n2011-01-01 01:00:00 -0.578449  6.100599  22.531275  ...  1.960914  23.186991  23.144293\n2011-01-01 02:00:00 -0.582087  6.071084  22.106222  ...  1.954744  22.747326  22.705514\n2011-01-01 03:00:00 -0.596210  6.139415  21.498113  ...  1.995093  22.117870  22.077360\n2011-01-01 04:00:00 -0.609622  6.164469  20.391625  ...  2.031251  20.979582  20.940890\n\n[5 rows x 585 columns]\n</code></pre> <p>For PyPSA users, note that this produces the same output as <code>n_base.buses_t.marginal_price</code> but provides a consistent interface across all platforms.</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#discovering-available-data","title":"Discovering Available Data","text":"<p>To see what data is available in a Dataset:</p> <pre><code>accepted_flags = ds_base.accepted_flags\nlist(sorted(accepted_flags))[:15]  # Just showing the first 15\n</code></pre> <pre><code>['buses',\n 'buses_t.marginal_price',\n 'buses_t.p',\n 'buses_t.q',\n 'buses_t.v_ang',\n 'buses_t.v_mag_pu',\n 'buses_t.v_mag_pu_set',\n 'carriers',\n 'control_areas',\n 'generators',\n 'generators_t.efficiency',\n 'generators_t.marginal_cost',\n 'generators_t.marginal_cost_quadratic',\n 'generators_t.mu_lower',\n 'generators_t.mu_p_set']\n</code></pre> <p>Or to find specific types of data:</p> <pre><code>accepted_flags_for_lines = ds_base.get_accepted_flags_containing_x('lines')\naccepted_flags_for_lines\n</code></pre> <pre><code>{'lines',\n 'lines_t.mu_lower',\n 'lines_t.mu_upper',\n 'lines_t.p0',\n 'lines_t.p1',\n 'lines_t.q0',\n 'lines_t.q1',\n 'lines_t.s_max_pu'}\n</code></pre>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#from-simple-to-powerful-scenario-collections","title":"From Simple to Powerful: Scenario Collections","text":"<p>While the individual Dataset interface is useful, MESQUAL's true power emerges when working with multiple scenarios.</p> <p>Let's fetch the marginal price data for all scenarios at once:</p> <pre><code>df_price = study.scen.fetch('buses_t.marginal_price')\nprint(df_price.head())\n</code></pre> <pre><code>dataset                  base                       ...  wind_200                      \nBus                         1        10        100  ...        98         99   99_220kV\nsnapshot                                            ...                                \n2011-01-01 00:00:00 -0.439753  5.772135  23.120287  ...  0.072306  23.828566  23.794191\n2011-01-01 01:00:00 -0.578449  6.100599  22.531275  ... -0.056472  22.380868  22.329086\n2011-01-01 02:00:00 -0.582087  6.071084  22.106222  ... -0.052310  20.490690  20.442924\n2011-01-01 03:00:00 -0.596210  6.139415  21.498113  ... -0.152777  18.640298  18.596510\n2011-01-01 04:00:00 -0.609622  6.164469  20.391625  ... -0.120450  15.923631  15.889469\n\n[5 rows x 2925 columns]\n</code></pre> <p>The result is a MultiIndex DataFrame with an additional 'dataset' level containing all scenario data in a single structure.</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#scenario-comparisons","title":"Scenario Comparisons","text":"<p>Similarly, we can get comparison data (deltas between scenarios):</p> <pre><code>df_price_change = study.comp.fetch('buses_t.marginal_price')\nprint(df_price_change.head())\n</code></pre> <pre><code>dataset             solar_150 vs base                      ... wind_200 vs base                    \nBus                                 1        10       100  ...               98        99  99_220kV\nsnapshot                                                   ...                                     \n2011-01-01 00:00:00          0.209435  0.022953  0.020959  ...        -1.818518  0.104774  0.108749\n2011-01-01 01:00:00          0.017964  0.064642 -0.080598  ...        -2.017386 -0.806122 -0.815207\n2011-01-01 02:00:00          0.013235  0.082471 -0.002842  ...        -2.007054 -2.256636 -2.262590\n2011-01-01 03:00:00          0.026943  0.012270 -0.156701  ...        -2.147870 -3.477572 -3.480850\n2011-01-01 04:00:00          0.000889 -0.124771 -0.887855  ...        -2.151701 -5.055950 -5.051421\n\n[5 rows x 2340 columns]\n</code></pre> <p>Each column in this DataFrame represents the difference between a variation scenario and the base scenario.</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#visualization-example","title":"Visualization Example","text":"<p>Now let's see this in action with a visualization. We'll create a unified analysis of average generation by carrier, control area, and scenario:</p> <pre><code>import plotly.express as px\nfrom mesqual.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query\n\ngenerators_model_df = study.scen.get_dataset('base').fetch('generators')\ndata = study.scen_comp.fetch('generators_t.p')\ndata = prepend_model_prop_levels(data, generators_model_df, 'bus_control_area', 'carrier')\ndata = data.mean().groupby(level=['dataset', 'bus_control_area', 'carrier']).sum()\ndata = data / 1e3  # MW to GW\ndata_flat = data.to_frame('value').reset_index()\nfig = px.bar(\n    data_frame=data_flat,\n    y='value',\n    x='dataset',\n    facet_col='bus_control_area',\n    color='carrier',\n    category_orders={'bus_control_area': ['Amprion', 'TransnetBW', 'TenneTDE', '50Hertz']},\n    labels={'value': 'Average generation [GW]'},\n)\nfig.update_layout(title='&lt;b&gt;Average generation per carrier and scenario (change per comparison)&lt;/b&gt;', width=1200)\nfig.update_xaxes(title=None)\n\n# Save and display the image\nimage_path = study.export_path('generation_barchart.png')\nfig.update_layout(height=600, width=1200)\nfig.write_image(image_path)\nImage(image_path)\n</code></pre> <p></p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_101_study_manager_and_basic_fetching/mescal_101_study_manager_and_basic_fetching/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Unified Interface: Whether working with individual scenarios or complex collections, the same methods apply</li> <li>Efficient Analysis: Analyze multiple scenarios with the same code you'd use for one</li> <li>Automatic Comparison: Calculate scenario deltas without manual calculations</li> <li>Hierarchical Organization: Study \u2192 Scenarios \u2192 Individual Datasets provides a logical structure</li> <li>Consistency Across Platforms: The same code works regardless of your modeling platform</li> </ul> <p>In the next notebook, we'll explore more advanced data fetching and transformation techniques that build on these fundamentals.</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/","title":"MESQUAL 102: Advanced Data Fetching Techniques","text":""},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#introduction","title":"Introduction","text":"<p>Building on the StudyManager fundamentals, this notebook demonstrates MESQUAL's advanced data fetching and transformation capabilities. These techniques significantly streamline multi-scenario analysis by leveraging pandas' power while abstracting away common boilerplate code.</p> <p>MESQUAL's data handling utilities provide efficient ways to: - Work with multi-index DataFrames from scenario and comparison collections - Filter data using model properties - Aggregate data across complex dimensions - Combine scenarios and comparisons in unified analyses</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#setup","title":"Setup","text":"<pre><code>import os\nimport logging\nimport warnings\nimport pandas as pd\nimport pypsa\nfrom IPython.display import Image\nimport plotly.express as px\n\nfrom mesqual import StudyManager\nfrom mesqual.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query\nfrom mesqual.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom mesqual_pypsa import PyPSADataset\n\n# Directory setup\nos.chdir(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))))\n\n# Configuration for cleaner output\nlogging.basicConfig(level=logging.ERROR)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\npd.set_option('display.max_columns', 6)\npd.set_option('display.width', 1000)\nPlotlyTheme().apply()\n</code></pre> <pre><code># Same study setup as in 101, only this time it's from a method so we can re-use it\nfrom studies.study_01_intro_to_mesqual.scripts.setup_study_manager import get_scigrid_de_study_manager\n\nstudy = get_scigrid_de_study_manager()\n</code></pre>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#working-with-multi-index-dataframes","title":"Working with Multi-Index DataFrames","text":"<p>Fetching data across multiple scenarios naturally creates multi-index DataFrames. Let's examine and transform these structures:</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#calculating-averages-per-scenario","title":"Calculating Averages Per Scenario","text":"<pre><code>df_price_change = study.comp.fetch('buses_t.marginal_price')\ndf_price_change_mean = df_price_change.mean().unstack('dataset')\nprint(df_price_change_mean)\n</code></pre> <pre><code>dataset    solar_150 vs base  solar_200 vs base  wind_150 vs base  wind_200 vs base\nBus                                                                                \n1                   0.210744           0.230623         -6.644124         -7.292750\n10                  0.250483           0.214094         -6.220950         -8.214383\n100                -0.712512          -1.279986         -1.500320         -3.403255\n100_220kV          -0.717737          -1.288928         -1.496799         -3.412561\n101                 0.144366           0.100898         -6.115556         -7.510163\n...                      ...                ...               ...               ...\n96_220kV           -0.173702          -0.405244         -4.028766         -4.810695\n97                 -0.694532          -1.244591         -1.385540         -2.557227\n98                  0.166729           0.132677         -6.155495         -7.608517\n99                 -0.711075          -1.284553         -1.324793         -3.213848\n99_220kV           -0.711609          -1.284904         -1.338588         -3.229785\n\n[585 rows x 4 columns]\n</code></pre> <p>The result is a DataFrame showing the average price change per bus for each scenario comparison.</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#combining-scenarios-and-comparisons","title":"Combining Scenarios and Comparisons","text":"<p>To analyze both raw values and deltas in one operation:</p> <pre><code>df_price_all = study.scen_comp.fetch('buses_t.marginal_price')\nprint(df_price_all.head())\n</code></pre> <pre><code>type                 scenario                       ...       comparison                    \ndataset                  base                       ... wind_200 vs base                    \nBus                         1        10        100  ...               98        99  99_220kV\nsnapshot                                            ...                                     \n2011-01-01 00:00:00 -0.439753  5.772135  23.120287  ...        -1.818518  0.104774  0.108749\n2011-01-01 01:00:00 -0.578449  6.100599  22.531275  ...        -2.017386 -0.806122 -0.815207\n2011-01-01 02:00:00 -0.582087  6.071084  22.106222  ...        -2.007054 -2.256636 -2.262590\n2011-01-01 03:00:00 -0.596210  6.139415  21.498113  ...        -2.147870 -3.477572 -3.480850\n2011-01-01 04:00:00 -0.609622  6.164469  20.391625  ...        -2.151701 -5.055950 -5.051421\n\n[5 rows x 5265 columns]\n</code></pre> <p>This DataFrame contains both the scenario data and comparison data, distinguished by the 'type' level in the MultiIndex.</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#unified-analysis","title":"Unified Analysis","text":"<p>We can perform operations on this unified structure:</p> <pre><code>df_price_all_mean = df_price_all.mean().droplevel('type').unstack('dataset').sort_index(axis=1)\nprint(df_price_all_mean)\n</code></pre> <pre><code>dataset         base  solar_150  solar_150 vs base  ...  wind_150 vs base   wind_200  wind_200 vs base\nBus                                                 ...                                               \n1           7.676257   7.887001           0.210744  ...         -6.644124   0.383507         -7.292750\n10         10.994609  11.245092           0.250483  ...         -6.220950   2.780226         -8.214383\n100        21.880995  21.168483          -0.712512  ...         -1.500320  18.477740         -3.403255\n100_220kV  21.977638  21.259901          -0.717737  ...         -1.496799  18.565077         -3.412561\n101         8.245265   8.389631           0.144366  ...         -6.115556   0.735102         -7.510163\n...              ...        ...                ...  ...               ...        ...               ...\n96_220kV   13.212272  13.038570          -0.173702  ...         -4.028766   8.401577         -4.810695\n97         21.288071  20.593539          -0.694532  ...         -1.385540  18.730844         -2.557227\n98          8.576509   8.743238           0.166729  ...         -6.155495   0.967991         -7.608517\n99         22.222045  21.510970          -0.711075  ...         -1.324793  19.008197         -3.213848\n99_220kV   22.202444  21.490835          -0.711609  ...         -1.338588  18.972659         -3.229785\n\n[585 rows x 9 columns]\n</code></pre> <p>This creates a table with average prices and deltas side by side, sorted alphabetically by dataset name.</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#integrating-model-data-with-time-series","title":"Integrating Model Data with Time Series","text":"<p>A powerful MESQUAL capability is the integration of static model data with time series data.</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#accessing-model-data","title":"Accessing Model Data","text":"<pre><code>buses_model_df = study.scen.get_dataset('base').fetch('buses')\nprint(buses_model_df)\n</code></pre> <pre><code>           v_nom type          x  ...  ref                   location control_area\nBus                               ...                                             \n1          220.0        9.522576  ...        POINT (9.52258 52.36041)     TenneTDE\n2          380.0        9.113210  ...        POINT (9.11321 52.54385)     TenneTDE\n3          380.0        9.389745  ...        POINT (9.38975 52.02631)     TenneTDE\n4          380.0        9.125266  ...        POINT (9.12527 52.53826)     TenneTDE\n5          380.0       10.366275  ...       POINT (10.36627 52.28465)     TenneTDE\n...          ...  ...        ...  ...  ...                        ...          ...\n404_220kV  220.0        8.232094  ...        POINT (8.23209 47.55614)   TransnetBW\n413_220kV  220.0        8.673717  ...        POINT (8.67372 49.29044)   TransnetBW\n421_220kV  220.0        9.091835  ...        POINT (9.09184 49.29462)   TransnetBW\n450_220kV  220.0        7.416708  ...        POINT (7.41671 51.45705)      Amprion\n458_220kV  220.0        7.419464  ...        POINT (7.41946 51.45751)      Amprion\n\n[585 rows x 21 columns]\n</code></pre>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#filtering-by-model-properties","title":"Filtering by Model Properties","text":"<p>Let's filter our time series to include only high voltage buses (v_nom &gt;= 380 kV):</p> <pre><code>filtered_price_df = filter_by_model_query(df_price_all, buses_model_df, 'v_nom &gt;= 380')\nprint(filtered_price_df.head())\n</code></pre> <pre><code>type                 scenario                       ...       comparison                    \ndataset                  base                       ... wind_200 vs base                    \nBus                        10        100       101  ...               97        98        99\nsnapshot                                            ...                                     \n2011-01-01 00:00:00  5.772135  23.120287  1.358870  ...        -1.095655 -1.818518  0.104774\n2011-01-01 01:00:00  6.100599  22.531275  1.347486  ...         0.510926 -2.017386 -0.806122\n2011-01-01 02:00:00  6.071084  22.106222  1.344766  ...        -1.208963 -2.007054 -2.256636\n2011-01-01 03:00:00  6.139415  21.498113  1.376138  ...        -2.351651 -2.147870 -3.477572\n2011-01-01 04:00:00  6.164469  20.391625  1.405119  ...        -5.122186 -2.151701 -5.055950\n\n[5 rows x 2592 columns]\n</code></pre> <p>The <code>filter_by_model_query</code> utility applies pandas query syntax to filter time series based on model properties.</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#prepending-model-properties","title":"Prepending Model Properties","text":"<p>We can add model properties as additional index levels:</p> <pre><code>price_with_control_area = prepend_model_prop_levels(filtered_price_df, buses_model_df, 'control_area')\nprint(price_with_control_area.head())\n</code></pre> <pre><code>control_area          50Hertz    Amprion   50Hertz  ...         TenneTDE          50Hertz          Amprion\ntype                 scenario   scenario  scenario  ...       comparison       comparison       comparison\ndataset                  base       base      base  ... wind_200 vs base wind_200 vs base wind_200 vs base\nBus                        10        100       101  ...               97               98               99\nsnapshot                                            ...                                                   \n2011-01-01 00:00:00  5.772135  23.120287  1.358870  ...        -1.095655        -1.818518         0.104774\n2011-01-01 01:00:00  6.100599  22.531275  1.347486  ...         0.510926        -2.017386        -0.806122\n2011-01-01 02:00:00  6.071084  22.106222  1.344766  ...        -1.208963        -2.007054        -2.256636\n2011-01-01 03:00:00  6.139415  21.498113  1.376138  ...        -2.351651        -2.147870        -3.477572\n2011-01-01 04:00:00  6.164469  20.391625  1.405119  ...        -5.122186        -2.151701        -5.055950\n\n[5 rows x 2592 columns]\n</code></pre> <p>This operation adds the 'control_area' property from the bus model as a new level in our multi-index DataFrame.</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#aggregating-by-model-properties","title":"Aggregating by Model Properties","text":"<p>Now we can efficiently aggregate by control area:</p> <pre><code>price_by_control_area = price_with_control_area.mean().groupby(level=['control_area', 'dataset']).mean().unstack('control_area')\nprint(price_by_control_area)\n</code></pre> <pre><code>control_area         50Hertz    Amprion   TenneTDE  TransnetBW\ndataset                                                       \nbase               11.657645  18.804042  15.284996   23.333147\nsolar_150          11.628050  18.179100  14.830030   22.741469\nsolar_150 vs base  -0.029595  -0.624942  -0.454965   -0.591678\nsolar_200          11.448025  17.720580  14.463570   22.177620\nsolar_200 vs base  -0.209620  -1.083462  -0.821425   -1.155527\nwind_150            7.087738  16.973092  12.332776   22.664264\nwind_150 vs base   -4.569907  -1.830950  -2.952220   -0.668883\nwind_200            4.765799  15.546364  10.669218   21.040701\nwind_200 vs base   -6.891846  -3.257678  -4.615778   -2.292446\n</code></pre>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#sophisticated-visualization-example","title":"Sophisticated Visualization Example","text":"<p>Let's demonstrate these techniques with a more sophisticated visualization. We'll create a boxplot showing the distribution of hourly prices by control area and scenario:</p> <pre><code>buses_model_df = study.scen.get_dataset('base').fetch('buses')\ndata = study.scen_comp.fetch('buses_t.marginal_price')\ndata = filter_by_model_query(data, buses_model_df, query='v_nom &gt;= 380')\ndata = prepend_model_prop_levels(data, buses_model_df, 'control_area')\ndata = data.T.groupby(level=['dataset', 'control_area']).mean().T\ndata_flat = flatten_df(data)\nfig = px.box(\n    data_frame=data_flat,\n    x='control_area',\n    color='dataset',\n    y='value',\n    category_orders={'control_area': ['Amprion', 'TransnetBW', 'TenneTDE', '50Hertz']},\n    labels={'value': 'Hourly marginal price [\u20ac/MWh]'},\n)\nfig.update_traces(boxmean=True)\nfig.update_layout(title='&lt;b&gt;Distribution of hourly price occurrences per scenario (price change per comparison)&lt;/b&gt;', width=1200)\nfig.update_xaxes(title=None)\n\n\n# Save and display the image (using this only so that the picture is shown on GitHub)\nimage_path = study.export_path('price_boxplot.png')\nfig.update_layout(height=600, width=1200)\nfig.write_image(image_path)\nImage(image_path)\n\n# fig.show()  # Use this instead when you are running locally and want to have full interactivity in the plot\n</code></pre> <p></p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#advanced-transformation-pipeline","title":"Advanced Transformation Pipeline","text":"<p>Let's trace through a complete data transformation pipeline that demonstrates MESQUAL's efficiency:</p> <pre><code># Start with raw data\ndf_raw = study.scen_comp.fetch('buses_t.marginal_price')\n\n# Filter to high voltage buses\nbuses_model_df = study.scen.get_dataset('base').fetch('buses')\ndf_filtered = filter_by_model_query(df_raw, buses_model_df, 'v_nom &gt;= 380')\n\n# Add control area information\ndf_with_areas = prepend_model_prop_levels(df_filtered, buses_model_df, 'control_area')\n\n# Calculate hourly area prices\nhourly_area_prices = df_with_areas.T.groupby(['dataset', 'control_area']).mean().T.unstack()\n\n# Reshape for analysis\narea_price_stats = hourly_area_prices.groupby(['dataset', 'control_area']).agg(['mean', 'std', 'min', 'max'])\n\nprint(area_price_stats)\n</code></pre> <pre><code>                                     mean       std        min        max\ndataset           control_area                                           \nbase              50Hertz       11.657645  4.898477   7.536946  22.367118\n                  Amprion       18.804042  3.200822  14.203696  23.352252\n                  TenneTDE      15.284996  4.839524   8.528858  22.238178\n                  TransnetBW    23.333147  2.799360  19.340758  25.851732\nsolar_150         50Hertz       11.628050  4.882195   7.616330  22.173180\n                  Amprion       18.179100  3.853877  12.146565  23.361021\n                  TenneTDE      14.830030  5.304740   7.223666  22.119134\n                  TransnetBW    22.741469  3.428135  16.896716  25.850474\nsolar_150 vs base 50Hertz       -0.029595  0.171920  -0.479044   0.261827\n                  Amprion       -0.624942  0.918766  -2.567606   0.157824\n                  TenneTDE      -0.454965  0.770981  -2.298001   0.484883\n                  TransnetBW    -0.591678  1.017996  -4.115210   0.065954\nsolar_200         50Hertz       11.448025  4.873367   6.854107  21.823334\n                  Amprion       17.720580  4.270065  11.023602  23.250576\n                  TenneTDE      14.463570  5.558940   6.534276  22.052944\n                  TransnetBW    22.177620  4.143174  14.744938  25.850372\nsolar_200 vs base 50Hertz       -0.209620  0.247018  -0.812198   0.268009\n                  Amprion       -1.083462  1.470448  -4.480616   0.099372\n                  TenneTDE      -0.821425  1.220437  -3.984937   0.524086\n                  TransnetBW    -1.155527  1.658060  -4.964659   0.177578\nwind_150          50Hertz        7.087738  2.572691   4.442086  14.417100\n                  Amprion       16.973092  2.468944  12.684241  20.197305\n                  TenneTDE      12.332776  3.101766   7.424682  17.277102\n                  TransnetBW    22.664264  3.346420  18.083046  25.879449\nwind_150 vs base  50Hertz       -4.569907  2.687352  -8.674692  -1.054884\n                  Amprion       -1.830950  1.360582  -3.565816   0.881127\n                  TenneTDE      -2.952220  2.143834  -6.159601  -0.075203\n                  TransnetBW    -0.668883  1.014195  -4.205612   0.135676\nwind_200          50Hertz        4.765799  2.160594   2.005988   8.848858\n                  Amprion       15.546364  3.183015  10.261299  19.354840\n                  TenneTDE      10.669218  3.488676   5.135119  15.777315\n                  TransnetBW    21.040701  5.060456  12.374443  25.935143\nwind_200 vs base  50Hertz       -6.891846  2.991629 -13.518259  -2.810534\n                  Amprion       -3.257678  1.231673  -5.057282   0.263154\n                  TenneTDE      -4.615778  2.107637  -8.368801  -0.593846\n                  TransnetBW    -2.292446  2.495500  -7.521899   0.191369\n</code></pre> <p>What would normally take dozens of lines of data manipulation code is condensed into a few expressive operations that maintain the relationships between different types of data.</p>"},{"location":"mesqual-study-01/non_versioned/_tmp/mescal_102_mastering_data_fetching/mescal_102_mastering_data_fetching/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Unified Analysis: Work with scenarios and comparisons in the same analytical framework</li> <li>Model and Time-Series Data Fetching: Unified framework for fetching model and time-series data</li> <li>Useful Integration Utilities: MESQUAL provides tools like <code>filter_by_model_query</code> and <code>prepend_model_prop_levels</code> that efficiently establish relationships between model properties and time series data and provide frequently needed filtering and property mapping features</li> </ul> <p>These capabilities make MESQUAL particularly powerful for complex multi-scenario analyses that would otherwise require extensive custom code.</p>"},{"location":"mesqual-study-01/notebooks/_mesqual_000_study_template/","title":"MESQUAL 000: Template Jupyter Notebook","text":"In\u00a0[4]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git\n        %cd mesqual-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mesqual -U\n        !pip install git+https://github.com/helgeesch/mesqual-pypsa -U\n        !pip install git+https://github.com/helgeesch/captain-arro -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git         %cd mesqual-vanilla-studies/          !pip install git+https://github.com/helgeesch/mesqual -U         !pip install git+https://github.com/helgeesch/mesqual-pypsa -U         !pip install git+https://github.com/helgeesch/captain-arro -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mesqual-vanilla-studies')\n    os.chdir('/content/mesqual-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mesqual import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mesqual-vanilla-studies')     os.chdir('/content/mesqual-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mesqual import StudyManager except ImportError:     raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\") In\u00a0[2]: Copied! <pre>from mesqual.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nPlotlyTheme().apply()\nrenderer = ConditionalRenderer()\n</pre> from mesqual.utils.plotly_utils.plotly_theme import PlotlyTheme from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() PlotlyTheme().apply() renderer = ConditionalRenderer()"},{"location":"mesqual-study-01/notebooks/_mesqual_000_study_template/#mesqual-000-template-jupyter-notebook","title":"MESQUAL 000: Template Jupyter Notebook\u00b6","text":""},{"location":"mesqual-study-01/notebooks/_mesqual_000_study_template/#introduction","title":"Introduction\u00b6","text":""},{"location":"mesqual-study-01/notebooks/_mesqual_000_study_template/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mesqual and all requirements are installed.</p>"},{"location":"mesqual-study-01/notebooks/_mesqual_000_study_template/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ul> <li>Bing</li> <li>Bong</li> </ul>"},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/","title":"MESQUAL 101: StudyManager and Dataset Fundamentals","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git\n        %cd mesqual-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mesqual -U\n        !pip install git+https://github.com/helgeesch/mesqual-pypsa -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git         %cd mesqual-vanilla-studies/          !pip install git+https://github.com/helgeesch/mesqual -U         !pip install git+https://github.com/helgeesch/mesqual-pypsa -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>\ud83d\udda5\ufe0f Running locally. No setup needed.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mesqual-vanilla-studies')\n    os.chdir('/content/mesqual-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mesqual import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mesqual-vanilla-studies')     os.chdir('/content/mesqual-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mesqual import StudyManager except ImportError:     raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\") <pre>\u2705 Environment ready. Let\u2019s go!\n</pre> In\u00a0[3]: Copied! <pre>import os\nimport pypsa\n\nfrom mesqual import StudyManager\nfrom mesqual.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom mesqual_pypsa import PyPSADataset\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nPlotlyTheme().apply()\nrenderer = ConditionalRenderer()\n</pre> import os import pypsa  from mesqual import StudyManager from mesqual.utils.plotly_utils.plotly_theme import PlotlyTheme from mesqual_pypsa import PyPSADataset from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() PlotlyTheme().apply() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Register study-specific interpreters (details on this will be covered in a later notebook)\nfrom studies.study_01_intro_to_mesqual.src.study_specific_model_interpreters import ControlAreaModelInterpreter, ScigridDEBusModelInterpreter\n\nPyPSADataset.register_interpreter(ControlAreaModelInterpreter)\nPyPSADataset.register_interpreter(ScigridDEBusModelInterpreter);\n</pre> # Register study-specific interpreters (details on this will be covered in a later notebook) from studies.study_01_intro_to_mesqual.src.study_specific_model_interpreters import ControlAreaModelInterpreter, ScigridDEBusModelInterpreter  PyPSADataset.register_interpreter(ControlAreaModelInterpreter) PyPSADataset.register_interpreter(ScigridDEBusModelInterpreter); In\u00a0[5]: Copied! <pre># Loading networks (all have already been optimized, so the results are included)\nstudy_folder = 'studies/study_01_intro_to_mesqual'\nnetworks_folder = os.path.join(study_folder, 'data/networks_scigrid_de')\n\nn_base = pypsa.Network(os.path.join(networks_folder, 'base.nc'))\nn_solar_150 = pypsa.Network(os.path.join(networks_folder, 'solar_150.nc'))\nn_solar_200 = pypsa.Network(os.path.join(networks_folder, 'solar_200.nc'))\nn_wind_150 = pypsa.Network(os.path.join(networks_folder, 'wind_150.nc'))\nn_wind_200 = pypsa.Network(os.path.join(networks_folder, 'wind_200.nc'))\n</pre> # Loading networks (all have already been optimized, so the results are included) study_folder = 'studies/study_01_intro_to_mesqual' networks_folder = os.path.join(study_folder, 'data/networks_scigrid_de')  n_base = pypsa.Network(os.path.join(networks_folder, 'base.nc')) n_solar_150 = pypsa.Network(os.path.join(networks_folder, 'solar_150.nc')) n_solar_200 = pypsa.Network(os.path.join(networks_folder, 'solar_200.nc')) n_wind_150 = pypsa.Network(os.path.join(networks_folder, 'wind_150.nc')) n_wind_200 = pypsa.Network(os.path.join(networks_folder, 'wind_200.nc')) In\u00a0[6]: Copied! <pre>study = StudyManager.factory_from_scenarios(\n    scenarios=[\n        PyPSADataset(n_base,        name='base'),\n        PyPSADataset(n_solar_150,   name='solar_150'),\n        PyPSADataset(n_solar_200,   name='solar_200'),\n        PyPSADataset(n_wind_150,    name='wind_150'),\n        PyPSADataset(n_wind_200,    name='wind_200'),\n    ],\n    comparisons=[('solar_150', 'base'), ('solar_200', 'base'), ('wind_150', 'base'), ('wind_200', 'base')],\n    export_folder=os.path.join(study_folder, 'non_versioned/output'),\n)\n</pre> study = StudyManager.factory_from_scenarios(     scenarios=[         PyPSADataset(n_base,        name='base'),         PyPSADataset(n_solar_150,   name='solar_150'),         PyPSADataset(n_solar_200,   name='solar_200'),         PyPSADataset(n_wind_150,    name='wind_150'),         PyPSADataset(n_wind_200,    name='wind_200'),     ],     comparisons=[('solar_150', 'base'), ('solar_200', 'base'), ('wind_150', 'base'), ('wind_200', 'base')],     export_folder=os.path.join(study_folder, 'non_versioned/output'), ) <p>In just a few lines of code, we've organized all scenarios and defined which comparisons we're interested in (here, comparing each scenario to the base case).</p> In\u00a0[7]: Copied! <pre>ds_base = study.scen.get_dataset('base')\n</pre> ds_base = study.scen.get_dataset('base') In\u00a0[8]: Copied! <pre>df_price_base = ds_base.fetch('buses_t.marginal_price')\nprint(df_price_base.head())\n</pre> df_price_base = ds_base.fetch('buses_t.marginal_price') print(df_price_base.head()) <pre>Bus                     1    10    100  ...    98     99  99_220kV\nsnapshot                                ...                       \n2011-01-01 00:00:00 -0.44  5.77  23.12  ...  1.89  23.72     23.69\n2011-01-01 01:00:00 -0.58  6.10  22.53  ...  1.96  23.19     23.14\n2011-01-01 02:00:00 -0.58  6.07  22.11  ...  1.95  22.75     22.71\n2011-01-01 03:00:00 -0.60  6.14  21.50  ...  2.00  22.12     22.08\n2011-01-01 04:00:00 -0.61  6.16  20.39  ...  2.03  20.98     20.94\n\n[5 rows x 585 columns]\n</pre> <p>For PyPSA users, note that this produces the same output as <code>n_base.buses_t.marginal_price</code> but provides a consistent interface across all platforms.</p> In\u00a0[9]: Copied! <pre>accepted_flags = ds_base.accepted_flags\nlist(sorted(accepted_flags))[:15]  # Just showing the first 15\n</pre> accepted_flags = ds_base.accepted_flags list(sorted(accepted_flags))[:15]  # Just showing the first 15 Out[9]: <pre>['buses',\n 'buses_t.marginal_price',\n 'buses_t.p',\n 'buses_t.q',\n 'buses_t.v_ang',\n 'buses_t.v_mag_pu',\n 'buses_t.v_mag_pu_set',\n 'carriers',\n 'control_areas',\n 'generators',\n 'generators_t.efficiency',\n 'generators_t.marginal_cost',\n 'generators_t.marginal_cost_quadratic',\n 'generators_t.mu_lower',\n 'generators_t.mu_p_set']</pre> <p>Or to find specific types of data:</p> In\u00a0[10]: Copied! <pre>accepted_flags_for_lines = ds_base.get_accepted_flags_containing_x('lines')\naccepted_flags_for_lines\n</pre> accepted_flags_for_lines = ds_base.get_accepted_flags_containing_x('lines') accepted_flags_for_lines Out[10]: <pre>{'lines',\n 'lines_t.mu_lower',\n 'lines_t.mu_upper',\n 'lines_t.p0',\n 'lines_t.p1',\n 'lines_t.q0',\n 'lines_t.q1',\n 'lines_t.s_max_pu'}</pre> In\u00a0[11]: Copied! <pre>df_price = study.scen.fetch('buses_t.marginal_price')\nprint(df_price.head())\n</pre> df_price = study.scen.fetch('buses_t.marginal_price') print(df_price.head()) <pre>dataset              base               ... wind_200                \nBus                     1    10    100  ...       98     99 99_220kV\nsnapshot                                ...                         \n2011-01-01 00:00:00 -0.44  5.77  23.12  ...     0.07  23.83    23.79\n2011-01-01 01:00:00 -0.58  6.10  22.53  ...    -0.06  22.38    22.33\n2011-01-01 02:00:00 -0.58  6.07  22.11  ...    -0.05  20.49    20.44\n2011-01-01 03:00:00 -0.60  6.14  21.50  ...    -0.15  18.64    18.60\n2011-01-01 04:00:00 -0.61  6.16  20.39  ...    -0.12  15.92    15.89\n\n[5 rows x 2925 columns]\n</pre> <p>The result is a MultiIndex DataFrame with an additional 'dataset' level containing all scenario data in a single structure.</p> In\u00a0[12]: Copied! <pre>df_price_change = study.comp.fetch('buses_t.marginal_price')\nprint(df_price_change.head())\n</pre> df_price_change = study.comp.fetch('buses_t.marginal_price') print(df_price_change.head()) <pre>dataset             solar_150 vs base                  ... wind_200 vs base               \nBus                                 1    10       100  ...               98    99 99_220kV\nsnapshot                                               ...                                \n2011-01-01 00:00:00          2.09e-01  0.02  2.10e-02  ...            -1.82  0.10     0.11\n2011-01-01 01:00:00          1.80e-02  0.06 -8.06e-02  ...            -2.02 -0.81    -0.82\n2011-01-01 02:00:00          1.32e-02  0.08 -2.84e-03  ...            -2.01 -2.26    -2.26\n2011-01-01 03:00:00          2.69e-02  0.01 -1.57e-01  ...            -2.15 -3.48    -3.48\n2011-01-01 04:00:00          8.89e-04 -0.12 -8.88e-01  ...            -2.15 -5.06    -5.05\n\n[5 rows x 2340 columns]\n</pre> <p>Each column in this DataFrame represents the difference between a variation scenario and the base scenario.</p> In\u00a0[13]: Copied! <pre>import plotly.express as px\nfrom mesqual.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query\n\ngenerators_model_df = study.scen.get_dataset('base').fetch('generators')\ndata = study.scen_comp.fetch('generators_t.p')\ndata = prepend_model_prop_levels(data, generators_model_df, 'bus_control_area', 'carrier')\ndata = data.mean().groupby(level=['dataset', 'bus_control_area', 'carrier']).sum()\ndata = data / 1e3  # MW to GW\ndata_flat = data.to_frame('value').reset_index()\nfig = px.bar(\n    data_frame=data_flat,\n    y='value',\n    x='dataset',\n    facet_col='bus_control_area',\n    color='carrier',\n    category_orders={'bus_control_area': ['Amprion', 'TransnetBW', 'TenneTDE', '50Hertz']},\n    labels={'value': 'Average generation [GW]'},\n)\nfig.update_layout(title='&lt;b&gt;Average generation per carrier and scenario (change per comparison)&lt;/b&gt;', width=1200)\nfig.update_xaxes(title=None)\n\nrenderer.show_plotly(fig)\n</pre> import plotly.express as px from mesqual.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query  generators_model_df = study.scen.get_dataset('base').fetch('generators') data = study.scen_comp.fetch('generators_t.p') data = prepend_model_prop_levels(data, generators_model_df, 'bus_control_area', 'carrier') data = data.mean().groupby(level=['dataset', 'bus_control_area', 'carrier']).sum() data = data / 1e3  # MW to GW data_flat = data.to_frame('value').reset_index() fig = px.bar(     data_frame=data_flat,     y='value',     x='dataset',     facet_col='bus_control_area',     color='carrier',     category_orders={'bus_control_area': ['Amprion', 'TransnetBW', 'TenneTDE', '50Hertz']},     labels={'value': 'Average generation [GW]'}, ) fig.update_layout(title='Average generation per carrier and scenario (change per comparison)', width=1200) fig.update_xaxes(title=None)  renderer.show_plotly(fig)"},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/#mesqual-101-studymanager-and-dataset-fundamentals","title":"MESQUAL 101: StudyManager and Dataset Fundamentals\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/#introduction","title":"Introduction\u00b6","text":"<p>This notebook demonstrates the core functionality of MESQUAL's StudyManager - the central component for handling multiple scenarios and scenario comparisons in energy system modeling studies. It showcases how MESQUAL's architecture simplifies working with complex multi-scenario analyses through a consistent and powerful interface.</p> <p>Rather than juggling separate data structures for each scenario, MESQUAL provides a unified framework where:</p> <ul> <li>Every data element is accessible through a consistent API</li> <li>Scenarios and comparisons are handled through the same paradigm</li> <li>Data relationships are automatically preserved and utilized</li> </ul> <p>We'll use PyPSA's Scigrid DE example dataset for this demonstration, but the same principles apply regardless of which modeling platform you use.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mesqual and all requirements are installed.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/#loading-example-data","title":"Loading Example Data\u00b6","text":"<p>For this demonstration, we use a PyPSA Scigrid DE example with a base network and four scenarios with increased solar and wind capacity. All networks have already been optimized.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/#the-studymanager","title":"The StudyManager\u00b6","text":"<p>The StudyManager is the central component of MESQUAL, organizing all scenarios and scenario comparisons for efficient access and analysis.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/#the-dataset-concept","title":"The Dataset Concept\u00b6","text":"<p>The core building block in MESQUAL is the Dataset class. The key insight is that:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/#everything-is-a-dataset","title":"Everything is a Dataset!\u00b6","text":"<ul> <li>Individual scenarios are Datasets</li> <li>Collections of scenarios are Datasets</li> <li>Scenario comparisons are Datasets</li> <li>Collections of comparisons are Datasets</li> </ul> <p>This means you interact with all entities through a consistent interface, regardless of whether you're working with a single scenario or a complex collection of scenario comparisons.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/#working-with-datasets","title":"Working with Datasets\u00b6","text":"<p>Let's explore the fundamental operations with Datasets:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/#accessing-a-single-dataset","title":"Accessing a Single Dataset\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/#fetching-data","title":"Fetching Data\u00b6","text":"<p>The primary method for interacting with Datasets is the <code>fetch()</code> method:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/#discovering-available-data","title":"Discovering Available Data\u00b6","text":"<p>To see what data is available in a Dataset:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/#from-simple-to-powerful-scenario-collections","title":"From Simple to Powerful: Scenario Collections\u00b6","text":"<p>While the individual Dataset interface is useful, MESQUAL's true power emerges when working with multiple scenarios.</p> <p>Let's fetch the marginal price data for all scenarios at once:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/#scenario-comparisons","title":"Scenario Comparisons\u00b6","text":"<p>Similarly, we can get comparison data (deltas between scenarios):</p>"},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/#visualization-example","title":"Visualization Example\u00b6","text":"<p>Now let's see this in action with a visualization. We'll create a unified analysis of average generation by carrier, control area, and scenario:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_101_study_manager_and_basic_fetching/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ul> <li>Unified Interface: Whether working with individual scenarios or complex collections, the same methods apply</li> <li>Efficient Analysis: Analyze multiple scenarios with the same code you'd use for one</li> <li>Automatic Comparison: Calculate scenario deltas without manual calculations</li> <li>Hierarchical Organization: Study \u2192 Scenarios \u2192 Individual Datasets provides a logical structure</li> <li>Consistency Across Platforms: The same code works regardless of your modeling platform</li> </ul> <p>In the next notebook, we'll explore more advanced data fetching and transformation techniques that build on these fundamentals.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/","title":"MESQUAL 102: Advanced Data Fetching Techniques","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git\n        %cd mesqual-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mesqual -U\n        !pip install git+https://github.com/helgeesch/mesqual-pypsa -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git         %cd mesqual-vanilla-studies/          !pip install git+https://github.com/helgeesch/mesqual -U         !pip install git+https://github.com/helgeesch/mesqual-pypsa -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>\ud83d\udda5\ufe0f Running locally. No setup needed.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mesqual-vanilla-studies')\n    os.chdir('/content/mesqual-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mesqual import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mesqual-vanilla-studies')     os.chdir('/content/mesqual-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mesqual import StudyManager except ImportError:     raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\") <pre>\u2705 Environment ready. Let\u2019s go!\n</pre> In\u00a0[3]: Copied! <pre>import plotly.express as px\n\nfrom mesqual.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query\nfrom mesqual.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nPlotlyTheme().apply()\nrenderer = ConditionalRenderer()\n</pre> import plotly.express as px  from mesqual.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query from mesqual.utils.plotly_utils.plotly_theme import PlotlyTheme from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() PlotlyTheme().apply() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Same study setup as in 101, only this time it's from a method so we can re-use it\nfrom studies.study_01_intro_to_mesqual.scripts.setup_study_manager import get_scigrid_de_study_manager\n\nstudy = get_scigrid_de_study_manager()\n</pre> # Same study setup as in 101, only this time it's from a method so we can re-use it from studies.study_01_intro_to_mesqual.scripts.setup_study_manager import get_scigrid_de_study_manager  study = get_scigrid_de_study_manager()  In\u00a0[5]: Copied! <pre>df_price_change = study.comp.fetch('buses_t.marginal_price')\ndf_price_change_mean = df_price_change.mean().unstack('dataset')\nprint(df_price_change_mean)\n</pre> df_price_change = study.comp.fetch('buses_t.marginal_price') df_price_change_mean = df_price_change.mean().unstack('dataset') print(df_price_change_mean) <pre>dataset    solar_150 vs base  solar_200 vs base  wind_150 vs base  wind_200 vs base\nBus                                                                                \n1                       0.21               0.23             -6.64             -7.29\n10                      0.25               0.21             -6.22             -8.21\n100                    -0.71              -1.28             -1.50             -3.40\n100_220kV              -0.72              -1.29             -1.50             -3.41\n101                     0.14               0.10             -6.12             -7.51\n...                      ...                ...               ...               ...\n96_220kV               -0.17              -0.41             -4.03             -4.81\n97                     -0.69              -1.24             -1.39             -2.56\n98                      0.17               0.13             -6.16             -7.61\n99                     -0.71              -1.28             -1.32             -3.21\n99_220kV               -0.71              -1.28             -1.34             -3.23\n\n[585 rows x 4 columns]\n</pre> <p>The result is a DataFrame showing the average price change per bus for each scenario comparison.</p> In\u00a0[6]: Copied! <pre>df_price_all = study.scen_comp.fetch('buses_t.marginal_price')\nprint(df_price_all.head())\n</pre> df_price_all = study.scen_comp.fetch('buses_t.marginal_price') print(df_price_all.head()) <pre>type                scenario               ...       comparison               \ndataset                 base               ... wind_200 vs base               \nBus                        1    10    100  ...               98    99 99_220kV\nsnapshot                                   ...                                \n2011-01-01 00:00:00    -0.44  5.77  23.12  ...            -1.82  0.10     0.11\n2011-01-01 01:00:00    -0.58  6.10  22.53  ...            -2.02 -0.81    -0.82\n2011-01-01 02:00:00    -0.58  6.07  22.11  ...            -2.01 -2.26    -2.26\n2011-01-01 03:00:00    -0.60  6.14  21.50  ...            -2.15 -3.48    -3.48\n2011-01-01 04:00:00    -0.61  6.16  20.39  ...            -2.15 -5.06    -5.05\n\n[5 rows x 5265 columns]\n</pre> <p>This DataFrame contains both the scenario data and comparison data, distinguished by the 'type' level in the MultiIndex.</p> In\u00a0[7]: Copied! <pre>df_price_all_mean = df_price_all.mean().droplevel('type').unstack('dataset').sort_index(axis=1)\nprint(df_price_all_mean)\n</pre> df_price_all_mean = df_price_all.mean().droplevel('type').unstack('dataset').sort_index(axis=1) print(df_price_all_mean) <pre>dataset     base  solar_150  solar_150 vs base  ...  wind_150 vs base  wind_200  wind_200 vs base\nBus                                             ...                                              \n1           7.68       7.89               0.21  ...             -6.64      0.38             -7.29\n10         10.99      11.25               0.25  ...             -6.22      2.78             -8.21\n100        21.88      21.17              -0.71  ...             -1.50     18.48             -3.40\n100_220kV  21.98      21.26              -0.72  ...             -1.50     18.57             -3.41\n101         8.25       8.39               0.14  ...             -6.12      0.74             -7.51\n...          ...        ...                ...  ...               ...       ...               ...\n96_220kV   13.21      13.04              -0.17  ...             -4.03      8.40             -4.81\n97         21.29      20.59              -0.69  ...             -1.39     18.73             -2.56\n98          8.58       8.74               0.17  ...             -6.16      0.97             -7.61\n99         22.22      21.51              -0.71  ...             -1.32     19.01             -3.21\n99_220kV   22.20      21.49              -0.71  ...             -1.34     18.97             -3.23\n\n[585 rows x 9 columns]\n</pre> <p>This creates a table with average prices and deltas side by side, sorted alphabetically by dataset name.</p> In\u00a0[8]: Copied! <pre>buses_model_df = study.scen.get_dataset('base').fetch('buses')\nprint(buses_model_df)\n</pre> buses_model_df = study.scen.get_dataset('base').fetch('buses') print(buses_model_df) <pre>           v_nom type      x  ...  frequency               typ control_area\nBus                           ...                                          \n1          220.0        9.52  ...         50        substation     TenneTDE\n2          380.0        9.11  ...         50        substation     TenneTDE\n3          380.0        9.39  ...         50        substation     TenneTDE\n4          380.0        9.13  ...         50        substation     TenneTDE\n5          380.0       10.37  ...         50        substation     TenneTDE\n...          ...  ...    ...  ...        ...               ...          ...\n404_220kV  220.0        8.23  ...                   substation   TransnetBW\n413_220kV  220.0        8.67  ...         50        substation   TransnetBW\n421_220kV  220.0        9.09  ...         50        substation   TransnetBW\n450_220kV  220.0        7.42  ...         50        substation      Amprion\n458_220kV  220.0        7.42  ...         50  auxillary_T_node      Amprion\n\n[585 rows x 21 columns]\n</pre> In\u00a0[9]: Copied! <pre>filtered_price_df = filter_by_model_query(df_price_all, buses_model_df, 'v_nom &gt;= 380')\nprint(filtered_price_df.head())\n</pre> filtered_price_df = filter_by_model_query(df_price_all, buses_model_df, 'v_nom &gt;= 380') print(filtered_price_df.head()) <pre>type                scenario               ...       comparison            \ndataset                 base               ... wind_200 vs base            \nBus                       10    100   101  ...               97    98    99\nsnapshot                                   ...                             \n2011-01-01 00:00:00     5.77  23.12  1.36  ...            -1.10 -1.82  0.10\n2011-01-01 01:00:00     6.10  22.53  1.35  ...             0.51 -2.02 -0.81\n2011-01-01 02:00:00     6.07  22.11  1.34  ...            -1.21 -2.01 -2.26\n2011-01-01 03:00:00     6.14  21.50  1.38  ...            -2.35 -2.15 -3.48\n2011-01-01 04:00:00     6.16  20.39  1.41  ...            -5.12 -2.15 -5.06\n\n[5 rows x 2592 columns]\n</pre> <p>The <code>filter_by_model_query</code> utility applies pandas query syntax to filter time series based on model properties.</p> In\u00a0[10]: Copied! <pre>price_with_control_area = prepend_model_prop_levels(filtered_price_df, buses_model_df, 'control_area')\nprint(price_with_control_area.head())\n</pre> price_with_control_area = prepend_model_prop_levels(filtered_price_df, buses_model_df, 'control_area') print(price_with_control_area.head()) <pre>control_area         50Hertz  Amprion  50Hertz  ...         TenneTDE          50Hertz          Amprion\ntype                scenario scenario scenario  ...       comparison       comparison       comparison\ndataset                 base     base     base  ... wind_200 vs base wind_200 vs base wind_200 vs base\nBus                       10      100      101  ...               97               98               99\nsnapshot                                        ...                                                   \n2011-01-01 00:00:00     5.77    23.12     1.36  ...            -1.10            -1.82             0.10\n2011-01-01 01:00:00     6.10    22.53     1.35  ...             0.51            -2.02            -0.81\n2011-01-01 02:00:00     6.07    22.11     1.34  ...            -1.21            -2.01            -2.26\n2011-01-01 03:00:00     6.14    21.50     1.38  ...            -2.35            -2.15            -3.48\n2011-01-01 04:00:00     6.16    20.39     1.41  ...            -5.12            -2.15            -5.06\n\n[5 rows x 2592 columns]\n</pre> <p>This operation adds the 'control_area' property from the bus model as a new level in our multi-index DataFrame.</p> In\u00a0[11]: Copied! <pre>price_by_control_area = price_with_control_area.mean().groupby(level=['control_area', 'dataset']).mean().unstack('control_area')\nprint(price_by_control_area)\n</pre> price_by_control_area = price_with_control_area.mean().groupby(level=['control_area', 'dataset']).mean().unstack('control_area') print(price_by_control_area) <pre>control_area       50Hertz  Amprion  TenneTDE  TransnetBW\ndataset                                                  \nbase                 11.66    18.88     15.09       23.33\nsolar_150            11.63    18.26     14.64       22.74\nsolar_150 vs base    -0.03    -0.63     -0.45       -0.59\nsolar_200            11.45    17.79     14.29       22.18\nsolar_200 vs base    -0.21    -1.09     -0.80       -1.16\nwind_150              7.09    17.08     12.08       22.66\nwind_150 vs base     -4.57    -1.81     -3.01       -0.67\nwind_200              4.77    15.64     10.42       21.04\nwind_200 vs base     -6.89    -3.24     -4.67       -2.29\n</pre> In\u00a0[12]: Copied! <pre>buses_model_df = study.scen.get_dataset('base').fetch('buses')\ndata = study.scen_comp.fetch('buses_t.marginal_price')\ndata = filter_by_model_query(data, buses_model_df, query='v_nom &gt;= 380')\ndata = prepend_model_prop_levels(data, buses_model_df, 'control_area')\ndata = data.T.groupby(level=['dataset', 'control_area']).mean().T\ndata_flat = flatten_df(data)\nfig = px.box(\n    data_frame=data_flat,\n    x='control_area',\n    color='dataset',\n    y='value',\n    category_orders={'control_area': ['Amprion', 'TransnetBW', 'TenneTDE', '50Hertz']},\n    labels={'value': 'Hourly marginal price [\u20ac/MWh]'},\n)\nfig.update_traces(boxmean=True)\nfig.update_layout(title='&lt;b&gt;Distribution of hourly price occurrences per scenario (price change per comparison)&lt;/b&gt;', width=1200)\nfig.update_xaxes(title=None)\n\nrenderer.show_plotly(fig)\n</pre> buses_model_df = study.scen.get_dataset('base').fetch('buses') data = study.scen_comp.fetch('buses_t.marginal_price') data = filter_by_model_query(data, buses_model_df, query='v_nom &gt;= 380') data = prepend_model_prop_levels(data, buses_model_df, 'control_area') data = data.T.groupby(level=['dataset', 'control_area']).mean().T data_flat = flatten_df(data) fig = px.box(     data_frame=data_flat,     x='control_area',     color='dataset',     y='value',     category_orders={'control_area': ['Amprion', 'TransnetBW', 'TenneTDE', '50Hertz']},     labels={'value': 'Hourly marginal price [\u20ac/MWh]'}, ) fig.update_traces(boxmean=True) fig.update_layout(title='Distribution of hourly price occurrences per scenario (price change per comparison)', width=1200) fig.update_xaxes(title=None)  renderer.show_plotly(fig) In\u00a0[13]: Copied! <pre># Start with raw data\ndf_raw = study.scen_comp.fetch('buses_t.marginal_price')\n\n# Filter to high voltage buses\nbuses_model_df = study.scen.get_dataset('base').fetch('buses')\ndf_filtered = filter_by_model_query(df_raw, buses_model_df, 'v_nom &gt;= 380')\n\n# Add control area information\ndf_with_areas = prepend_model_prop_levels(df_filtered, buses_model_df, 'control_area')\n\n# Calculate hourly area prices\nhourly_area_prices = df_with_areas.T.groupby(['dataset', 'control_area']).mean().T.unstack()\n\n# Reshape for analysis\narea_price_stats = hourly_area_prices.groupby(['dataset', 'control_area']).agg(['mean', 'std', 'min', 'max'])\n\nprint(area_price_stats)\n</pre> # Start with raw data df_raw = study.scen_comp.fetch('buses_t.marginal_price')  # Filter to high voltage buses buses_model_df = study.scen.get_dataset('base').fetch('buses') df_filtered = filter_by_model_query(df_raw, buses_model_df, 'v_nom &gt;= 380')  # Add control area information df_with_areas = prepend_model_prop_levels(df_filtered, buses_model_df, 'control_area')  # Calculate hourly area prices hourly_area_prices = df_with_areas.T.groupby(['dataset', 'control_area']).mean().T.unstack()  # Reshape for analysis area_price_stats = hourly_area_prices.groupby(['dataset', 'control_area']).agg(['mean', 'std', 'min', 'max'])  print(area_price_stats) <pre>                                 mean   std    min    max\ndataset           control_area                           \nbase              50Hertz       11.66  4.90   7.54  22.37\n                  Amprion       18.88  3.17  14.31  23.37\n                  TenneTDE      15.09  4.93   8.25  22.19\n                  TransnetBW    23.33  2.80  19.34  25.85\nsolar_150         50Hertz       11.63  4.88   7.62  22.17\n                  Amprion       18.26  3.83  12.23  23.38\n                  TenneTDE      14.64  5.38   6.99  22.07\n                  TransnetBW    22.74  3.43  16.90  25.85\nsolar_150 vs base 50Hertz       -0.03  0.17  -0.48   0.26\n                  Amprion       -0.63  0.92  -2.59   0.15\n                  TenneTDE      -0.45  0.76  -2.27   0.51\n                  TransnetBW    -0.59  1.02  -4.12   0.07\nsolar_200         50Hertz       11.45  4.87   6.85  21.82\n                  Amprion       17.79  4.25  11.10  23.27\n                  TenneTDE      14.29  5.63   6.32  22.00\n                  TransnetBW    22.18  4.14  14.74  25.85\nsolar_200 vs base 50Hertz       -0.21  0.25  -0.81   0.27\n                  Amprion       -1.09  1.48  -4.50   0.09\n                  TenneTDE      -0.80  1.20  -3.94   0.55\n                  TransnetBW    -1.16  1.66  -4.96   0.18\nwind_150          50Hertz        7.09  2.57   4.44  14.42\n                  Amprion       17.08  2.48  12.77  20.28\n                  TenneTDE      12.08  3.11   7.18  17.09\n                  TransnetBW    22.66  3.35  18.08  25.88\nwind_150 vs base  50Hertz       -4.57  2.69  -8.67  -1.05\n                  Amprion       -1.81  1.33  -3.49   0.88\n                  TenneTDE      -3.01  2.22  -6.34  -0.06\n                  TransnetBW    -0.67  1.01  -4.21   0.14\nwind_200          50Hertz        4.77  2.16   2.01   8.85\n                  Amprion       15.64  3.20  10.32  19.47\n                  TenneTDE      10.42  3.47   4.93  15.57\n                  TransnetBW    21.04  5.06  12.37  25.94\nwind_200 vs base  50Hertz       -6.89  2.99 -13.52  -2.81\n                  Amprion       -3.24  1.21  -5.10   0.26\n                  TenneTDE      -4.67  2.20  -8.58  -0.60\n                  TransnetBW    -2.29  2.50  -7.52   0.19\n</pre> <p>What would normally take dozens of lines of data manipulation code is condensed into a few expressive operations that maintain the relationships between different types of data.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/#mesqual-102-advanced-data-fetching-techniques","title":"MESQUAL 102: Advanced Data Fetching Techniques\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/#introduction","title":"Introduction\u00b6","text":"<p>Building on the StudyManager fundamentals, this notebook demonstrates MESQUAL's advanced data fetching and transformation capabilities. These techniques significantly streamline multi-scenario analysis by leveraging pandas' power while abstracting away common boilerplate code.</p> <p>MESQUAL's data handling utilities provide efficient ways to:</p> <ul> <li>Work with multi-index DataFrames from scenario and comparison collections</li> <li>Filter data using model properties</li> <li>Aggregate data across complex dimensions</li> <li>Combine scenarios and comparisons in unified analyses</li> </ul>"},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mesqual and all requirements are installed.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/#working-with-multi-index-dataframes","title":"Working with Multi-Index DataFrames\u00b6","text":"<p>Fetching data across multiple scenarios naturally creates multi-index DataFrames. Let's examine and transform these structures:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/#calculating-averages-per-scenario-comparison","title":"Calculating Averages Per Scenario Comparison\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/#combining-scenarios-and-comparisons","title":"Combining Scenarios and Comparisons\u00b6","text":"<p>To analyze both raw values and deltas in one operation:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/#unified-analysis","title":"Unified Analysis\u00b6","text":"<p>We can perform operations on this unified structure:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/#integrating-model-data-with-time-series","title":"Integrating Model Data with Time Series\u00b6","text":"<p>A powerful MESQUAL capability is the integration of static model data with time series data.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/#accessing-model-data","title":"Accessing Model Data\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/#filtering-by-model-properties","title":"Filtering by Model Properties\u00b6","text":"<p>Let's filter our time series to include only high voltage buses (v_nom &gt;= 380 kV):</p>"},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/#prepending-model-properties","title":"Prepending Model Properties\u00b6","text":"<p>We can add model properties as additional index levels:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/#aggregating-by-model-properties","title":"Aggregating by Model Properties\u00b6","text":"<p>Now we can efficiently aggregate by control area:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/#sophisticated-visualization-example","title":"Sophisticated Visualization Example\u00b6","text":"<p>Let's demonstrate these techniques with a more sophisticated visualization. We'll create a boxplot showing the distribution of hourly prices by control area and scenario:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/#advanced-transformation-pipeline","title":"Advanced Transformation Pipeline\u00b6","text":"<p>Let's trace through a complete data transformation pipeline that demonstrates MESQUAL's efficiency:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_102_mastering_data_fetching/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ul> <li>Unified Analysis: Work with scenarios and comparisons in the same analytical framework</li> <li>Model and Time-Series Data Fetching: Unified framework for fetching model and time-series data</li> <li>Useful Integration Utilities: MESQUAL provides tools like <code>filter_by_model_query</code> and <code>prepend_model_prop_levels</code> that efficiently establish relationships between model properties and time series data and provide frequently needed filtering and property mapping features</li> </ul> <p>These capabilities make MESQUAL particularly powerful for complex multi-scenario analyses that would otherwise require extensive custom code.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_103_scenario_attributes/","title":"MESQUAL 103: Scenario Attributes","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git\n        %cd mesqual-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mesqual -U\n        !pip install git+https://github.com/helgeesch/mesqual-pypsa -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git         %cd mesqual-vanilla-studies/          !pip install git+https://github.com/helgeesch/mesqual -U         !pip install git+https://github.com/helgeesch/mesqual-pypsa -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>\ud83d\udda5\ufe0f Running locally. No setup needed.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mesqual-vanilla-studies')\n    os.chdir('/content/mesqual-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mesqual import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mesqual-vanilla-studies')     os.chdir('/content/mesqual-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mesqual import StudyManager except ImportError:     raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\") <pre>\u2705 Environment ready. Let\u2019s go!\n</pre> In\u00a0[3]: Copied! <pre>import os\nimport plotly.express as px\nimport pypsa\n\nfrom mesqual import StudyManager\nfrom mesqual.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query\nfrom mesqual.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom mesqual_pypsa import PyPSADataset\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nPlotlyTheme().apply()\nrenderer = ConditionalRenderer()\n</pre> import os import plotly.express as px import pypsa  from mesqual import StudyManager from mesqual.utils.pandas_utils import flatten_df, prepend_model_prop_levels, filter_by_model_query from mesqual.utils.plotly_utils.plotly_theme import PlotlyTheme from mesqual_pypsa import PyPSADataset from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() PlotlyTheme().apply() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Register study-specific interpreters (details on this will be covered in a later notebook)\nfrom studies.study_01_intro_to_mesqual.src.study_specific_model_interpreters import ControlAreaModelInterpreter, ScigridDEBusModelInterpreter\n\nPyPSADataset.register_interpreter(ControlAreaModelInterpreter)\nPyPSADataset.register_interpreter(ScigridDEBusModelInterpreter)\n\n# setup up study same as in previous notebooks, but this time also set attributes\nstudy_folder = 'studies/study_01_intro_to_mesqual'\nnetworks_folder = os.path.join(study_folder, 'data/networks_scigrid_de')\n\nn_base = pypsa.Network(os.path.join(networks_folder, 'base.nc'))\nn_solar_150 = pypsa.Network(os.path.join(networks_folder, 'solar_150.nc'))\nn_solar_200 = pypsa.Network(os.path.join(networks_folder, 'solar_200.nc'))\nn_wind_150 = pypsa.Network(os.path.join(networks_folder, 'wind_150.nc'))\nn_wind_200 = pypsa.Network(os.path.join(networks_folder, 'wind_200.nc'))\n\nstudy = StudyManager.factory_from_scenarios(\n    scenarios=[\n        PyPSADataset(n_base,        name='base'),\n        PyPSADataset(n_solar_150,   name='solar_150', attributes=dict(res_tech='solar', scaling_factor=150)),\n        PyPSADataset(n_solar_200,   name='solar_200', attributes=dict(res_tech='solar', scaling_factor=200)),\n        PyPSADataset(n_wind_150,    name='wind_150', attributes=dict(res_tech='wind', scaling_factor=150)),\n        PyPSADataset(n_wind_200,    name='wind_200', attributes=dict(res_tech='wind', scaling_factor=200)),\n    ],\n    comparisons=[('solar_150', 'base'), ('solar_200', 'base'), ('wind_150', 'base'), ('wind_200', 'base')],\n    export_folder=os.path.join(study_folder, 'non_versioned/output'),\n)\n</pre> # Register study-specific interpreters (details on this will be covered in a later notebook) from studies.study_01_intro_to_mesqual.src.study_specific_model_interpreters import ControlAreaModelInterpreter, ScigridDEBusModelInterpreter  PyPSADataset.register_interpreter(ControlAreaModelInterpreter) PyPSADataset.register_interpreter(ScigridDEBusModelInterpreter)  # setup up study same as in previous notebooks, but this time also set attributes study_folder = 'studies/study_01_intro_to_mesqual' networks_folder = os.path.join(study_folder, 'data/networks_scigrid_de')  n_base = pypsa.Network(os.path.join(networks_folder, 'base.nc')) n_solar_150 = pypsa.Network(os.path.join(networks_folder, 'solar_150.nc')) n_solar_200 = pypsa.Network(os.path.join(networks_folder, 'solar_200.nc')) n_wind_150 = pypsa.Network(os.path.join(networks_folder, 'wind_150.nc')) n_wind_200 = pypsa.Network(os.path.join(networks_folder, 'wind_200.nc'))  study = StudyManager.factory_from_scenarios(     scenarios=[         PyPSADataset(n_base,        name='base'),         PyPSADataset(n_solar_150,   name='solar_150', attributes=dict(res_tech='solar', scaling_factor=150)),         PyPSADataset(n_solar_200,   name='solar_200', attributes=dict(res_tech='solar', scaling_factor=200)),         PyPSADataset(n_wind_150,    name='wind_150', attributes=dict(res_tech='wind', scaling_factor=150)),         PyPSADataset(n_wind_200,    name='wind_200', attributes=dict(res_tech='wind', scaling_factor=200)),     ],     comparisons=[('solar_150', 'base'), ('solar_200', 'base'), ('wind_150', 'base'), ('wind_200', 'base')],     export_folder=os.path.join(study_folder, 'non_versioned/output'), ) In\u00a0[5]: Copied! <pre>scen_attributes_df = study.scen.get_attributes_concat_df()\nprint(scen_attributes_df)\n</pre> scen_attributes_df = study.scen.get_attributes_concat_df() print(scen_attributes_df) <pre>attribute res_tech scaling_factor\ndataset                          \nbase           NaN            NaN\nsolar_150    solar            150\nsolar_200    solar            200\nwind_150      wind            150\nwind_200      wind            200\n</pre> <p>Of course, the same method can be called for the collection of comparisons and returns a similar df.</p> In\u00a0[6]: Copied! <pre>comp_attributes_df = study.comp.get_attributes_concat_df()\nprint(comp_attributes_df)\n</pre> comp_attributes_df = study.comp.get_attributes_concat_df() print(comp_attributes_df) <pre>attribute         res_tech scaling_factor variation_dataset reference_dataset\ndataset                                                                      \nsolar_150 vs base    solar            150         solar_150              base\nsolar_200 vs base    solar            200         solar_200              base\nwind_150 vs base      wind            150          wind_150              base\nwind_200 vs base      wind            200          wind_200              base\n</pre> <p>As you can see, for DatasetComparisons, the method automatically uses the attributes of the variation dataset while also including the names to variation as well as reference dataset. You could, of course, also set attributes explicitly for every comparison dataset individually.</p> <p>From the previous notebooks, you might also remember that there is even a combined dataset for all scenarios + comparisons. The method even works for this one, as you can see below.</p> In\u00a0[7]: Copied! <pre>print(study.scen_comp.get_attributes_concat_df())\n</pre> print(study.scen_comp.get_attributes_concat_df()) <pre>attribute                    res_tech scaling_factor variation_dataset reference_dataset\ntype       dataset                                                                      \nscenario   base                   NaN            NaN               NaN               NaN\n           solar_150            solar            150               NaN               NaN\n           solar_200            solar            200               NaN               NaN\n           wind_150              wind            150               NaN               NaN\n           wind_200              wind            200               NaN               NaN\ncomparison solar_150 vs base    solar            150         solar_150              base\n           solar_200 vs base    solar            200         solar_200              base\n           wind_150 vs base      wind            150          wind_150              base\n           wind_200 vs base      wind            200          wind_200              base\n</pre> <p>You might be wondering, when would we need these attributes? Making use of Dataset attributes is entirely optional, but is extremely useful, for example when you want to categorize or group your assessment by different attributes.</p> <p>In the example below, we want to show the average price change per control area and categorize the scenarios by res_tech and scaling_factor, here we go:</p> In\u00a0[8]: Copied! <pre>comp_attributes_df = study.comp.get_attributes_concat_df()\nbuses_model_df = study.scen.get_dataset('base').fetch('buses')\ndata = study.comp.fetch('buses_t.marginal_price')\ndata = prepend_model_prop_levels(data, buses_model_df, 'control_area')\ndata = data.T.groupby(level=['dataset', 'control_area']).mean().T\ndata = data.mean().to_frame('value')\ndata = prepend_model_prop_levels(data, comp_attributes_df, 'res_tech', 'scaling_factor')\ndata = data.reset_index()\nfig = px.bar(\n    data_frame=data,\n    x='scaling_factor',\n    y='value',\n    color='res_tech',\n    facet_col='control_area',\n    labels={'scaling_factor': 'RES scaled to [%]', 'value': 'average price change compared to base [\u20ac/MWh]'},\n    barmode='group',\n)\nfig.update_layout(title='&lt;b&gt;Average price change per control area and scenario&lt;/b&gt;', width=1200)\n\nrenderer.show_plotly(fig)\n</pre> comp_attributes_df = study.comp.get_attributes_concat_df() buses_model_df = study.scen.get_dataset('base').fetch('buses') data = study.comp.fetch('buses_t.marginal_price') data = prepend_model_prop_levels(data, buses_model_df, 'control_area') data = data.T.groupby(level=['dataset', 'control_area']).mean().T data = data.mean().to_frame('value') data = prepend_model_prop_levels(data, comp_attributes_df, 'res_tech', 'scaling_factor') data = data.reset_index() fig = px.bar(     data_frame=data,     x='scaling_factor',     y='value',     color='res_tech',     facet_col='control_area',     labels={'scaling_factor': 'RES scaled to [%]', 'value': 'average price change compared to base [\u20ac/MWh]'},     barmode='group', ) fig.update_layout(title='Average price change per control area and scenario', width=1200)  renderer.show_plotly(fig) <p>You might have spottet that the prepend_model_prop_levels method is applied in combination with the comp_attributes_df. In this application, the attributes_df basically just functions as a model_df (mapping from object names to properties/memberships/attributes) for the index level with dataset names.</p> <p>In many cases, dataset attributes are actually retrievable from the raw-data folder / file and can be interpreted based on a rule. This is especially useful if you are managing more than just a hand full of scenarios with potentially many attribute dimensions.</p> <p>The used method in the example below utilizes the rule based attribute setting and sets up the study manager instance by a method call. Visit the source code if you want to see the implementation for yourself.</p> In\u00a0[9]: Copied! <pre>from studies.study_01_intro_to_mesqual.scripts.setup_study_manager import get_scigrid_de_study_manager\n\nstudy = get_scigrid_de_study_manager()  # same study setup as from above, using rule based attribute attribution and a re-usable script.\n</pre> from studies.study_01_intro_to_mesqual.scripts.setup_study_manager import get_scigrid_de_study_manager  study = get_scigrid_de_study_manager()  # same study setup as from above, using rule based attribute attribution and a re-usable script."},{"location":"mesqual-study-01/notebooks/mesqual_103_scenario_attributes/#mesqual-103-scenario-attributes","title":"MESQUAL 103: Scenario Attributes\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_103_scenario_attributes/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mesqual and all requirements are installed.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_103_scenario_attributes/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ul> <li>Dataset Attribute Feature: Describe scenarios and scenario comparisons with attributes and have the information at hand during your assessment.</li> <li>Modularity of descriptive <code>model_df</code> framework: Treat the attributes_df as if it was a <code>model_df</code> for the dataset names and make use of tools like <code>filter_by_model_query</code> and <code>prepend_model_prop_levels</code> in combination with multi-scenario (multi-scenario-comparison) dataframes.</li> </ul>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/","title":"MESQUAL 201: KPI Framework and Unit Handling","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git\n        %cd mesqual-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mesqual -U\n        !pip install git+https://github.com/helgeesch/mesqual-pypsa -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git         %cd mesqual-vanilla-studies/          !pip install git+https://github.com/helgeesch/mesqual -U         !pip install git+https://github.com/helgeesch/mesqual-pypsa -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>\ud83d\udda5\ufe0f Running locally. No setup needed.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mesqual-vanilla-studies')\n    os.chdir('/content/mesqual-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mesqual import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let's go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mesqual-vanilla-studies')     os.chdir('/content/mesqual-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mesqual import StudyManager except ImportError:     raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let's go!\") <pre>\u2705 Environment ready. Let's go!\n</pre> In\u00a0[3]: Copied! <pre>import pandas as pd\nfrom mesqual import kpis\nfrom mesqual.units import Units\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\n\nconfigure_clean_output_for_jupyter_notebook()\n</pre> import pandas as pd from mesqual import kpis from mesqual.units import Units from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook  configure_clean_output_for_jupyter_notebook() In\u00a0[4]: Copied! <pre>from studies.study_01_intro_to_mesqual.scripts.setup_study_manager import get_scigrid_de_study_manager\n\nstudy = get_scigrid_de_study_manager()\n\nprint(\"Study scenarios:\")\nfor dataset in study.scen.datasets:\n    print(f\"  \ud83d\udcca {dataset.name}\")\n\n# Get base dataset for exploration\nds_base = study.scen.get_dataset('base')\nprint(f\"\\n\u2705 Loaded dataset: {ds_base.name}\")\n</pre> from studies.study_01_intro_to_mesqual.scripts.setup_study_manager import get_scigrid_de_study_manager  study = get_scigrid_de_study_manager()  print(\"Study scenarios:\") for dataset in study.scen.datasets:     print(f\"  \ud83d\udcca {dataset.name}\")  # Get base dataset for exploration ds_base = study.scen.get_dataset('base') print(f\"\\n\u2705 Loaded dataset: {ds_base.name}\") <pre>Study scenarios:\n  \ud83d\udcca base\n  \ud83d\udcca solar_150\n  \ud83d\udcca solar_200\n  \ud83d\udcca wind_150\n  \ud83d\udcca wind_200\n\n\u2705 Loaded dataset: base\n</pre> In\u00a0[5]: Copied! <pre># Define KPI using builder pattern\nprice_kpi_definition = (\n    kpis.FlagAggKPIBuilder()\n    .for_flag('control_areas_t.vol_weighted_marginal_price')\n    .with_aggregation(kpis.Aggregations.Mean)\n    .for_objects(['TenneTDE', '50Hertz'])\n    .build()\n)\n\n# Generate KPI\nds_base.add_kpis_from_definitions(price_kpi_definition)\n\n# Examine the KPI\nkpi = ds_base.kpi_collection._kpis[0]\nprint(f\"KPI Name: {kpi.name}\")\nprint(f\"Value: {kpi.value:.2f}\")\nprint(f\"Quantity: {kpi.quantity}\")\nprint(f\"\\nKPI Attributes:\")\nfor attr, value in kpi.attributes.as_dict(primitive_values=True).items():\n    print(f\"  {attr}: {value}\")\n</pre> # Define KPI using builder pattern price_kpi_definition = (     kpis.FlagAggKPIBuilder()     .for_flag('control_areas_t.vol_weighted_marginal_price')     .with_aggregation(kpis.Aggregations.Mean)     .for_objects(['TenneTDE', '50Hertz'])     .build() )  # Generate KPI ds_base.add_kpis_from_definitions(price_kpi_definition)  # Examine the KPI kpi = ds_base.kpi_collection._kpis[0] print(f\"KPI Name: {kpi.name}\") print(f\"Value: {kpi.value:.2f}\") print(f\"Quantity: {kpi.quantity}\") print(f\"\\nKPI Attributes:\") for attr, value in kpi.attributes.as_dict(primitive_values=True).items():     print(f\"  {attr}: {value}\") <pre>KPI Name: control_areas_t.vol_weighted_marginal_price Mean TenneTDE\nValue: 16.91\nQuantity: 16.906856836420513 EUR_per_MWh\n\nKPI Attributes:\n  flag: control_areas_t.vol_weighted_marginal_price\n  model_flag: control_areas\n  object_name: TenneTDE\n  aggregation: Mean\n  dataset_name: base\n  dataset_type: &lt;class 'studies.study_01_intro_to_mesqual.scripts.setup_study_manager.ScigridDEDataset'&gt;\n  value_comparison: None\n  arithmetic_operation: None\n  reference_dataset_name: None\n  variation_dataset_name: None\n  name_prefix: \n  name_suffix: \n  custom_name: None\n  unit: EUR_per_MWh\n</pre> In\u00a0[6]: Copied! <pre># Get all control areas\ncontrol_areas = ds_base.fetch('control_areas').index.to_list()\nprint(f\"Control areas: {control_areas}\")\n\n# Create KPIs for all control areas at once\nall_price_kpis_def = (\n    kpis.FlagAggKPIBuilder()\n    .for_flag('control_areas_t.vol_weighted_marginal_price')\n    .with_aggregation(kpis.Aggregations.Mean)\n    # .for_all_objects()  # is the default, can be skipped\n    .build()\n)\n\nds_base.add_kpis_from_definitions(all_price_kpis_def)\n\nprint(f\"\\n\u2705 Generated {len(all_price_kpis_def)} KPIs in one operation\")\nprint(\"\\nSample KPIs:\")\nfor kpi in ds_base.kpi_collection[:20]:\n    print(f\"  {kpi.name}: {Units.get_pretty_text_for_quantity(kpi.quantity)}\")\n</pre> # Get all control areas control_areas = ds_base.fetch('control_areas').index.to_list() print(f\"Control areas: {control_areas}\")  # Create KPIs for all control areas at once all_price_kpis_def = (     kpis.FlagAggKPIBuilder()     .for_flag('control_areas_t.vol_weighted_marginal_price')     .with_aggregation(kpis.Aggregations.Mean)     # .for_all_objects()  # is the default, can be skipped     .build() )  ds_base.add_kpis_from_definitions(all_price_kpis_def)  print(f\"\\n\u2705 Generated {len(all_price_kpis_def)} KPIs in one operation\") print(\"\\nSample KPIs:\") for kpi in ds_base.kpi_collection[:20]:     print(f\"  {kpi.name}: {Units.get_pretty_text_for_quantity(kpi.quantity)}\") <pre>Control areas: ['50Hertz', 'TenneTDE', 'TransnetBW', 'Amprion']\n\n\u2705 Generated 1 KPIs in one operation\n\nSample KPIs:\n  control_areas_t.vol_weighted_marginal_price Mean TenneTDE: 16.9 \u20ac/MWh\n  control_areas_t.vol_weighted_marginal_price Mean 50Hertz: 11.9 \u20ac/MWh\n  control_areas_t.vol_weighted_marginal_price Mean Amprion: 20.3 \u20ac/MWh\n  control_areas_t.vol_weighted_marginal_price Mean TransnetBW: 23.6 \u20ac/MWh\n</pre> In\u00a0[7]: Copied! <pre># Create KPIs with different kpis.Aggregations\nkpi_defs = (\n    kpis.FlagAggKPIBuilder()\n    .for_flag('control_areas_t.vol_weighted_marginal_price')\n    .with_aggregations([kpis.Aggregations.Sum, kpis.Aggregations.Max, kpis.Aggregations.Min])\n    .for_objects(['TenneTDE', '50Hertz'])\n    .build()\n)\nds_base.add_kpis_from_definitions(kpi_defs)\nkpi = ds_base.kpi_collection[-1]\nprint(f\"{kpi.name:10s}: {Units.get_pretty_text_for_quantity(kpi.quantity)}\")\n</pre> # Create KPIs with different kpis.Aggregations kpi_defs = (     kpis.FlagAggKPIBuilder()     .for_flag('control_areas_t.vol_weighted_marginal_price')     .with_aggregations([kpis.Aggregations.Sum, kpis.Aggregations.Max, kpis.Aggregations.Min])     .for_objects(['TenneTDE', '50Hertz'])     .build() ) ds_base.add_kpis_from_definitions(kpi_defs) kpi = ds_base.kpi_collection[-1] print(f\"{kpi.name:10s}: {Units.get_pretty_text_for_quantity(kpi.quantity)}\") <pre>control_areas_t.vol_weighted_marginal_price Min 50Hertz: 6.76 \u20ac/MWh\n</pre> In\u00a0[8]: Copied! <pre># Create generation KPI\ngen_kpi_def = (\n    kpis.FlagAggKPIBuilder()\n    .for_flag('generators_t.p')\n    .with_aggregation(kpis.Aggregations.Mean)\n    .build()\n)\n\nds_base.add_kpis_from_definitions(gen_kpi_def)\n\n# Show original and pretty units\nsample_kpi = ds_base.kpi_collection[-1]\nprint(f\"Original quantity: {sample_kpi.quantity}\")\nprint(f\"Pretty unit: {Units.get_quantity_in_pretty_unit(sample_kpi.quantity)}\")\nprint(f\"Pretty text: {Units.get_pretty_text_for_quantity(Units.get_quantity_in_pretty_unit(sample_kpi.quantity), thousands_separator=' ')}\")\n</pre> # Create generation KPI gen_kpi_def = (     kpis.FlagAggKPIBuilder()     .for_flag('generators_t.p')     .with_aggregation(kpis.Aggregations.Mean)     .build() )  ds_base.add_kpis_from_definitions(gen_kpi_def)  # Show original and pretty units sample_kpi = ds_base.kpi_collection[-1] print(f\"Original quantity: {sample_kpi.quantity}\") print(f\"Pretty unit: {Units.get_quantity_in_pretty_unit(sample_kpi.quantity)}\") print(f\"Pretty text: {Units.get_pretty_text_for_quantity(Units.get_quantity_in_pretty_unit(sample_kpi.quantity), thousands_separator=' ')}\") <pre>Original quantity: 0.7883543532348286 MW\nPretty unit: 788.3543532348286 kW\nPretty text: 788 kW\n</pre> In\u00a0[9]: Copied! <pre># Get a KPI with energy units\nprint(\"Original KPI:\")\nprint(f\"  Value: {sample_kpi.value:.2f}\")\nprint(f\"  Unit: {sample_kpi.attributes.unit}\")\nprint(f\"  Quantity: {sample_kpi.quantity}\")\n\n# Convert to different unit\nquantity_in_GW = Units.get_quantity_in_target_unit(sample_kpi.quantity, Units.GW)\nGW_text = Units.get_pretty_text_for_quantity(quantity_in_GW)\nprint(f\"\\nConverted to GW: {GW_text}\")\n\n# Get in pretty unit\nquantity_pretty = Units.get_quantity_in_pretty_unit(quantity_in_GW)\ntext_pretty = Units.get_pretty_text_for_quantity(quantity_pretty)\nprint(f\"\\nPretty unit quantity: {text_pretty}\")\n</pre> # Get a KPI with energy units print(\"Original KPI:\") print(f\"  Value: {sample_kpi.value:.2f}\") print(f\"  Unit: {sample_kpi.attributes.unit}\") print(f\"  Quantity: {sample_kpi.quantity}\")  # Convert to different unit quantity_in_GW = Units.get_quantity_in_target_unit(sample_kpi.quantity, Units.GW) GW_text = Units.get_pretty_text_for_quantity(quantity_in_GW) print(f\"\\nConverted to GW: {GW_text}\")  # Get in pretty unit quantity_pretty = Units.get_quantity_in_pretty_unit(quantity_in_GW) text_pretty = Units.get_pretty_text_for_quantity(quantity_pretty) print(f\"\\nPretty unit quantity: {text_pretty}\") <pre>Original KPI:\n  Value: 0.79\n  Unit: MW\n  Quantity: 0.7883543532348286 MW\n\nConverted to GW: 0.00079 GW\n\nPretty unit quantity: 788 kW\n</pre> In\u00a0[10]: Copied! <pre># Create KPI with custom attributes\nkpi_def = (\n    kpis.FlagAggKPIBuilder()\n    .for_flag('control_areas_t.vol_weighted_marginal_price')\n    .with_aggregation(kpis.Aggregations.Mean)\n    .for_objects(['TenneTDE', '50Hertz'])\n    .with_custom_name('Price')\n    .with_extra_attributes(\n        custom_category='custom value 123'\n    )\n    .build()\n)\nds_base.add_kpis_from_definitions(kpi_def)\n\n# Show original and pretty units\nsample_kpi = ds_base.kpi_collection[0]\n\nprint(\"KPI Attributes:\")\nattrs = kpi.attributes.as_dict(primitive_values=True)\nfor key, value in sorted(attrs.items()):\n    if value is not None:\n        print(f\"  {key:20s}: {value}\")\n</pre> # Create KPI with custom attributes kpi_def = (     kpis.FlagAggKPIBuilder()     .for_flag('control_areas_t.vol_weighted_marginal_price')     .with_aggregation(kpis.Aggregations.Mean)     .for_objects(['TenneTDE', '50Hertz'])     .with_custom_name('Price')     .with_extra_attributes(         custom_category='custom value 123'     )     .build() ) ds_base.add_kpis_from_definitions(kpi_def)  # Show original and pretty units sample_kpi = ds_base.kpi_collection[0]  print(\"KPI Attributes:\") attrs = kpi.attributes.as_dict(primitive_values=True) for key, value in sorted(attrs.items()):     if value is not None:         print(f\"  {key:20s}: {value}\") <pre>KPI Attributes:\n  aggregation         : Min\n  dataset_name        : base\n  dataset_type        : &lt;class 'studies.study_01_intro_to_mesqual.scripts.setup_study_manager.ScigridDEDataset'&gt;\n  flag                : control_areas_t.vol_weighted_marginal_price\n  model_flag          : control_areas\n  name_prefix         : \n  name_suffix         : \n  object_name         : 50Hertz\n  unit                : EUR_per_MWh\n</pre> In\u00a0[11]: Copied! <pre># Clear existing KPIs\nstudy.scen.clear_kpi_collection_for_all_child_datasets()\n\n# Generate price KPIs for all scenarios\nprice_def = (\n    kpis.FlagAggKPIBuilder()\n    .for_flag('control_areas_t.vol_weighted_marginal_price')\n    .with_aggregation(kpis.Aggregations.Mean)\n    .for_all_objects()\n    .build()\n)\n\nfor dataset in study.scen.datasets:\n    dataset.add_kpis_from_definitions(price_def)\n    print(f\"\u2705 Generated {len(price_def)} KPIs for {dataset.name}\")\n\n# Get merged collection\nall_kpis = study.scen.get_merged_kpi_collection()\nprint(f\"\\n\ud83d\udcca Total KPIs across all scenarios: {all_kpis.size}\")\n</pre> # Clear existing KPIs study.scen.clear_kpi_collection_for_all_child_datasets()  # Generate price KPIs for all scenarios price_def = (     kpis.FlagAggKPIBuilder()     .for_flag('control_areas_t.vol_weighted_marginal_price')     .with_aggregation(kpis.Aggregations.Mean)     .for_all_objects()     .build() )  for dataset in study.scen.datasets:     dataset.add_kpis_from_definitions(price_def)     print(f\"\u2705 Generated {len(price_def)} KPIs for {dataset.name}\")  # Get merged collection all_kpis = study.scen.get_merged_kpi_collection() print(f\"\\n\ud83d\udcca Total KPIs across all scenarios: {all_kpis.size}\") <pre>\u2705 Generated 1 KPIs for base\n\u2705 Generated 1 KPIs for solar_150\n\u2705 Generated 1 KPIs for solar_200\n\u2705 Generated 1 KPIs for wind_150\n\u2705 Generated 1 KPIs for wind_200\n\n\ud83d\udcca Total KPIs across all scenarios: 20\n</pre> In\u00a0[12]: Copied! <pre># Filter by attributes\ntennet_kpis = all_kpis.filter(object_name='TenneTDE')\nprint(f\"KPIs for TenneTDE: {tennet_kpis.size}\")\n\n# Filter by dataset\nbase_kpis = all_kpis.filter(dataset_name='base')\nprint(f\"KPIs for base scenario: {base_kpis.size}\")\n\n# Combined filtering\ntennet_base = all_kpis.filter(\n    object_name='TenneTDE',\n    dataset_name='base',\n    aggregation=kpis.Aggregations.Mean\n)\nprint(f\"\\nFiltered result: {tennet_base.size} KPI(s)\")\nif tennet_base.size &gt; 0:\n    kpi = tennet_base[0]\n    print(f\"  {kpi.name}: {Units.get_pretty_text_for_quantity(kpi.quantity)}\")\n</pre> # Filter by attributes tennet_kpis = all_kpis.filter(object_name='TenneTDE') print(f\"KPIs for TenneTDE: {tennet_kpis.size}\")  # Filter by dataset base_kpis = all_kpis.filter(dataset_name='base') print(f\"KPIs for base scenario: {base_kpis.size}\")  # Combined filtering tennet_base = all_kpis.filter(     object_name='TenneTDE',     dataset_name='base',     aggregation=kpis.Aggregations.Mean ) print(f\"\\nFiltered result: {tennet_base.size} KPI(s)\") if tennet_base.size &gt; 0:     kpi = tennet_base[0]     print(f\"  {kpi.name}: {Units.get_pretty_text_for_quantity(kpi.quantity)}\") <pre>KPIs for TenneTDE: 5\nKPIs for base scenario: 4\n\nFiltered result: 1 KPI(s)\n  control_areas_t.vol_weighted_marginal_price Mean TenneTDE: 16.9 \u20ac/MWh\n</pre> In\u00a0[13]: Copied! <pre># Group by dataset\nby_dataset = all_kpis.group_by('dataset_name')\n\nprint(\"KPIs grouped by dataset:\")\nfor (dataset,), kpi_collection in by_dataset.items():\n    print(f\"  {dataset}: {kpi_collection.size} KPIs\")\n\n# Group by object and dataset\nby_object_dataset = all_kpis.group_by('object_name', 'dataset_name')\n\nprint(f\"\\nTotal groups (object \u00d7 dataset): {len(by_object_dataset)}\")\nprint(\"\\nSample groups:\")\nfor i, ((obj, ds), kpis) in enumerate(by_object_dataset.items()):\n    if i &lt; 3:\n        print(f\"  {obj} \u00d7 {ds}: {kpis.size} KPI(s)\")\n</pre> # Group by dataset by_dataset = all_kpis.group_by('dataset_name')  print(\"KPIs grouped by dataset:\") for (dataset,), kpi_collection in by_dataset.items():     print(f\"  {dataset}: {kpi_collection.size} KPIs\")  # Group by object and dataset by_object_dataset = all_kpis.group_by('object_name', 'dataset_name')  print(f\"\\nTotal groups (object \u00d7 dataset): {len(by_object_dataset)}\") print(\"\\nSample groups:\") for i, ((obj, ds), kpis) in enumerate(by_object_dataset.items()):     if i &lt; 3:         print(f\"  {obj} \u00d7 {ds}: {kpis.size} KPI(s)\") <pre>KPIs grouped by dataset:\n  base: 4 KPIs\n  solar_150: 4 KPIs\n  solar_200: 4 KPIs\n  wind_150: 4 KPIs\n  wind_200: 4 KPIs\n\nTotal groups (object \u00d7 dataset): 20\n\nSample groups:\n  50Hertz \u00d7 base: 1 KPI(s)\n  Amprion \u00d7 base: 1 KPI(s)\n  TenneTDE \u00d7 base: 1 KPI(s)\n</pre>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#mesqual-201-kpi-framework-and-unit-handling","title":"MESQUAL 201: KPI Framework and Unit Handling\u00b6","text":"<p>This notebook demonstrates how to build a structured KPI (Key Performance Indicator) system with proper unit handling using MESQUAL's new KPI framework.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#introduction","title":"Introduction\u00b6","text":"<p>MESQUAL's KPI framework provides a sophisticated system for creating, managing, and analyzing performance metrics across energy scenarios. This notebook covers:</p> <ol> <li>KPI Builder Pattern: Declarative KPI creation with fluent API</li> <li>Batch Generation: Efficient computation across multiple objects</li> <li>Unit Handling: Automatic unit conversion and pretty formatting</li> <li>Aggregations: Various statistical aggregations (Sum, Mean, Max, etc.)</li> <li>KPI Attributes: Rich metadata for filtering and organization</li> <li>Comparison KPIs: Scenario comparison workflows</li> </ol> <p>The new KPI system is designed for 10x performance improvement through batch-first architecture while providing a cleaner, more intuitive API.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mesqual and all requirements are installed.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#load-study-data","title":"Load Study Data\u00b6","text":"<p>Let's load our study data using the same Scigrid-DE setup:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#part-1-building-kpis-with-the-builder-pattern","title":"Part 1: Building KPIs with the Builder Pattern\u00b6","text":"<p>The KPI framework uses a fluent builder pattern for declarative KPI creation:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#creating-a-simple-kpi","title":"Creating a Simple KPI\u00b6","text":"<p>Let's create a KPI for average market price in a control area:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#batch-kpi-generation","title":"Batch KPI Generation\u00b6","text":"<p>The real power comes from batch generation across multiple objects:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#part-2-aggregation-types","title":"Part 2: Aggregation Types\u00b6","text":"<p>MESQUAL supports various aggregation types:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#part-3-unit-handling","title":"Part 3: Unit Handling\u00b6","text":"<p>MESQUAL's unit system provides automatic conversion and pretty formatting:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#automatic-pretty-units","title":"Automatic Pretty Units\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#kpi-unit-conversion-methods","title":"KPI Unit Conversion Methods\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#part-4-kpi-attributes-and-metadata","title":"Part 4: KPI Attributes and Metadata\u00b6","text":"<p>Every KPI carries rich metadata for filtering and organization:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#part-5-multi-scenario-kpi-generation","title":"Part 5: Multi-Scenario KPI Generation\u00b6","text":"<p>Generate KPIs across multiple scenarios efficiently:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#part-6-kpi-collection-filtering","title":"Part 6: KPI Collection Filtering\u00b6","text":"<p>The KPI collection provides powerful filtering capabilities:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#part-7-kpi-grouping","title":"Part 7: KPI Grouping\u00b6","text":"<p>Group KPIs by attributes for analysis:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#summary-kpi-framework-capabilities","title":"Summary: KPI Framework Capabilities\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#key-features-demonstrated","title":"Key Features Demonstrated:\u00b6","text":"<ol> <li><p>Fluent Builder API</p> <ul> <li><code>.for_flag()</code> - Specify data source</li> <li><code>.with_aggregation()</code> - Choose aggregation type</li> <li><code>.for_objects()</code> - Select objects</li> <li><code>.with_attributes()</code> - Add custom metadata</li> <li><code>.build()</code> - Create definition</li> </ul> </li> <li><p>Batch Operations</p> <ul> <li>Single fetch per flag (not per object)</li> <li>Column-wise aggregation</li> <li>10x performance improvement</li> </ul> </li> <li><p>Unit Handling</p> <ul> <li>Automatic unit tracking</li> <li>Pretty unit conversion</li> <li><code>get_kpi_in_unit()</code> - Convert to specific unit</li> <li><code>get_kpi_in_pretty_unit()</code> - Auto-select readable unit</li> </ul> </li> <li><p>Rich Metadata</p> <ul> <li>Object name, flag, aggregation</li> <li>Dataset name and type</li> <li>Custom attributes</li> <li>Extra attributes dictionary</li> </ul> </li> <li><p>Collection Operations</p> <ul> <li><code>.filter()</code> - Filter by attributes</li> <li><code>.group_by()</code> - Group by attributes</li> <li><code>.get_related()</code> - Find related KPIs</li> <li><code>.to_dataframe()</code> - Export to pandas</li> </ul> </li> </ol>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#performance-benefits","title":"Performance Benefits:\u00b6","text":"<ul> <li>Old System: N fetches + N kpis.Aggregations (one per object)</li> <li>New System: 1 fetch + 1 aggregation (batch operation)</li> <li>Result: Up to 10x faster for large datasets</li> </ul>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#next-steps","title":"Next Steps:\u00b6","text":"<p>In notebook 202, we'll explore:</p> <ul> <li>Exporting KPI collections to DataFrames</li> <li>Creating pretty tables with automatic unit normalization</li> <li>Comparison KPIs for scenario analysis</li> <li>Advanced filtering and visualization</li> </ul>"},{"location":"mesqual-study-01/notebooks/mesqual_201_kpi_framework_and_units/#conclusion","title":"Conclusion\u00b6","text":"<p>The MESQUAL KPI framework provides:</p> <ul> <li>\u2705 Clean, declarative API - Builder pattern for readable code</li> <li>\u2705 High performance - Batch operations for efficiency</li> <li>\u2705 Automatic unit handling - No manual conversion needed</li> <li>\u2705 Rich metadata - Everything you need for filtering and analysis</li> <li>\u2705 Flexible filtering - Find exactly the KPIs you need s This foundation enables sophisticated multi-scenario energy system analysis with minimal code.</li> </ul>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/","title":"MESQUAL 202: KPI Collections and Pretty Tables","text":"In\u00a0[18]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git\n        %cd mesqual-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mesqual -U\n        !pip install git+https://github.com/helgeesch/mesqual-pypsa -U\n        !pip install git+https://github.com/helgeesch/captain-arro -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git         %cd mesqual-vanilla-studies/          !pip install git+https://github.com/helgeesch/mesqual -U         !pip install git+https://github.com/helgeesch/mesqual-pypsa -U         !pip install git+https://github.com/helgeesch/captain-arro -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>\ud83d\udda5\ufe0f Running locally. No setup needed.\n</pre> In\u00a0[19]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mesqual-vanilla-studies')\n    os.chdir('/content/mesqual-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mesqual import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let's go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mesqual-vanilla-studies')     os.chdir('/content/mesqual-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mesqual import StudyManager except ImportError:     raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let's go!\") <pre>\u2705 Environment ready. Let's go!\n</pre> In\u00a0[20]: Copied! <pre>import pandas as pd\nfrom mesqual import kpis\nfrom mesqual.units import Units\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\n\nconfigure_clean_output_for_jupyter_notebook()\n\n# Pandas display options for better tables\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 2)\npd.set_option('display.width', None)\n</pre> import pandas as pd from mesqual import kpis from mesqual.units import Units from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook  configure_clean_output_for_jupyter_notebook()  # Pandas display options for better tables pd.set_option('display.max_columns', None) pd.set_option('display.precision', 2) pd.set_option('display.width', None) In\u00a0[21]: Copied! <pre>from studies.study_01_intro_to_mesqual.scripts.setup_study_manager import get_scigrid_de_study_manager\n\nstudy = get_scigrid_de_study_manager()\n\n# Clear any existing KPIs\nstudy.scen.clear_kpi_collection_for_all_child_datasets()\nstudy.comp.clear_kpi_collection_for_all_child_datasets()\n\nprint(\"\u2705 Study loaded successfully\")\n</pre> from studies.study_01_intro_to_mesqual.scripts.setup_study_manager import get_scigrid_de_study_manager  study = get_scigrid_de_study_manager()  # Clear any existing KPIs study.scen.clear_kpi_collection_for_all_child_datasets() study.comp.clear_kpi_collection_for_all_child_datasets()  print(\"\u2705 Study loaded successfully\") <pre>\u2705 Study loaded successfully\n</pre> In\u00a0[22]: Copied! <pre># Generate price KPIs for all scenarios\nprice_def = (\n    kpis.FlagAggKPIBuilder()\n    .for_flag('control_areas_t.vol_weighted_marginal_price')\n    .with_aggregation(kpis.Aggregations.Mean)\n    .for_all_objects()\n    .build()\n)\n\nstudy.scen.add_kpis_from_definitions_to_all_child_datasets(price_def)\n    \nprint(f\"\u2705 Generated price KPIs for {len(study.scen.datasets)} scenarios\")\nprint(f\"   Total KPIs: {study.scen.get_merged_kpi_collection().size}\")\n</pre> # Generate price KPIs for all scenarios price_def = (     kpis.FlagAggKPIBuilder()     .for_flag('control_areas_t.vol_weighted_marginal_price')     .with_aggregation(kpis.Aggregations.Mean)     .for_all_objects()     .build() )  study.scen.add_kpis_from_definitions_to_all_child_datasets(price_def)      print(f\"\u2705 Generated price KPIs for {len(study.scen.datasets)} scenarios\") print(f\"   Total KPIs: {study.scen.get_merged_kpi_collection().size}\") <pre>\u2705 Generated price KPIs for 5 scenarios\n   Total KPIs: 20\n</pre> In\u00a0[23]: Copied! <pre># Get KPI collection and export to DataFrame\nkpi_collection = study.scen.get_merged_kpi_collection()\n\ndf = kpi_collection.to_dataframe()\n\nprint(f\"DataFrame shape: {df.shape}\")\nprint(f\"\\nColumns: {df.columns.tolist()}\")\nprint(f\"\\nFirst few rows:\")\nprint(df.head())\n</pre> # Get KPI collection and export to DataFrame kpi_collection = study.scen.get_merged_kpi_collection()  df = kpi_collection.to_dataframe()  print(f\"DataFrame shape: {df.shape}\") print(f\"\\nColumns: {df.columns.tolist()}\") print(f\"\\nFirst few rows:\") print(df.head()) <pre>DataFrame shape: (20, 18)\n\nColumns: ['name', 'flag', 'model_flag', 'object_name', 'aggregation', 'dataset_name', 'dataset_type', 'value_comparison', 'arithmetic_operation', 'reference_dataset_name', 'variation_dataset_name', 'name_prefix', 'name_suffix', 'custom_name', 'unit', 'value', 'res_tech', 'scaling_factor']\n\nFirst few rows:\n                                                name  \\\n0  control_areas_t.vol_weighted_marginal_price Me...   \n1  control_areas_t.vol_weighted_marginal_price Me...   \n2  control_areas_t.vol_weighted_marginal_price Me...   \n3  control_areas_t.vol_weighted_marginal_price Me...   \n4  control_areas_t.vol_weighted_marginal_price Me...   \n\n                                          flag     model_flag object_name  \\\n0  control_areas_t.vol_weighted_marginal_price  control_areas     50Hertz   \n1  control_areas_t.vol_weighted_marginal_price  control_areas     Amprion   \n2  control_areas_t.vol_weighted_marginal_price  control_areas    TenneTDE   \n3  control_areas_t.vol_weighted_marginal_price  control_areas  TransnetBW   \n4  control_areas_t.vol_weighted_marginal_price  control_areas     50Hertz   \n\n  aggregation dataset_name                                       dataset_type  \\\n0        Mean         base  &lt;class 'studies.study_01_intro_to_mesqual.scri...   \n1        Mean         base  &lt;class 'studies.study_01_intro_to_mesqual.scri...   \n2        Mean         base  &lt;class 'studies.study_01_intro_to_mesqual.scri...   \n3        Mean         base  &lt;class 'studies.study_01_intro_to_mesqual.scri...   \n4        Mean    solar_150  &lt;class 'studies.study_01_intro_to_mesqual.scri...   \n\n  value_comparison arithmetic_operation reference_dataset_name  \\\n0             None                 None                   None   \n1             None                 None                   None   \n2             None                 None                   None   \n3             None                 None                   None   \n4             None                 None                   None   \n\n  variation_dataset_name name_prefix name_suffix custom_name         unit  \\\n0                   None                                None  EUR_per_MWh   \n1                   None                                None  EUR_per_MWh   \n2                   None                                None  EUR_per_MWh   \n3                   None                                None  EUR_per_MWh   \n4                   None                                None  EUR_per_MWh   \n\n   value res_tech scaling_factor  \n0  11.90      NaN            NaN  \n1  20.25      NaN            NaN  \n2  16.91      NaN            NaN  \n3  23.62      NaN            NaN  \n4  11.87    solar            150  \n</pre> In\u00a0[24]: Copied! <pre>df_original = kpi_collection.to_dataframe(unit_handling='original')\nprint(\"Sample values with original units:\")\nprint(df_original[['object_name', 'dataset_name', 'value', 'unit']].head())\n</pre> df_original = kpi_collection.to_dataframe(unit_handling='original') print(\"Sample values with original units:\") print(df_original[['object_name', 'dataset_name', 'value', 'unit']].head()) <pre>Sample values with original units:\n  object_name dataset_name  value         unit\n0     50Hertz         base  11.90  EUR_per_MWh\n1     Amprion         base  20.25  EUR_per_MWh\n2    TenneTDE         base  16.91  EUR_per_MWh\n3  TransnetBW         base  23.62  EUR_per_MWh\n4     50Hertz    solar_150  11.87  EUR_per_MWh\n</pre> In\u00a0[25]: Copied! <pre>df_pretty = kpi_collection.to_dataframe(unit_handling='auto_convert')\nprint(\"Sample values with auto-converted pretty units:\")\nprint(df_pretty[['object_name', 'dataset_name', 'value', 'unit']].head())\n</pre> df_pretty = kpi_collection.to_dataframe(unit_handling='auto_convert') print(\"Sample values with auto-converted pretty units:\") print(df_pretty[['object_name', 'dataset_name', 'value', 'unit']].head()) <pre>Sample values with auto-converted pretty units:\n  object_name dataset_name  value         unit\n0     50Hertz         base  11.90  EUR_per_MWh\n1     Amprion         base  20.25  EUR_per_MWh\n2    TenneTDE         base  16.91  EUR_per_MWh\n3  TransnetBW         base  23.62  EUR_per_MWh\n4     50Hertz    solar_150  11.87  EUR_per_MWh\n</pre> In\u00a0[26]: Copied! <pre>df_normalized = kpi_collection.to_dataframe(normalize_to_collection=True)\nprint(\"All values normalized to common unit:\")\nprint(df_normalized[['object_name', 'dataset_name', 'value', 'unit']].head())\nprint(f\"\\nAll units: {df_normalized['unit'].unique()}\")\n</pre> df_normalized = kpi_collection.to_dataframe(normalize_to_collection=True) print(\"All values normalized to common unit:\") print(df_normalized[['object_name', 'dataset_name', 'value', 'unit']].head()) print(f\"\\nAll units: {df_normalized['unit'].unique()}\") <pre>All values normalized to common unit:\n  object_name dataset_name  value         unit\n0     50Hertz         base  11.90  EUR_per_MWh\n1     Amprion         base  20.25  EUR_per_MWh\n2    TenneTDE         base  16.91  EUR_per_MWh\n3  TransnetBW         base  23.62  EUR_per_MWh\n4     50Hertz    solar_150  11.87  EUR_per_MWh\n\nAll units: ['EUR_per_MWh']\n</pre> In\u00a0[27]: Copied! <pre>df_eur_mwh = kpi_collection.to_dataframe(\n    unit_handling='target',\n    target_unit=Units.EUR_per_MWh\n)\nprint(\"All values in EUR/MWh:\")\nprint(df_eur_mwh[['object_name', 'dataset_name', 'value', 'unit']].head())\n</pre> df_eur_mwh = kpi_collection.to_dataframe(     unit_handling='target',     target_unit=Units.EUR_per_MWh ) print(\"All values in EUR/MWh:\") print(df_eur_mwh[['object_name', 'dataset_name', 'value', 'unit']].head()) <pre>All values in EUR/MWh:\n  object_name dataset_name  value         unit\n0     50Hertz         base  11.90  EUR_per_MWh\n1     Amprion         base  20.25  EUR_per_MWh\n2    TenneTDE         base  16.91  EUR_per_MWh\n3  TransnetBW         base  23.62  EUR_per_MWh\n4     50Hertz    solar_150  11.87  EUR_per_MWh\n</pre> In\u00a0[28]: Copied! <pre># Export with normalized units\ndf = kpi_collection.to_dataframe(normalize_to_collection=True)\n\n# Create pivot table\npivot = df.pivot(index='object_name', columns='dataset_name', values='value')\n\n# Get the common unit for display\ncommon_unit = df['unit'].iloc[0]\n\nprint(f\"Average Market Prices [{common_unit}]\")\nprint(\"=\" * 80)\nprint(pivot)\n</pre> # Export with normalized units df = kpi_collection.to_dataframe(normalize_to_collection=True)  # Create pivot table pivot = df.pivot(index='object_name', columns='dataset_name', values='value')  # Get the common unit for display common_unit = df['unit'].iloc[0]  print(f\"Average Market Prices [{common_unit}]\") print(\"=\" * 80) print(pivot) <pre>Average Market Prices [EUR_per_MWh]\n================================================================================\ndataset_name   base  solar_150  solar_200  wind_150  wind_200\nobject_name                                                  \n50Hertz       11.90      11.87      11.68      6.67      4.28\nAmprion       20.25      19.58      19.05     18.49     16.85\nTenneTDE      16.91      16.34      15.91     14.59     13.27\nTransnetBW    23.62      23.05      22.52     22.93     21.25\n</pre> In\u00a0[29]: Copied! <pre># Create styled pivot table\nstyled = (\n    pivot\n    .style\n    .format(\"{:.2f}\")\n    .background_gradient(cmap='RdYlGn_r', axis=None)\n    .set_caption(f\"Average Market Prices by Control Area and Scenario [{common_unit}]\")\n)\n\nstyled\n</pre> # Create styled pivot table styled = (     pivot     .style     .format(\"{:.2f}\")     .background_gradient(cmap='RdYlGn_r', axis=None)     .set_caption(f\"Average Market Prices by Control Area and Scenario [{common_unit}]\") )  styled Out[29]: Average Market Prices by Control Area and Scenario [EUR_per_MWh] dataset_name base solar_150 solar_200 wind_150 wind_200 object_name 50Hertz 11.90 11.87 11.68 6.67 4.28 Amprion 20.25 19.58 19.05 18.49 16.85 TenneTDE 16.91 16.34 15.91 14.59 13.27 TransnetBW 23.62 23.05 22.52 22.93 21.25 In\u00a0[30]: Copied! <pre># Generate comparison KPIs\ncomp_def = (\n    kpis.ComparisonKPIBuilder(price_def)\n    .with_comparisons([\n        kpis.ValueComparisons.Increase,\n        kpis.ValueComparisons.PercentageIncrease\n    ])\n    .build()\n)\nstudy.comp.add_kpis_from_definitions_to_all_child_datasets(comp_def)\n\nprint(f\"\u2705 Generated comparison KPIs for {len(study.comp.datasets)} comparisons\")\nprint(f\"   Total comparison KPIs: {study.comp.get_merged_kpi_collection().size}\")\n</pre> # Generate comparison KPIs comp_def = (     kpis.ComparisonKPIBuilder(price_def)     .with_comparisons([         kpis.ValueComparisons.Increase,         kpis.ValueComparisons.PercentageIncrease     ])     .build() ) study.comp.add_kpis_from_definitions_to_all_child_datasets(comp_def)  print(f\"\u2705 Generated comparison KPIs for {len(study.comp.datasets)} comparisons\") print(f\"   Total comparison KPIs: {study.comp.get_merged_kpi_collection().size}\") <pre>\u2705 Generated comparison KPIs for 4 comparisons\n   Total comparison KPIs: 64\n</pre> In\u00a0[31]: Copied! <pre># Get absolute difference KPIs\ncomp_kpis = study.comp.get_merged_kpi_collection()\nabs_diff_kpis = comp_kpis.filter(value_comparison=kpis.ValueComparisons.Increase)\n\n# Export and pivot\ndf_comp = abs_diff_kpis.to_dataframe(normalize_to_collection=True)\npivot_comp = df_comp.pivot(index='object_name', columns='dataset_name', values='value')\n\ncomp_unit = df_comp['unit'].iloc[0]\n\nprint(f\"Price Changes vs Base Scenario [{comp_unit}]\")\nprint(\"=\" * 80)\nprint(pivot_comp)\n</pre> # Get absolute difference KPIs comp_kpis = study.comp.get_merged_kpi_collection() abs_diff_kpis = comp_kpis.filter(value_comparison=kpis.ValueComparisons.Increase)  # Export and pivot df_comp = abs_diff_kpis.to_dataframe(normalize_to_collection=True) pivot_comp = df_comp.pivot(index='object_name', columns='dataset_name', values='value')  comp_unit = df_comp['unit'].iloc[0]  print(f\"Price Changes vs Base Scenario [{comp_unit}]\") print(\"=\" * 80) print(pivot_comp) <pre>Price Changes vs Base Scenario [EUR_per_MWh]\n================================================================================\ndataset_name  solar_150 vs base  solar_200 vs base  wind_150 vs base  \\\nobject_name                                                            \n50Hertz                   -0.03              -0.23             -5.24   \nAmprion                   -0.67              -1.20             -1.76   \nTenneTDE                  -0.57              -0.99             -2.32   \nTransnetBW                -0.57              -1.10             -0.69   \n\ndataset_name  wind_200 vs base  \nobject_name                     \n50Hertz                  -7.62  \nAmprion                  -3.40  \nTenneTDE                 -3.64  \nTransnetBW               -2.36  \n</pre> In\u00a0[32]: Copied! <pre># Get relative difference KPIs\nrel_diff_kpis = comp_kpis.filter(value_comparison=kpis.ValueComparisons.PercentageIncrease)\n\n# Export and pivot\ndf_rel = rel_diff_kpis.to_dataframe()\npivot_rel = df_rel.pivot(index='object_name', columns='dataset_name', values='value')\n\n# Convert to percentage\npivot_rel = pivot_rel * 100\n\nprint(f\"Relative Price Changes vs Base Scenario [%]\")\nprint(\"=\" * 80)\nprint(pivot_rel)\n</pre> # Get relative difference KPIs rel_diff_kpis = comp_kpis.filter(value_comparison=kpis.ValueComparisons.PercentageIncrease)  # Export and pivot df_rel = rel_diff_kpis.to_dataframe() pivot_rel = df_rel.pivot(index='object_name', columns='dataset_name', values='value')  # Convert to percentage pivot_rel = pivot_rel * 100  print(f\"Relative Price Changes vs Base Scenario [%]\") print(\"=\" * 80) print(pivot_rel) <pre>Relative Price Changes vs Base Scenario [%]\n================================================================================\ndataset_name  solar_150 vs base  solar_200 vs base  wind_150 vs base  \\\nobject_name                                                            \n50Hertz                  -25.46            -189.91          -4399.98   \nAmprion                 -332.43            -592.08           -867.61   \nTenneTDE                -335.60            -588.05          -1371.59   \nTransnetBW              -241.53            -463.81           -290.87   \n\ndataset_name  wind_200 vs base  \nobject_name                     \n50Hertz               -6400.48  \nAmprion               -1680.10  \nTenneTDE              -2153.83  \nTransnetBW            -1000.40  \n</pre> In\u00a0[33]: Copied! <pre># Get KPIs for TenneTDE control area only\nall_kpis = study.scen.get_merged_kpi_collection()\n\ntennet_kpis = all_kpis.filter_by_kpi_attributes(\n    attributes=dict(object_name='TenneTDE')\n)\n\nprint(f\"KPIs for TenneTDE: {tennet_kpis.size}\")\n\n# Create table for single control area across scenarios\ndf_tennet = tennet_kpis.to_dataframe(normalize_to_collection=True)\nprint(f\"\\nTenneTDE Average Prices [{df_tennet['unit'].iloc[0]}]:\")\nprint(df_tennet[['dataset_name', 'value']].set_index('dataset_name'))\n</pre> # Get KPIs for TenneTDE control area only all_kpis = study.scen.get_merged_kpi_collection()  tennet_kpis = all_kpis.filter_by_kpi_attributes(     attributes=dict(object_name='TenneTDE') )  print(f\"KPIs for TenneTDE: {tennet_kpis.size}\")  # Create table for single control area across scenarios df_tennet = tennet_kpis.to_dataframe(normalize_to_collection=True) print(f\"\\nTenneTDE Average Prices [{df_tennet['unit'].iloc[0]}]:\") print(df_tennet[['dataset_name', 'value']].set_index('dataset_name')) <pre>KPIs for TenneTDE: 5\n\nTenneTDE Average Prices [EUR_per_MWh]:\n              value\ndataset_name       \nbase          16.91\nsolar_150     16.34\nsolar_200     15.91\nwind_150      14.59\nwind_200      13.27\n</pre> In\u00a0[34]: Copied! <pre># Get both scenario and comparison KPIs\nscenario_kpis = study.scen.get_merged_kpi_collection()\ncomparison_kpis = study.comp.get_merged_kpi_collection().filter(\n    value_comparison=kpis.ValueComparisons.Increase\n)\n\n# Export both\ndf_scen = scenario_kpis.to_dataframe(normalize_to_collection=True)\ndf_comp = comparison_kpis.to_dataframe(normalize_to_collection=True)\n\n# Add type indicator\ndf_scen['type'] = 'scenario'\ndf_comp['type'] = 'change'\n\n# Combine\ndf_combined = pd.concat([df_scen, df_comp], ignore_index=True)\n\n# Pivot for comparison\npivot_combined = df_combined.pivot(\n    index='object_name',\n    columns=['type', 'dataset_name'],\n    values='value'\n)\n\n# Sort columns for better readability\npivot_combined = pivot_combined.sort_index(axis=1, level=[1, 0])\n\nprint(\"Combined Scenario Values and Changes\")\nprint(\"=\" * 120)\nprint(pivot_combined)\n</pre> # Get both scenario and comparison KPIs scenario_kpis = study.scen.get_merged_kpi_collection() comparison_kpis = study.comp.get_merged_kpi_collection().filter(     value_comparison=kpis.ValueComparisons.Increase )  # Export both df_scen = scenario_kpis.to_dataframe(normalize_to_collection=True) df_comp = comparison_kpis.to_dataframe(normalize_to_collection=True)  # Add type indicator df_scen['type'] = 'scenario' df_comp['type'] = 'change'  # Combine df_combined = pd.concat([df_scen, df_comp], ignore_index=True)  # Pivot for comparison pivot_combined = df_combined.pivot(     index='object_name',     columns=['type', 'dataset_name'],     values='value' )  # Sort columns for better readability pivot_combined = pivot_combined.sort_index(axis=1, level=[1, 0])  print(\"Combined Scenario Values and Changes\") print(\"=\" * 120) print(pivot_combined) <pre>Combined Scenario Values and Changes\n========================================================================================================================\ntype         scenario                      change  scenario            change  \\\ndataset_name     base solar_150 solar_150 vs base solar_200 solar_200 vs base   \nobject_name                                                                     \n50Hertz         11.90     11.87             -0.03     11.68             -0.23   \nAmprion         20.25     19.58             -0.67     19.05             -1.20   \nTenneTDE        16.91     16.34             -0.57     15.91             -0.99   \nTransnetBW      23.62     23.05             -0.57     22.52             -1.10   \n\ntype         scenario           change scenario           change  \ndataset_name wind_150 wind_150 vs base wind_200 wind_200 vs base  \nobject_name                                                       \n50Hertz          6.67            -5.24     4.28            -7.62  \nAmprion         18.49            -1.76    16.85            -3.40  \nTenneTDE        14.59            -2.32    13.27            -3.64  \nTransnetBW      22.93            -0.69    21.25            -2.36  \n</pre>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#mesqual-202-kpi-collections-and-pretty-tables","title":"MESQUAL 202: KPI Collections and Pretty Tables\u00b6","text":"<p>This notebook demonstrates how to extract, organize, and present KPI data in well-formatted tables with automatic unit normalization.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#introduction","title":"Introduction\u00b6","text":"<p>Building on the KPI framework fundamentals from notebook 201, this notebook focuses on practical data extraction and presentation:</p> <ol> <li>DataFrame Export: Converting KPI collections to pandas DataFrames</li> <li>Unit Normalization: Automatic unit selection for readable tables</li> <li>Comparison KPIs: Creating and analyzing scenario differences</li> <li>Advanced Filtering: Model property-based filtering</li> <li>Pretty Tables: Publication-ready table formatting</li> <li>MultiIndex Support: Hierarchical table organization</li> </ol> <p>These techniques enable rapid creation of analysis-ready tables from complex multi-scenario KPI collections.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mesqual and all requirements are installed.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#load-study-and-generate-kpis","title":"Load Study and Generate KPIs\u00b6","text":"<p>Let's load our study and create a comprehensive set of KPIs:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#part-1-basic-dataframe-export","title":"Part 1: Basic DataFrame Export\u00b6","text":"<p>The simplest way to export KPIs is to convert them to a DataFrame:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#part-2-unit-handling-strategies","title":"Part 2: Unit Handling Strategies\u00b6","text":"<p>The KPI collection supports multiple unit handling strategies:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#strategy-1-original-units-default","title":"Strategy 1: Original Units (Default)\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#strategy-2-auto-convert-to-pretty-units","title":"Strategy 2: Auto-Convert to Pretty Units\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#strategy-3-normalize-to-collection","title":"Strategy 3: Normalize to Collection\u00b6","text":"<p>Find a single \"pretty\" unit that works well for the entire collection:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#strategy-4-target-unit","title":"Strategy 4: Target Unit\u00b6","text":"<p>Convert all KPIs to a specific unit:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#part-3-creating-pretty-tables","title":"Part 3: Creating Pretty Tables\u00b6","text":"<p>Let's create analysis-ready tables with proper organization:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#pivot-table-scenarios-objects","title":"Pivot Table: Scenarios \u00d7 Objects\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#styled-table-with-formatting","title":"Styled Table with Formatting\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#part-4-comparison-kpis","title":"Part 4: Comparison KPIs\u00b6","text":"<p>Create and analyze scenario comparison KPIs:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#comparison-table-absolute-differences","title":"Comparison Table: Absolute Differences\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#relative-differences-percentage-changes","title":"Relative Differences (Percentage Changes)\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#part-5-advanced-filtering","title":"Part 5: Advanced Filtering\u00b6","text":"<p>Use model properties to filter KPIs:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#part-6-combined-analysis-tables","title":"Part 6: Combined Analysis Tables\u00b6","text":"<p>Combine scenarios and comparisons in unified tables:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#summary-kpi-collection-and-table-capabilities","title":"Summary: KPI Collection and Table Capabilities\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#dataframe-export-strategies","title":"DataFrame Export Strategies:\u00b6","text":"<ol> <li><p>Original Units (<code>unit_handling='original'</code>)</p> <ul> <li>Keep units as computed</li> <li>Best for raw data export</li> </ul> </li> <li><p>Auto-Convert (<code>unit_handling='auto_convert'</code>)</p> <ul> <li>Each KPI converted to its own pretty unit</li> <li>Good for mixed KPI types</li> </ul> </li> <li><p>Normalize to Collection (<code>normalize_to_collection=True</code>)</p> <ul> <li>Single common unit for entire collection</li> <li>Best for tables and comparisons</li> <li>Uses median order of magnitude</li> </ul> </li> <li><p>Target Unit (<code>unit_handling='target', target_unit=...</code>)</p> <ul> <li>Explicit unit specification</li> <li>Useful for standard reporting formats</li> </ul> </li> </ol>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#table-creation-patterns","title":"Table Creation Patterns:\u00b6","text":"<ul> <li>Pivot Tables: <code>.pivot(index='object_name', columns='dataset_name', values='value')</code></li> </ul>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#comparison-workflows","title":"Comparison Workflows:\u00b6","text":"<ol> <li>Generate scenario KPIs</li> <li>Create comparison KPIs with <code>kpis.ComparisonKPIBuilder</code></li> <li>Filter by <code>value_comparison</code> type</li> <li>Export to DataFrame with normalized units</li> <li>Pivot and format</li> </ol>"},{"location":"mesqual-study-01/notebooks/mesqual_202_kpi_collections_and_tables/#conclusion","title":"Conclusion\u00b6","text":"<p>The MESQUAL KPI collection system provides:</p> <ul> <li>\u2705 Flexible Export - Multiple unit handling strategies</li> <li>\u2705 Automatic Normalization - Smart unit selection for readable tables</li> <li>\u2705 Rich Filtering - By attributes and model properties</li> <li>\u2705 Easy Pivoting - Transform to analysis-ready formats</li> <li>\u2705 Publication Quality - Styled tables with proper formatting</li> </ul> <p>These capabilities enable rapid creation of professional analysis tables from complex multi-scenario energy system studies, with minimal code and maximum clarity.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_301_time_series_dashboard/","title":"MESQUAL 301: Time Series Dashboard","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git\n        %cd mesqual-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mesqual -U\n        !pip install git+https://github.com/helgeesch/mesqual-pypsa -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git         %cd mesqual-vanilla-studies/          !pip install git+https://github.com/helgeesch/mesqual -U         !pip install git+https://github.com/helgeesch/mesqual-pypsa -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>\ud83d\udda5\ufe0f Running locally. No setup needed.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mesqual-vanilla-studies')\n    os.chdir('/content/mesqual-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mesqual import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mesqual-vanilla-studies')     os.chdir('/content/mesqual-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mesqual import StudyManager except ImportError:     raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\") <pre>\u2705 Environment ready. Let\u2019s go!\n</pre> In\u00a0[3]: Copied! <pre>import os\nimport pandas as pd\n\nfrom mesqual.visualizations.plotly_figures import TimeSeriesDashboardGenerator\nfrom mesqual.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom mesqual.utils.pandas_utils import xs_df\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nPlotlyTheme().apply()\nrenderer = ConditionalRenderer(height=800, width=1600)\n</pre> import os import pandas as pd  from mesqual.visualizations.plotly_figures import TimeSeriesDashboardGenerator from mesqual.utils.plotly_utils.plotly_theme import PlotlyTheme from mesqual.utils.pandas_utils import xs_df from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() PlotlyTheme().apply() renderer = ConditionalRenderer(height=800, width=1600) In\u00a0[4]: Copied! <pre># Load sample data\nurl = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\"\ndf_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1)\n\n# Create scenario dataset with variations\ndf_raw_scenarios = pd.concat(\n    {\n        'base': df_raw,\n        'scen1': df_raw ** 0.5,  # numerical transformation to mock a \"scenario\"\n        'scen2': df_raw ** 0.2,  # numerical transformation to mock a \"scenario\"\n    },\n    axis=1,\n    names=['dataset']\n)\n\n# Create dataset with renewable generation (as percentages)\ndf_res = df_raw[['onwind', 'offwind', 'solar']].copy() * 100\n# Remove one data point to show handling of missing data\ndf_res_scenarios = xs_df(df_raw_scenarios.copy(), ['onwind', 'offwind', 'solar'], level='variable', axis=1).drop(('scen1', 'offwind'), axis=1)\n\n# Create mixed dataset with price, load, and solar data\ndf_mixed = df_raw[['prices', 'load', 'solar']].copy()\ndf_mixed['solar'] *= 100  # Convert solar to percentage\ndf_mixed_scenarios = xs_df(df_raw_scenarios.copy(), ['prices', 'load', 'solar'], level='variable', axis=1)\nfor c in df_mixed_scenarios.columns:\n    if 'solar' in c:\n        df_mixed_scenarios[c] *= 100\n</pre> # Load sample data url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\" df_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1)  # Create scenario dataset with variations df_raw_scenarios = pd.concat(     {         'base': df_raw,         'scen1': df_raw ** 0.5,  # numerical transformation to mock a \"scenario\"         'scen2': df_raw ** 0.2,  # numerical transformation to mock a \"scenario\"     },     axis=1,     names=['dataset'] )  # Create dataset with renewable generation (as percentages) df_res = df_raw[['onwind', 'offwind', 'solar']].copy() * 100 # Remove one data point to show handling of missing data df_res_scenarios = xs_df(df_raw_scenarios.copy(), ['onwind', 'offwind', 'solar'], level='variable', axis=1).drop(('scen1', 'offwind'), axis=1)  # Create mixed dataset with price, load, and solar data df_mixed = df_raw[['prices', 'load', 'solar']].copy() df_mixed['solar'] *= 100  # Convert solar to percentage df_mixed_scenarios = xs_df(df_raw_scenarios.copy(), ['prices', 'load', 'solar'], level='variable', axis=1) for c in df_mixed_scenarios.columns:     if 'solar' in c:         df_mixed_scenarios[c] *= 100 In\u00a0[5]: Copied! <pre># Display RES ts\nprint(\"Renewable generation data shape:\", df_res.shape, \"\\n\")\nprint(df_res.head())\n</pre> # Display RES ts print(\"Renewable generation data shape:\", df_res.shape, \"\\n\") print(df_res.head()) <pre>Renewable generation data shape: (8760, 3) \n\nvariable             onwind  offwind  solar\n2015-01-01 00:00:00   15.66    70.30    0.0\n2015-01-01 01:00:00   16.59    68.75    0.0\n2015-01-01 02:00:00   17.46    65.35    0.0\n2015-01-01 03:00:00   17.45    68.03    0.0\n2015-01-01 04:00:00   18.26    72.72    0.0\n</pre> In\u00a0[6]: Copied! <pre>print(\"Mixed data shape:\", df_mixed.shape, \"\\n\")\nprint(df_mixed.tail())\n</pre> print(\"Mixed data shape:\", df_mixed.shape, \"\\n\") print(df_mixed.tail()) <pre>Mixed data shape: (8760, 3) \n\nvariable             prices   load  solar\n2015-12-31 19:00:00   36.79  47.72    0.0\n2015-12-31 20:00:00   28.81  45.91    0.0\n2015-12-31 21:00:00   26.27  45.61    0.0\n2015-12-31 22:00:00   29.99  43.76    0.0\n2015-12-31 23:00:00   31.59  41.91    0.0\n</pre> In\u00a0[7]: Copied! <pre>print(\"Scenarios RES data shape:\", df_res_scenarios.shape, \"\\n\")\nprint(df_res_scenarios.head())\n</pre> print(\"Scenarios RES data shape:\", df_res_scenarios.shape, \"\\n\") print(df_res_scenarios.head()) <pre>Scenarios RES data shape: (8760, 8) \n\ndataset               base                ...  scen2              \nvariable            onwind offwind solar  ... onwind offwind solar\n2015-01-01 00:00:00   0.16    0.70   0.0  ...   0.69    0.93   0.0\n2015-01-01 01:00:00   0.17    0.69   0.0  ...   0.70    0.93   0.0\n2015-01-01 02:00:00   0.17    0.65   0.0  ...   0.71    0.92   0.0\n2015-01-01 03:00:00   0.17    0.68   0.0  ...   0.71    0.93   0.0\n2015-01-01 04:00:00   0.18    0.73   0.0  ...   0.71    0.94   0.0\n\n[5 rows x 8 columns]\n</pre> In\u00a0[8]: Copied! <pre>print(\"Scenarios Mixed data shape:\", df_mixed_scenarios.shape, \"\\n\")\nprint(df_mixed_scenarios.head())\n</pre> print(\"Scenarios Mixed data shape:\", df_mixed_scenarios.shape, \"\\n\") print(df_mixed_scenarios.head()) <pre>Scenarios Mixed data shape: (8760, 9) \n\ndataset               base               ... scen2             \nvariable              load solar prices  ...  load solar prices\n2015-01-01 00:00:00  41.15   0.0    NaN  ...  2.10   0.0    NaN\n2015-01-01 01:00:00  40.13   0.0    NaN  ...  2.09   0.0    NaN\n2015-01-01 02:00:00  39.11   0.0    NaN  ...  2.08   0.0    NaN\n2015-01-01 03:00:00  38.77   0.0    NaN  ...  2.08   0.0    NaN\n2015-01-01 04:00:00  38.94   0.0    NaN  ...  2.08   0.0    NaN\n\n[5 rows x 9 columns]\n</pre> In\u00a0[9]: Copied! <pre># Basic visualization with variables as rows\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis='date',                      # X-axis per date (other options are aggregations per: week, month, year_week, year_month)\n    color_continuous_scale='viridis',   # Color scale\n    facet_row='variable',               # Each variable gets its own row\n    facet_row_order=['solar', 'onwind', 'offwind']\n)\n\nfig = generator.get_figure(df_res, title='Renewable Generation Patterns')\n\nrenderer.show_plotly(fig)\n</pre> # Basic visualization with variables as rows generator = TimeSeriesDashboardGenerator(     x_axis='date',                      # X-axis per date (other options are aggregations per: week, month, year_week, year_month)     color_continuous_scale='viridis',   # Color scale     facet_row='variable',               # Each variable gets its own row     facet_row_order=['solar', 'onwind', 'offwind'] )  fig = generator.get_figure(df_res, title='Renewable Generation Patterns')  renderer.show_plotly(fig) <p>You can see a dashboard with three time-series visualizations as heatmaps and some KPIs right next to the heatmaps. The time-series heatmaps have the date on the x-axis and the time of the day on the y-axis. This enables you to quickly understand any intra-daily patterns, such as the one of the solar profile.</p> <p>The stats on the right-hand side are an efficient way to immediately understand some KPIs for each time-series, and how they differ between the different variables.</p> In\u00a0[10]: Copied! <pre>from mesqual.visualizations.plotly_figures.timeseries_dashboard import DashboardConfig\n\nstats = DashboardConfig.DEFAULT_STATISTICS.copy()\nstats.pop('Datums')\nstats.pop('Abs max')\nstats.pop('Abs mean')\nstats['Median'] = DashboardConfig.STATISTICS_LIBRARY['Median']\nstats['% == 0'] = lambda x: (x == 0).sum() / len(x) * 100  # custom agg should be a callable of a pd.Series\nstats['% &gt; 50'] = lambda x: (x &gt; 50).sum() / len(x) * 100  # custom agg should be a callable of a pd.Series\n\n# Basic visualization with custom set of stats\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis='date',                      # X-axis per date (other options are aggregations per: week, month, year_week, year_month)\n    color_continuous_scale='viridis',   # Color scale\n    facet_row='variable',               # Each variable gets its own row\n    facet_row_order=['solar', 'onwind', 'offwind'],\n    stat_aggs=stats,\n)\n\nfig = generator.get_figure(df_res, title='Renewable Generation Patterns with custom stats')\n\nrenderer.show_plotly(fig)\n</pre> from mesqual.visualizations.plotly_figures.timeseries_dashboard import DashboardConfig  stats = DashboardConfig.DEFAULT_STATISTICS.copy() stats.pop('Datums') stats.pop('Abs max') stats.pop('Abs mean') stats['Median'] = DashboardConfig.STATISTICS_LIBRARY['Median'] stats['% == 0'] = lambda x: (x == 0).sum() / len(x) * 100  # custom agg should be a callable of a pd.Series stats['% &gt; 50'] = lambda x: (x &gt; 50).sum() / len(x) * 100  # custom agg should be a callable of a pd.Series  # Basic visualization with custom set of stats generator = TimeSeriesDashboardGenerator(     x_axis='date',                      # X-axis per date (other options are aggregations per: week, month, year_week, year_month)     color_continuous_scale='viridis',   # Color scale     facet_row='variable',               # Each variable gets its own row     facet_row_order=['solar', 'onwind', 'offwind'],     stat_aggs=stats, )  fig = generator.get_figure(df_res, title='Renewable Generation Patterns with custom stats')  renderer.show_plotly(fig) In\u00a0[11]: Copied! <pre># Organizing variables as columns with wrapping\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis='date',\n    color_continuous_scale='viridis',\n    facet_col='variable',\n    facet_col_order=['solar', 'onwind', 'offwind'],\n    facet_col_wrap=2  # Wrap after 2 columns\n)\n\nfig = generator.get_figure(df_res, title='Variables as Columns (Wrapped)')\n\nrenderer.show_plotly(fig)\n</pre> # Organizing variables as columns with wrapping generator = TimeSeriesDashboardGenerator(     x_axis='date',     color_continuous_scale='viridis',     facet_col='variable',     facet_col_order=['solar', 'onwind', 'offwind'],     facet_col_wrap=2  # Wrap after 2 columns )  fig = generator.get_figure(df_res, title='Variables as Columns (Wrapped)')  renderer.show_plotly(fig) In\u00a0[12]: Copied! <pre># Creating a grid of scenarios (columns) and variables (rows)\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis='date',\n    facet_col='dataset',  # Scenarios as columns\n    facet_row='variable', # Variables as rows\n    facet_col_order=['base', 'scen1', 'scen2'],\n    facet_row_order=['onwind', 'solar', 'offwind'],\n    color_continuous_scale='viridis',\n    subplots_horizontal_spacing=0.05  # Adjust spacing between columns\n)\n\nfig = generator.get_figure(df_res_scenarios, title='Variable Patterns by Scenario')\n\nrenderer.show_plotly(fig)\n</pre> # Creating a grid of scenarios (columns) and variables (rows) generator = TimeSeriesDashboardGenerator(     x_axis='date',     facet_col='dataset',  # Scenarios as columns     facet_row='variable', # Variables as rows     facet_col_order=['base', 'scen1', 'scen2'],     facet_row_order=['onwind', 'solar', 'offwind'],     color_continuous_scale='viridis',     subplots_horizontal_spacing=0.05  # Adjust spacing between columns )  fig = generator.get_figure(df_res_scenarios, title='Variable Patterns by Scenario')  renderer.show_plotly(fig) In\u00a0[13]: Copied! <pre># Custom color scales by row (variable)\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis='date',\n    facet_col='dataset',\n    facet_row='variable',\n    facet_col_order=['base', 'scen1', 'scen2'],\n    facet_row_order=['solar', 'load', 'prices'],\n    per_facet_row_colorscale=True,              # Different color scale per row\n    facet_row_color_settings={\n        'solar': {'color_continuous_scale': 'Reds', 'range_color': [0, 100]},\n        'load': {'color_continuous_scale': 'Blues'},\n        'prices': {'color_continuous_scale': 'Portland', 'color_continuous_midpoint': 0},\n    },\n    subplots_horizontal_spacing=0.05  # Adjust spacing between columns\n)\n\nfig = generator.get_figure(df_mixed_scenarios, title='Variable-specific Color Scales')\n\nrenderer.show_plotly(fig)\n</pre> # Custom color scales by row (variable) generator = TimeSeriesDashboardGenerator(     x_axis='date',     facet_col='dataset',     facet_row='variable',     facet_col_order=['base', 'scen1', 'scen2'],     facet_row_order=['solar', 'load', 'prices'],     per_facet_row_colorscale=True,              # Different color scale per row     facet_row_color_settings={         'solar': {'color_continuous_scale': 'Reds', 'range_color': [0, 100]},         'load': {'color_continuous_scale': 'Blues'},         'prices': {'color_continuous_scale': 'Portland', 'color_continuous_midpoint': 0},     },     subplots_horizontal_spacing=0.05  # Adjust spacing between columns )  fig = generator.get_figure(df_mixed_scenarios, title='Variable-specific Color Scales')  renderer.show_plotly(fig) In\u00a0[14]: Copied! <pre># Multiple x-axis time aggregations with custom color scales\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis=['date', 'week', 'year_month'],      # Multiple time aggregations\n    color_continuous_scale='viridis',\n    facet_col='x_axis',                         # Aggregations as columns\n    facet_row='variable',\n    facet_row_order=['solar', 'load', 'prices'],\n    per_facet_row_colorscale=True,\n    facet_row_color_settings={\n        'solar': {'color_continuous_scale': 'Reds', 'range_color': [0, 100]},\n        'load': {'color_continuous_scale': 'Blues'},\n        'prices': {'color_continuous_scale': 'Portland', 'color_continuous_midpoint': 0},\n    },\n    subplots_horizontal_spacing=0.05  # Adjust spacing between columns\n)\n\nfig = generator.get_figure(df_mixed, title='Multiple Time Aggregations')\n\nrenderer.show_plotly(fig)\n</pre> # Multiple x-axis time aggregations with custom color scales generator = TimeSeriesDashboardGenerator(     x_axis=['date', 'week', 'year_month'],      # Multiple time aggregations     color_continuous_scale='viridis',     facet_col='x_axis',                         # Aggregations as columns     facet_row='variable',     facet_row_order=['solar', 'load', 'prices'],     per_facet_row_colorscale=True,     facet_row_color_settings={         'solar': {'color_continuous_scale': 'Reds', 'range_color': [0, 100]},         'load': {'color_continuous_scale': 'Blues'},         'prices': {'color_continuous_scale': 'Portland', 'color_continuous_midpoint': 0},     },     subplots_horizontal_spacing=0.05  # Adjust spacing between columns )  fig = generator.get_figure(df_mixed, title='Multiple Time Aggregations')  renderer.show_plotly(fig) In\u00a0[15]: Copied! <pre># Multiple statistical aggregations\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis='year_month',                        # Aggregated per year and month\n    facet_col='groupby_aggregation',            # Different stats as columns\n    groupby_aggregation=['min', 'mean', 'max'], # Stats to calculate\n    color_continuous_scale='viridis',\n    facet_row='variable',\n    facet_row_order=['solar', 'onwind', 'offwind'],\n    subplots_horizontal_spacing=0.05  # Adjust spacing between columns\n)\n\nfig = generator.get_figure(df_res, title='Min/Mean/Max Aggregations by Month')\n\nrenderer.show_plotly(fig)\n</pre> # Multiple statistical aggregations generator = TimeSeriesDashboardGenerator(     x_axis='year_month',                        # Aggregated per year and month     facet_col='groupby_aggregation',            # Different stats as columns     groupby_aggregation=['min', 'mean', 'max'], # Stats to calculate     color_continuous_scale='viridis',     facet_row='variable',     facet_row_order=['solar', 'onwind', 'offwind'],     subplots_horizontal_spacing=0.05  # Adjust spacing between columns )  fig = generator.get_figure(df_res, title='Min/Mean/Max Aggregations by Month')  renderer.show_plotly(fig) In\u00a0[16]: Copied! <pre># Custom color scales per statistic type\ngenerator = TimeSeriesDashboardGenerator(\n    x_axis='month',\n    facet_col='groupby_aggregation',\n    groupby_aggregation=['min', 'mean', 'max'],\n    facet_row='variable',\n    facet_row_order=['solar', 'onwind', 'offwind'],\n    per_facet_col_colorscale=True,  # Different color scale per column\n    facet_col_color_settings={  # Color scales for columns\n        'min': {'color_continuous_scale': 'Blues', 'range_color': [0, 10]},\n        'mean': {'color_continuous_scale': 'Greens', 'range_color': [0, 70]},\n        'max': {'color_continuous_scale': 'Reds', 'range_color': [0, 100]}\n    },\n    subplots_horizontal_spacing=0.05  # Adjust spacing between columns\n)\n\nfig = generator.get_figure(df_res, title='Custom Colors by Statistic Type')\n\nrenderer.show_plotly(fig)\n</pre> # Custom color scales per statistic type generator = TimeSeriesDashboardGenerator(     x_axis='month',     facet_col='groupby_aggregation',     groupby_aggregation=['min', 'mean', 'max'],     facet_row='variable',     facet_row_order=['solar', 'onwind', 'offwind'],     per_facet_col_colorscale=True,  # Different color scale per column     facet_col_color_settings={  # Color scales for columns         'min': {'color_continuous_scale': 'Blues', 'range_color': [0, 10]},         'mean': {'color_continuous_scale': 'Greens', 'range_color': [0, 70]},         'max': {'color_continuous_scale': 'Reds', 'range_color': [0, 100]}     },     subplots_horizontal_spacing=0.05  # Adjust spacing between columns )  fig = generator.get_figure(df_res, title='Custom Colors by Statistic Type')  renderer.show_plotly(fig)"},{"location":"mesqual-study-01/notebooks/mesqual_301_time_series_dashboard/#mesqual-301-time-series-dashboard","title":"MESQUAL 301: Time Series Dashboard\u00b6","text":"<p>This notebook demonstrates the TimeSeriesDashboard module for effective visualization of complex time-series data across multiple dimensions, such as multi-scenario and multi-object time-series DataFrames.</p> <p>The TimeSeriesDashboard module can be used as a standalone module and does not require the use of any further MESQUAL modules, so it can be used outside of the KPI framework, as well as outside of the StudyManager and Dataset framework. You can use this module with any time-series that you have in a pd DataFrame format with a DatetimeIndex. In fact, for the examples, we will use an example time-series from one of TU-Berlins public lecture documents.</p> <p>Key features of TimeSeriesDashboard:</p> <ul> <li>Heatmap visualization of time-series patterns</li> <li>Multi-faceted display for comparing scenarios and variables, following the plotly-express framework of facet_row and facet_col</li> <li>Flexible time-horizon aggregation (daily, weekly, monthly) combined with flexible statistical aggregations (e.g. mean, min, max, ...)</li> <li>Statistical summaries alongside visualizations, fully customizable</li> <li>Customizable color scales, optionally even per facet category</li> </ul>"},{"location":"mesqual-study-01/notebooks/mesqual_301_time_series_dashboard/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mesqual and all requirements are installed.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_301_time_series_dashboard/#loading-example-data","title":"Loading Example Data\u00b6","text":"<p>We'll work with renewable energy generation and market data from TU-Berlin for our examples.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_301_time_series_dashboard/#basic-timeseriesdashboard-visualization","title":"Basic TimeSeriesDashboard Visualization\u00b6","text":"<p>Let's visualize hourly patterns throughout different time periods, with variables organized as facet rows.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_301_time_series_dashboard/#custom-statistics-in-stats-column","title":"Custom Statistics in stats column\u00b6","text":"<p>You can fully customize the statistics shown in the stats column on the dashbaord. The Config has a default set and a library of pre-implemented statistical aggregations (feel free to check out). Custom aggregations can be implemented as needed, you just need to provide a callable for a pandas series that returns the KPI.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_301_time_series_dashboard/#facet-column-layouts","title":"Facet Column Layouts\u00b6","text":"<p>Instead of rows, we can also organize variables as columns, giving full control over the layout.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_301_time_series_dashboard/#multi-dimensional-facets-scenarios-variables","title":"Multi-dimensional Facets (Scenarios \u00d7 Variables)\u00b6","text":"<p>Visualize multiple scenarios and variables in a grid layout.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_301_time_series_dashboard/#custom-color-scales-per-variable","title":"Custom Color Scales Per Variable\u00b6","text":"<p>In many cases the different variables (or different objects / comparisons / scenarios) will have different orders-of-magnitude, making it useful to have a different color scale for the different categories.</p> <p>For this, just set <code>per_facet_row_colorscale</code> (or col) to True, and set the <code>facet_row_color_settings</code> (or col) accordingly. See an example below.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_301_time_series_dashboard/#multiple-time-aggregations","title":"Multiple Time Aggregations\u00b6","text":"<p>For the x_axis, you can also set a list of time aggregation methods, and then show them side-by-side (or row-by-row in case of facet_row, respectively).</p>"},{"location":"mesqual-study-01/notebooks/mesqual_301_time_series_dashboard/#statistical-aggregations","title":"Statistical Aggregations\u00b6","text":"<p>The same principle of setting multiple x_axis aggregations and comparing them can also be applied to Compare different statistical aggregations (min, mean, max) across each time-horizon.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_301_time_series_dashboard/#custom-colors-per-statistical-aggregation","title":"Custom Colors Per Statistical Aggregation\u00b6","text":"<p>The principle of custom color settings can, of course, also be applied per facet_col and in this case, for different statistical aggregations. Let's have a look.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_301_time_series_dashboard/#conclusion","title":"Conclusion\u00b6","text":"<p>The TimeSeriesDashboard is a powerful tool for visualizing time-series data across multiple dimensions. Key benefits include:</p> <ul> <li>Revealing hourly patterns across days, weeks, or months</li> <li>Easy comparison of multiple scenarios, variables, objects, ...</li> <li>Easy detection of structural errors in your data</li> <li>KPI summaries alongside visualizations</li> <li>Highly customizable with flexible KPI summaries, categorical faceting, and color scales</li> </ul> <p>Keep in mind, the TimeSeriesDashboard can be used as a standalone module, as long as you have your data in a pandas DataFrame format with a DatetimeIndex.</p> <p>This module becomes especially powerful once you have your multi-scenario study loaded into a StudyManager, because the StudyManager gives you efficient access to the multi-scenario DataFrames in proper MultiIndex format.</p> <p>Try for yourself! :)</p>"},{"location":"mesqual-study-01/notebooks/mesqual_302_segmented_colormap/","title":"MESQUAL 302: Segmented Colormap","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git\n        %cd mesqual-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mesqual -U\n        !pip install git+https://github.com/helgeesch/mesqual-pypsa -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git         %cd mesqual-vanilla-studies/          !pip install git+https://github.com/helgeesch/mesqual -U         !pip install git+https://github.com/helgeesch/mesqual-pypsa -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>\ud83d\udda5\ufe0f Running locally. No setup needed.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mesqual-vanilla-studies')\n    os.chdir('/content/mesqual-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mesqual import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mesqual-vanilla-studies')     os.chdir('/content/mesqual-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mesqual import StudyManager except ImportError:     raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\")  <pre>\u2705 Environment ready. Let\u2019s go!\n</pre> In\u00a0[3]: Copied! <pre>import numpy as np\nimport folium\nimport plotly.graph_objects as go\n\nimport mesqual.visualizations.folium_viz_system as folviz\nimport mesqual.visualizations.value_mapping_system as valmap\n\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\n\nconfigure_clean_output_for_jupyter_notebook()\nrenderer = ConditionalRenderer()\n</pre>  import numpy as np import folium import plotly.graph_objects as go  import mesqual.visualizations.folium_viz_system as folviz import mesqual.visualizations.value_mapping_system as valmap  from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer   configure_clean_output_for_jupyter_notebook() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Define our market price colormap segments\nprice_segments = {\n    (-500, -150): ['#000080'],  # Navy for extreme negative prices\n    (-150, 0): ['#0000FF', '#87CEFA'],  # Blue to light blue for negative prices\n    (0, 150): ['#00C300', '#FFFB00'],  # Light green to yellow-green for low positive prices\n    (150, 500): ['#FFFB00', '#FF9300'],  # Yellow-green to orange for high positive prices\n    (500, 10000): ['#FF0000']  # Red for extreme positive prices\n}\n\n# Create a colormap without folium legend functionality \nprice_colorscale = valmap.SegmentedContinuousColorscale(price_segments)\n\n# Create a legend for Folium maps\nprice_legend = folviz.legends.ContinuousColorscaleLegend(\n    mapping=price_colorscale,\n    title=\"Market Price (\u20ac/MWh)\",\n    width=350,\n    padding=20,\n    background_color=\"#FFFFFF\",\n    n_ticks_per_segment=2\n)\n\n# Show the legend in a Folium map\nm = folium.Map(location=[50.5, 10.5], zoom_start=6, tiles='CartoDB Positron')\nprice_legend.add_to(m)\n\n\nrenderer.show_folium(m)\n</pre> # Define our market price colormap segments price_segments = {     (-500, -150): ['#000080'],  # Navy for extreme negative prices     (-150, 0): ['#0000FF', '#87CEFA'],  # Blue to light blue for negative prices     (0, 150): ['#00C300', '#FFFB00'],  # Light green to yellow-green for low positive prices     (150, 500): ['#FFFB00', '#FF9300'],  # Yellow-green to orange for high positive prices     (500, 10000): ['#FF0000']  # Red for extreme positive prices }  # Create a colormap without folium legend functionality  price_colorscale = valmap.SegmentedContinuousColorscale(price_segments)  # Create a legend for Folium maps price_legend = folviz.legends.ContinuousColorscaleLegend(     mapping=price_colorscale,     title=\"Market Price (\u20ac/MWh)\",     width=350,     padding=20,     background_color=\"#FFFFFF\",     n_ticks_per_segment=2 )  # Show the legend in a Folium map m = folium.Map(location=[50.5, 10.5], zoom_start=6, tiles='CartoDB Positron') price_legend.add_to(m)   renderer.show_folium(m) <p>This legend will be used in the next notebook (mesqual_303_geospatial_visualization.ipynb) for our geospatial visualizations of market prices.</p> In\u00a0[5]: Copied! <pre># Get the colorscale for Plotly\nplotly_colorscale = price_colorscale.to_normalized_colorscale(num_reference_points_per_segment=5)\n\n# Create some sample data with a range of price values\nnp.random.seed(42)\nhours = 24\ndays = 7\nz = np.random.normal(loc=100, scale=150, size=(days, hours))\nz[2, 8:12] = -200  # Add some extreme negative values\nz[5, 15:20] = 700  # Add some extreme positive values\n\n# Create a heatmap with our custom colorscale\nfig = go.Figure(data=go.Heatmap(\n    z=z,\n    x=[f\"{h:02d}:00\" for h in range(hours)],\n    y=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"],\n    colorscale=plotly_colorscale,\n    zmin=-500,\n    zmax=10000,\n))\n\nfig.update_layout(\n    title=\"Example: Market Prices with Segmented Colormap\",\n    xaxis_title=\"Hour of Day\",\n    yaxis_title=\"Day of Week\",\n)\n\nrenderer.show_plotly(fig)\n</pre> # Get the colorscale for Plotly plotly_colorscale = price_colorscale.to_normalized_colorscale(num_reference_points_per_segment=5)  # Create some sample data with a range of price values np.random.seed(42) hours = 24 days = 7 z = np.random.normal(loc=100, scale=150, size=(days, hours)) z[2, 8:12] = -200  # Add some extreme negative values z[5, 15:20] = 700  # Add some extreme positive values  # Create a heatmap with our custom colorscale fig = go.Figure(data=go.Heatmap(     z=z,     x=[f\"{h:02d}:00\" for h in range(hours)],     y=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"],     colorscale=plotly_colorscale,     zmin=-500,     zmax=10000, ))  fig.update_layout(     title=\"Example: Market Prices with Segmented Colormap\",     xaxis_title=\"Hour of Day\",     yaxis_title=\"Day of Week\", )  renderer.show_plotly(fig)"},{"location":"mesqual-study-01/notebooks/mesqual_302_segmented_colormap/#mesqual-302-segmented-colormap","title":"MESQUAL 302: Segmented Colormap\u00b6","text":"<p>A useful module to merge multiple linear segments into a single cohesive colormap.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_302_segmented_colormap/#introduction","title":"Introduction\u00b6","text":"<p>Visualizing data with complex color scales can be challenging. Some datasets have natural break points where you want different color behavior:</p> <ul> <li>Extreme negative values in one color range</li> <li>Near-zero values with fine gradients</li> <li>Positive values in another color range</li> <li>Extreme positive values with distinct colors</li> </ul> <p>Traditional linear colormaps don't handle these multi-segment needs well. The <code>SegmentedColorMap</code> and its companion <code>SegmentedColorMapLegend</code> solve this problem by allowing you to define custom segments with distinct color behaviors.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_302_segmented_colormap/#key-features","title":"Key Features\u00b6","text":"<ul> <li>Define different color gradients for different value ranges</li> <li>Seamless transitions between segments</li> <li>Support for both continuous gradients and discrete colors</li> <li>Integration with both Folium (via legend) and Plotly (via normalized colorscale)</li> <li>Configurable ticks, labels, and visual styling</li> </ul>"},{"location":"mesqual-study-01/notebooks/mesqual_302_segmented_colormap/#creating-a-market-price-colormap","title":"Creating a Market Price Colormap\u00b6","text":"<p>Let's create a colormap for electricity market prices (\u20ac/MWh) with these segments:</p> <ul> <li>Extreme negative prices: -500 to -150 \u20ac/MWh (navy blue)</li> <li>Negative prices: -150 to 0 \u20ac/MWh (gradient from blue to light blue)</li> <li>Low positive prices: 0 to 150 \u20ac/MWh (gradient from light green to yellow-green)</li> <li>High positive prices: 150 to 500 \u20ac/MWh (gradient from yellow-green to orange)</li> <li>Extreme positive prices: 500 to 10000 \u20ac/MWh (red)</li> </ul>"},{"location":"mesqual-study-01/notebooks/mesqual_302_segmented_colormap/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mesqual and all requirements are installed.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_302_segmented_colormap/#using-the-colormap-with-plotly","title":"Using the Colormap with Plotly\u00b6","text":"<p>The <code>to_normalized_colorscale()</code> method converts our segmented colormap to a format compatible with Plotly:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_302_segmented_colormap/#benefit-of-segmented-colormaps","title":"Benefit of Segmented Colormaps\u00b6","text":"<p>With our segmented colormap, we can:</p> <ol> <li>Define a color scale split in custom segments</li> <li>Assign distinct colors to extreme values</li> <li>Create fine color gradients for the most common value ranges</li> <li>Use appropriate colors for different segments (e.g., red for extreme positive prices)</li> <li>Maintain consistent coloring across different visualizations</li> </ol> <p>This approach is particularly useful for electricity market data, where extreme prices are significant but rare, and the common price range needs detailed color differentiation.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_302_segmented_colormap/#conclusion","title":"Conclusion\u00b6","text":"<p>The <code>SegmentedColorMap</code> module provides a powerful tool for creating color scales that match the natural segments in your data. When combined with folium and plotly, it enables consistent, informative visualizations across different platforms and chart types.</p> <p>In the next notebook (mesqual_303), we'll apply this colormap to geospatial visualizations of electricity market data.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/","title":"MESQUAL 401: Folium Model Data Visualization - Areas, Points, and Lines","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git\n        %cd mesqual-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mesqual -U\n        !pip install git+https://github.com/helgeesch/mesqual-pypsa -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git         %cd mesqual-vanilla-studies/          !pip install git+https://github.com/helgeesch/mesqual -U         !pip install git+https://github.com/helgeesch/mesqual-pypsa -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>\ud83d\udda5\ufe0f Running locally. No setup needed.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mesqual-vanilla-studies')\n    os.chdir('/content/mesqual-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mesqual import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let's go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mesqual-vanilla-studies')     os.chdir('/content/mesqual-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mesqual import StudyManager except ImportError:     raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let's go!\") <pre>\u2705 Environment ready. Let's go!\n</pre> In\u00a0[3]: Copied! <pre>import os\nimport folium\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\n\nfrom mesqual import StudyManager\nfrom mesqual_pypsa import PyPSADataset\nimport mesqual.visualizations.folium_viz_system as folviz\nfrom mesqual.utils.folium_utils.background_color import set_background_color_of_map\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\n# Register study-specific interpreters\nfrom studies.study_01_intro_to_mesqual.src.study_specific_model_interpreters import (\n    ControlAreaModelInterpreter, \n    ScigridDEBusModelInterpreter\n)\n\nPyPSADataset.register_interpreter(ControlAreaModelInterpreter)\nPyPSADataset.register_interpreter(ScigridDEBusModelInterpreter)\n\nconfigure_clean_output_for_jupyter_notebook()\nrenderer = ConditionalRenderer()\n</pre> import os import folium import geopandas as gpd import pandas as pd import numpy as np  from mesqual import StudyManager from mesqual_pypsa import PyPSADataset import mesqual.visualizations.folium_viz_system as folviz from mesqual.utils.folium_utils.background_color import set_background_color_of_map from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  # Register study-specific interpreters from studies.study_01_intro_to_mesqual.src.study_specific_model_interpreters import (     ControlAreaModelInterpreter,      ScigridDEBusModelInterpreter )  PyPSADataset.register_interpreter(ControlAreaModelInterpreter) PyPSADataset.register_interpreter(ScigridDEBusModelInterpreter)  configure_clean_output_for_jupyter_notebook() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Load the StudyManager with PyPSA Scigrid-DE network data\nfrom studies.study_01_intro_to_mesqual.scripts.setup_study_manager import get_scigrid_de_study_manager\nstudy = get_scigrid_de_study_manager()\n\n# Get the base dataset for our visualizations\nds_base = study.scen.get_dataset('base')\n\nprint(\"Available data flags:\")\nfor flag in sorted(ds_base.accepted_flags):\n    if any(geo_term in flag for geo_term in ['control_areas', 'buses', 'lines']):\n        print(f\"  \ud83d\udccd {flag}\")\n    else:\n        print(f\"  \ud83d\udcca {flag}\")\n</pre> # Load the StudyManager with PyPSA Scigrid-DE network data from studies.study_01_intro_to_mesqual.scripts.setup_study_manager import get_scigrid_de_study_manager study = get_scigrid_de_study_manager()  # Get the base dataset for our visualizations ds_base = study.scen.get_dataset('base')  print(\"Available data flags:\") for flag in sorted(ds_base.accepted_flags):     if any(geo_term in flag for geo_term in ['control_areas', 'buses', 'lines']):         print(f\"  \ud83d\udccd {flag}\")     else:         print(f\"  \ud83d\udcca {flag}\") <pre>Available data flags:\n  \ud83d\udccd buses\n  \ud83d\udccd buses_t.marginal_price\n  \ud83d\udccd buses_t.p\n  \ud83d\udccd buses_t.q\n  \ud83d\udccd buses_t.v_ang\n  \ud83d\udccd buses_t.v_mag_pu\n  \ud83d\udccd buses_t.v_mag_pu_set\n  \ud83d\udcca carriers\n  \ud83d\udccd control_areas\n  \ud83d\udccd control_areas_t.vol_weighted_marginal_price\n  \ud83d\udcca generators\n  \ud83d\udcca generators_t.efficiency\n  \ud83d\udcca generators_t.marginal_cost\n  \ud83d\udcca generators_t.marginal_cost_quadratic\n  \ud83d\udcca generators_t.mu_lower\n  \ud83d\udcca generators_t.mu_p_set\n  \ud83d\udcca generators_t.mu_ramp_limit_down\n  \ud83d\udcca generators_t.mu_ramp_limit_up\n  \ud83d\udcca generators_t.mu_upper\n  \ud83d\udcca generators_t.p\n  \ud83d\udcca generators_t.p_max_pu\n  \ud83d\udcca generators_t.p_min_pu\n  \ud83d\udcca generators_t.p_set\n  \ud83d\udcca generators_t.q\n  \ud83d\udcca generators_t.q_set\n  \ud83d\udcca generators_t.ramp_limit_down\n  \ud83d\udcca generators_t.ramp_limit_up\n  \ud83d\udcca generators_t.shut_down\n  \ud83d\udcca generators_t.stand_by_cost\n  \ud83d\udcca generators_t.start_up\n  \ud83d\udcca generators_t.status\n  \ud83d\udcca global_constraints\n  \ud83d\udcca line_types\n  \ud83d\udccd lines\n  \ud83d\udccd lines_t.mu_lower\n  \ud83d\udccd lines_t.mu_upper\n  \ud83d\udccd lines_t.p0\n  \ud83d\udccd lines_t.p1\n  \ud83d\udccd lines_t.q0\n  \ud83d\udccd lines_t.q1\n  \ud83d\udccd lines_t.s_max_pu\n  \ud83d\udcca links\n  \ud83d\udcca links_t.efficiency\n  \ud83d\udcca links_t.marginal_cost\n  \ud83d\udcca links_t.marginal_cost_quadratic\n  \ud83d\udcca links_t.mu_lower\n  \ud83d\udcca links_t.mu_p_set\n  \ud83d\udcca links_t.mu_ramp_limit_down\n  \ud83d\udcca links_t.mu_ramp_limit_up\n  \ud83d\udcca links_t.mu_upper\n  \ud83d\udcca links_t.p0\n  \ud83d\udcca links_t.p1\n  \ud83d\udcca links_t.p_max_pu\n  \ud83d\udcca links_t.p_min_pu\n  \ud83d\udcca links_t.p_set\n  \ud83d\udcca links_t.ramp_limit_down\n  \ud83d\udcca links_t.ramp_limit_up\n  \ud83d\udcca links_t.shut_down\n  \ud83d\udcca links_t.stand_by_cost\n  \ud83d\udcca links_t.start_up\n  \ud83d\udcca links_t.status\n  \ud83d\udcca loads\n  \ud83d\udcca loads_t.p\n  \ud83d\udcca loads_t.p_set\n  \ud83d\udcca loads_t.q\n  \ud83d\udcca loads_t.q_set\n  \ud83d\udcca objective\n  \ud83d\udcca shapes\n  \ud83d\udcca shunt_impedances\n  \ud83d\udcca shunt_impedances_t.p\n  \ud83d\udcca shunt_impedances_t.q\n  \ud83d\udcca storage_units\n  \ud83d\udcca storage_units_t.efficiency_dispatch\n  \ud83d\udcca storage_units_t.efficiency_store\n  \ud83d\udcca storage_units_t.inflow\n  \ud83d\udcca storage_units_t.marginal_cost\n  \ud83d\udcca storage_units_t.marginal_cost_quadratic\n  \ud83d\udcca storage_units_t.marginal_cost_storage\n  \ud83d\udcca storage_units_t.mu_energy_balance\n  \ud83d\udcca storage_units_t.mu_lower\n  \ud83d\udcca storage_units_t.mu_state_of_charge_set\n  \ud83d\udcca storage_units_t.mu_upper\n  \ud83d\udcca storage_units_t.p\n  \ud83d\udcca storage_units_t.p_dispatch\n  \ud83d\udcca storage_units_t.p_max_pu\n  \ud83d\udcca storage_units_t.p_min_pu\n  \ud83d\udcca storage_units_t.p_set\n  \ud83d\udcca storage_units_t.p_store\n  \ud83d\udcca storage_units_t.q\n  \ud83d\udcca storage_units_t.q_set\n  \ud83d\udcca storage_units_t.spill\n  \ud83d\udcca storage_units_t.spill_cost\n  \ud83d\udcca storage_units_t.standing_loss\n  \ud83d\udcca storage_units_t.state_of_charge\n  \ud83d\udcca storage_units_t.state_of_charge_set\n  \ud83d\udcca stores\n  \ud83d\udcca stores_t.e\n  \ud83d\udcca stores_t.e_max_pu\n  \ud83d\udcca stores_t.e_min_pu\n  \ud83d\udcca stores_t.marginal_cost\n  \ud83d\udcca stores_t.marginal_cost_quadratic\n  \ud83d\udcca stores_t.marginal_cost_storage\n  \ud83d\udcca stores_t.mu_energy_balance\n  \ud83d\udcca stores_t.mu_lower\n  \ud83d\udcca stores_t.mu_upper\n  \ud83d\udcca stores_t.p\n  \ud83d\udcca stores_t.p_set\n  \ud83d\udcca stores_t.q\n  \ud83d\udcca stores_t.q_set\n  \ud83d\udcca stores_t.standing_loss\n  \ud83d\udcca sub_networks\n  \ud83d\udcca transformer_types\n  \ud83d\udcca transformers\n  \ud83d\udcca transformers_t.mu_lower\n  \ud83d\udcca transformers_t.mu_upper\n  \ud83d\udcca transformers_t.p0\n  \ud83d\udcca transformers_t.p1\n  \ud83d\udcca transformers_t.q0\n  \ud83d\udcca transformers_t.q1\n  \ud83d\udcca transformers_t.s_max_pu\n</pre> In\u00a0[5]: Copied! <pre># Load control areas directly from GeoJSON file\ngeojson_path = 'studies/study_01_intro_to_mesqual/data/DE_control_areas.geojson'\ncontrol_areas_gdf = gpd.read_file(geojson_path)\ncontrol_areas_gdf = control_areas_gdf.rename(columns={'tso': 'control_area'})\ncontrol_areas_gdf = control_areas_gdf.set_index('control_area')\ncontrol_areas_gdf = control_areas_gdf.set_crs(epsg=4326)\n\nprint(\"Control Areas from GeoJSON:\")\nprint(control_areas_gdf.head())\nprint(f\"\\nGeometry types: {control_areas_gdf.geometry.geom_type.unique()}\")\nprint(f\"CRS: {control_areas_gdf.crs}\")\n</pre> # Load control areas directly from GeoJSON file geojson_path = 'studies/study_01_intro_to_mesqual/data/DE_control_areas.geojson' control_areas_gdf = gpd.read_file(geojson_path) control_areas_gdf = control_areas_gdf.rename(columns={'tso': 'control_area'}) control_areas_gdf = control_areas_gdf.set_index('control_area') control_areas_gdf = control_areas_gdf.set_crs(epsg=4326)  print(\"Control Areas from GeoJSON:\") print(control_areas_gdf.head()) print(f\"\\nGeometry types: {control_areas_gdf.geometry.geom_type.unique()}\") print(f\"CRS: {control_areas_gdf.crs}\") <pre>Control Areas from GeoJSON:\n                                                       geometry\ncontrol_area                                                   \n50Hertz       MULTIPOLYGON (((11.2639 50.48074, 11.2474 50.4...\nTenneTDE      MULTIPOLYGON (((8.45592 50.41449, 8.38936 50.4...\nTransnetBW    MULTIPOLYGON (((8.62585 47.64422, 8.59569 47.6...\nAmprion       MULTIPOLYGON (((7.09992 49.15556, 7.0803 49.14...\n\nGeometry types: ['MultiPolygon']\nCRS: EPSG:4326\n</pre> In\u00a0[6]: Copied! <pre># Create a basic map with control areas\nm1 = folium.Map(\n    location=[51, 11],  # Center of Germany\n    zoom_start=6,\n    tiles=None\n)\nm1 = set_background_color_of_map(m1, color='#f8f9fa')\n\n# Create a simple area generator with static styling\nbasic_area_generator = folviz.AreaGenerator(\n    folviz.AreaFeatureResolver(\n        fill_color='#e3f2fd',\n        fill_opacity=0.6,\n        border_color='#1976d2',\n        border_width=2,\n        tooltip=True  # Auto-generate tooltips from data\n    )\n)\n\n# Create a feature group and add areas\nfeature_group = folium.FeatureGroup(name='Control Areas (Direct GeoJSON)')\nbasic_area_generator.generate_objects_for_model_df(control_areas_gdf, feature_group)\nfeature_group.add_to(m1)\n\nfolium.LayerControl().add_to(m1)\nrenderer.show_folium(m1)\n</pre> # Create a basic map with control areas m1 = folium.Map(     location=[51, 11],  # Center of Germany     zoom_start=6,     tiles=None ) m1 = set_background_color_of_map(m1, color='#f8f9fa')  # Create a simple area generator with static styling basic_area_generator = folviz.AreaGenerator(     folviz.AreaFeatureResolver(         fill_color='#e3f2fd',         fill_opacity=0.6,         border_color='#1976d2',         border_width=2,         tooltip=True  # Auto-generate tooltips from data     ) )  # Create a feature group and add areas feature_group = folium.FeatureGroup(name='Control Areas (Direct GeoJSON)') basic_area_generator.generate_objects_for_model_df(control_areas_gdf, feature_group) feature_group.add_to(m1)  folium.LayerControl().add_to(m1) renderer.show_folium(m1) In\u00a0[7]: Copied! <pre># Fetch control areas using MESQUAL dataset (via ControlAreaModelInterpreter)\ncontrol_areas_mesqual = ds_base.fetch('control_areas')\n\nprint(\"Control Areas from MESQUAL Dataset:\")\nprint(control_areas_mesqual.head())\nprint(f\"\\nSame data? {control_areas_mesqual.equals(control_areas_gdf)}\")\n</pre> # Fetch control areas using MESQUAL dataset (via ControlAreaModelInterpreter) control_areas_mesqual = ds_base.fetch('control_areas')  print(\"Control Areas from MESQUAL Dataset:\") print(control_areas_mesqual.head()) print(f\"\\nSame data? {control_areas_mesqual.equals(control_areas_gdf)}\") <pre>Control Areas from MESQUAL Dataset:\n                                                       geometry\ncontrol_area                                                   \n50Hertz       MULTIPOLYGON (((11.2639 50.48074, 11.2474 50.4...\nTenneTDE      MULTIPOLYGON (((8.45592 50.41449, 8.38936 50.4...\nTransnetBW    MULTIPOLYGON (((8.62585 47.64422, 8.59569 47.6...\nAmprion       MULTIPOLYGON (((7.09992 49.15556, 7.0803 49.14...\n\nSame data? True\n</pre> In\u00a0[8]: Copied! <pre>from mesqual.visualizations.value_mapping_system import DiscreteColorMapping\n\n# Create a more sophisticated visualization with color mapping\nm2 = folium.Map(\n    location=[51, 11],\n    zoom_start=6,\n    tiles=None\n)\nm2 = set_background_color_of_map(m2, color='#ffffff')\n\n# Define explicit colors for control areas\ncontrol_area_colors = {\n    'Amprion': '#FF6B6B',      # Red\n    'TenneTDE': '#4ECDC4',     # Teal  \n    'TransnetBW': '#96CEB4',   # Blue\n    # '50Hertz': '#45B7D1'     # outcommented to showcase the \"auto_assign\" feature of the DiscreteColorMapping class\n}\n\n#\ncontrol_areas_mesqual['fill_opacity'] = [1.0, 0.5, 1.0, 1.0]\n\ncolor_mapping = DiscreteColorMapping(control_area_colors, mode=\"auto_assign\")\n\n# Create area generator with dynamic coloring\ncolored_area_generator = folviz.AreaGenerator(\n    folviz.AreaFeatureResolver(\n        fill_color=folviz.PropertyMapper.from_item_attr('control_area', color_mapping),\n        fill_opacity=0.7,\n        border_color='#2c3e50',\n        border_width=2,\n        highlight_border_width=3,\n        tooltip=True\n    )\n)\n\n# Add colored areas\ncolored_feature_group = folium.FeatureGroup(name='Control Areas (Colored)')\ncolored_area_generator.generate_objects_for_model_df(control_areas_mesqual, colored_feature_group)\ncolored_feature_group.add_to(m2)\n\nfolium.LayerControl().add_to(m2)\nrenderer.show_folium(m2)\n</pre> from mesqual.visualizations.value_mapping_system import DiscreteColorMapping  # Create a more sophisticated visualization with color mapping m2 = folium.Map(     location=[51, 11],     zoom_start=6,     tiles=None ) m2 = set_background_color_of_map(m2, color='#ffffff')  # Define explicit colors for control areas control_area_colors = {     'Amprion': '#FF6B6B',      # Red     'TenneTDE': '#4ECDC4',     # Teal       'TransnetBW': '#96CEB4',   # Blue     # '50Hertz': '#45B7D1'     # outcommented to showcase the \"auto_assign\" feature of the DiscreteColorMapping class }  # control_areas_mesqual['fill_opacity'] = [1.0, 0.5, 1.0, 1.0]  color_mapping = DiscreteColorMapping(control_area_colors, mode=\"auto_assign\")  # Create area generator with dynamic coloring colored_area_generator = folviz.AreaGenerator(     folviz.AreaFeatureResolver(         fill_color=folviz.PropertyMapper.from_item_attr('control_area', color_mapping),         fill_opacity=0.7,         border_color='#2c3e50',         border_width=2,         highlight_border_width=3,         tooltip=True     ) )  # Add colored areas colored_feature_group = folium.FeatureGroup(name='Control Areas (Colored)') colored_area_generator.generate_objects_for_model_df(control_areas_mesqual, colored_feature_group) colored_feature_group.add_to(m2)  folium.LayerControl().add_to(m2) renderer.show_folium(m2) In\u00a0[9]: Copied! <pre># Fetch buses using MESQUAL (includes control area assignment via ScigridDEBusModelInterpreter)\nbuses_gdf = ds_base.fetch('buses')\n\nprint(\"Bus data structure:\")\nprint(buses_gdf.head())\nprint(f\"\\nColumns: {list(buses_gdf.columns)}\")\nprint(f\"Geometry type: {buses_gdf.geometry.geom_type.unique()}\")\nprint(f\"Total buses: {len(buses_gdf)}\")\nprint(f\"Buses per control area:\\n{buses_gdf['control_area'].value_counts()}\")\n</pre> # Fetch buses using MESQUAL (includes control area assignment via ScigridDEBusModelInterpreter) buses_gdf = ds_base.fetch('buses')  print(\"Bus data structure:\") print(buses_gdf.head()) print(f\"\\nColumns: {list(buses_gdf.columns)}\") print(f\"Geometry type: {buses_gdf.geometry.geom_type.unique()}\") print(f\"Total buses: {len(buses_gdf)}\") print(f\"Buses per control area:\\n{buses_gdf['control_area'].value_counts()}\") <pre>Bus data structure:\n     v_nom type      x  ...  frequency         typ control_area\nBus                     ...                                    \n1    220.0        9.52  ...         50  substation     TenneTDE\n2    380.0        9.11  ...         50  substation     TenneTDE\n3    380.0        9.39  ...         50  substation     TenneTDE\n4    380.0        9.13  ...         50  substation     TenneTDE\n5    380.0       10.37  ...         50  substation     TenneTDE\n\n[5 rows x 21 columns]\n\nColumns: ['v_nom', 'type', 'x', 'y', 'carrier', 'unit', 'location', 'v_mag_pu_set', 'v_mag_pu_min', 'v_mag_pu_max', 'control', 'generator', 'sub_network', 'osm_name', 'operator', 'voltage', 'wkt_srid_4326', 'ref', 'frequency', 'typ', 'control_area']\nGeometry type: ['Point']\nTotal buses: 585\nBuses per control area:\ncontrol_area\nAmprion       241\nTenneTDE      176\n50Hertz        87\nTransnetBW     81\nName: count, dtype: int64\n</pre> In\u00a0[10]: Copied! <pre># Create map with buses as points\nm3 = folium.Map(\n    location=[51, 11],\n    zoom_start=6,\n    tiles=None\n)\nm3 = set_background_color_of_map(m3, color='#f8f9fa')\n\n# Add control areas as background\nbg_areas = folium.FeatureGroup(name='Control Areas (Background)')\nbasic_area_generator = folviz.AreaGenerator(\n    folviz.AreaFeatureResolver(\n        fill_color=folviz.PropertyMapper.from_static_value('#e8e8e8'),\n        fill_opacity=0.3,\n        border_color=folviz.PropertyMapper.from_static_value('#666666'),\n        border_width=1\n    )\n)\nbasic_area_generator.generate_objects_for_model_df(control_areas_mesqual, bg_areas)\nbg_areas.add_to(m3)\n\n# Function to get bus color based on voltage level\ndef get_bus_color_by_voltage(data_item):\n    \"\"\"Color buses by voltage level\"\"\"\n    v_nom = data_item.get_object_attribute('v_nom')\n    if v_nom &gt;= 380:\n        return '#d32f2f'  # Red for extra high voltage\n    elif v_nom &gt;= 220:\n        return '#f57c00'  # Orange for high voltage  \n    elif v_nom &gt;= 110:\n        return '#fbc02d'  # Yellow for medium voltage\n    else:\n        return '#388e3c'  # Green for low voltage\n\n# Function to get bus size based on voltage level\ndef get_bus_size_by_voltage(data_item):\n    \"\"\"Size buses by voltage level\"\"\"\n    v_nom = data_item.get_object_attribute('v_nom')\n    if v_nom &gt;= 380:\n        return 8\n    elif v_nom &gt;= 220:\n        return 6\n    elif v_nom &gt;= 110:\n        return 4\n    else:\n        return 2\n\n# Create circle marker generator for buses\nbus_generator = folviz.CircleMarkerGenerator(\n    folviz.CircleMarkerFeatureResolver(\n        radius=folviz.PropertyMapper(get_bus_size_by_voltage),\n        fill_color=folviz.PropertyMapper(get_bus_color_by_voltage),\n        color=folviz.PropertyMapper.from_static_value('#2c3e50'),\n        weight=1,\n        fill_opacity=0.8,\n        tooltip=True\n    )\n)\n\n# Add buses to map\nbus_feature_group = folium.FeatureGroup(name='Buses (by Voltage Level)')\nbus_generator.generate_objects_for_model_df(buses_gdf, bus_feature_group)\nbus_feature_group.add_to(m3)\n\n# Add a custom legend\nlegend_html = '''\n&lt;div style=\"position: fixed; \n            bottom: 50px; right: 50px; width: 200px; height: 160px;\n            background-color: white; border:2px solid grey; z-index:9999; \n            font-size:14px; padding: 10px\n            \"&gt;\n&lt;h4&gt;Bus Voltage Levels&lt;/h4&gt;\n&lt;p&gt;&lt;i class=\"fa fa-circle\" style=\"color:#d32f2f\"&gt;&lt;/i&gt; \u2265380 kV (Extra High)&lt;/p&gt;\n&lt;p&gt;&lt;i class=\"fa fa-circle\" style=\"color:#f57c00\"&gt;&lt;/i&gt; \u2265220 kV (High)&lt;/p&gt;\n&lt;p&gt;&lt;i class=\"fa fa-circle\" style=\"color:#fbc02d\"&gt;&lt;/i&gt; \u2265110 kV (Medium)&lt;/p&gt;\n&lt;p&gt;&lt;i class=\"fa fa-circle\" style=\"color:#388e3c\"&gt;&lt;/i&gt; &lt;110 kV (Low)&lt;/p&gt;\n&lt;/div&gt;\n'''\nm3.get_root().html.add_child(folium.Element(legend_html))\n\nfolium.LayerControl().add_to(m3)\nrenderer.show_folium(m3)\n</pre> # Create map with buses as points m3 = folium.Map(     location=[51, 11],     zoom_start=6,     tiles=None ) m3 = set_background_color_of_map(m3, color='#f8f9fa')  # Add control areas as background bg_areas = folium.FeatureGroup(name='Control Areas (Background)') basic_area_generator = folviz.AreaGenerator(     folviz.AreaFeatureResolver(         fill_color=folviz.PropertyMapper.from_static_value('#e8e8e8'),         fill_opacity=0.3,         border_color=folviz.PropertyMapper.from_static_value('#666666'),         border_width=1     ) ) basic_area_generator.generate_objects_for_model_df(control_areas_mesqual, bg_areas) bg_areas.add_to(m3)  # Function to get bus color based on voltage level def get_bus_color_by_voltage(data_item):     \"\"\"Color buses by voltage level\"\"\"     v_nom = data_item.get_object_attribute('v_nom')     if v_nom &gt;= 380:         return '#d32f2f'  # Red for extra high voltage     elif v_nom &gt;= 220:         return '#f57c00'  # Orange for high voltage       elif v_nom &gt;= 110:         return '#fbc02d'  # Yellow for medium voltage     else:         return '#388e3c'  # Green for low voltage  # Function to get bus size based on voltage level def get_bus_size_by_voltage(data_item):     \"\"\"Size buses by voltage level\"\"\"     v_nom = data_item.get_object_attribute('v_nom')     if v_nom &gt;= 380:         return 8     elif v_nom &gt;= 220:         return 6     elif v_nom &gt;= 110:         return 4     else:         return 2  # Create circle marker generator for buses bus_generator = folviz.CircleMarkerGenerator(     folviz.CircleMarkerFeatureResolver(         radius=folviz.PropertyMapper(get_bus_size_by_voltage),         fill_color=folviz.PropertyMapper(get_bus_color_by_voltage),         color=folviz.PropertyMapper.from_static_value('#2c3e50'),         weight=1,         fill_opacity=0.8,         tooltip=True     ) )  # Add buses to map bus_feature_group = folium.FeatureGroup(name='Buses (by Voltage Level)') bus_generator.generate_objects_for_model_df(buses_gdf, bus_feature_group) bus_feature_group.add_to(m3)  # Add a custom legend legend_html = '''  Bus Voltage Levels <p> \u2265380 kV (Extra High)</p> <p> \u2265220 kV (High)</p> <p> \u2265110 kV (Medium)</p> <p> &lt;110 kV (Low)</p>  ''' m3.get_root().html.add_child(folium.Element(legend_html))  folium.LayerControl().add_to(m3) renderer.show_folium(m3) In\u00a0[11]: Copied! <pre># Fetch transmission lines\nlines_gdf = ds_base.fetch('lines')\n\nprint(\"Transmission lines data:\")\nprint(lines_gdf.head())\n</pre> # Fetch transmission lines lines_gdf = ds_base.fetch('lines')  print(\"Transmission lines data:\") print(lines_gdf.head()) <pre>Transmission lines data:\n         bus0     bus1                         type  ...  bus_generator_combo_opposite  bus_v_nom_combo_opposite                                           geometry\nLine                                                 ...                                                                                                           \n1           1  2_220kV  Al/St 240/40 2-bundle 220.0  ...                       - 1 Gas             220.0 - 220.0  LINESTRING (9.52257596986 52.3604090558, 9.113...\n2           3        4  Al/St 240/40 4-bundle 380.0  ...                            -              380.0 - 380.0  LINESTRING (9.38974509625 52.026313066, 9.1252...\n3     5_220kV        6  Al/St 240/40 2-bundle 220.0  ...                            -              220.0 - 220.0  LINESTRING (10.3662749375 52.2846467462, 9.918...\n4           7        5  Al/St 240/40 4-bundle 380.0  ...                            -              380.0 - 380.0  LINESTRING (9.91717971972 52.2781686139, 10.36...\n5           8        9  Al/St 240/40 4-bundle 380.0  ...                            -              380.0 - 380.0  LINESTRING (10.4149923382 53.412606883, 10.378...\n\n[5 rows x 155 columns]\n</pre> In\u00a0[12]: Copied! <pre># Create comprehensive network visualization\nm4 = folium.Map(\n    location=[51, 11],\n    zoom_start=6,\n    tiles=None\n)\nm4 = set_background_color_of_map(m4, color='#ffffff')\n\n# Add control areas as background (lighter)\nbg_areas = folium.FeatureGroup(name='Control Areas')\narea_bg_generator = folviz.AreaGenerator(\n    folviz.AreaFeatureResolver(\n        fill_color=folviz.PropertyMapper.from_static_value('#f5f5f5'),\n        fill_opacity=0.4,\n        border_color=folviz.PropertyMapper.from_static_value('#bdbdbd'),\n        border_width=1.5\n    )\n)\narea_bg_generator.generate_objects_for_model_df(control_areas_mesqual, bg_areas)\nbg_areas.add_to(m4)\n\n# Function to style lines by voltage\ndef get_line_color_by_voltage(data_item):\n    \"\"\"Color lines by voltage level\"\"\"\n    v_nom = data_item.get_object_attribute('v_nom')\n    if v_nom &gt;= 380:\n        return '#c62828'  # Dark red for 380+ kV\n    elif v_nom &gt;= 220:\n        return '#ef6c00'  # Orange for 220+ kV\n    else:\n        return '#2e7d32'  # Green for &lt;220 kV\n\ndef get_line_width_by_voltage(data_item):\n    \"\"\"Width lines by voltage level\"\"\"\n    v_nom = data_item.get_object_attribute('v_nom')\n    if v_nom &gt;= 380:\n        return 4\n    elif v_nom &gt;= 220:\n        return 3\n    else:\n        return 2\n\n# Create line generator\nline_generator = folviz.LineGenerator(\n    folviz.LineFeatureResolver(\n        color=folviz.PropertyMapper(get_line_color_by_voltage),\n        weight=folviz.PropertyMapper(get_line_width_by_voltage),\n        opacity=0.8,\n        tooltip=True\n    )\n)\n\n# Add transmission lines\nlines_feature_group = folium.FeatureGroup(name='Transmission Lines')\nline_generator.generate_objects_for_model_df(lines_gdf, lines_feature_group)\nlines_feature_group.add_to(m4)\n\n# Add key buses (high voltage only)\nhigh_voltage_buses = buses_gdf[buses_gdf['v_nom'] &gt;= 220]\nkey_bus_generator = folviz.CircleMarkerGenerator(\n    folviz.CircleMarkerFeatureResolver(\n        radius=folviz.PropertyMapper(lambda item: 5 if item.get_object_attribute('v_nom') &gt;= 380 else 3),\n        fill_color='#1a237e',\n        color='#ffffff',\n        weight=2,\n        fill_opacity=0.9,\n        tooltip=True\n    )\n)\n\nkey_buses_group = folium.FeatureGroup(name='Key Buses (\u2265220kV)')\nkey_bus_generator.generate_objects_for_model_df(high_voltage_buses, key_buses_group)\nkey_buses_group.add_to(m4)\n\n# Add network legend\nnetwork_legend_html = '''\n&lt;div style=\"position: fixed; \n            bottom: 50px; right: 50px; width: 220px; height: 180px;\n            background-color: white; border:2px solid grey; z-index:9999; \n            font-size:14px; padding: 10px\n            \"&gt;\n&lt;h4&gt;Network Elements&lt;/h4&gt;\n&lt;p&gt;&lt;span style=\"color:#c62828; font-weight:bold;\"&gt;\u2501\u2501\u2501&lt;/span&gt; \u2265380 kV Lines&lt;/p&gt;\n&lt;p&gt;&lt;span style=\"color:#ef6c00; font-weight:bold;\"&gt;\u2501\u2501\u2501&lt;/span&gt; \u2265220 kV Lines&lt;/p&gt;\n&lt;p&gt;&lt;span style=\"color:#2e7d32; font-weight:bold;\"&gt;\u2501\u2501\u2501&lt;/span&gt; &lt;220 kV Lines&lt;/p&gt;\n&lt;p&gt;&lt;i class=\"fa fa-circle\" style=\"color:#1a237e\"&gt;&lt;/i&gt; Key Buses (\u2265220kV)&lt;/p&gt;\n&lt;p style=\"color:#bdbdbd;\"&gt;\u25a2 Control Areas&lt;/p&gt;\n&lt;/div&gt;\n'''\nm4.get_root().html.add_child(folium.Element(network_legend_html))\n\nfolium.LayerControl().add_to(m4)\nrenderer.show_folium(m4)\n</pre> # Create comprehensive network visualization m4 = folium.Map(     location=[51, 11],     zoom_start=6,     tiles=None ) m4 = set_background_color_of_map(m4, color='#ffffff')  # Add control areas as background (lighter) bg_areas = folium.FeatureGroup(name='Control Areas') area_bg_generator = folviz.AreaGenerator(     folviz.AreaFeatureResolver(         fill_color=folviz.PropertyMapper.from_static_value('#f5f5f5'),         fill_opacity=0.4,         border_color=folviz.PropertyMapper.from_static_value('#bdbdbd'),         border_width=1.5     ) ) area_bg_generator.generate_objects_for_model_df(control_areas_mesqual, bg_areas) bg_areas.add_to(m4)  # Function to style lines by voltage def get_line_color_by_voltage(data_item):     \"\"\"Color lines by voltage level\"\"\"     v_nom = data_item.get_object_attribute('v_nom')     if v_nom &gt;= 380:         return '#c62828'  # Dark red for 380+ kV     elif v_nom &gt;= 220:         return '#ef6c00'  # Orange for 220+ kV     else:         return '#2e7d32'  # Green for &lt;220 kV  def get_line_width_by_voltage(data_item):     \"\"\"Width lines by voltage level\"\"\"     v_nom = data_item.get_object_attribute('v_nom')     if v_nom &gt;= 380:         return 4     elif v_nom &gt;= 220:         return 3     else:         return 2  # Create line generator line_generator = folviz.LineGenerator(     folviz.LineFeatureResolver(         color=folviz.PropertyMapper(get_line_color_by_voltage),         weight=folviz.PropertyMapper(get_line_width_by_voltage),         opacity=0.8,         tooltip=True     ) )  # Add transmission lines lines_feature_group = folium.FeatureGroup(name='Transmission Lines') line_generator.generate_objects_for_model_df(lines_gdf, lines_feature_group) lines_feature_group.add_to(m4)  # Add key buses (high voltage only) high_voltage_buses = buses_gdf[buses_gdf['v_nom'] &gt;= 220] key_bus_generator = folviz.CircleMarkerGenerator(     folviz.CircleMarkerFeatureResolver(         radius=folviz.PropertyMapper(lambda item: 5 if item.get_object_attribute('v_nom') &gt;= 380 else 3),         fill_color='#1a237e',         color='#ffffff',         weight=2,         fill_opacity=0.9,         tooltip=True     ) )  key_buses_group = folium.FeatureGroup(name='Key Buses (\u2265220kV)') key_bus_generator.generate_objects_for_model_df(high_voltage_buses, key_buses_group) key_buses_group.add_to(m4)  # Add network legend network_legend_html = '''  Network Elements <p>\u2501\u2501\u2501 \u2265380 kV Lines</p> <p>\u2501\u2501\u2501 \u2265220 kV Lines</p> <p>\u2501\u2501\u2501 &lt;220 kV Lines</p> <p> Key Buses (\u2265220kV)</p> <p>\u25a2 Control Areas</p>  ''' m4.get_root().html.add_child(folium.Element(network_legend_html))  folium.LayerControl().add_to(m4) renderer.show_folium(m4) In\u00a0[13]: Copied! <pre># Create final comprehensive map with text overlays\nm5 = folium.Map(\n    location=[51, 11],\n    zoom_start=6,\n    tiles=None\n)\nm5 = set_background_color_of_map(m5, color='#ffffff')\n\n# Add colored control areas\nareas_group = folium.FeatureGroup(name='Control Areas')\ncolored_area_generator.generate_objects_for_model_df(control_areas_mesqual, areas_group)\nareas_group.add_to(m5)\n\n# Add control area labels using folviz.TextOverlayGenerator\ntext_generator = folviz.TextOverlayGenerator(\n    folviz.TextOverlayFeatureResolver(\n        text=folviz.PropertyMapper.from_item_attr('control_area'),  # Use area name\n        font_size='16pt',\n        font_weight='bold',\n        color='#2c3e50',\n        background_color='#ffffff',\n        border_color='#bdc3c7',\n        border_width=1,\n        padding=5\n    )\n)\n\nlabels_group = folium.FeatureGroup(name='Area Labels')\ntext_generator.generate_objects_for_model_df(control_areas_mesqual, labels_group)\nlabels_group.add_to(m5)\n\n# Add transmission lines (extra high voltage only for cleaner view)\nehv_lines = lines_gdf[lines_gdf['v_nom'] &gt;= 380]\nehv_line_generator = folviz.LineGenerator(\n    folviz.LineFeatureResolver(\n        color='#c62828',\n        weight=3,\n        opacity=0.8,\n        tooltip=True\n    )\n)\n\nehv_lines_group = folium.FeatureGroup(name='Extra High Voltage Lines (\u2265380kV)')\nehv_line_generator.generate_objects_for_model_df(ehv_lines, ehv_lines_group)\nehv_lines_group.add_to(m5)\n\n# Add major bus nodes\nmajor_buses = buses_gdf[buses_gdf['v_nom'] &gt;= 380]\nmajor_bus_generator = folviz.CircleMarkerGenerator(\n    folviz.CircleMarkerFeatureResolver(\n        radius=6,\n        fill_color=folviz.PropertyMapper.from_static_value('#1a237e'),\n        color=folviz.PropertyMapper.from_static_value('#ffffff'),\n        weight=2,\n        fill_opacity=1.0,\n        tooltip=True\n    )\n)\n\nmajor_buses_group = folium.FeatureGroup(name='Major Bus Nodes (\u2265380kV)')\nmajor_bus_generator.generate_objects_for_model_df(major_buses, major_buses_group)\nmajor_buses_group.add_to(m5)\n\nfolium.LayerControl().add_to(m5)\nrenderer.show_folium(m5)\n</pre> # Create final comprehensive map with text overlays m5 = folium.Map(     location=[51, 11],     zoom_start=6,     tiles=None ) m5 = set_background_color_of_map(m5, color='#ffffff')  # Add colored control areas areas_group = folium.FeatureGroup(name='Control Areas') colored_area_generator.generate_objects_for_model_df(control_areas_mesqual, areas_group) areas_group.add_to(m5)  # Add control area labels using folviz.TextOverlayGenerator text_generator = folviz.TextOverlayGenerator(     folviz.TextOverlayFeatureResolver(         text=folviz.PropertyMapper.from_item_attr('control_area'),  # Use area name         font_size='16pt',         font_weight='bold',         color='#2c3e50',         background_color='#ffffff',         border_color='#bdc3c7',         border_width=1,         padding=5     ) )  labels_group = folium.FeatureGroup(name='Area Labels') text_generator.generate_objects_for_model_df(control_areas_mesqual, labels_group) labels_group.add_to(m5)  # Add transmission lines (extra high voltage only for cleaner view) ehv_lines = lines_gdf[lines_gdf['v_nom'] &gt;= 380] ehv_line_generator = folviz.LineGenerator(     folviz.LineFeatureResolver(         color='#c62828',         weight=3,         opacity=0.8,         tooltip=True     ) )  ehv_lines_group = folium.FeatureGroup(name='Extra High Voltage Lines (\u2265380kV)') ehv_line_generator.generate_objects_for_model_df(ehv_lines, ehv_lines_group) ehv_lines_group.add_to(m5)  # Add major bus nodes major_buses = buses_gdf[buses_gdf['v_nom'] &gt;= 380] major_bus_generator = folviz.CircleMarkerGenerator(     folviz.CircleMarkerFeatureResolver(         radius=6,         fill_color=folviz.PropertyMapper.from_static_value('#1a237e'),         color=folviz.PropertyMapper.from_static_value('#ffffff'),         weight=2,         fill_opacity=1.0,         tooltip=True     ) )  major_buses_group = folium.FeatureGroup(name='Major Bus Nodes (\u2265380kV)') major_bus_generator.generate_objects_for_model_df(major_buses, major_buses_group) major_buses_group.add_to(m5)  folium.LayerControl().add_to(m5) renderer.show_folium(m5)"},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/#mesqual-401-folium-model-data-visualization-areas-points-and-lines","title":"MESQUAL 401: Folium Model Data Visualization - Areas, Points, and Lines\u00b6","text":"<p>This notebook demonstrates how to visualize model DataFrames using MESQUAL's folium visualization system. We'll explore how to create interactive maps showing network topology, geographic data, and model properties across different geometric object types.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/#introduction","title":"Introduction\u00b6","text":"<p>MESQUAL's folium visualization system provides a powerful way to create interactive geospatial visualizations of energy system data. In this notebook, we'll cover:</p> <ol> <li>Area Visualization: Control areas and regions using folviz.AreaGenerator</li> <li>Point Visualization: Buses and generators using folviz.CircleMarkerGenerator</li> <li>Line Visualization: Transmission lines using folviz.LineGenerator</li> <li>Text Overlays: Labels and annotations using folviz.TextOverlayGenerator</li> <li>folviz.PropertyMapper System: Dynamic styling based on model properties</li> </ol> <p>We'll use both external geospatial data (GeoJSON files) and integrated MESQUAL datasets with geographic information.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mesqual and all requirements are installed.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/#load-study-data","title":"Load Study Data\u00b6","text":"<p>Let's load our PyPSA study data which includes networks with geographic information:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/#part-1-area-visualization-control-areas","title":"Part 1: Area Visualization - Control Areas\u00b6","text":"<p>Let's start by visualizing control areas using both direct GeoJSON loading and the integrated MESQUAL dataset approach.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/#approach-1-direct-geojson-loading","title":"Approach 1: Direct GeoJSON Loading\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/#approach-2-using-mesqual-dataset-integration","title":"Approach 2: Using MESQUAL Dataset Integration\u00b6","text":""},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/#part-2-point-visualization-buses-and-generators","title":"Part 2: Point Visualization - Buses and Generators\u00b6","text":"<p>Now let's visualize buses as points on the map, showing how the ScigridDEBusModelInterpreter enriches bus data with control area information.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/#part-3-line-visualization-transmission-lines","title":"Part 3: Line Visualization - Transmission Lines\u00b6","text":"<p>Let's visualize the transmission lines connecting the buses in our network.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/#part-4-text-overlays-and-labels","title":"Part 4: Text Overlays and Labels\u00b6","text":"<p>Finally, let's add text overlays to label our control areas and key network elements.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/#summary-folvizpropertymapper-system","title":"Summary: folviz.PropertyMapper System\u00b6","text":"<p>Throughout this notebook, we've demonstrated the power of MESQUAL's folviz.PropertyMapper system:</p> <ol> <li><p>Static Values:</p> <p>folviz.PropertyMapper.from_static_value('#e3f2fd')</p> <p>\u2192 Always returns the same value</p> <p>\u2192 In the FeatureResolver, you can also set the feature attribute=static_value and the FeatureResolver will automatically create a static mapping.</p> </li> <li><p>Object Attributes and Attribute Mappings:</p> <p>folviz.PropertyMapper.from_item_attr('name') \u2192 Extracts 'name' attribute from data objects</p> <p>folviz.PropertyMapper.from_item_attr('name', custom_name_mapping_function) \u2192 Extracts 'name' attribute and maps it through custom_name_mapping_function</p> </li> <li><p>Custom Functions:</p> <p>folviz.PropertyMapper(get_area_color) \u2192 Applies custom logic directly to the data-item to determine properties and map them according to your logic. Especially useful when you have conditional logic, like if object_value &gt; object_threshold then ... else ...</p> </li> <li><p>Default Mappers:</p> <p>\u2192 Geometries, text content, and other feature-properties will be intelligently generated based on default if no specifics are defined</p> </li> <li><p>Dynamic Styling Examples:</p> <ul> <li>Bus colors/sizes based on voltage levels</li> <li>Line colors/widths based on voltage levels</li> <li>Area colors based on control area names</li> <li>Text labels from object attributes</li> </ul> </li> </ol>"},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook, we've explored MESQUAL's folium visualization system for model DataFrames:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/#key-concepts-covered","title":"Key Concepts Covered:\u00b6","text":"<ol> <li><p>Multiple Data Sources: Both direct GeoJSON loading and integrated MESQUAL datasets</p> </li> <li><p>Object Type Visualization:</p> <ul> <li>Areas: Control regions with folviz.AreaGenerator</li> <li>Points: Buses and generators with folviz.CircleMarkerGenerator</li> <li>Lines: Transmission lines with folviz.LineGenerator</li> <li>Text: Labels and annotations with folviz.TextOverlayGenerator</li> </ul> </li> <li><p>folviz.PropertyMapper System: Dynamic styling based on data attributes</p> </li> <li><p>FeatureResolver Pattern: Configurable styling and behavior</p> </li> <li><p>Layer Management: Organized visualization with folium FeatureGroups</p> </li> </ol>"},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/#next-steps","title":"Next Steps:\u00b6","text":"<p>In the next notebook (mesqual_304), we'll explore how to visualize KPI data using the same system, showing how computed metrics can be mapped to geographic visualizations with automatic coloring, legends, and interactive features.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_303_folium_model_df_map/#key-takeaways","title":"Key Takeaways:\u00b6","text":"<ul> <li>MESQUAL's folium system provides a unified approach to geospatial visualization</li> <li>The folviz.PropertyMapper system enables highly flexible, data-driven styling</li> <li>Integration with MESQUAL datasets allows seamless visualization of model data</li> <li>Multiple object types can be combined to create comprehensive network visualizations</li> </ul>"},{"location":"mesqual-study-01/notebooks/mesqual_304_folium_area_kpi_map/","title":"MESQUAL 304: Folium KPI Visualization System","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git\n        %cd mesqual-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mesqual -U\n        !pip install git+https://github.com/helgeesch/mesqual-pypsa -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git         %cd mesqual-vanilla-studies/          !pip install git+https://github.com/helgeesch/mesqual -U         !pip install git+https://github.com/helgeesch/mesqual-pypsa -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>\ud83d\udda5\ufe0f Running locally. No setup needed.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mesqual-vanilla-studies')\n    os.chdir('/content/mesqual-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mesqual import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let's go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mesqual-vanilla-studies')     os.chdir('/content/mesqual-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mesqual import StudyManager except ImportError:     raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let's go!\") <pre>\u2705 Environment ready. Let's go!\n</pre> In\u00a0[3]: Copied! <pre>import folium\n\nfrom mesqual import kpis\nimport mesqual.visualizations.folium_viz_system as folviz\nimport mesqual.visualizations.value_mapping_system as valmap\nfrom mesqual.utils.folium_utils.background_color import set_background_color_of_map\nfrom mesqual.utils.plotly_utils.plotly_theme import colors\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nrenderer = ConditionalRenderer()\n</pre> import folium  from mesqual import kpis import mesqual.visualizations.folium_viz_system as folviz import mesqual.visualizations.value_mapping_system as valmap from mesqual.utils.folium_utils.background_color import set_background_color_of_map from mesqual.utils.plotly_utils.plotly_theme import colors from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Load the StudyManager with PyPSA Scigrid-DE network data\nfrom studies.study_01_intro_to_mesqual.scripts.setup_study_manager import get_scigrid_de_study_manager\nstudy = get_scigrid_de_study_manager()\n\nprint(\"Study scenarios:\")\nfor dataset in study.scen.datasets:\n    print(f\"  \ud83d\udcca {dataset.name}\")\n    \nprint(\"\\nComparisons:\")\nfor dataset in study.comp.datasets:\n    print(f\"  \ud83d\udd04 {dataset.name}\")\n</pre> # Load the StudyManager with PyPSA Scigrid-DE network data from studies.study_01_intro_to_mesqual.scripts.setup_study_manager import get_scigrid_de_study_manager study = get_scigrid_de_study_manager()  print(\"Study scenarios:\") for dataset in study.scen.datasets:     print(f\"  \ud83d\udcca {dataset.name}\")      print(\"\\nComparisons:\") for dataset in study.comp.datasets:     print(f\"  \ud83d\udd04 {dataset.name}\") <pre>Study scenarios:\n  \ud83d\udcca base\n  \ud83d\udcca solar_150\n  \ud83d\udcca solar_200\n  \ud83d\udcca wind_150\n  \ud83d\udcca wind_200\n\nComparisons:\n  \ud83d\udd04 solar_150 vs base\n  \ud83d\udd04 solar_200 vs base\n  \ud83d\udd04 wind_150 vs base\n  \ud83d\udd04 wind_200 vs base\n</pre> In\u00a0[5]: Copied! <pre># Clear any existing KPIs\nstudy.scen.clear_kpi_collection_for_all_child_datasets()\nstudy.comp.clear_kpi_collection_for_all_child_datasets()\n\n# Get base dataset to access control areas\nds_base = study.scen.get_dataset('base')\ngenerators = ds_base.fetch('generators').query(\"carrier.isin(['Gas', 'Hard Coal', 'Brown Coal'])\").index.to_list()\n\n# Define scenario KPIs using KPI system\n\nmarket_price_definitions = (\n    kpis.FlagAggKPIBuilder()\n    .for_flag('control_areas_t.vol_weighted_marginal_price')\n    .with_aggregation(kpis.Aggregations.Mean)\n    .build()\n)\n\n# Generation capacity utilization KPIs\ngeneration_definitions = (\n    kpis.FlagAggKPIBuilder()\n    .for_flag('generators_t.p')\n    .with_aggregation(kpis.Aggregations.Mean)\n    .build()  # .for_all_objects() is default\n)\nkpi_defs = market_price_definitions + generation_definitions\nstudy.scen.add_kpis_from_definitions_to_all_child_datasets(kpi_defs)\nprint(f\"Added KPIs to scenario datasets.\")\n\n# Define comparison KPIs (calculate changes between scenarios)\nprint(\"\\nGenerating comparison KPIs...\")\ncomparison_defs = (\n    kpis.ComparisonKPIBuilder(kpi_defs)\n    .with_comparisons([\n        kpis.ValueComparisons.Increase,  # Absolute change\n        kpis.ValueComparisons.Delta      # Also Delta for compatibility\n    ])\n    .build()\n)\nstudy.comp.add_kpis_from_definitions_to_all_child_datasets(comparison_defs)\nprint(f\"Added comparison KPIs to comparison datasets\")\n\n# Get merged collections\nscenario_kpi_collection = study.scen.get_merged_kpi_collection()\ncomparison_kpi_collection = study.comp.get_merged_kpi_collection()\n\nprint(f\"\\nTotal scenario KPIs: {scenario_kpi_collection.size}\")\nprint(f\"Total comparison KPIs: {comparison_kpi_collection.size}\")\n\nprint(\"\\n\u2705 All KPIs generated successfully!\")\n</pre> # Clear any existing KPIs study.scen.clear_kpi_collection_for_all_child_datasets() study.comp.clear_kpi_collection_for_all_child_datasets()  # Get base dataset to access control areas ds_base = study.scen.get_dataset('base') generators = ds_base.fetch('generators').query(\"carrier.isin(['Gas', 'Hard Coal', 'Brown Coal'])\").index.to_list()  # Define scenario KPIs using KPI system  market_price_definitions = (     kpis.FlagAggKPIBuilder()     .for_flag('control_areas_t.vol_weighted_marginal_price')     .with_aggregation(kpis.Aggregations.Mean)     .build() )  # Generation capacity utilization KPIs generation_definitions = (     kpis.FlagAggKPIBuilder()     .for_flag('generators_t.p')     .with_aggregation(kpis.Aggregations.Mean)     .build()  # .for_all_objects() is default ) kpi_defs = market_price_definitions + generation_definitions study.scen.add_kpis_from_definitions_to_all_child_datasets(kpi_defs) print(f\"Added KPIs to scenario datasets.\")  # Define comparison KPIs (calculate changes between scenarios) print(\"\\nGenerating comparison KPIs...\") comparison_defs = (     kpis.ComparisonKPIBuilder(kpi_defs)     .with_comparisons([         kpis.ValueComparisons.Increase,  # Absolute change         kpis.ValueComparisons.Delta      # Also Delta for compatibility     ])     .build() ) study.comp.add_kpis_from_definitions_to_all_child_datasets(comparison_defs) print(f\"Added comparison KPIs to comparison datasets\")  # Get merged collections scenario_kpi_collection = study.scen.get_merged_kpi_collection() comparison_kpi_collection = study.comp.get_merged_kpi_collection()  print(f\"\\nTotal scenario KPIs: {scenario_kpi_collection.size}\") print(f\"Total comparison KPIs: {comparison_kpi_collection.size}\")  print(\"\\n\u2705 All KPIs generated successfully!\") <pre>Added KPIs to scenario datasets.\n\nGenerating comparison KPIs...\nAdded comparison KPIs to comparison datasets\n\nTotal scenario KPIs: 7135\nTotal comparison KPIs: 22832\n\n\u2705 All KPIs generated successfully!\n</pre> In\u00a0[6]: Copied! <pre># Create colorscale for market prices\nprice_colorscale = valmap.SegmentedContinuousColorscale(\n    segments={\n        (-25, 0): colors.sequential.shades_of_blue[::-1],\n        (0, 25): colors.sequential.shades_of_pink,\n    },\n    nan_fallback='#CCCCCC'\n)\n\n# Create map\nm1 = folium.Map(\n    location=[51, 11],\n    zoom_start=6,\n    tiles=None\n)\nm1 = set_background_color_of_map(m1, color='#ffffff')\n\n# Add legend\nfolviz.legends.ContinuousColorscaleLegend(\n    mapping=price_colorscale,\n    title=\"Market Price (\u20ac/MWh)\",\n    width=300,\n    position={'bottom': 50, 'right': 50}\n).add_to(m1)\n\n# Create area generator with KPI-based coloring\nprice_area_generator = folviz.AreaGenerator(\n    folviz.AreaFeatureResolver(\n        fill_color=folviz.PropertyMapper.from_kpi_value(price_colorscale),\n        fill_opacity=0.8,\n        border_color='#2c3e50',\n        border_width=2,\n        tooltip=True  # Auto-generates tooltip with KPI info\n    )\n)\n\n# Filter KPIs to show only market prices using new API\nprice_kpis = scenario_kpi_collection.filter_by_kpi_attributes(\n    filter_funcs=dict(flag=lambda f: 'price' in f)\n)\n\nprint(f\"Found {price_kpis.size} price KPIs\")\n\n# Use KPICollectionMapVisualizer for automatic visualization\nfolviz.KPICollectionMapVisualizer(\n    generators=[price_area_generator, folviz.TextOverlayGenerator()]\n).generate_and_add_feature_groups_to_map(price_kpis, m1, show='last')\n\nfolium.LayerControl(collapsed=False).add_to(m1)\nrenderer.show_folium(m1)\n</pre> # Create colorscale for market prices price_colorscale = valmap.SegmentedContinuousColorscale(     segments={         (-25, 0): colors.sequential.shades_of_blue[::-1],         (0, 25): colors.sequential.shades_of_pink,     },     nan_fallback='#CCCCCC' )  # Create map m1 = folium.Map(     location=[51, 11],     zoom_start=6,     tiles=None ) m1 = set_background_color_of_map(m1, color='#ffffff')  # Add legend folviz.legends.ContinuousColorscaleLegend(     mapping=price_colorscale,     title=\"Market Price (\u20ac/MWh)\",     width=300,     position={'bottom': 50, 'right': 50} ).add_to(m1)  # Create area generator with KPI-based coloring price_area_generator = folviz.AreaGenerator(     folviz.AreaFeatureResolver(         fill_color=folviz.PropertyMapper.from_kpi_value(price_colorscale),         fill_opacity=0.8,         border_color='#2c3e50',         border_width=2,         tooltip=True  # Auto-generates tooltip with KPI info     ) )  # Filter KPIs to show only market prices using new API price_kpis = scenario_kpi_collection.filter_by_kpi_attributes(     filter_funcs=dict(flag=lambda f: 'price' in f) )  print(f\"Found {price_kpis.size} price KPIs\")  # Use KPICollectionMapVisualizer for automatic visualization folviz.KPICollectionMapVisualizer(     generators=[price_area_generator, folviz.TextOverlayGenerator()] ).generate_and_add_feature_groups_to_map(price_kpis, m1, show='last')  folium.LayerControl(collapsed=False).add_to(m1) renderer.show_folium(m1) <pre>Found 20 price KPIs\n</pre> <pre>KPICollectionMapVisualizer:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 16/20 [00:00&lt;00:00, 503.21it/s]\n</pre> In\u00a0[7]: Copied! <pre># Create diverging colorscale for price changes\nprice_change_colormap = valmap.SegmentedContinuousColorscale(\n    segments={\n        (-10, 0): colors.sequential.shades_of_blue[::-1],\n        (0, 10): colors.sequential.shades_of_pink,\n    },\n    nan_fallback='#CCCCCC'\n)\n\n# Create comparison visualization map\nm2 = folium.Map(\n    location=[51, 11],\n    zoom_start=6,\n    tiles=None\n)\nm2 = set_background_color_of_map(m2, color='#ffffff')\n\n# Add legend for price changes\nfolviz.legends.ContinuousColorscaleLegend(\n    mapping=price_change_colormap,\n    title=\"Price Change (\u20ac/MWh)\",\n    width=300,\n    position={'bottom': 50, 'right': 50}\n).add_to(m2)\n\n# Create area generator for price changes\nprice_change_generator = folviz.AreaGenerator(\n    folviz.AreaFeatureResolver(\n        fill_color=folviz.PropertyMapper.from_kpi_value(price_change_colormap),\n        fill_opacity=0.8,\n        border_color='#34495e',\n        border_width=2,\n        tooltip=True\n    )\n)\n\n# Filter comparison KPIs for price changes using new API\nprice_change_kpis = comparison_kpi_collection.filter_by_kpi_attributes(\n    filter_funcs=dict(flag=lambda f: 'price' in f)\n)\n\nprint(f\"Found {price_change_kpis.size} price change KPIs\")\n\n# Visualize price changes\nfolviz.KPICollectionMapVisualizer(\n    generators=[price_change_generator, folviz.TextOverlayGenerator()]\n).generate_and_add_feature_groups_to_map(price_change_kpis, m2, show='last')\n\nfolium.LayerControl(collapsed=False).add_to(m2)\nrenderer.show_folium(m2)\n</pre> # Create diverging colorscale for price changes price_change_colormap = valmap.SegmentedContinuousColorscale(     segments={         (-10, 0): colors.sequential.shades_of_blue[::-1],         (0, 10): colors.sequential.shades_of_pink,     },     nan_fallback='#CCCCCC' )  # Create comparison visualization map m2 = folium.Map(     location=[51, 11],     zoom_start=6,     tiles=None ) m2 = set_background_color_of_map(m2, color='#ffffff')  # Add legend for price changes folviz.legends.ContinuousColorscaleLegend(     mapping=price_change_colormap,     title=\"Price Change (\u20ac/MWh)\",     width=300,     position={'bottom': 50, 'right': 50} ).add_to(m2)  # Create area generator for price changes price_change_generator = folviz.AreaGenerator(     folviz.AreaFeatureResolver(         fill_color=folviz.PropertyMapper.from_kpi_value(price_change_colormap),         fill_opacity=0.8,         border_color='#34495e',         border_width=2,         tooltip=True     ) )  # Filter comparison KPIs for price changes using new API price_change_kpis = comparison_kpi_collection.filter_by_kpi_attributes(     filter_funcs=dict(flag=lambda f: 'price' in f) )  print(f\"Found {price_change_kpis.size} price change KPIs\")  # Visualize price changes folviz.KPICollectionMapVisualizer(     generators=[price_change_generator, folviz.TextOverlayGenerator()] ).generate_and_add_feature_groups_to_map(price_change_kpis, m2, show='last')  folium.LayerControl(collapsed=False).add_to(m2) renderer.show_folium(m2) <pre>Found 64 price change KPIs\n</pre> <pre>KPICollectionMapVisualizer:  50%|\u2588\u2588\u2588\u2588\u2588     | 32/64 [00:01&lt;00:01, 16.44it/s]\n</pre> In\u00a0[8]: Copied! <pre># Create comprehensive map with multiple KPI layers\nm3 = folium.Map(\n    location=[51, 11],\n    zoom_start=6,\n    tiles=None\n)\nm3 = set_background_color_of_map(m3, color='#ffffff')\n\n# Define different colorscales for different KPI types\ngeneration_colorscale = valmap.SegmentedContinuousColorscale.single_segment_autoscale_factory_from_array(\n    values=[0, 1000],\n    color_range='viridis',\n)\n\ncolorscales = {\n    'price': {'pos': {'bottom': 50, 'left': 50}, 'scale': price_colorscale, 'title': \"Average Market Price (\u20ac/MWh)\"},\n    'change': {'pos': {'bottom': 200, 'left': 50}, 'scale': price_change_colormap, 'title': \"Price Change (\u20ac/MWh)\"},\n    'generation': {'pos': {'bottom': 350, 'left': 50}, 'scale': generation_colorscale, 'title': \"Average Generation (MW)\"},\n}\n\nfor k, v in colorscales.items():\n    folviz.legends.ContinuousColorscaleLegend(\n        mapping=v['scale'],\n        title=v['title'],\n        width=280,\n        position=v['pos']\n    ).add_to(m3)\n\nfrom shapely import Point\n\n# In this case, we don't have a \"location\" column ready in the generator model df, so we create the location on demand\ndef _get_location_of_generator(gen_data_item: folviz.VisualizableDataItem) -&gt; Point:\n    lat = gen_data_item.get_object_attribute('bus_y')\n    lon = gen_data_item.get_object_attribute('bus_x')\n    return Point([lon, lat])\n\n# Visualize market prices for scenarios using new filtering API\nprice_fgs = (\n    folviz.KPICollectionMapVisualizer(\n        generators=[\n            folviz.AreaGenerator(\n                folviz.AreaFeatureResolver(\n                    fill_color=folviz.PropertyMapper.from_kpi_value(colorscales['price']['scale']),\n                    fill_opacity=0.8,\n                    border_width=2,\n                    tooltip=True\n                )\n            ),\n            folviz.TextOverlayGenerator()\n        ]\n    )\n    .generate_and_add_feature_groups_to_map(\n        kpi_collection=scenario_kpi_collection.filter_by_kpi_attributes(filter_funcs=dict(flag=lambda f: 'price' in f)),\n        folium_map=m3,\n        show=\"none\"\n    )\n)\n\n# Visualize price changes for comparisons - filter by both flag and value_comparison\nprice_change_fgs = (\n    folviz.KPICollectionMapVisualizer(\n        generators=[\n            folviz.AreaGenerator(\n                folviz.AreaFeatureResolver(\n                    fill_color=folviz.PropertyMapper.from_kpi_value(colorscales['change']['scale']),\n                    fill_opacity=0.8,\n                    border_width=2,\n                    tooltip=True\n                )\n            ),\n            folviz.TextOverlayGenerator()\n        ]\n    )\n    .generate_and_add_feature_groups_to_map(\n        kpi_collection=(\n            comparison_kpi_collection\n            .filter_by_kpi_attributes(filter_funcs=dict(flag=lambda f: 'price' in f))\n            .filter(value_comparison=kpis.ValueComparisons.Increase)\n        ),\n        folium_map=m3,\n        show='first'\n    )\n)\n\n# Visualize generation for scenarios\ngeneration_fgs = (\n    folviz.KPICollectionMapVisualizer(\n        generators=[\n            folviz.CircleMarkerGenerator(\n                folviz.CircleMarkerFeatureResolver(\n                    fill_color=folviz.PropertyMapper.from_kpi_value(colorscales['generation']['scale']),\n                    radius=folviz.PropertyMapper.from_item_attr('p_nom', lambda p: max(min(20, p / 50), 3)),\n                    fill_opacity=0.8,\n                    border_width=1,\n                    tooltip=True,\n                    location=folviz.PropertyMapper(_get_location_of_generator)\n                )\n            ),\n            folviz.TextOverlayGenerator()\n        ]\n    )\n    .generate_and_add_feature_groups_to_map(\n        kpi_collection=scenario_kpi_collection.filter(flag='generators_t.p'),\n        folium_map=m3,\n        show='first'\n    )\n)\n\nfolium.LayerControl(collapsed=True).add_to(m3)\n\nfrom folium.plugins import GroupedLayerControl\nGroupedLayerControl(\n    groups={'Area Fgs': price_fgs + price_change_fgs, 'Generator Fgs': generation_fgs},\n    exclusive_groups=False,\n    collapsed=True,\n).add_to(m3)\n\n\nrenderer.show_folium(m3)\n</pre> # Create comprehensive map with multiple KPI layers m3 = folium.Map(     location=[51, 11],     zoom_start=6,     tiles=None ) m3 = set_background_color_of_map(m3, color='#ffffff')  # Define different colorscales for different KPI types generation_colorscale = valmap.SegmentedContinuousColorscale.single_segment_autoscale_factory_from_array(     values=[0, 1000],     color_range='viridis', )  colorscales = {     'price': {'pos': {'bottom': 50, 'left': 50}, 'scale': price_colorscale, 'title': \"Average Market Price (\u20ac/MWh)\"},     'change': {'pos': {'bottom': 200, 'left': 50}, 'scale': price_change_colormap, 'title': \"Price Change (\u20ac/MWh)\"},     'generation': {'pos': {'bottom': 350, 'left': 50}, 'scale': generation_colorscale, 'title': \"Average Generation (MW)\"}, }  for k, v in colorscales.items():     folviz.legends.ContinuousColorscaleLegend(         mapping=v['scale'],         title=v['title'],         width=280,         position=v['pos']     ).add_to(m3)  from shapely import Point  # In this case, we don't have a \"location\" column ready in the generator model df, so we create the location on demand def _get_location_of_generator(gen_data_item: folviz.VisualizableDataItem) -&gt; Point:     lat = gen_data_item.get_object_attribute('bus_y')     lon = gen_data_item.get_object_attribute('bus_x')     return Point([lon, lat])  # Visualize market prices for scenarios using new filtering API price_fgs = (     folviz.KPICollectionMapVisualizer(         generators=[             folviz.AreaGenerator(                 folviz.AreaFeatureResolver(                     fill_color=folviz.PropertyMapper.from_kpi_value(colorscales['price']['scale']),                     fill_opacity=0.8,                     border_width=2,                     tooltip=True                 )             ),             folviz.TextOverlayGenerator()         ]     )     .generate_and_add_feature_groups_to_map(         kpi_collection=scenario_kpi_collection.filter_by_kpi_attributes(filter_funcs=dict(flag=lambda f: 'price' in f)),         folium_map=m3,         show=\"none\"     ) )  # Visualize price changes for comparisons - filter by both flag and value_comparison price_change_fgs = (     folviz.KPICollectionMapVisualizer(         generators=[             folviz.AreaGenerator(                 folviz.AreaFeatureResolver(                     fill_color=folviz.PropertyMapper.from_kpi_value(colorscales['change']['scale']),                     fill_opacity=0.8,                     border_width=2,                     tooltip=True                 )             ),             folviz.TextOverlayGenerator()         ]     )     .generate_and_add_feature_groups_to_map(         kpi_collection=(             comparison_kpi_collection             .filter_by_kpi_attributes(filter_funcs=dict(flag=lambda f: 'price' in f))             .filter(value_comparison=kpis.ValueComparisons.Increase)         ),         folium_map=m3,         show='first'     ) )  # Visualize generation for scenarios generation_fgs = (     folviz.KPICollectionMapVisualizer(         generators=[             folviz.CircleMarkerGenerator(                 folviz.CircleMarkerFeatureResolver(                     fill_color=folviz.PropertyMapper.from_kpi_value(colorscales['generation']['scale']),                     radius=folviz.PropertyMapper.from_item_attr('p_nom', lambda p: max(min(20, p / 50), 3)),                     fill_opacity=0.8,                     border_width=1,                     tooltip=True,                     location=folviz.PropertyMapper(_get_location_of_generator)                 )             ),             folviz.TextOverlayGenerator()         ]     )     .generate_and_add_feature_groups_to_map(         kpi_collection=scenario_kpi_collection.filter(flag='generators_t.p'),         folium_map=m3,         show='first'     ) )  folium.LayerControl(collapsed=True).add_to(m3)  from folium.plugins import GroupedLayerControl GroupedLayerControl(     groups={'Area Fgs': price_fgs + price_change_fgs, 'Generator Fgs': generation_fgs},     exclusive_groups=False,     collapsed=True, ).add_to(m3)   renderer.show_folium(m3) <pre>KPICollectionMapVisualizer:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 16/20 [00:00&lt;00:00, 588.22it/s]\nKPICollectionMapVisualizer: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:00&lt;00:00, 207.99it/s]\nKPICollectionMapVisualizer:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 5692/7115 [00:05&lt;00:01, 964.73it/s] \n</pre>"},{"location":"mesqual-study-01/notebooks/mesqual_304_folium_area_kpi_map/#mesqual-304-folium-kpi-visualization-system","title":"MESQUAL 304: Folium KPI Visualization System\u00b6","text":"<p>This notebook demonstrates how to visualize KPI data using MESQUAL's folium system. We'll explore how computed metrics and indicators can be mapped to interactive geographic visualizations with automatic coloring, legends, and multi-scenario analysis.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_304_folium_area_kpi_map/#introduction","title":"Introduction\u00b6","text":"<p>Building on the model data visualization from mesqual_401, this notebook focuses on visualizing computed KPIs (Key Performance Indicators). We'll cover:</p> <ol> <li>KPI Data Items: How KPIs integrate with the folium visualization system</li> <li>KPI Collection Visualization: Automatic mapping of KPI collections to maps</li> <li>Dynamic Coloring: Using PropertyMapper.from_kpi_value() for data-driven styling</li> <li>Multi-Scenario KPI Maps: Comparing KPIs across different scenarios</li> <li>KPI Filtering and Grouping: Selective visualization of KPI subsets</li> <li>Interactive Features: Tooltips, popups, and layer controls for KPI exploration</li> </ol> <p>This approach enables sophisticated analysis of energy system performance metrics with geographic context.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_304_folium_area_kpi_map/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mesqual and all requirements are installed.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_304_folium_area_kpi_map/#load-study-data-and-setup-kpis","title":"Load Study Data and Setup KPIs\u00b6","text":"<p>Let's load our study data and define comprehensive KPIs for visualization:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_304_folium_area_kpi_map/#part-1-basic-kpi-visualization","title":"Part 1: Basic KPI Visualization\u00b6","text":"<p>Let's start with a simple visualization of market prices across control areas for all scenarios:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_304_folium_area_kpi_map/#part-2-comparison-visualization","title":"Part 2: Comparison Visualization\u00b6","text":"<p>Now let's visualize price changes between scenarios using comparison KPIs:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_304_folium_area_kpi_map/#part-3-multi-kpi-visualization","title":"Part 3: Multi-KPI Visualization\u00b6","text":"<p>Let's create a comprehensive map showing multiple KPI types with layer controls:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_304_folium_area_kpi_map/#summary-kpi-visualization-system-architecture","title":"Summary: KPI Visualization System Architecture\u00b6","text":"<p>Let's summarize the key components and patterns we've demonstrated:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_304_folium_area_kpi_map/#key-components-demonstrated","title":"Key Components Demonstrated:\u00b6","text":"<ul> <li>\ud83d\udcca KPIDataItem: Wraps KPI objects for visualization pipeline</li> <li>\ud83c\udfa8 PropertyMapper.from_kpi_value(): Maps KPI values to visual properties</li> <li>\ud83d\udd0d KPI Collection Filtering: By attributes, scenarios, and types</li> <li>\ud83d\uddfa\ufe0f KPICollectionMapVisualizer: Automatic map generation</li> <li>\ud83c\udf9b\ufe0f Multi-layer visualization: Different KPI types on same map</li> <li>\ud83d\udcc8 Custom styling: Based on KPI magnitude and significance</li> </ul>"},{"location":"mesqual-study-01/notebooks/mesqual_304_folium_area_kpi_map/#visualization-patterns","title":"Visualization Patterns:\u00b6","text":"<ul> <li>Scenario Comparison: Toggle and Delta visualization with diverging colors</li> <li>Multi-KPI Dashboards: Layer controls for different metrics</li> <li>Significance Encoding: Opacity/border based on value magnitude</li> <li>Interactive Features: Tooltips, popups, and layer controls</li> </ul>"},{"location":"mesqual-study-01/notebooks/mesqual_304_folium_area_kpi_map/#advanced-features","title":"Advanced Features:\u00b6","text":"<ul> <li>\u2728 Automatic tooltip generation from KPI attributes</li> <li>\ud83c\udfaf KPI filtering for flexible selection</li> <li>\ud83d\udccf Custom colorscales for different data ranges</li> <li>\ud83d\udd04 Dynamic styling based on KPI properties</li> <li>\ud83d\udcca Integration with MESQUAL's scenario comparison framework</li> </ul>"},{"location":"mesqual-study-01/notebooks/mesqual_307_country_plotter_util/","title":"MESQUAL 307: Country Plotter Utility Module","text":"In\u00a0[7]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git\n        %cd mesqual-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mesqual -U\n        !pip install git+https://github.com/helgeesch/mesqual-pypsa -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git         %cd mesqual-vanilla-studies/          !pip install git+https://github.com/helgeesch/mesqual -U         !pip install git+https://github.com/helgeesch/mesqual-pypsa -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>\ud83d\udda5\ufe0f Running locally. No setup needed.\n</pre> In\u00a0[8]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mesqual-vanilla-studies')\n    os.chdir('/content/mesqual-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mesqual import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mesqual-vanilla-studies')     os.chdir('/content/mesqual-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mesqual import StudyManager except ImportError:     raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\") <pre>\u2705 Environment ready. Let\u2019s go!\n</pre> In\u00a0[9]: Copied! <pre>import os\nimport folium\n\nfrom mesqual.utils.folium_utils import set_background_color_of_map, MapCountryPlotter\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nrenderer = ConditionalRenderer()\n</pre> import os import folium  from mesqual.utils.folium_utils import set_background_color_of_map, MapCountryPlotter from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() renderer = ConditionalRenderer() In\u00a0[10]: Copied! <pre># Create a basic map centered on Europe\nm = folium.Map(location=[46, 10], zoom_start=4.75, tiles=None)\n\n# Set a clean white background\nm = set_background_color_of_map(m, color='#ffffff')\n\n# Initialize the plotter (uses default geojson)\nplotter = MapCountryPlotter()\n\n# Create a feature group for our countries\ncountries_fg = folium.FeatureGroup(name=\"Selected Countries\")\n\n# Add some countries with default styling\nplotter.add_countries_to_feature_group(\n    countries_fg,\n    countries=[\"DE\", \"FR\", \"IT\", \"ES\"],\n)\ncountries_fg.add_to(m)\n\n# Let's add layer control\nfolium.LayerControl().add_to(m)\n\nrenderer.show_folium(m)\n</pre> # Create a basic map centered on Europe m = folium.Map(location=[46, 10], zoom_start=4.75, tiles=None)  # Set a clean white background m = set_background_color_of_map(m, color='#ffffff')  # Initialize the plotter (uses default geojson) plotter = MapCountryPlotter()  # Create a feature group for our countries countries_fg = folium.FeatureGroup(name=\"Selected Countries\")  # Add some countries with default styling plotter.add_countries_to_feature_group(     countries_fg,     countries=[\"DE\", \"FR\", \"IT\", \"ES\"], ) countries_fg.add_to(m)  # Let's add layer control folium.LayerControl().add_to(m)  renderer.show_folium(m) <p>The plotter looks up countries using standard ISO country codes (ISO-A2, ISO-A3, SOV-A3). This is particularly useful when working with data that uses these standard codes.</p> In\u00a0[11]: Copied! <pre># Create a map\ncustom_map = folium.Map(location=[60, 14], zoom_start=5, tiles=None)\n\n# Create feature group with custom styling\nnordic_fg = folium.FeatureGroup(name=\"Nordic Countries\")\n\n# Add Nordic countries with blue styling\nplotter.add_countries_to_feature_group(\n    nordic_fg,\n    countries=[\"NOR\", \"SE\", \"FI\", \"DK\"],\n    style={\n        \"fillColor\": \"#3388ff\",\n        \"color\": \"black\",\n        \"weight\": 1,\n        \"fillOpacity\": 0.4\n    }\n)\nnordic_fg.add_to(custom_map)\n\nfolium.LayerControl().add_to(custom_map)\n\nrenderer.show_folium(m)\n</pre> # Create a map custom_map = folium.Map(location=[60, 14], zoom_start=5, tiles=None)  # Create feature group with custom styling nordic_fg = folium.FeatureGroup(name=\"Nordic Countries\")  # Add Nordic countries with blue styling plotter.add_countries_to_feature_group(     nordic_fg,     countries=[\"NOR\", \"SE\", \"FI\", \"DK\"],     style={         \"fillColor\": \"#3388ff\",         \"color\": \"black\",         \"weight\": 1,         \"fillOpacity\": 0.4     } ) nordic_fg.add_to(custom_map)  folium.LayerControl().add_to(custom_map)  renderer.show_folium(m) <p>The style dictionary accepts the same parameters as Folium's GeoJSON styling. You can customize:</p> <ul> <li><code>fillColor</code>: The color to fill the country</li> <li><code>color</code>: The border color</li> <li><code>weight</code>: The border thickness</li> <li><code>fillOpacity</code>: The transparency of the fill color</li> </ul> <p>This flexibility allows you to create visually distinct regions or apply data-driven styling, which is perfect for market zonal analysis.Excluding Countries</p> In\u00a0[12]: Copied! <pre># Get GeoJSON for Germany\ngermany_gdf = plotter.get_geojson_for_country(\"DE\")\n\n# Create a focused map\ngermany_map = folium.Map(location=[51, 10], zoom_start=6, tiles=None)\n\n# Add Germany with custom styling\nfolium.GeoJson(\n    germany_gdf,\n    style_function=lambda x: {\n        \"fillColor\": \"#ff0000\",\n        \"color\": \"black\",\n        \"weight\": 2,\n        \"fillOpacity\": 0.6\n    }\n).add_to(germany_map)\n\nrenderer.show_folium(germany_map)\n</pre> # Get GeoJSON for Germany germany_gdf = plotter.get_geojson_for_country(\"DE\")  # Create a focused map germany_map = folium.Map(location=[51, 10], zoom_start=6, tiles=None)  # Add Germany with custom styling folium.GeoJson(     germany_gdf,     style_function=lambda x: {         \"fillColor\": \"#ff0000\",         \"color\": \"black\",         \"weight\": 2,         \"fillOpacity\": 0.6     } ).add_to(germany_map)  renderer.show_folium(germany_map) <p>This direct access to country GeoJSON is useful when you need to:</p> <ul> <li>Perform spatial analysis with country boundaries</li> <li>Combine country shapes with other geometries</li> <li>Add custom interactive elements to specific countries</li> <li>Calculate centroids or other geometric properties for positioning labels or markers</li> </ul> <p>Feel free to combine this module with the map created in any of the previous modules (<code>mesqual_303, mesqual_304, mesqual_305, mesqual_306</code>) to show neighboring countries in the same map.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_307_country_plotter_util/#mesqual-307-country-plotter-utility-module","title":"MESQUAL 307: Country Plotter Utility Module\u00b6","text":"<p>This notebook demonstrates a simple but useful module to project countries from the country library to a folium map.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_307_country_plotter_util/#module-use-case","title":"Module Use-Case\u00b6","text":"<p>There are two primary use-cases for the module:</p> <p>Using it as a library with geo data for countries. Using it to project countries that are not the focus of your study (e.g. neighboring countries) in a neutral color to your map.</p> <p>Whenever you are creating a KPI Map for your study (e.g. a map with European prices or anything alike), you often want to project the countries that are not part of your study itself (e.g. neighboring countries) to the map as well. You can, of course, also use Folium's native tile layers. However, those are often overloaded with unnecessary texts and geo information that distracts from your actual data. The module enables you to include other countries that are not the focus of your study, without distracting from the information you are trying to communicate.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_307_country_plotter_util/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mesqual and all requirements are installed.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_307_country_plotter_util/#basic-usage","title":"Basic Usage\u00b6","text":"<p>The MapCountryPlotter class provides a simple interface to add country geometries to folium maps. It handles the loading of GeoJSON data and provides methods to easily add countries to feature groups.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_307_country_plotter_util/#custom-styling","title":"Custom Styling\u00b6","text":"<p>One of the key benefits of using this plotter is the ability to customize the styling of countries. This is useful when you want to highlight specific regions or apply styling based on data values.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_307_country_plotter_util/#working-with-individual-countries","title":"Working with Individual Countries\u00b6","text":"<p>Sometimes you need more direct access to country geometries. The `get_geojson_for_country method allows you to retrieve the raw GeoJSON for specific processing.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_307_country_plotter_util/#conclusion","title":"Conclusion\u00b6","text":"<p>The MapCountryPlotter module is a simple but powerful tool for extending clean, informative geographical visualizations. By providing an easy way to add country shapes to your maps, it helps you to a) provide geo shapes for countries; b) focus on communicating your data rather than relying on the rather distractive default tile_layers in folium.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_308_html_dashboards/","title":"MESQUAL 305: HTML Dashboard Utility","text":"In\u00a0[1]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import importlib.util\n\n    def is_module_available(module_name):\n        return importlib.util.find_spec(module_name) is not None\n\n    if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):\n        print(\"\u2705 Environment already set up. Skipping installation.\")\n    else:\n        print(\"\ud83d\udd27 Setting up Colab environment...\")\n        !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git\n        %cd mesqual-vanilla-studies/\n\n        !pip install git+https://github.com/helgeesch/mesqual -U\n        !pip install git+https://github.com/helgeesch/mesqual-pypsa -U\n        !pip install -r requirements.txt\n\n        print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.')\nelse:\n    print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import importlib.util      def is_module_available(module_name):         return importlib.util.find_spec(module_name) is not None      if os.path.exists(\"mesqual-vanilla-studies\") and is_module_available(\"mesqual\"):         print(\"\u2705 Environment already set up. Skipping installation.\")     else:         print(\"\ud83d\udd27 Setting up Colab environment...\")         !git clone --recursive https://github.com/helgeesch/mesqual-vanilla-studies.git         %cd mesqual-vanilla-studies/          !pip install git+https://github.com/helgeesch/mesqual -U         !pip install git+https://github.com/helgeesch/mesqual-pypsa -U         !pip install -r requirements.txt          print('\u2705 Setup complete. \ud83d\udd01 Restart the session, then skip this cell and continue with the next one.') else:     print(\"\ud83d\udda5\ufe0f Running locally. No setup needed.\") <pre>\ud83d\udda5\ufe0f Running locally. No setup needed.\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nif \"COLAB_RELEASE_TAG\" in os.environ:\n    import sys\n    sys.path.append('/content/mesqual-vanilla-studies')\n    os.chdir('/content/mesqual-vanilla-studies')\nelse:\n    def setup_notebook_env():\n        \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"\n        import os\n        import sys\n        from pathlib import Path\n\n        def find_repo_root(start_path: Path) -&gt; Path:\n            current = start_path.resolve()\n            while current != current.parent:\n                if (current / 'vanilla').exists():\n                    return current\n                current = current.parent\n            raise FileNotFoundError(f\"Repository root not found from: {start_path}\")\n\n        repo_root = find_repo_root(Path.cwd())\n        os.chdir(repo_root)\n        if str(repo_root) not in sys.path:\n            sys.path.insert(0, str(repo_root))\n\n    setup_notebook_env()\n\ntry:\n    from mesqual import StudyManager\nexcept ImportError:\n    raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")\n\nif not os.path.isdir(\"studies\"):\n    raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")\n\nprint(\"\u2705 Environment ready. Let\u2019s go!\")\n</pre> import os  if \"COLAB_RELEASE_TAG\" in os.environ:     import sys     sys.path.append('/content/mesqual-vanilla-studies')     os.chdir('/content/mesqual-vanilla-studies') else:     def setup_notebook_env():         \"\"\"Set working directory to repo root and ensure it's in sys.path.\"\"\"         import os         import sys         from pathlib import Path          def find_repo_root(start_path: Path) -&gt; Path:             current = start_path.resolve()             while current != current.parent:                 if (current / 'vanilla').exists():                     return current                 current = current.parent             raise FileNotFoundError(f\"Repository root not found from: {start_path}\")          repo_root = find_repo_root(Path.cwd())         os.chdir(repo_root)         if str(repo_root) not in sys.path:             sys.path.insert(0, str(repo_root))      setup_notebook_env()  try:     from mesqual import StudyManager except ImportError:     raise ImportError(\"\u274c 'mesqual' not found. If you're running locally, make sure you've installed all dependencies as described in the README.\")  if not os.path.isdir(\"studies\"):     raise RuntimeError(f\"\u274c 'studies' folder not found. Make sure your working directory is set to the mesqual-vanilla-studies root. Current working directory: {os.getcwd()}\")  print(\"\u2705 Environment ready. Let\u2019s go!\") <pre>\u2705 Environment ready. Let\u2019s go!\n</pre> In\u00a0[3]: Copied! <pre>import os\nimport plotly.express as px\nimport folium\n\nfrom mesqual.visualizations.html_dashboard import HTMLDashboard\nfrom mesqual.utils.folium_utils import MapCountryPlotter\nfrom mesqual.utils.plotly_utils.plotly_theme import PlotlyTheme\nfrom vanilla.notebook_config import configure_clean_output_for_jupyter_notebook\nfrom vanilla.conditional_renderer import ConditionalRenderer\n\nconfigure_clean_output_for_jupyter_notebook()\nPlotlyTheme().apply()\nrenderer = ConditionalRenderer()\n</pre> import os import plotly.express as px import folium  from mesqual.visualizations.html_dashboard import HTMLDashboard from mesqual.utils.folium_utils import MapCountryPlotter from mesqual.utils.plotly_utils.plotly_theme import PlotlyTheme from vanilla.notebook_config import configure_clean_output_for_jupyter_notebook from vanilla.conditional_renderer import ConditionalRenderer  configure_clean_output_for_jupyter_notebook() PlotlyTheme().apply() renderer = ConditionalRenderer() In\u00a0[4]: Copied! <pre># Sample dataset\ndata = px.data.gapminder().query(\"year == 2007\")\n\n# Create first visualization: GDP vs Life Expectancy\nfig1 = px.scatter(\n    data, \n    x=\"gdpPercap\", \n    y=\"lifeExp\", \n    size=\"pop\", \n    color=\"continent\",\n    hover_name=\"country\", \n    log_x=True, \n    size_max=60,\n    title=\"GDP vs Life Expectancy (2007)\"\n)\nfig1.update_layout(height=500)\n\n# Create second visualization: Population by Continent\nfig2 = px.bar(\n    data.groupby(\"continent\")[\"pop\"].sum().reset_index(), \n    x=\"continent\", \n    y=\"pop\", \n    color=\"continent\",\n    title=\"Population by Continent (2007)\"\n)\nfig2.update_layout(height=500)\n\n# Create third visualization: Life Expectancy Distribution\nfig3 = px.box(\n    data, \n    x=\"continent\", \n    y=\"lifeExp\", \n    color=\"continent\",\n    title=\"Life Expectancy Distribution by Continent (2007)\"\n)\nfig3.update_layout(height=500);\n</pre> # Sample dataset data = px.data.gapminder().query(\"year == 2007\")  # Create first visualization: GDP vs Life Expectancy fig1 = px.scatter(     data,      x=\"gdpPercap\",      y=\"lifeExp\",      size=\"pop\",      color=\"continent\",     hover_name=\"country\",      log_x=True,      size_max=60,     title=\"GDP vs Life Expectancy (2007)\" ) fig1.update_layout(height=500)  # Create second visualization: Population by Continent fig2 = px.bar(     data.groupby(\"continent\")[\"pop\"].sum().reset_index(),      x=\"continent\",      y=\"pop\",      color=\"continent\",     title=\"Population by Continent (2007)\" ) fig2.update_layout(height=500)  # Create third visualization: Life Expectancy Distribution fig3 = px.box(     data,      x=\"continent\",      y=\"lifeExp\",      color=\"continent\",     title=\"Life Expectancy Distribution by Continent (2007)\" ) fig3.update_layout(height=500); In\u00a0[5]: Copied! <pre># Create a map centered in Europe\nm = folium.Map(location=[50, 10], zoom_start=3)\n\n# Initialize country plotter\nplotter = MapCountryPlotter()\n\n# Create a simple choropleth-like map with country data\ncountries = [\"DE\", \"FR\", \"IT\", \"ES\", \"GB\", \"SE\", \"NO\", \"FI\", \"PL\", \"RO\"]\ncolors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \n          \"#8c564b\", \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\"]\n\n# Create feature group\nfg = folium.FeatureGroup(name=\"Europe\")\n\n# Add countries with different colors\nfor country, color in zip(countries, colors):\n    plotter.add_countries_to_feature_group(\n        fg,\n        countries=[country],\n        style={\n            \"fillColor\": color,\n            \"color\": \"white\",\n            \"weight\": 1,\n            \"fillOpacity\": 0.7\n        }\n    )\n\n# Add to map\nfg.add_to(m)\n\n# Add background countries\nbackground_fg = folium.FeatureGroup(name=\"Background\")\nplotter.add_all_countries_except(\n    background_fg,\n    excluded_countries=countries,\n    style={\"fillColor\": \"#f8f8f8\", \"color\": \"#e0e0e0\", \"weight\": 0.5, \"fillOpacity\": 0.5}\n)\nbackground_fg.add_to(m)\n\n# Add layer control\nfolium.LayerControl().add_to(m);\n</pre> # Create a map centered in Europe m = folium.Map(location=[50, 10], zoom_start=3)  # Initialize country plotter plotter = MapCountryPlotter()  # Create a simple choropleth-like map with country data countries = [\"DE\", \"FR\", \"IT\", \"ES\", \"GB\", \"SE\", \"NO\", \"FI\", \"PL\", \"RO\"] colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\",            \"#8c564b\", \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\"]  # Create feature group fg = folium.FeatureGroup(name=\"Europe\")  # Add countries with different colors for country, color in zip(countries, colors):     plotter.add_countries_to_feature_group(         fg,         countries=[country],         style={             \"fillColor\": color,             \"color\": \"white\",             \"weight\": 1,             \"fillOpacity\": 0.7         }     )  # Add to map fg.add_to(m)  # Add background countries background_fg = folium.FeatureGroup(name=\"Background\") plotter.add_all_countries_except(     background_fg,     excluded_countries=countries,     style={\"fillColor\": \"#f8f8f8\", \"color\": \"#e0e0e0\", \"weight\": 0.5, \"fillOpacity\": 0.5} ) background_fg.add_to(m)  # Add layer control folium.LayerControl().add_to(m); In\u00a0[6]: Copied! <pre>from mesqual.visualizations.html_table import HTMLTable\n\ntable = HTMLTable(\n    df=data,\n    title=\"Gapminder Dataset (2007)\",\n    height=\"500px\",\n    theme=\"modern\",\n    pagination=\"local\",\n    page_size=15,\n)\n</pre> from mesqual.visualizations.html_table import HTMLTable  table = HTMLTable(     df=data,     title=\"Gapminder Dataset (2007)\",     height=\"500px\",     theme=\"modern\",     pagination=\"local\",     page_size=15, ) In\u00a0[7]: Copied! <pre># Create a new dashboard\ndashboard = HTMLDashboard(name=\"Global Development Dashboard\")\n\n# Add title section\ndashboard.add_section_divider(\n    title=\"Global Development Dashboard\",\n    subtitle=\"Analysis of development indicators across countries and continents\",\n    background_color=\"#f0f7fa\",\n    padding=\"30px\",\n    border_bottom=\"3px solid #3498db\"\n)\n\n# Add plotly figures\ndashboard.add_plotly_figure(fig1, height=\"500px\", name=\"gdp_vs_lifeexp\")\ndashboard.add_plotly_figure(fig2, height=\"500px\", name=\"population_by_continent\")\ndashboard.add_plotly_figure(fig3, height=\"500px\", name=\"lifeexp_distribution\")\n\n# Add geographic section\ndashboard.add_section_divider(\n    title=\"Geographic Visualization\",\n    subtitle=\"European country map example\",\n    background_color=\"#e8f4f9\"\n)\n\n# Add folium map directly using the new method\ndashboard.add_folium_map(m)\n\n# Add data section\ndashboard.add_section_divider(\n    title=\"Data Overview\",\n    subtitle=\"Sample of the dataset used for analysis\",\n    background_color=\"#f0f7ed\",\n    border_left=\"5px solid #2ecc71\"\n)\n\n# Add data table\ndashboard.add_table(table, name=\"data_table\")\n\n# Add footer\ndashboard.add_section_divider(\n    title=\"\",\n    subtitle=\"Created with MESQUAL HTMLDashboard | 2025\",\n    background_color=\"#f9f9f9\",\n    padding=\"10px\",\n    subtitle_color=\"#888\",\n    border_top=\"1px solid #ddd\"\n)\n\n# Save the dashboard\n# dashboard.save('your/dashboard/export/path.html')\n\n# View dashboard\nrenderer.show_html_dashboard(dashboard)\n</pre> # Create a new dashboard dashboard = HTMLDashboard(name=\"Global Development Dashboard\")  # Add title section dashboard.add_section_divider(     title=\"Global Development Dashboard\",     subtitle=\"Analysis of development indicators across countries and continents\",     background_color=\"#f0f7fa\",     padding=\"30px\",     border_bottom=\"3px solid #3498db\" )  # Add plotly figures dashboard.add_plotly_figure(fig1, height=\"500px\", name=\"gdp_vs_lifeexp\") dashboard.add_plotly_figure(fig2, height=\"500px\", name=\"population_by_continent\") dashboard.add_plotly_figure(fig3, height=\"500px\", name=\"lifeexp_distribution\")  # Add geographic section dashboard.add_section_divider(     title=\"Geographic Visualization\",     subtitle=\"European country map example\",     background_color=\"#e8f4f9\" )  # Add folium map directly using the new method dashboard.add_folium_map(m)  # Add data section dashboard.add_section_divider(     title=\"Data Overview\",     subtitle=\"Sample of the dataset used for analysis\",     background_color=\"#f0f7ed\",     border_left=\"5px solid #2ecc71\" )  # Add data table dashboard.add_table(table, name=\"data_table\")  # Add footer dashboard.add_section_divider(     title=\"\",     subtitle=\"Created with MESQUAL HTMLDashboard | 2025\",     background_color=\"#f9f9f9\",     padding=\"10px\",     subtitle_color=\"#888\",     border_top=\"1px solid #ddd\" )  # Save the dashboard # dashboard.save('your/dashboard/export/path.html')  # View dashboard renderer.show_html_dashboard(dashboard)"},{"location":"mesqual-study-01/notebooks/mesqual_308_html_dashboards/#mesqual-305-html-dashboard-utility","title":"MESQUAL 305: HTML Dashboard Utility\u00b6","text":"<p>This notebook demonstrates how to use the HTMLDashboard utility to combine multiple visualizations into a single HTML file for easy sharing and viewing.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_308_html_dashboards/#introduction","title":"Introduction\u00b6","text":"<p>When working with multiple visualizations (Plotly figures, Folium maps, etc.), it can be cumbersome to have separate HTML files for each one. The HTMLDashboard utility provides a simple way to combine multiple visualizations into a single, well-structured HTML file that can be easily shared with stakeholders or published online.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_308_html_dashboards/#setup","title":"Setup\u00b6","text":"<p>First, we need to set up the environment. If you are on Colab, the first cell will clone and install all dependencies. You will have to restart the session afterwards and continue with cell 2. If you are in a local environment, make sure that you have followed the Getting started steps in the README, so that mesqual and all requirements are installed.</p>"},{"location":"mesqual-study-01/notebooks/mesqual_308_html_dashboards/#creating-plotly-visualizations","title":"Creating Plotly Visualizations\u00b6","text":"<p>Let's create some sample Plotly visualizations that we'll include in our dashboard:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_308_html_dashboards/#creating-a-folium-map","title":"Creating a Folium Map\u00b6","text":"<p>Next, let's create a Folium map visualization:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_308_html_dashboards/#creating-a-simple-data-table","title":"Creating a Simple Data Table\u00b6","text":"<p>Let's also create a simple HTML table to include in our dashboard:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_308_html_dashboards/#building-the-dashboard","title":"Building the Dashboard\u00b6","text":"<p>Now let's combine all our visualizations into a single HTML dashboard:</p>"},{"location":"mesqual-study-01/notebooks/mesqual_308_html_dashboards/#use-cases-and-best-practices","title":"Use Cases and Best Practices\u00b6","text":"<p>The HTMLDashboard utility is particularly useful for:</p> <ol> <li>Sharing Results: Create comprehensive reports combining multiple visualizations</li> <li>Market Analysis: Build dashboards showing electricity prices, flows, and generation mixes</li> <li>Comparative Studies: Combine maps, charts, and tables to show scenario differences</li> <li>Static Reporting: Create standalone HTML files that don't require server infrastructure</li> </ol> <p>Best practices:</p> <ul> <li>Name your elements: Using meaningful names makes reordering easier</li> <li>Organize content logically: Group related visualizations together</li> <li>Consider load time: Many large visualizations may slow down page loading</li> <li>Use section headers: Divide your dashboard into clear sections with headings</li> <li>Add context: Include text explanations to help readers understand your visualizations</li> </ul>"},{"location":"mesqual-study-01/notebooks/mesqual_308_html_dashboards/#conclusion","title":"Conclusion\u00b6","text":"<p>The HTMLDashboard utility provides a simple yet powerful way to combine multiple visualizations into a single HTML file. This makes it easy to create comprehensive dashboards and reports that can be shared with stakeholders or published online.</p> <p>By leveraging HTML, Plotly, and Folium, you can create rich, interactive visualizations that work in any modern web browser without requiring special software or server infrastructure.</p>"}]}